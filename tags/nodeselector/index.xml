<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>NodeSelector on TechBlog about OpenShift/Ansible/Satellite and much more</title><link>https://blog.stderr.at/tags/nodeselector/</link><description>TechBlog about OpenShift/Ansible/Satellite and much more</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Toni Schmidbauer &amp; Thomas Jungbauer</copyright><lastBuildDate>Wed, 12 Jan 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.stderr.at/tags/nodeselector/index.xml" rel="self" type="application/rss+xml"/><item><title>Labels &amp; Environments</title><link>https://blog.stderr.at/day-2/labels_environmets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/labels_environmets/</guid><description/></item><item><title>Pod Placement</title><link>https://blog.stderr.at/day-2/pod-placement/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/</guid><description/></item><item><title>Working with Environments</title><link>https://blog.stderr.at/day-2/labels_environmets/2022-01-12-creatingenvironment/</link><pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/labels_environmets/2022-01-12-creatingenvironment/</guid><description>&lt;div class="paragraph">
&lt;p>Imagine you have one OpenShift cluster and you would like to create 2 or more environments inside this cluster, but also separate them and force the environments to specific nodes, or use specific inbound routers. All this can be achieved using labels, IngressControllers and so on. The following article will guide you to set up dedicated compute nodes for infrastructure, development and test environments as well as the creation of IngressController which are bound to the appropriate nodes.&lt;/p>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_prerequisites">Prerequisites&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Before we start we need an OpenShift cluster of course. In this example we have a cluster with typical 3 control plane nodes (labelled as master) and 7 compute nodes (labelled as worker)&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get nodes
NAME STATUS ROLES AGE VERSION
ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632
ip-10-0-149-168.us-east-2.compute.internal Ready worker 13h v1.21.1+6438632 # &amp;lt;-- will become infra
ip-10-0-154-244.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # &amp;lt;-- will become infra
ip-10-0-158-44.us-east-2.compute.internal Ready worker 15m v1.21.1+6438632 # &amp;lt;-- will become infra
ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632
ip-10-0-188-198.us-east-2.compute.internal Ready worker 13h v1.21.1+6438632 # &amp;lt;-- will become worker-test
ip-10-0-191-9.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # &amp;lt;-- will become worker-test
ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632
ip-10-0-195-201.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # &amp;lt;-- will become worker-dev
ip-10-0-199-235.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # &amp;lt;-- will become worker-dev&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>We will use the 7 nodes to create the environments for:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>Infrastructure Services (3 nodes) - will be labelled as &lt;code>infra&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Development Environment (2 nodes) - will be labelled as &lt;code>worker-dev&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Test Environment (2 nodes) - will be labelled as &lt;code>worker-test&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>To do this, we will label the nodes and create dedicated roles for them.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_create_nodes_labels_and_machineconfigpools">Create Nodes Labels and MachineConfigPools&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Let’s create a maschine config pool for the different environments.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The pool inherits the configuration from &lt;code>worker&lt;/code> nodes by default, which means that any new update on the worker configuration will also update custom labeled nodes.
Or in other words: it is possible to remove the worker label from the custom pools.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Create the following objects:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml"># Infrastructure
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
name: infra
spec:
machineConfigSelector:
matchExpressions:
- {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,infra]} &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>
maxUnavailable: 1 &lt;i class="conum" data-value="2">&lt;/i>&lt;b>(2)&lt;/b>
nodeSelector:
matchLabels:
node-role.kubernetes.io/infra: &amp;#34;&amp;#34; &lt;i class="conum" data-value="3">&lt;/i>&lt;b>(3)&lt;/b>
paused: false
---
# Worker-DEV
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
name: worker-dev
spec:
machineConfigSelector:
matchExpressions:
- {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-dev]}
nodeSelector:
matchLabels:
node-role.kubernetes.io/worker-dev: &amp;#34;&amp;#34;
---
# Worker-TEST
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
name: worker-test
spec:
machineConfigSelector:
matchExpressions:
- {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-test]}
nodeSelector:
matchLabels:
node-role.kubernetes.io/worker-test: &amp;#34;&amp;#34;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="colist arabic">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td>&lt;i class="conum" data-value="1">&lt;/i>&lt;b>1&lt;/b>&lt;/td>
&lt;td>User worker and infra so that the default worker configuration gets applied during upgrades&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="2">&lt;/i>&lt;b>2&lt;/b>&lt;/td>
&lt;td>Whenever an update happens, do only 1 at a time&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="3">&lt;/i>&lt;b>3&lt;/b>&lt;/td>
&lt;td>This pool is valid for nodes which are labelled as &lt;code>infra&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Now let’s label our nodes.
First add the new, additional label:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash"># Add the label &amp;#34;infra&amp;#34;
oc label node ip-10-0-149-168.us-east-2.compute.internal ip-10-0-154-244.us-east-2.compute.internal ip-10-0-158-44.us-east-2.compute.internal node-role.kubernetes.io/infra=
# Add the label &amp;#34;worker-dev&amp;#34;
oc label node ip-10-0-195-201.us-east-2.compute.internal ip-10-0-199-235.us-east-2.compute.internal node-role.kubernetes.io/worker-dev=
# Add the label &amp;#34;worker-test&amp;#34;
oc label node ip-10-0-188-198.us-east-2.compute.internal ip-10-0-191-9.us-east-2.compute.internal node-role.kubernetes.io/worker-test=&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This will result in:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">NAME STATUS ROLES AGE VERSION
ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632
ip-10-0-149-168.us-east-2.compute.internal Ready infra,worker 13h v1.21.1+6438632
ip-10-0-154-244.us-east-2.compute.internal Ready infra,worker 20m v1.21.1+6438632
ip-10-0-158-44.us-east-2.compute.internal Ready infra,worker 19m v1.21.1+6438632
ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632
ip-10-0-188-198.us-east-2.compute.internal Ready worker,worker-test 13h v1.21.1+6438632
ip-10-0-191-9.us-east-2.compute.internal Ready worker,worker-test 20m v1.21.1+6438632
ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632
ip-10-0-195-201.us-east-2.compute.internal Ready worker,worker-dev 20m v1.21.1+6438632
ip-10-0-199-235.us-east-2.compute.internal Ready worker,worker-dev 20m v1.21.1+6438632&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>We can remove the worker label from the worker-dev and worker-test nodes now.&lt;/p>
&lt;/div>
&lt;div class="admonitionblock warning">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-warning" title="Warning">&lt;/i>
&lt;/td>
&lt;td class="content">
Keep the label &amp;#34;worker&amp;#34; for the infra nodes as this is the default worker label which is used when no nodeselector is in use. You can use any other node, just keep in mind that per default new applications will be started on nodes with the labels &amp;#34;worker&amp;#34;. As an alternative, you can also define a cluster-wide default node selector.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc label node ip-10-0-195-201.us-east-2.compute.internal ip-10-0-199-235.us-east-2.compute.internal node-role.kubernetes.io/worker-
oc label node ip-10-0-188-198.us-east-2.compute.internal ip-10-0-191-9.us-east-2.compute.internal node-role.kubernetes.io/worker-&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The final node labels will look like the following:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">NAME STATUS ROLES AGE VERSION
ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632
ip-10-0-149-168.us-east-2.compute.internal Ready infra,worker 13h v1.21.1+6438632
ip-10-0-154-244.us-east-2.compute.internal Ready infra,worker 22m v1.21.1+6438632
ip-10-0-158-44.us-east-2.compute.internal Ready infra,worker 21m v1.21.1+6438632
ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632
ip-10-0-188-198.us-east-2.compute.internal Ready worker-test 13h v1.21.1+6438632
ip-10-0-191-9.us-east-2.compute.internal Ready worker-test 21m v1.21.1+6438632
ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632
ip-10-0-195-201.us-east-2.compute.internal Ready worker-dev 21m v1.21.1+6438632
ip-10-0-199-235.us-east-2.compute.internal Ready worker-dev 22m v1.21.1+6438632&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="admonitionblock note">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-note" title="Note">&lt;/i>
&lt;/td>
&lt;td class="content">
Since the custom pools (infra, worker-test and worker-dev) inherit their configuration from the default worker pool, no changes on the files on the nodes themselves are triggered at this point.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_create_custom_configuration">Create Custom Configuration&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Let’s test our setup by deploying a configuration on specific nodes. The following MaschineConfig objects will create a file at &lt;strong>/etc/myfile&lt;/strong> on the nodes labelled either &lt;em>infra&lt;/em>, &lt;em>worker-dev&lt;/em> or &lt;em>worker_test&lt;/em>.
Dependent on the node role the content of the file will vary.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
labels:
machineconfiguration.openshift.io/role: infra &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>
name: 55-infra
spec:
config:
ignition:
version: 2.2.0
storage:
files:
- contents:
source: data:,infra &lt;i class="conum" data-value="2">&lt;/i>&lt;b>(2)&lt;/b>
filesystem: root
mode: 0644
path: /etc/myfile &lt;i class="conum" data-value="3">&lt;/i>&lt;b>(3)&lt;/b>
---
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
labels:
machineconfiguration.openshift.io/role: worker-dev
name: 55-worker-dev
spec:
config:
ignition:
version: 2.2.0
storage:
files:
- contents:
source: data:,worker-dev
filesystem: root
mode: 0644
path: /etc/myfile
---
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
labels:
machineconfiguration.openshift.io/role: worker-test
name: 55-worker-test
spec:
config:
ignition:
version: 2.2.0
storage:
files:
- contents:
source: data:,worker-test
filesystem: root
mode: 0644
path: /etc/myfile&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="colist arabic">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td>&lt;i class="conum" data-value="1">&lt;/i>&lt;b>1&lt;/b>&lt;/td>
&lt;td>Valid for node with the role xyz&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="2">&lt;/i>&lt;b>2&lt;/b>&lt;/td>
&lt;td>Content of the file&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="3">&lt;/i>&lt;b>3&lt;/b>&lt;/td>
&lt;td>File to be created&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Since this is a new configuration, all nodes will get reconfigured.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">NAME STATUS ROLES AGE VERSION
ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632
ip-10-0-149-168.us-east-2.compute.internal Ready,SchedulingDisabled infra,worker 13h v1.21.1+6438632
ip-10-0-154-244.us-east-2.compute.internal Ready infra,worker 28m v1.21.1+6438632
ip-10-0-158-44.us-east-2.compute.internal Ready infra,worker 27m v1.21.1+6438632
ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632
ip-10-0-188-198.us-east-2.compute.internal Ready worker-test 13h v1.21.1+6438632
ip-10-0-191-9.us-east-2.compute.internal NotReady,SchedulingDisabled worker-test 27m v1.21.1+6438632
ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632
ip-10-0-195-201.us-east-2.compute.internal Ready worker-dev 27m v1.21.1+6438632
ip-10-0-199-235.us-east-2.compute.internal NotReady,SchedulingDisabled worker-dev 28m v1.21.1+6438632&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Wait until all nodes are ready and test the configuration by verifying the content of &lt;em>/etc/myfile&lt;/em>:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">###
# infra nodes:
###
oc get pods -n openshift-machine-config-operator -l k8s-app=machine-config-daemon --field-selector &amp;#34;spec.nodeName=ip-10-0-149-168.us-east-2.compute.internal&amp;#34;
NAME READY STATUS RESTARTS AGE
machine-config-daemon-f85kd 2/2 Running 6 16h
# Get file content
oc rsh -n openshift-machine-config-operator machine-config-daemon-f85kd chroot /rootfs cat /etc/myfile
Defaulted container &amp;#34;machine-config-daemon&amp;#34; out of: machine-config-daemon, oauth-proxy
infra
###
#worker-dev:
###
oc get pods -n openshift-machine-config-operator -l k8s-app=machine-config-daemon --field-selector &amp;#34;spec.nodeName=ip-10-0-195-201.us-east-2.compute.internal&amp;#34;
NAME READY STATUS RESTARTS AGE
machine-config-daemon-s6rr5 2/2 Running 4 3h5m
# Get file content
oc rsh -n openshift-machine-config-operator machine-config-daemon-s6rr5 chroot /rootfs cat /etc/myfile
Defaulted container &amp;#34;machine-config-daemon&amp;#34; out of: machine-config-daemon, oauth-proxy
worker-dev
###
# worker-test:
###
oc get pods -n openshift-machine-config-operator -l k8s-app=machine-config-daemon --field-selector &amp;#34;spec.nodeName=ip-10-0-188-198.us-east-2.compute.internal&amp;#34;
NAME READY STATUS RESTARTS AGE
machine-config-daemon-m22rf 2/2 Running 6 16h
# Get file content
oc rsh -n openshift-machine-config-operator machine-config-daemon-m22rf chroot /rootfs cat /etc/myfile
Defaulted container &amp;#34;machine-config-daemon&amp;#34; out of: machine-config-daemon, oauth-proxy
worker-test&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The file /etc/myfile exists on all nodes and depending on their role the files have a different content.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_bind_an_application_to_a_specific_environment">Bind an Application to a Specific Environment&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>The following will label the nodes with a specific environment and will deploy an example application, which should only be executed on the appropriate nodes.&lt;/p>
&lt;/div>
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>Let’s label the nodes with &lt;strong>environment=worker-dev&lt;/strong> and &lt;strong>environment=worker-test&lt;/strong>:&lt;/p>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc label node ip-10-0-195-201.us-east-2.compute.internal ip-10-0-199-235.us-east-2.compute.internal environment=worker-dev
oc label node ip-10-0-188-198.us-east-2.compute.internal ip-10-0-191-9.us-east-2.compute.internal environment=worker-test&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>Create a namespace for the example application&lt;/p>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc new-project bookinfo&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>Create an annotation and a label for the namespace. The annotation will make sure that the application will only be started on nodes with the same label. The label will be later used for the IngressController setup.&lt;/p>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc annotate namespace bookinfo environment=worker-dev
oc annotate namespace bookinfo openshift.io/node-selector: environment=worker-test
oc label namespace bookinfo environment=worker-dev&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>Deploy the example application. In this article the sample application of Istio was used:&lt;/p>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc apply -f https://raw.githubusercontent.com/istio/istio/release-1.11/samples/bookinfo/platform/kube/bookinfo.yaml&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This will start the application on &lt;code>worker-dev`&lt;/code> nodes only, because the annotation in the namespace was created accordingly.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n bookinfo -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
details-v1-86dfdc4b95-v8zfv 1/1 Running 0 9m19s 10.130.2.17 ip-10-0-199-235.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
productpage-v1-658849bb5-8gcl7 1/1 Running 0 7m17s 10.128.4.21 ip-10-0-195-201.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
ratings-v1-76b8c9cbf9-cc4js 1/1 Running 0 9m19s 10.130.2.19 ip-10-0-199-235.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
reviews-v1-58b8568645-mbgth 1/1 Running 0 7m44s 10.128.4.20 ip-10-0-195-201.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
reviews-v2-5d8f8b6775-qkdmz 1/1 Running 0 9m19s 10.130.2.21 ip-10-0-199-235.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
reviews-v3-666b89cfdf-8zv8w 1/1 Running 0 9m18s 10.130.2.22 ip-10-0-199-235.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_create_dedicated_ingresscontroller">Create Dedicated IngressController&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>IngressController are responsible to bring the traffic into the cluster. OpenShift comes with one default controller, but it is possible to create more in order to use different domains and separate the incoming traffic to different nodes.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Bind the default ingress controller to the infra labeled nodes, so we can be sure that the default router pods are executed only on these nodes:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch=&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;nodePlacement&amp;#34;:{&amp;#34;nodeSelector&amp;#34;: {&amp;#34;matchLabels&amp;#34;:{&amp;#34;node-role.kubernetes.io/infra&amp;#34;:&amp;#34;&amp;#34;}}}}}&amp;#39;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The pods will get restarted, to be sure they are running on infra:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre>oc get pods -n openshift-ingress -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
router-default-78f8dd6f69-dbtbv 0/1 ContainerCreating 0 2s &amp;lt;none&amp;gt; ip-10-0-149-168.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
router-default-78f8dd6f69-wwpgb 0/1 ContainerCreating 0 2s &amp;lt;none&amp;gt; ip-10-0-158-44.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
router-default-7bbbc8f9bd-vfh84 1/1 Running 0 22m 10.129.4.6 ip-10-0-158-44.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
router-default-7bbbc8f9bd-wggrx 1/1 Terminating 0 19m 10.128.2.8 ip-10-0-149-168.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Create the following IngressController objects for &lt;strong>worker-dev&lt;/strong> and &lt;strong>worker-test&lt;/strong>. Replace with the domain of your choice&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
name: ingress-worker-dev
namespace: openshift-ingress-operator
spec:
domain: worker-dev.&amp;lt;yourdomain&amp;gt; &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>
endpointPublishingStrategy:
type: HostNetwork
httpErrorCodePages:
name: &amp;#39;&amp;#39;
namespaceSelector:
matchLabels:
environment: worker-dev &lt;i class="conum" data-value="2">&lt;/i>&lt;b>(2)&lt;/b>
nodePlacement:
nodeSelector:
matchLabels:
node-role.kubernetes.io/worker-dev: &amp;#39;&amp;#39; &lt;i class="conum" data-value="3">&lt;/i>&lt;b>(3)&lt;/b>
replicas: 3
tuningOptions: {}
unsupportedConfigOverrides: null
---
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
name: ingress-worker-test
namespace: openshift-ingress-operator
spec:
domain: worker-test.&amp;lt;yourdomain&amp;gt;
endpointPublishingStrategy:
type: HostNetwork
httpErrorCodePages:
name: &amp;#39;&amp;#39;
namespaceSelector:
matchLabels:
environment: worker-test
nodePlacement:
nodeSelector:
matchLabels:
node-role.kubernetes.io/worker-test: &amp;#39;&amp;#39;
replicas: 3
tuningOptions: {}
unsupportedConfigOverrides: null&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="colist arabic">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td>&lt;i class="conum" data-value="1">&lt;/i>&lt;b>1&lt;/b>&lt;/td>
&lt;td>Domainname which is used by this Controller&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="2">&lt;/i>&lt;b>2&lt;/b>&lt;/td>
&lt;td>Namespace selector …​ namespaces with such label will be handled by this IngressController&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="3">&lt;/i>&lt;b>3&lt;/b>&lt;/td>
&lt;td>Node Placement …​ This Controller should run on nodes with this label/role&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This will spin up additional router pods on the collect labelled nodes:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n openshift-ingress -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
router-default-78f8dd6f69-dbtbv 1/1 Running 0 8m7s 10.128.2.11 ip-10-0-149-168.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
router-default-78f8dd6f69-wwpgb 1/1 Running 0 8m7s 10.129.4.10 ip-10-0-158-44.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
router-ingress-worker-dev-76b65cf558-mspvb 1/1 Running 0 113s 10.130.2.13 ip-10-0-199-235.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
router-ingress-worker-dev-76b65cf558-p2jpg 1/1 Running 0 113s 10.128.4.12 ip-10-0-195-201.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
router-ingress-worker-test-6bbf9967f-4whfs 1/1 Running 0 113s 10.131.2.13 ip-10-0-191-9.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
router-ingress-worker-test-6bbf9967f-jht4w 1/1 Running 0 113s 10.131.0.8 ip-10-0-188-198.us-east-2.compute.internal &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_verify_ingress_configuration">Verify Ingress Configuration&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>To test our new ingress router lets create a route object for our example application:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">kind: Route
apiVersion: route.openshift.io/v1
metadata:
name: productpage
namespace: bookinfo
spec:
host: productpage-bookinfo.worker-dev.&amp;lt;yourdomain&amp;gt;
to:
kind: Service
name: productpage
weight: 100
port:
targetPort: http
wildcardPolicy: None
---
kind: Route
apiVersion: route.openshift.io/v1
metadata:
name: productpage-worker-test
namespace: bookinfo
spec:
host: productpage-bookinfo.worker-test.&amp;lt;yourdomain&amp;gt;
to:
kind: Service
name: productpage
weight: 100
port:
targetPort: http
wildcardPolicy: None&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="admonitionblock warning">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-warning" title="Warning">&lt;/i>
&lt;/td>
&lt;td class="content">
Be sure that the name is resolvable and a load balancer is configured accordingly
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Verify that the router pod has the correct configuration in the file &lt;strong>haproxy.config&lt;/strong> :&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n openshift-ingress
NAME READY STATUS RESTARTS AGE
router-default-78f8dd6f69-dbtbv 1/1 Running 0 95m
router-default-78f8dd6f69-wwpgb 1/1 Running 0 95m
router-ingress-worker-dev-76b65cf558-mspvb 1/1 Running 0 88m
router-ingress-worker-dev-76b65cf558-p2jpg 1/1 Running 0 88m
router-ingress-worker-test-6bbf9967f-4whfs 1/1 Running 0 88m
router-ingress-worker-test-6bbf9967f-jht4w 1/1 Running 0 88m&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Verify the content of the haproxy configuration for one of the &lt;code>worker-dev&lt;/code> router&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc rsh -n openshift-ingress router-ingress-worker-dev-76b65cf558-mspvb cat haproxy.config | grep productpage
backend be_http:bookinfo:productpage
server pod:productpage-v1-658849bb5-8gcl7:productpage:http:10.128.4.21:9080 10.128.4.21:9080 cookie 3758caf21badd7e4f729209173eece08 weight 256&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Compare with &lt;code>worker-test&lt;/code> router&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc rsh -n openshift-ingress router-ingress-worker-test-6bbf9967f-jht4w cat haproxy.config | grep productpage
--&amp;gt; Empty result, this router is not configured with that route.&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Compare with &lt;code>default&lt;/code> router:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">backend be_http:bookinfo:productpage
server pod:productpage-v1-658849bb5-8gcl7:productpage:http:10.128.4.21:9080 10.128.4.21:9080 cookie 3758caf21badd7e4f729209173eece08 weight 256&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Why does this happen? Why are the default router and the router for worker-dev configured?
This happens because it is the default router and we must explicitly tell it to ignore certain labels.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Modify the default IngressController&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc edit ingresscontroller.operator default -n openshift-ingress-operator&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Add the following&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml"> namespaceSelector:
matchExpressions:
- key: environment
operator: NotIn
values:
- worker-dev
- worker-test&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This will tell the default IngressController to ignore selectors on &lt;code>worker-dev&lt;/code> and &lt;code>worker-test&lt;/code>&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Wait a few seconds until the route pods have been restarted:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n openshift-ingress
NAME READY STATUS RESTARTS AGE
router-default-744998df46-8lh4t 1/1 Running 0 2m32s
router-default-744998df46-hztgf 1/1 Running 0 2m31s
router-ingress-worker-dev-76b65cf558-mspvb 1/1 Running 0 96m
router-ingress-worker-dev-76b65cf558-p2jpg 1/1 Running 0 96m
router-ingress-worker-test-6bbf9967f-4whfs 1/1 Running 0 96m
router-ingress-worker-test-6bbf9967f-jht4w 1/1 Running 0 96m&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>And test again&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc rsh -n openshift-ingress router-default-744998df46-8lh4t cat haproxy.config | grep productpage
--&amp;gt; empty result&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="admonitionblock caution">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-caution" title="Caution">&lt;/i>
&lt;/td>
&lt;td class="content">
At this point the new router feels responsible. Be sure to have a load balancer configured correctly.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_appendix">Appendix&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Bind other infra-workload to infrastructure nodes:&lt;/p>
&lt;/div>
&lt;div class="sect2">
&lt;h3 id="_internal_registry">Internal Registry&lt;/h3>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc patch configs.imageregistry.operator.openshift.io/cluster -n openshift-image-registry --type=merge --patch &amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;nodeSelector&amp;#34;:{&amp;#34;node-role.kubernetes.io/infra&amp;#34;:&amp;#34;&amp;#34;}}}&amp;#39;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect2">
&lt;h3 id="_openshift_monitoring_workload">OpenShift Monitoring Workload&lt;/h3>
&lt;div class="paragraph">
&lt;p>Create the following file and apply it.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">cat &amp;lt;&amp;lt;&amp;#39;EOF&amp;#39; &amp;gt; cluster-monitoring-config-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
name: cluster-monitoring-config
namespace: openshift-monitoring
data:
config.yaml: |+
alertmanagerMain:
nodeSelector:
node-role.kubernetes.io/infra: &amp;#34;&amp;#34;
prometheusK8s:
nodeSelector:
node-role.kubernetes.io/infra: &amp;#34;&amp;#34;
prometheusOperator:
nodeSelector:
node-role.kubernetes.io/infra: &amp;#34;&amp;#34;
grafana:
nodeSelector:
node-role.kubernetes.io/infra: &amp;#34;&amp;#34;
k8sPrometheusAdapter:
nodeSelector:
node-role.kubernetes.io/infra: &amp;#34;&amp;#34;
kubeStateMetrics:
nodeSelector:
node-role.kubernetes.io/infra: &amp;#34;&amp;#34;
telemeterClient:
nodeSelector:
node-role.kubernetes.io/infra: &amp;#34;&amp;#34;
EOF&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc create -f cluster-monitoring-config-cm.yaml&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Introduction</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/</guid><description>&lt;div class="paragraph">
&lt;p>Pod scheduling is an internal process that determines placement of new pods onto nodes within the cluster. It is probably one of the most important tasks for a Day-2 scenario and should be considered at a very early stage for a new cluster. OpenShift/Kubernetes is already shipped with a &lt;strong>default scheduler&lt;/strong> which schedules pods as they get created accross the cluster, without any manual steps.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>However, there are scenarios where a more advanced approach is required, like for example using a specifc group of nodes for dedicated workload or make sure that certain applications do not run on the same nodes. Kubernetes provides different options:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>Controlling placement with node selectors&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with pod/node affinity/anti-affinity rules&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with taints and tolerations&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with topology spread constraints&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This series will try to go into the detail of the different options and explains in simple examples how to work with pod placement rules.
It is not a replacement for any official documentation, so always check out Kubernetes and or OpenShift documentations.&lt;/p>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_pod_placement_series">Pod Placement Series&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-nodeselector/">NodeSelector&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Affinity and Anti Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-node-affinity/">Node Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-taints-and-tolerations">Taints and Tolerations&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-topology-spread-constraints/">Topology Spread Constraints&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/descheduler/">Descheduler&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_prerequisites">Prerequisites&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="admonitionblock note">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-note" title="Note">&lt;/i>
&lt;/td>
&lt;td class="content">
The following prerequisites are used for all examples.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Let’s image that our cluster (OpenShift 4) has 4 compute nodes&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get node --selector=&amp;#39;node-role.kubernetes.io/worker&amp;#39;
NAME STATUS ROLES AGE VERSION
compute-0 Ready worker 7h1m v1.19.0+d59ce34
compute-1 Ready worker 7h1m v1.19.0+d59ce34
compute-2 Ready worker 7h1m v1.19.0+d59ce34
compute-3 Ready worker 7h1m v1.19.0+d59ce34&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>An example application (from the catalog Django + Postgres) has been deployed in the namespace &lt;code>podtesting&lt;/code>. It contains by default 1 pod for a PostGresql database and one pod for a frontend web application.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide | grep Running
django-psql-example-1-h6kst 1/1 Running 0 20m 10.130.2.97 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
postgresql-1-4pcm4 1/1 Running 0 21m 10.131.0.51 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Without any configuration the OpenShift scheduler will try to spread the pods evenly accross the cluster.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Let’s increase the replica of the web frontend:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc scale --replicas=4 dc/django-psql-example -n podtesting&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Eventually 4 additional pods will be started accross the compute nodes of the cluster:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide | grep Running
django-psql-example-1-842fl 1/1 Running 0 2m7s 10.131.0.65 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-1-h6kst 1/1 Running 0 24m 10.130.2.97 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-1-pxhlv 1/1 Running 0 2m7s 10.128.2.13 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-1-xms7x 1/1 Running 0 2m7s 10.129.2.10 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
postgresql-1-4pcm4 1/1 Running 0 26m 10.131.0.51 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>As you can see, the scheduler already tries to spread the pods evenly, so that every worker node will host one frontend pod.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>However, let’s try to apply a more advanced configuration for the pod placement, starting with the
&lt;a href="https://blog.stderr.at/openshift/day-2/2021-08-27-podplacement/">Pod Placement - NodeSelector&lt;/a>&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>NodeSelector</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/</guid><description>&lt;div class="paragraph">
&lt;p>One of the easiest ways to tell your Kubernetes cluster where to put certain pods is to use a &lt;code>nodeSelector&lt;/code> specification. A nodeSelector defines a key-value pair and are defined inside the specification of the pods and as a label on one or multiple nodes (or machine set or machine config). Only if selector matches the node label, the pod is allowed to be scheduled on that node.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Kubernetes distingushes between 2 types of selectors:&lt;/p>
&lt;/div>
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>&lt;em>cluster-wide node selectors&lt;/em>: defined by the cluster administrators and valid for the whole cluster&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>project node selectors&lt;/em>: to place new pods inside projects into specific nodes.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_pod_placement_series">Pod Placement Series&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Please check out other ways of pod placements:&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
&lt;a>Expand me...&lt;/a>
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-nodeselector/">NodeSelector&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Affinity and Anti Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-node-affinity/">Node Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-taints-and-tolerations">Taints and Tolerations&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-topology-spread-constraints/">Topology Spread Constraints&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/descheduler/">Descheduler&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_using_nodeselector">Using nodeSelector&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>As previously described, we have a cluster with an example application scheduled accross the worker nodes evenly by the scheduler.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide | grep Running
django-psql-example-1-842fl 1/1 Running 0 2m7s 10.131.0.65 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-1-h6kst 1/1 Running 0 24m 10.130.2.97 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-1-pxhlv 1/1 Running 0 2m7s 10.128.2.13 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-1-xms7x 1/1 Running 0 2m7s 10.129.2.10 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
postgresql-1-4pcm4 1/1 Running 0 26m 10.131.0.51 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>However, our 4 compute nodes are assembled with different hardware specification and are using different harddisks (sdd vs hdd).&lt;/p>
&lt;/div>
&lt;div class="imageblock">
&lt;div class="content">
&lt;img src="https://blog.stderr.at/day-2/images/nodeselector-disktypes.png" alt="Node with different disktypes"/>
&lt;/div>
&lt;div class="title">Figure 1. Nodes with Different Specifications&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Since our web application must run on fast disks must configure the cluster to schedule the pods on nodes with SSD only.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>To start using nodeSelectors we first &lt;strong>label our nodes&lt;/strong> accordingly:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>compute-0 and compute-1 are faster nodes with an SSD attached.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>compute-2 and compute-2 have a HDD attached.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc label nodes compute-0 compute-1 disktype=ssd &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>
oc label nodes compute-2 compute-3 disktype=hdd&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="colist arabic">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td>&lt;i class="conum" data-value="1">&lt;/i>&lt;b>1&lt;/b>&lt;/td>
&lt;td>as key we are using &lt;strong>disktype&lt;/strong>&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>As crosscheck we can list nodes with a specific label:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get nodes -l disktype=ssd
NAME STATUS ROLES AGE VERSION
compute-0 Ready worker 7h32m v1.19.0+d59ce34
compute-1 Ready worker 7h31m v1.19.0+d59ce34
oc get nodes -l disktype=hdd
NAME STATUS ROLES AGE VERSION
compute-2 Ready worker 7h32m v1.19.0+d59ce34
compute-3 Ready worker 7h32m v1.19.0+d59ce34&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="admonitionblock warning">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-warning" title="Warning">&lt;/i>
&lt;/td>
&lt;td class="content">
If no matching label is found, the pod cannot be scheduled. Therefore, &lt;strong>always&lt;/strong> label the nodes first.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The 2nd step is to add the node selector to the specification of the pod. In our example we are using a DeploymentConfig, so let’s add it there:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc patch dc django-psql-example -n podtesting --patch &amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;template&amp;#34;:{&amp;#34;spec&amp;#34;:{&amp;#34;nodeSelector&amp;#34;:{&amp;#34;disktype&amp;#34;:&amp;#34;ssd&amp;#34;}}}}}&amp;#39;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This adds the nodeSelector into: spec/template/spec&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml"> nodeSelector:
disktype: ssd&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Kubernetes will now trigger a restart of the pods on the supposed nodes.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide | grep Running
django-psql-example-3-4j92k 1/1 Running 0 42s 10.129.2.7 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-3-d7hsd 1/1 Running 0 42s 10.129.2.8 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-3-fkbfm 1/1 Running 0 14m 10.128.2.18 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-3-psskb 1/1 Running 0 14m 10.128.2.17 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>As you can see, only nodes with a SSD (compute-0 and compute-1) are being used.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_controlling_pod_placement_with_project_wide_selector">Controlling pod placement with project-wide selector&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Adding a nodeSelector to a deployment seems fine…​ until somebody forgets to add it. Then the pods would be started anywhere the scheduler finds suitable. Therefore, it might make sense to use a project-wide node selector, which will automatically be applied on all pods on that project. The project selector is added by the cluster administrator to the &lt;strong>Namespace&lt;/strong> object (no matter what the OpenShift documentation says in it’s example) as &lt;code>openshift.io/node-selector&lt;/code> parameter.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Let’s remove our previous configuration and add the setting to our namespace &lt;em>podtesting&lt;/em>:&lt;/p>
&lt;/div>
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>Cleanup&lt;/p>
&lt;div class="paragraph">
&lt;p>Remove the nodeSelector from the deployment configuration and wait until all pods have been reshuffeld&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc patch dc django-psql-example -n podtesting --type=&amp;#39;json&amp;#39; -p=&amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;remove&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/template/spec/nodeSelector&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;disktype=ssd&amp;#34; }]&amp;#39;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>Add the label to the project&lt;/p>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc annotate ns/podtesting openshift.io/node-selector=&amp;#34;disktype=ssd&amp;#34;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The OpenShift scheduler will now spread the Pods accross compute-0 or compute-1 again, but not on compute-2 or 3.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>We can prove that by stressing our cluster (and nodes) a little bit and scale our frontend application to 10:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide | grep Running
django-psql-example-4-2jn2l 1/1 Running 0 27s 10.128.2.8 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-6g7ks 1/1 Running 0 7m47s 10.129.2.23 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-752nm 1/1 Running 0 7m47s 10.128.2.7 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-c5jvm 1/1 Running 0 27s 10.129.2.4 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-f5kwg 1/1 Running 0 27s 10.129.2.5 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-g7bcs 1/1 Running 0 7m47s 10.129.2.24 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-h5tgb 1/1 Running 0 27s 10.129.2.6 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-spvpp 1/1 Running 0 28s 10.128.2.5 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-v9qwj 1/1 Running 0 7m48s 10.129.2.22 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-zgwcv 1/1 Running 0 27s 10.128.2.6 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>As you can see compute-0 and compute-1 are the only nodes which are used.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_well_known_labels">Well-Known Labels&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>nodeSelector is one of the easiest ways to control where an application shall be started. Working with labels is therefore very important as soon as workload shall be added to the cluster.
Kubernetes reserves some labels which can be leveraged and some are already predefined on the nodes, for example:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>beta.kubernetes.io/arch=amd64&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kubernetes.io/hostname=compute-0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kubernetes.io/os=linux&lt;/p>
&lt;/li>
&lt;li>
&lt;p>node-role.kubernetes.io/worker=&lt;/p>
&lt;/li>
&lt;li>
&lt;p>node.openshift.io/os_id=rhcos&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>A list of all known can be found at: [&lt;a href="#source_1">1&lt;/a>]&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Two of them I would like to mention here, since they might become very important when designing the placement of pods:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>topology.kubernetes.io/zone&lt;/p>
&lt;/li>
&lt;li>
&lt;p>topology.kubernetes.io/region&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>With these two labels you can create availability zones for your cluster. A &lt;strong>zone&lt;/strong> can be seen a logical failure domain and a cluster is typically spanned across multiple zones. This could be a rack in a data center for example, hardware which is sharing the same switch or simply different data centers. Zones are seen as independent to each other.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>A &lt;strong>region&lt;/strong> is made up of one or more zones. A cluster is usually not spanned across multiple region.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Kubernetes makes a few assumptions about the structure of zones and regions:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>regions and zones are hierarchical: zones are strict subsets of regions and no zone can be in 2 regions&lt;/p>
&lt;/li>
&lt;li>
&lt;p>zone names are unique across regions; for example region &amp;#34;africa-east-1&amp;#34; might be comprised of zones &amp;#34;africa-east-1a&amp;#34; and &amp;#34;africa-east-1b&amp;#34;&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_cleanup">Cleanup&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>This concludes the chapter about nodeSelectors. For the next chapter of the Pod Placement Series (&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Affinity and Anti Affinity&lt;/a>) we need to cleanup our configuration.&lt;/p>
&lt;/div>
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>Scale the frontend down to 2&lt;/p>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc scale --replicas=2 dc/django-psql-example -n podtesting&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>Remove the label from the namespace&lt;/p>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc annotate ns/podtesting openshift.io/node-selector- &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="colist arabic">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td>&lt;i class="conum" data-value="1">&lt;/i>&lt;b>1&lt;/b>&lt;/td>
&lt;td>The minus at the end defines that this annotation shall be removed&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>And, just to be sure if you have not done this before, remove the nodeSelector from the DeploymentConfig&lt;/p>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc patch dc django-psql-example -n podtesting --type=&amp;#39;json&amp;#39; -p=&amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;remove&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/template/spec/nodeSelector&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;disktype=ssd&amp;#34; }]&amp;#39;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_sources">Sources&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>&lt;a id="source_1">&lt;/a>[1]: &lt;a href="https://kubernetes.io/docs/reference/labels-annotations-taints/" target="_blank" rel="noopener">Well-Known Labels, Annotations and Taints&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>YAUB Yet Another Useless Blog</title><link>https://blog.stderr.at/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/</guid><description>
&lt;h1 class="blog-title gradient-header">Welcome to Yet Another Useless Blog&lt;/h1>
&lt;p>Well we hope the articles here are not totally useless :)&lt;/p>
&lt;p>Who are we, you might ask.
We (Thomas Jungbauer and Toni Schmidbauer) are two old IT guys, working in the business since more than 20 years. At the moment we are architects at Red Hat Austria, mainly responsible helping customers with OpenShift or Ansible architectures. &lt;/p>
&lt;p>The articles in this blog shall help to easily test and understand specific issues so they can be reproduced and tested. We simply wrote down what we saw in the field and of what we thought it might be helpful, so no frustrating searches in documentations or manual testing is required. &lt;/p>
&lt;p>If you have any question, please feel free to send us an e-mail or create a &lt;a href="https://github.com/stderrat/stderrat.github.io/issues" >GitHub issue&lt;/a>&lt;/p></description></item></channel></rss>