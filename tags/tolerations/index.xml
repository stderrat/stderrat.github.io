<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tolerations on TechBlog about OpenShift/Ansible/Satellite and much more</title><link>https://blog.stderr.at/tags/tolerations/</link><description>TechBlog about OpenShift/Ansible/Satellite and much more</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Toni Schmidbauer &amp; Thomas Jungbauer</copyright><lastBuildDate>Thu, 26 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.stderr.at/tags/tolerations/index.xml" rel="self" type="application/rss+xml"/><item><title>Taints and Tolerations</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/</guid><description>&lt;div class="paragraph">
&lt;p>While Node Affinity is a property of pods that attracts them to a set of nodes, taints are the exact opposite. Nodes can be configured with one or more taints, which mark the node in a way to only accept pods that do tolerate the taints. The tolerations themselves are applied to pods, telling the scheduler to accept a taints and start the workload on a tainted node.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>A common use case would be to mark certain nodes as infrastructure nodes, where only specific pods are allowed to be executed or to taint nodes with a special hardware (i.e. GPU).&lt;/p>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_pod_placement_series">Pod Placement Series&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Please check out other ways of pod placements:&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
&lt;a>Expand me...&lt;/a>
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-nodeselector/">NodeSelector&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Affinity and Anti Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-node-affinity/">Node Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-taints-and-tolerations">Taints and Tolerations&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-topology-spread-constraints/">Topology Spread Constraints&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/descheduler/">Descheduler&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_understanding_taints_and_tolerations">Understanding Taints and Tolerations&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Matching taints and tolerations is defined by a key/value pair and a taint effect.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>For example, a node can be tainted as:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
spec:
taints:
- effect: NoExecute
key: key1
value: value1
[...]&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>And a pod can have the matching toleration:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
spec:
tolerations:
- key: &amp;#34;key1&amp;#34;
operator: &amp;#34;Equal&amp;#34;
value: &amp;#34;value1&amp;#34;
effect: &amp;#34;NoExecute&amp;#34;
tolerationSeconds: 3600
[...]&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This means that the pod is tolerating the taint of the node with the pair &lt;strong>key1=value1&lt;/strong>.&lt;/p>
&lt;/div>
&lt;div class="sect2">
&lt;h3 id="_parameter_effect">Parameter: effect&lt;/h3>
&lt;div class="paragraph">
&lt;p>Above example is using as effect &lt;code>NoExecute&lt;/code>. The following effects are possible:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>&lt;code>NoSchedule&lt;/code> - new pods are not scheduled, existing pods remain&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>PreferNoSchedule&lt;/code> - new pods are not preferred but can still be scheduled but the scheduler tries to avoid that, existing pods remain&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>NoExecute&lt;/code> - new pods are not scheduled, existing pods (without matching toleration) are &lt;strong>removed&lt;/strong>! The setting &lt;code>tolerationSeconds&lt;/code> is used to define a maximum time until a pod is allowed to stay.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;div class="sect2">
&lt;h3 id="_parameter_operator">Parameter: operator&lt;/h3>
&lt;div class="paragraph">
&lt;p>For the operator two options are possible:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>&lt;code>Exists&lt;/code> - simply checks if a key exists and key and effect matches. No value should be configured in this case.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Equal&lt;/code> (default) - with this option key, value and effect must match exactly.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_configuring_taints_and_tolerations">Configuring Taints and Tolerations&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Our compute-0 node is a special node with a GPU installed. We would like that our web application is running on this node and ONLY our web application is running on that node. To achieve this, we will taint the node accordingly with the key/value &lt;code>gpu=enabled&lt;/code> and the effect &lt;code>NoExecute&lt;/code> and configure a toleration to the DeploymentConfig of our web fronted.&lt;/p>
&lt;/div>
&lt;div class="imageblock">
&lt;div class="content">
&lt;img src="https://blog.stderr.at/day-2/images/tainting-node.png" alt="Tainting compute-0"/>
&lt;/div>
&lt;div class="title">Figure 1. Tainting Node&lt;/div>
&lt;/div>
&lt;div class="admonitionblock warning">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-warning" title="Warning">&lt;/i>
&lt;/td>
&lt;td class="content">
Always configure tolerations before you taint a node. Otherwise the scheduler might not be able to start a pod until it has been configured to tolerate the taints.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>First we will set the toleration to the DeploymentConfig:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc edit dc/django-psql-example -n podtesting&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>And add a toleration for the key/value pair, the effect NoExecute and a tolerationSeconds of X seconds.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
spec:
tolerations:
- key: &amp;#34;gpu&amp;#34;
value: &amp;#34;enabled&amp;#34;
operator: &amp;#34;Equal&amp;#34;
effect: &amp;#34;NoExecute&amp;#34;
tolerationSeconds: 30 &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="colist arabic">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td>&lt;i class="conum" data-value="1">&lt;/i>&lt;b>1&lt;/b>&lt;/td>
&lt;td>I am setting the tolerationSeconds to 30 seconds to get it done quicker.&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Before we taint our node, let’s check which pods are currently running there:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -A -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,STATE:.status.phase | grep compute-0 | grep Running
tuned-hrvl4 compute-0 Running
dns-default-ln9s4 compute-0 Running
node-resolver-qk24n compute-0 Running
node-ca-lxrvf compute-0 Running
ingress-canary-fv5w6 compute-0 Running
machine-config-daemon-558v4 compute-0 Running
grafana-78ccdb8c9d-8rqsp compute-0 Running
node-exporter-rn8h4 compute-0 Running
prometheus-adapter-66976bf759-fxdcd compute-0 Running
multus-additional-cni-plugins-29qgq compute-0 Running
multus-mkd87 compute-0 Running
network-metrics-daemon-64hrw compute-0 Running
network-check-target-l6l7n compute-0 Running
ovnkube-node-hrtln compute-0 Running
django-psql-example-24-c599b compute-0 Running
django-psql-example-24-l7znv compute-0 Running
django-psql-example-24-zpg77 compute-0 Running&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Multiple different pods are running here, as well as two of our web frontend (django-psql-example)&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The other django-psql-example pods are started accross the cluster:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide
oc get pods -n podtesting -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
django-psql-example-25-8ppxc 1/1 Running 0 21s 10.128.0.61 master-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-8rv76 1/1 Running 0 21s 10.130.2.92 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-9m8k2 1/1 Running 0 21s 10.129.0.67 master-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-fhvxg 1/1 Running 0 31s 10.128.2.126 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-kqgwz 0/1 Running 0 21s 10.130.0.71 master-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-m66nn 1/1 Running 0 21s 10.130.0.70 master-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-ntjqb 1/1 Running 0 21s 10.128.0.60 master-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-nxxqh 1/1 Running 0 21s 10.130.2.93 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-p8nbz 1/1 Running 0 21s 10.131.1.183 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-ttr9g 1/1 Running 0 21s 10.129.2.57 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-xn4fp 1/1 Running 0 21s 10.129.2.56 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-xpqf4 1/1 Running 0 21s 10.128.2.127 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-xwmwv 1/1 Running 0 21s 10.131.1.184 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>To taint the node we can simply execute the following command. Be sure to use the same values as in the toleration:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc adm taint nodes compute-0 gpu=enabled:NoExecute&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This will create the following specification in the node object:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
taints:
- effect: NoExecute
key: gpu
value: enabled&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>OpenShift will allow pods, which are not tolerating the taints, to keep on running for 30 seconds. After that, these pods will be evicted and started elsewhere.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>When we check after the tolerationSeconds time has passed which pods are running on the node compute-0, we will see that most pods have disappeared, except the pods for the webapplication and pods which are part of DaemonSets. (DaemonSets are defined to run on all or specific nodes and are not evicted. The cluster-DaemonSets are tolerating everything)&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -A -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,STATE:.status.phase | grep compute-0 | grep Running
tuned-hrvl4 compute-0 Running
node-resolver-qk24n compute-0 Running
node-ca-lxrvf compute-0 Running
machine-config-daemon-558v4 compute-0 Running
node-exporter-rn8h4 compute-0 Running
multus-additional-cni-plugins-29qgq compute-0 Running
multus-mkd87 compute-0 Running
network-metrics-daemon-64hrw compute-0 Running
network-check-target-l6l7n compute-0 Running
ovnkube-node-hrtln compute-0 Running
django-psql-example-25-8dzqh compute-0 Running
django-psql-example-25-9p4sb compute-0 Running
django-psql-example-25-pqbvn compute-0 Running
django-psql-example-25-sb6hr compute-0 Running&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_removing_taints">Removing taints&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Taints can be simply removed with the oc command added a trailing &lt;code>-&lt;/code>&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc adm taint nodes compute-0 gpu=enabled:NoExecute-&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_built_in_taints_taint_nodes_by_condition">Built-in Taints - Taint Nodes by Condition&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Several taints are built into OpenShift and are set during certain events (aka Taint Nodes by Condition) and cleared when the condition is resolved.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The following list is quoted from the OpenShift documentation and provides a list of taints which are automatically set. For example, when a node becomes unavailable:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>&lt;code>node.kubernetes.io/not-ready&lt;/code>: The node is not ready. This corresponds to the node condition Ready=False.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>node.kubernetes.io/unreachable&lt;/code>: The node is unreachable from the node controller. This corresponds to the node condition Ready=Unknown.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>node.kubernetes.io/out-of-disk&lt;/code>: The node has insufficient free space on the node for adding new pods. This corresponds to the node condition OutOfDisk=True.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>node.kubernetes.io/memory-pressure&lt;/code>: The node has memory pressure issues. This corresponds to the node condition MemoryPressure=True.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>node.kubernetes.io/disk-pressure&lt;/code>: The node has disk pressure issues. This corresponds to the node condition DiskPressure=True.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>node.kubernetes.io/network-unavailable&lt;/code>: The node network is unavailable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>node.kubernetes.io/unschedulable&lt;/code>: The node is unschedulable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>node.cloudprovider.kubernetes.io/uninitialized&lt;/code>: When the node controller is started with an external cloud provider, this taint is set on a node to mark it as unusable. After a controller from the cloud-controller-manager initializes this node, the kubelet removes this taint.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Depending on the condition, the node will either have the effect &lt;code>NoSchedule&lt;/code>, which means no new pods will be started there (unless a toleration is configured) or the effect &lt;code>NoExecute&lt;/code>, which will evict pods with no tolerations from the node. Typical examples for NoSchedule condition would be: memory-pressure or disk-pressure. If a node is unreachable or not-ready, then it will be automatically tainted with the effect NoExecute. &lt;code>tolerationSeconds&lt;/code> (default 300) will be respected.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_tolerating_all_taints">Tolerating all taints&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Tolerations can be configured to tolerate all possible taints. In such case no &lt;code>value&lt;/code> or &lt;code>key&lt;/code> is configured and &lt;code>operator: &amp;#34;Exists&amp;#34;&lt;/code> is used:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
spec:
tolerations:
- operator: “Exists”&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="admonitionblock note">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-note" title="Note">&lt;/i>
&lt;/td>
&lt;td class="content">
Some cluster daemonsets (i.e. tuned) are configured this way.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_cleanup">Cleanup&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Remove the taint from the node:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc adm taint nodes compute-0 gpu=enabled:NoExecute-&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>And remove the toleration specification from the DeploymentConfig&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
spec:
tolerations:
- key: &amp;#34;gpu&amp;#34;
value: &amp;#34;enabled&amp;#34;
operator: &amp;#34;Equal&amp;#34;
effect: &amp;#34;NoExecute&amp;#34;
tolerationSeconds: 30 &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div></description></item></channel></rss>