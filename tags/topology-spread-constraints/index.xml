<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Topology Spread Constraints on TechBlog about OpenShift/Ansible/Satellite and much more</title><link>https://blog.stderr.at/tags/topology-spread-constraints/</link><description>TechBlog about OpenShift/Ansible/Satellite and much more</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Toni Schmidbauer &amp; Thomas Jungbauer</copyright><lastBuildDate>Thu, 26 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.stderr.at/tags/topology-spread-constraints/index.xml" rel="self" type="application/rss+xml"/><item><title>Topology Spread Constraints</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/</guid><description>&lt;div class="paragraph">
&lt;p>&lt;strong>Topology spread constraints&lt;/strong> is a new feature since Kubernetes 1.19 (OpenShift 4.6) and another way to control where pods shall be started. It allows to use failure-domains, like zones or regions or to define custom topology domains. It heavily relies on configured node labels, which are used to define topology domains. This feature is a more granular approach than affinity, allowing to achieve higher availability.&lt;/p>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_pod_placement_series">Pod Placement Series&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Please check out other ways of pod placements:&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
&lt;a>Expand me...&lt;/a>
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-nodeselector/">NodeSelector&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Affinity and Anti Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-node-affinity/">Node Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-taints-and-tolerations">Taints and Tolerations&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-topology-spread-constraints/">Topology Spread Constraints&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/descheduler/">Descheduler&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_understanding_topology_spread_constraints">Understanding Topology Spread Constraints&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Imagine, like for pod affinity, we have two zones (east and west) in our 4 compute node cluster.&lt;/p>
&lt;/div>
&lt;div class="imageblock">
&lt;div class="content">
&lt;img src="https://blog.stderr.at/day-2/images/affinity-kubernetes.zones.png" alt="Node Zones"/>
&lt;/div>
&lt;div class="title">Figure 1. Node Zones&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>These zones are defined by node labels, which can be created with the following commands:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc label nodes compute-0 compute-1 topology.kubernetes.io/zone=east
oc label nodes compute-2 compute-3 topology.kubernetes.io/zone=west&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Now let’s scale our web application to 3 pods. The OpenShift scheduler will try to evenly spread the pods accross the cluster:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide
django-psql-example-31-jrhsr 0/1 Running 0 8s 10.130.2.112 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-31-q66hk 0/1 Running 0 8s 10.131.1.219 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-31-xv7jc 0/1 Running 0 8s 10.128.3.115 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="imageblock">
&lt;div class="content">
&lt;img src="https://blog.stderr.at/day-2/images/topologyspreadconstraints1.png" alt="Running Pods"/>
&lt;/div>
&lt;div class="title">Figure 2. Running Pods&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>If a new pod is started the scheduler may try to start it on compute-1. However, this is done based on best effort. The scheduler does not guarantee that and may try to start it on one of the nodes in zone &amp;#34;West&amp;#34;. With the configuration &lt;code>topologySpreadConstraints&lt;/code> this can be controlled and incoming pods can only be scheduled on a node of zone &amp;#34;East&amp;#34; (either on compute-0 or compute-1).&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_configure_topologyspreadcontraint">Configure TopologySpreadContraint&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Let’s configure our topology by adding the following into the DeploymentConfig:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
topologySpreadConstraints:
- maxSkew: 1
topologyKey: topology.kubernetes.io/zone
whenUnsatisfiable: DoNotSchedule
labelSelector:
matchLabels:
name: django-psql-example&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This defines the following parameter:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>&lt;code>maxSkew&lt;/code> - defines the degree to which pods may be unevenly distributed (must be greater than zero). Depending on the &lt;code>whenUnsatisfiable&lt;/code> parameter the bahaviour differs:&lt;/p>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>whenUnsatisfiable == DoNotSchedule: would not schedule if the maxSkew is is not met.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>whenUnsatisfiable == ScheduleAnyway: the scheduler will still schedule and gives the topology which would decrease the skew a better score&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>&lt;code>topologyKey&lt;/code> - key of node lables&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>whenUnsatisfiable&lt;/code> - defines what to do with a pod if the spread constraint is not met. Can be either &lt;code>DoNotSchedule&lt;/code> (default) or &lt;code>ScheduleAnyway&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>lableSelector&lt;/code> - used to find matching pods. Found pods are considered to be part of the topology domain. In our example we simply use a default label of the application: &lt;code>name: django-psql-example&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>If now a 4th pod shall be started the topologySpreadConstraints will allow this pod on one of the nodes of zone=east (either compute-0 or compute-1). It cannot be scheduled on nodes in zone=west since it would violate the &lt;code>maxSkew: 1&lt;/code>&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre>Pod distribution would be 1 in zone east and 3 on zone west.
--&amp;gt; the skew would then be 3 - 1 = 2 which is not equal to 1&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_configure_multiple_topologyspreadcontraint">Configure multiple TopologySpreadContraint&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>It is possible to configure multiple TopologySpreadConstraints. In such a case all contrains must meet the requirement (logical AND). For example:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
topologySpreadConstraints:
- maxSkew: 1
topologyKey: topology.kubernetes.io/zone
whenUnsatisfiable: DoNotSchedule
labelSelector:
matchLabels:
name: django-psql-example
- maxSkew: 1
topologyKey: kubernetes.io/hostname
whenUnsatisfiable: DoNotSchedule
labelSelector:
matchLabels:
name: django-psql-example&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The first part is identical to our first example and would allow the schedule to start a pod in &lt;code>topology.kubernetes.io/zone: east&lt;/code> (compute-0 or compute-1). The second configuration defines not a zone but a node hostname. Now the scheduler can only deploy into zone=east AND onto node=compute-1&lt;/p>
&lt;/div>
&lt;div class="sect2">
&lt;h3 id="_conflicts_with_multiple_topologyspreadconstraints">Conflicts with multiple TopologySpreadConstraints&lt;/h3>
&lt;div class="paragraph">
&lt;p>If you use multiple constraints conflicts are possible. For example, you have a 3 node cluster and the 2 zones east and west. In such cases the maxSkew might be increased or the &lt;code>whenUnsatisfiable&lt;/code> might be set to ScheduleAnyway&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>See &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#example-multiple-topologyspreadconstraints" class="bare">https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#example-multiple-topologyspreadconstraints&lt;/a> for further information.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_cleanup">Cleanup&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Remove the topologySpreadConstraint&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
topologySpreadConstraints:
- maxSkew: 1
topologyKey: topology.kubernetes.io/zone
whenUnsatisfiable: DoNotSchedule
labelSelector:
matchLabels:
name: django-psql-example&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div></description></item></channel></rss>