<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Day-2 on TechBlog about OpenShift/Ansible/Satellite and much more</title><link>https://blog.stderr.at/tags/day-2/</link><description>Recent content in Day-2 on TechBlog about OpenShift/Ansible/Satellite and much more</description><generator>Hugo</generator><language>en-us</language><copyright>Toni Schmidbauer &amp; Thomas Jungbauer</copyright><lastBuildDate>Fri, 28 Nov 2025 16:12:42 +0100</lastBuildDate><atom:link href="https://blog.stderr.at/tags/day-2/index.xml" rel="self" type="application/rss+xml"/><item><title>etcd</title><link>https://blog.stderr.at/day-2/etcd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/etcd/</guid><description/></item><item><title>Labels &amp; Environments</title><link>https://blog.stderr.at/day-2/labels_environmets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/labels_environmets/</guid><description/></item><item><title>Observability</title><link>https://blog.stderr.at/day-2/observability/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/observability/</guid><description/></item><item><title>Pod Placement</title><link>https://blog.stderr.at/day-2/pod-placement/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/</guid><description/></item><item><title>OpenShift</title><link>https://blog.stderr.at/openshift/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/</guid><description/></item><item><title>The Hitchhiker's Guide to Observability Introduction - Part 1</title><link>https://blog.stderr.at/day-2/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/</link><pubDate>Sun, 23 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/</guid><description>&lt;div class="paragraph">
&lt;p>In modern microservices architectures, understanding how requests flow through your distributed system is crucial for debugging, performance optimization, and maintaining system health. &lt;strong>Distributed tracing&lt;/strong> provides visibility into these complex interactions by tracking requests as they traverse multiple services.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>With this article I would like to summarize and, especially, remember my setup. This is Part 1 of a series of articles that I split up so it is easier to read and understand and not too long. Initially, there will be 6 parts, but I will add more as needed.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This whole guide demonstrates how to set up a distributed tracing infrastructure using&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>&lt;strong>OpenShift&lt;/strong> (4.16+) as base platform&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Red Hat Build of OpenTelemetry&lt;/strong> - The observability framework based on &lt;a href="https://opentelemetry.io/" target="_blank" rel="noopener">OpenTelemetry&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>TempoStack&lt;/strong> - &lt;a href="https://grafana.com/docs/tempo/latest/" target="_blank" rel="noopener">Grafana’s distributed&lt;/a> tracing backend for Kubernetes&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Multi-tenant architecture&lt;/strong> - Isolating traces by team or environment&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Cluster Observability Operator&lt;/strong> - For now this Operator is only used to extend the OpenShift UI with the tracing UI.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div></description></item><item><title>The Hitchhiker's Guide to Observability - Grafana Tempo - Part 2</title><link>https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/</link><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/</guid><description>&lt;div class="paragraph">
&lt;p>After covering the fundamentals and architecture in Part 1, it’s time to get our hands dirty! This article walks through the complete implementation of a distributed tracing infrastructure on OpenShift.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>We’ll deploy and configure the &lt;strong>Tempo Operator&lt;/strong> and a multi-tenant &lt;strong>TempoStack&lt;/strong> instance. For S3 storage we will use the integrated OpenShift Data Foundation. However, you can use whatever S3-compatible storage you have available.&lt;/p>
&lt;/div></description></item><item><title>The Hitchhiker's Guide to Observability - Central Collector - Part 3</title><link>https://blog.stderr.at/day-2/observability/2025-11-25-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part3/</link><pubDate>Tue, 25 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/observability/2025-11-25-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part3/</guid><description>&lt;div class="paragraph">
&lt;p>With the architecture defined in &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/">Part 1&lt;/a> and TempoStack deployed in &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/">Part 2&lt;/a>, it’s time to tackle the heart of our distributed tracing system: the &lt;strong>Central OpenTelemetry Collector&lt;/strong>. This is the critical component that sits between your application namespaces and TempoStack, orchestrating trace flow, metadata enrichment, and tenant routing.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>In this article, we’ll configure the RBAC permissions required for the Central Collector to enrich traces with Kubernetes metadata and deploy the Central OpenTelemetry Collector with its complete configuration. You’ll learn how to set up receivers for accepting traces from local collectors, configure processors to enrich traces with Kubernetes and OpenShift metadata, and implement routing connectors to direct traces to the appropriate TempoStack tenants based on namespace.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>To be honest, this was the most challenging part of the entire setup to get right, as you can easily miss a setting, or misconfigure a part of the configuration. But once you understand the configuration, it becomes straightforward to extend and modify.&lt;/p>
&lt;/div></description></item><item><title>The Hitchhiker's Guide to Observability - Example Applications - Part 4</title><link>https://blog.stderr.at/day-2/observability/2025-11-26-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part4/</link><pubDate>Wed, 26 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/observability/2025-11-26-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part4/</guid><description>&lt;div class="paragraph">
&lt;p>With the &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/">architecture defined&lt;/a>, &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/">TempoStack deployed&lt;/a>, and the &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-25-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part3/">Central Collector configured&lt;/a>, we’re now ready to complete the distributed tracing pipeline. It’s time to deploy real applications and see traces flowing through the entire system!&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>In this fourth installment, we’ll focus on the &lt;strong>application layer&lt;/strong> - deploying &lt;strong>Local OpenTelemetry Collectors&lt;/strong> in team namespaces and configuring example applications to generate traces. You’ll see how applications automatically get enriched with Kubernetes metadata, how namespace-based routing directs traces to the correct TempoStack tenants, and how the entire two-tier architecture comes together.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>We’ll deploy an example application (&lt;strong>Mockbin&lt;/strong>) into two different namespaces. Together with the application, we will deploy a local OpenTelemetry Collector. One with sidecar mode, one with deployment mode.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>You’ll learn how to configure local collectors to add namespace attributes, forward traces to the central collector, and verify that traces appear in the UI with full Kubernetes context.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>By the end of this article, you’ll have a fully functional distributed tracing system with applications generating traces, local collectors forwarding them, and the central collector routing everything to the appropriate TempoStack tenants. Let’s bring this architecture to life!&lt;/p>
&lt;/div></description></item><item><title>The Hitchhiker's Guide to Observability - Understanding Traces - Part 5</title><link>https://blog.stderr.at/day-2/observability/2025-11-27-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part5/</link><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/observability/2025-11-27-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part5/</guid><description>&lt;div class="paragraph">
&lt;p>With the &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/">architecture established&lt;/a>, &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/">TempoStack deployed&lt;/a>, &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-25-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part3/">the Central Collector configured&lt;/a>, and &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-26-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part4/">applications generating traces&lt;/a>, it’s time to take a step back and understand what we’re actually building. Before you deploy more applications and start troubleshooting performance issues, you need to &lt;strong>understand how to read and interpret distributed traces&lt;/strong>.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Let’s decode the matrix of distributed tracing!&lt;/p>
&lt;/div></description></item><item><title>The Hitchhiker's Guide to Observability - Adding A New Tenant - Part 6</title><link>https://blog.stderr.at/day-2/observability/2025-11-28-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part6/</link><pubDate>Fri, 28 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/observability/2025-11-28-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part6/</guid><description>&lt;div class="paragraph">
&lt;p>While we have created our distributed tracing infrastructure, we created two tenants as an example. In this article, I will show you how to add a new tenant and which changes must be made in the TempoStack and the OpenTelemetry Collector.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This article was mainly created as a quick reference guide to see which changes must be made when adding new tenants.&lt;/p>
&lt;/div></description></item><item><title>Secrets Management - Vault on OpenShift</title><link>https://blog.stderr.at/openshift/2022/08/secrets-management-vault-on-openshift/</link><pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2022/08/secrets-management-vault-on-openshift/</guid><description>&lt;div class="paragraph">
&lt;p>Sensitive information in OpenShift or Kubernetes is stored as a so-called Secret. The management of these Secrets is one of the most important questions,
when it comes to OpenShift. Secrets in Kubernetes are encoded in base64. This is &lt;strong>not&lt;/strong> an encryption format.
Even if etcd is encrypted at rest, everybody can decode a given base64 string which is stored in the Secret.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>For example: The string &lt;code>Thomas&lt;/code> encoded as base64 is &lt;code>VGhvbWFzCg==&lt;/code>. This is simply a masked plain text and it is not secure to share these values, especially not on Git.
To make your CI/CD pipelines or Gitops process secure, you need to think of a secure way to manage your Secrets. Thus, your Secret objects must be encrypted somehow. HashiCorp Vault is one option to achieve this requirement.&lt;/p>
&lt;/div></description></item><item><title>Automated ETCD Backup</title><link>https://blog.stderr.at/day-2/etcd/2022-01-29-automatedetcdbackup/</link><pubDate>Sat, 29 Jan 2022 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/etcd/2022-01-29-automatedetcdbackup/</guid><description>&lt;div class="paragraph">
&lt;p>Securing ETCD is one of the major Day-2 tasks for a Kubernetes cluster. This article will explain how to create a backup using OpenShift Cronjob.&lt;/p>
&lt;/div></description></item><item><title>Working with Environments</title><link>https://blog.stderr.at/day-2/labels_environmets/2022-01-12-creatingenvironment/</link><pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/labels_environmets/2022-01-12-creatingenvironment/</guid><description>&lt;div class="paragraph">
&lt;p>Imagine you have one OpenShift cluster and you would like to create 2 or more environments inside this cluster, but also separate them and force the environments to specific nodes, or use specific inbound routers. All this can be achieved using labels, IngressControllers and so on. The following article will guide you to set up dedicated compute nodes for infrastructure, development and test environments as well as the creation of IngressController which are bound to the appropriate nodes.&lt;/p>
&lt;/div></description></item><item><title>Introduction</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/</guid><description>&lt;div class="paragraph">
&lt;p>Pod scheduling is an internal process that determines placement of new pods onto nodes within the cluster. It is probably one of the most important tasks for a Day-2 scenario and should be considered at a very early stage for a new cluster. OpenShift/Kubernetes is already shipped with a &lt;strong>default scheduler&lt;/strong> which schedules pods as they get created accross the cluster, without any manual steps.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>However, there are scenarios where a more advanced approach is required, like for example using a specifc group of nodes for dedicated workload or make sure that certain applications do not run on the same nodes. Kubernetes provides different options:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>Controlling placement with node selectors&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with pod/node affinity/anti-affinity rules&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with taints and tolerations&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with topology spread constraints&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This series will try to go into the detail of the different options and explains in simple examples how to work with pod placement rules.
It is not a replacement for any official documentation, so always check out Kubernetes and or OpenShift documentations.&lt;/p>
&lt;/div></description></item><item><title>Node Affinity</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-29-node-affinity/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-29-node-affinity/</guid><description>&lt;div class="paragraph">
&lt;p>Node Affinity allows to place a pod to a specific group of nodes. For example, it is possible to run a pod only on nodes with a specific CPU or disktype. The disktype was used as an example for the &lt;code>nodeSelector&lt;/code> and yes …​ Node Affinity is conceptually similar to nodeSelector but allows a more granular configuration.&lt;/p>
&lt;/div></description></item><item><title>NodeSelector</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/</guid><description>&lt;div class="paragraph">
&lt;p>One of the easiest ways to tell your Kubernetes cluster where to put certain pods is to use a &lt;code>nodeSelector&lt;/code> specification. A nodeSelector defines a key-value pair and are defined inside the specification of the pods and as a label on one or multiple nodes (or machine set or machine config). Only if selector matches the node label, the pod is allowed to be scheduled on that node.&lt;/p>
&lt;/div></description></item><item><title>Pod Affinity/Anti-Affinity</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-28-affinity/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-28-affinity/</guid><description>&lt;div class="paragraph">
&lt;p>While noteSelector provides a very easy way to control where a pod shall be scheduled, the affinity/anti-affinity feature, expands this configuration with more expressive rules like logical AND operators, constraints against labels on other pods or soft rules instead of hard requirements.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The feature comes with two types:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>pod affinity/anti-affinity - allows constrains against other pod labels rather than node labels.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>node affinity - allows pods to specify a group of nodes they can be placed on&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div></description></item><item><title>Taints and Tolerations</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/</guid><description>&lt;div class="paragraph">
&lt;p>While Node Affinity is a property of pods that attracts them to a set of nodes, taints are the exact opposite. Nodes can be configured with one or more taints, which mark the node in a way to only accept pods that do tolerate the taints. The tolerations themselves are applied to pods, telling the scheduler to accept a taints and start the workload on a tainted node.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>A common use case would be to mark certain nodes as infrastructure nodes, where only specific pods are allowed to be executed or to taint nodes with a special hardware (i.e. GPU).&lt;/p>
&lt;/div></description></item><item><title>Topology Spread Constraints</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/</guid><description>&lt;div class="paragraph">
&lt;p>&lt;strong>Topology spread constraints&lt;/strong> is a new feature since Kubernetes 1.19 (OpenShift 4.6) and another way to control where pods shall be started. It allows to use failure-domains, like zones or regions or to define custom topology domains. It heavily relies on configured node labels, which are used to define topology domains. This feature is a more granular approach than affinity, allowing to achieve higher availability.&lt;/p>
&lt;/div></description></item></channel></rss>