<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Pod Placement on TechBlog about OpenShift/Ansible/Satellite and much more</title><link>https://blog.stderr.at/tags/pod-placement/</link><description>Recent content in Pod Placement on TechBlog about OpenShift/Ansible/Satellite and much more</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Toni Schmidbauer &amp; Thomas Jungbauer</copyright><lastBuildDate>Thu, 26 Aug 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.stderr.at/tags/pod-placement/index.xml" rel="self" type="application/rss+xml"/><item><title>Pod Placement - Introduction</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/</guid><description>&lt;div class="paragraph">
&lt;p>Pod scheduling is an internal process that determines placement of new pods onto nodes within the cluster. It is probably one of the most important tasks for a Day-2 scenario and should be considered at a very early stage for a new cluster. OpenShift/Kubernetes is already shipped with a &lt;strong>default scheduler&lt;/strong> which schedules pods as they get created accross the cluster, without any manual steps.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>However, there are scenarios where a more advanced approach is required, like for example using a specifc group of nodes for dedicated workload or make sure that certain applications do not run on the same nodes. Kubernetes provides different options:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>Controlling placement with node selectors&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with pod/node affinity/anti-affinity rules&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with taints and tolerations&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with topology spread constraints&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This series will try to go into the detail of the different options and explains in simple examples how to work with pod placement rules.
It is not a replacement for any official documentation, so always check out Kubernetes and or OpenShift documentations.&lt;/p>
&lt;/div></description></item><item><title>Pod Placement - Node Affinity</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-29-node-affinity/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-29-node-affinity/</guid><description>&lt;div class="paragraph">
&lt;p>Node Affinity allows to place a pod to a specific group of nodes. For example, it is possible to run a pod only on nodes with a specific CPU or disktype. The disktype was used as an example for the &lt;code>nodeSelector&lt;/code> and yes …​ Node Affinity is conceptually similar to nodeSelector but allows a more granular configuration.&lt;/p>
&lt;/div></description></item><item><title>Pod Placement - NodeSelector</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/</guid><description>&lt;div class="paragraph">
&lt;p>One of the easiest ways to tell your Kubernetes cluster where to put certain pods is to use a &lt;code>nodeSelector&lt;/code> specification. A nodeSelector defines a key-value pair and are defined inside the specification of the pods and as a label on one or multiple nodes (or machine set or machine config). Only if selector matches the node label, the pod is allowed to be scheduled on that node.&lt;/p>
&lt;/div></description></item><item><title>Pod Placement - Pod Affinity/Anti-Affinity</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-28-affinity/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-28-affinity/</guid><description>&lt;div class="paragraph">
&lt;p>While noteSelector provides a very easy way to control where a pod shall be scheduled, the affinity/anti-affinity feature, expands this configuration with more expressive rules like logical AND operators, constraints against labels on other pods or soft rules instead of hard requirements.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The feature comes with two types:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>pod affinity/anti-affinity - allows constrains against other pod labels rather than node labels.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>node affinity - allows pods to specify a group of nodes they can be placed on&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div></description></item><item><title>Pod Placement - Taints and Tolerations</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/</guid><description>&lt;div class="paragraph">
&lt;p>While Node Affinity is a property of pods that attracts them to a set of nodes, taints are the exact opposite. Nodes can be configured with one or more taints, which mark the node in a way to only accept pods that do tolerate the taints. The tolerations themselves are applied to pods, telling the scheduler to accept a taints and start the workload on a tainted node.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>A common use case would be to mark certain nodes as infrastructure nodes, where only specific pods are allowed to be executed or to taint nodes with a special hardware (i.e. GPU).&lt;/p>
&lt;/div></description></item><item><title>Pod Placement - Topology Spread Constraints</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/</guid><description>&lt;div class="paragraph">
&lt;p>&lt;strong>Topology spread constraints&lt;/strong> is a new feature since Kubernetes 1.19 (OpenShift 4.6) and another way to control where pods shall be started. It allows to use failure-domains, like zones or regions or to define custom topology domains. It heavily relies on configured node labels, which are used to define topology domains. This feature is a more granular approach than affinity, allowing to achieve higher availability.&lt;/p>
&lt;/div></description></item></channel></rss>