<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>OpenShift on TechBlog about OpenShift/Ansible/Satellite and much more</title><link>https://blog.stderr.at/tags/openshift/</link><description>Recent content in OpenShift on TechBlog about OpenShift/Ansible/Satellite and much more</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Toni Schmidbauer &amp; Thomas Jungbauer</copyright><lastBuildDate>Fri, 04 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.stderr.at/tags/openshift/index.xml" rel="self" type="application/rss+xml"/><item><title>Using ServerSideApply with ArgoCD</title><link>https://blog.stderr.at/openshift/2022-11-04-argocd-and-serversideapply/</link><pubDate>Fri, 04 Nov 2022 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2022-11-04-argocd-and-serversideapply/</guid><description>&lt;div class="paragraph">
&lt;p>„&lt;em>If it is not in GitOps, it does not exist&lt;/em>“ - However, managing objects partially only by Gitops was always an issue, since ArgoCD would like to manage the whole object. For example, when you tried to work with node labels and would like to manage them via Gitops, you would need to put the whole node object into ArgoCD. This is impractical since the node object is very complex and typically managed by the cluster.
There were 3rd party solutions (like the patch operator), that helped with this issue.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>However, with the Kubernetes feature &lt;strong>Server-Side Apply&lt;/strong> this problem is solved. Read further to see a working example of this feature.&lt;/p>
&lt;/div></description></item><item><title>Secrets Management - Vault on OpenShift</title><link>https://blog.stderr.at/openshift/2022-08-16-hashicorp-vault/</link><pubDate>Tue, 16 Aug 2022 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2022-08-16-hashicorp-vault/</guid><description>&lt;div class="paragraph">
&lt;p>Sensitive information in OpenShift or Kubernetes is stored as a so-called Secret. The management of these Secrets is one of the most important questions,
when it comes to OpenShift. Secrets in Kubernetes are encoded in base64. This is &lt;strong>not&lt;/strong> an encryption format.
Even if etcd is encrypted at rest, everybody can decode a given base64 string which is stored in the Secret.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>For example: The string &lt;code>Thomas&lt;/code> encoded as base64 is &lt;code>VGhvbWFzCg==&lt;/code>. This is simply a masked plain text and it is not secure to share these values, especially not on Git.
To make your CI/CD pipelines or Gitops process secure, you need to think of a secure way to manage your Secrets. Thus, your Secret objects must be encrypted somehow. HashiCorp Vault is one option to achieve this requirement.&lt;/p>
&lt;/div></description></item><item><title>Automated ETCD Backup</title><link>https://blog.stderr.at/day-2/etcd/2022-01-29-automatedetcdbackup/</link><pubDate>Sat, 29 Jan 2022 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/etcd/2022-01-29-automatedetcdbackup/</guid><description>&lt;div class="paragraph">
&lt;p>Securing ETCD is one of the major Day-2 tasks for a Kubernetes cluster. This article will explain how to create a backup using OpenShift Cronjob.&lt;/p>
&lt;/div></description></item><item><title>Working with Environments</title><link>https://blog.stderr.at/day-2/labels_environmets/2022-01-12-creatingenvironment/</link><pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/labels_environmets/2022-01-12-creatingenvironment/</guid><description>&lt;div class="paragraph">
&lt;p>Imagine you have one OpenShift cluster and you would like to create 2 or more environments inside this cluster, but also separate them and force the environments to specific nodes, or use specific inbound routers. All this can be achieved using labels, IngressControllers and so on. The following article will guide you to set up dedicated compute nodes for infrastructure, development and test environments as well as the creation of IngressController which are bound to the appropriate nodes.&lt;/p>
&lt;/div></description></item><item><title>Advanced Cluster Security - Authentication</title><link>https://blog.stderr.at/acs/2021-12-11-acsauth/</link><pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/acs/2021-12-11-acsauth/</guid><description>&lt;div class="paragraph">
&lt;p>Red Hat Advanced Cluster Security (RHACS) Central is installed with one administrator user by default. Typically, customers request an integration with existing Identity Provider(s) (IDP). RHACS offers different options for such integration. In this article 2 IDPs will be configured as an example. First OpenShift Auth and second Red Hat Single Sign On (RHSSO) based on Keycloak&lt;/p>
&lt;/div></description></item><item><title>Secure your secrets with Sealed Secrets</title><link>https://blog.stderr.at/openshift/2021-09-25-sealed_secrets/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2021-09-25-sealed_secrets/</guid><description>&lt;div class="paragraph">
&lt;p>Working with a GitOps approach is a good way to keep all configurations and settings versioned and in sync on Git. Sensitive data, such as passwords to a database connection, will quickly come around.
Obviously, it is not a idea to store clear text strings in a, maybe even public, Git repository. Therefore, all sensitive information should be stored in a secret object. The problem with secrets in Kubernetes is that they are actually not encrypted. Instead, strings are base64 encoded which can be decoded as well. Thats not good …​ it should not be possible to decrypt secured data. Sealed Secret will help here…​&lt;/p>
&lt;/div></description></item><item><title>Introduction</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/</guid><description>&lt;div class="paragraph">
&lt;p>Pod scheduling is an internal process that determines placement of new pods onto nodes within the cluster. It is probably one of the most important tasks for a Day-2 scenario and should be considered at a very early stage for a new cluster. OpenShift/Kubernetes is already shipped with a &lt;strong>default scheduler&lt;/strong> which schedules pods as they get created accross the cluster, without any manual steps.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>However, there are scenarios where a more advanced approach is required, like for example using a specifc group of nodes for dedicated workload or make sure that certain applications do not run on the same nodes. Kubernetes provides different options:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>Controlling placement with node selectors&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with pod/node affinity/anti-affinity rules&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with taints and tolerations&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with topology spread constraints&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This series will try to go into the detail of the different options and explains in simple examples how to work with pod placement rules.
It is not a replacement for any official documentation, so always check out Kubernetes and or OpenShift documentations.&lt;/p>
&lt;/div></description></item><item><title>Node Affinity</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-29-node-affinity/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-29-node-affinity/</guid><description>&lt;div class="paragraph">
&lt;p>Node Affinity allows to place a pod to a specific group of nodes. For example, it is possible to run a pod only on nodes with a specific CPU or disktype. The disktype was used as an example for the &lt;code>nodeSelector&lt;/code> and yes …​ Node Affinity is conceptually similar to nodeSelector but allows a more granular configuration.&lt;/p>
&lt;/div></description></item><item><title>NodeSelector</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/</guid><description>&lt;div class="paragraph">
&lt;p>One of the easiest ways to tell your Kubernetes cluster where to put certain pods is to use a &lt;code>nodeSelector&lt;/code> specification. A nodeSelector defines a key-value pair and are defined inside the specification of the pods and as a label on one or multiple nodes (or machine set or machine config). Only if selector matches the node label, the pod is allowed to be scheduled on that node.&lt;/p>
&lt;/div></description></item><item><title>Pod Affinity/Anti-Affinity</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-28-affinity/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-28-affinity/</guid><description>&lt;div class="paragraph">
&lt;p>While noteSelector provides a very easy way to control where a pod shall be scheduled, the affinity/anti-affinity feature, expands this configuration with more expressive rules like logical AND operators, constraints against labels on other pods or soft rules instead of hard requirements.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The feature comes with two types:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>pod affinity/anti-affinity - allows constrains against other pod labels rather than node labels.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>node affinity - allows pods to specify a group of nodes they can be placed on&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div></description></item><item><title>Taints and Tolerations</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/</guid><description>&lt;div class="paragraph">
&lt;p>While Node Affinity is a property of pods that attracts them to a set of nodes, taints are the exact opposite. Nodes can be configured with one or more taints, which mark the node in a way to only accept pods that do tolerate the taints. The tolerations themselves are applied to pods, telling the scheduler to accept a taints and start the workload on a tainted node.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>A common use case would be to mark certain nodes as infrastructure nodes, where only specific pods are allowed to be executed or to taint nodes with a special hardware (i.e. GPU).&lt;/p>
&lt;/div></description></item><item><title>Topology Spread Constraints</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/</guid><description>&lt;div class="paragraph">
&lt;p>&lt;strong>Topology spread constraints&lt;/strong> is a new feature since Kubernetes 1.19 (OpenShift 4.6) and another way to control where pods shall be started. It allows to use failure-domains, like zones or regions or to define custom topology domains. It heavily relies on configured node labels, which are used to define topology domains. This feature is a more granular approach than affinity, allowing to achieve higher availability.&lt;/p>
&lt;/div></description></item><item><title>Using Descheduler</title><link>https://blog.stderr.at/day-2/pod-placement/2021-09-01-descheduler/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-09-01-descheduler/</guid><description>&lt;div class="paragraph">
&lt;p>&lt;strong>Descheduler&lt;/strong> is a new feature which is GA since OpenShift 4.7. It can be used to evict pods from nodes based on specific strategies. The evicted pod is then scheduled on another node (by the Scheduler) which is more suitable.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This feature can be used when:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>nodes are under/over-utilized&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pod or node affinity, taints or labels have changed and are no longer valid for a running pod&lt;/p>
&lt;/li>
&lt;li>
&lt;p>node failures&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pods have been restarted too many times&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div></description></item><item><title>oc compliance command line plugin</title><link>https://blog.stderr.at/compliance/2021/07/oc-compliance-command-line-plugin/</link><pubDate>Tue, 20 Jul 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/compliance/2021/07/oc-compliance-command-line-plugin/</guid><description>&lt;div class="paragraph">
&lt;p>As described at &lt;a href="https://blog.stderr.at/compliance/2021/07/compliance-operator/">Compliance Operator&lt;/a> the Compliance Operator can be used to scan the OpenShift cluster environment against security benchmark, like CIS.
Fetching the actual results might be a bit tricky tough.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>With OpenShift 4.8 plugins to the &lt;code>oc&lt;/code> command are allowed. One of these plugin os &lt;code>oc compliance&lt;/code>, which allows you to easily fetch scan results, re-run scans and so on.
Let’s install and try it out.&lt;/p>
&lt;/div></description></item><item><title>Compliance Operator</title><link>https://blog.stderr.at/compliance/2021/07/compliance-operator/</link><pubDate>Mon, 19 Jul 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/compliance/2021/07/compliance-operator/</guid><description>&lt;div class="paragraph">
&lt;p>OpenShift comes out of the box with a highly secure operating system, called Red Hat CoreOS. This OS is immutable, which means that no direct changes are done inside the OS, instead any configuration is managed by OpenShift itself using MachineConfig objects. Nevertheless, hardening certain settings must still be considered. Red Hat released a hardening guide (CIS Benchmark) which can be downloaded at &lt;a href="https://www.cisecurity.org/" class="bare">https://www.cisecurity.org/&lt;/a>.&lt;/p>
&lt;/div></description></item><item><title>Understanding RWO block device handling in OpenShift</title><link>https://blog.stderr.at/openshift/2021-02-27-understanding-block-devices/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2021-02-27-understanding-block-devices/</guid><description>&lt;div class="paragraph">
&lt;p>In this blog post we would like to explore OpenShift / Kubernetes
block device handling. We try to answer the following questions:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>What happens if multiple pods try to access the same block device?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>What happens if we scale a deployment using block devices to more than one replica?&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div></description></item><item><title>Writing Operator using Ansible</title><link>https://blog.stderr.at/openshift/2021-01-27-writingoperatoransible/</link><pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2021-01-27-writingoperatoransible/</guid><description>&lt;div class="paragraph">
&lt;p>This quick post shall explain, without any fancy details, how to write an Operator based on Ansible. It is assumed that you know what purpose an Operator has.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>As a short summary: Operators are a way to create custom controllers in OpenShift or Kubernetes. It watches for custom resource objects and creates the application based on the parameters in such custom resource object.
Often written in &lt;strong>Go&lt;/strong>, the SDK supports &lt;strong>Ansible&lt;/strong>, &lt;strong>Helm&lt;/strong> and (new) &lt;strong>Java&lt;/strong> as well.&lt;/p>
&lt;/div></description></item><item><title>Thanos Querier vs Thanos Querier</title><link>https://blog.stderr.at/openshift/2020-12-10-thanos-vs-thanos/</link><pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2020-12-10-thanos-vs-thanos/</guid><description>&lt;div class="paragraph">
&lt;p>OpenShift comes per default with a static Grafana dashboard, which will present cluster metrics to cluster administrators. It is not possible to customize this Grafana instance.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>However, many customers would like to create their own dashboards, their own monitoring and their own alerting while leveraging the possibilities of OpenShift at the same time and without installing a completely separated monitoring stack.&lt;/p>
&lt;/div></description></item><item><title>GitOps - Argo CD</title><link>https://blog.stderr.at/openshift/2020-08-06-argocd/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2020-08-06-argocd/</guid><description>&lt;div class="paragraph">
&lt;p>&lt;a href="https://argoproj.github.io/argo-cd/">Argo CD&lt;/a> &lt;em>is a declarative, GitOps continuous delivery tool for Kubernetes. GitOps itself uses Git pull request to manager infrastructure and application configuration.&lt;/em>&lt;/p>
&lt;/div></description></item><item><title>Enable Automatic Route Creation</title><link>https://blog.stderr.at/service-mesh/2020/05/enable-automatic-route-creation/</link><pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/service-mesh/2020/05/enable-automatic-route-creation/</guid><description>&lt;div class="paragraph">
&lt;p>Red Hat Service Mesh 1.1 allows you to enable a &amp;#34;&lt;strong>Automatic Route Creation&lt;/strong>&amp;#34; which will take care about the routes for a specific Gateway. Instead of defining * for hosts, a list of domains can be defined. The Istio OpenShift Routing (ior) synchronizes the routes and creates them inside the Istio namespace. If a Gateway is deleted, the routes will also be removed again.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This new features makes the manual creation of the route obsolete, as it was explained here: &lt;a href="https://blog.stderr.at/service-mesh/2020/03/ingress-with-custom-domain/">Openshift 4 and Service Mesh 4 - Ingress with custom domain&lt;/a>&lt;/p>
&lt;/div></description></item></channel></rss>