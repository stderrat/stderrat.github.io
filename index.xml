<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>YAUB Yet Another Useless Blog on TechBlog about OpenShift/Ansible/Satellite and much more</title><link>https://blog.stderr.at/</link><description>Recent content in YAUB Yet Another Useless Blog on TechBlog about OpenShift/Ansible/Satellite and much more</description><generator>Hugo</generator><language>en-us</language><copyright>Toni Schmidbauer &amp; Thomas Jungbauer</copyright><atom:link href="https://blog.stderr.at/index.xml" rel="self" type="application/rss+xml"/><item><title>The Hitchhiker's Guide to Observability Introduction - Part 1</title><link>https://blog.stderr.at/day-2/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/</link><pubDate>Sun, 23 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/</guid><description>&lt;div class="paragraph">
&lt;p>In modern microservices architectures, understanding how requests flow through your distributed system is crucial for debugging, performance optimization, and maintaining system health. &lt;strong>Distributed tracing&lt;/strong> provides visibility into these complex interactions by tracking requests as they traverse multiple services.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>With this article I would like to summarize and, especially, remember my setup. This is Part 1 of a series of articles that I split up so it is easier to read and understand and not too long. Initially, there will be 6 parts, but I will add more as needed.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This whole guide demonstrates how to set up a distributed tracing infrastructure using&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>&lt;strong>OpenShift&lt;/strong> (4.16+) as base platform&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Red Hat Build of OpenTelemetry&lt;/strong> - The observability framework based on &lt;a href="https://opentelemetry.io/" target="_blank" rel="noopener">OpenTelemetry&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>TempoStack&lt;/strong> - &lt;a href="https://grafana.com/docs/tempo/latest/" target="_blank" rel="noopener">Grafana’s distributed&lt;/a> tracing backend for Kubernetes&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Multi-tenant architecture&lt;/strong> - Isolating traces by team or environment&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Cluster Observability Operator&lt;/strong> - For now this Operator is only used to extend the OpenShift UI with the tracing UI.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div></description></item><item><title>The Hitchhiker's Guide to Observability - Grafana Tempo - Part 2</title><link>https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/</link><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/</guid><description>&lt;div class="paragraph">
&lt;p>After covering the fundamentals and architecture in Part 1, it’s time to get our hands dirty! This article walks through the complete implementation of a distributed tracing infrastructure on OpenShift.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>We’ll deploy and configure the &lt;strong>Tempo Operator&lt;/strong> and a multi-tenant &lt;strong>TempoStack&lt;/strong> instance. For S3 storage we will use the integrated OpenShift Data Foundation. However, you can use whatever S3-compatible storage you have available.&lt;/p>
&lt;/div></description></item><item><title>The Hitchhiker's Guide to Observability - Central Collector - Part 3</title><link>https://blog.stderr.at/day-2/observability/2025-11-25-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part3/</link><pubDate>Tue, 25 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/observability/2025-11-25-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part3/</guid><description>&lt;div class="paragraph">
&lt;p>With the architecture defined in &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/">Part 1&lt;/a> and TempoStack deployed in &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/">Part 2&lt;/a>, it’s time to tackle the heart of our distributed tracing system: the &lt;strong>Central OpenTelemetry Collector&lt;/strong>. This is the critical component that sits between your application namespaces and TempoStack, orchestrating trace flow, metadata enrichment, and tenant routing.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>In this article, we’ll configure the RBAC permissions required for the Central Collector to enrich traces with Kubernetes metadata and deploy the Central OpenTelemetry Collector with its complete configuration. You’ll learn how to set up receivers for accepting traces from local collectors, configure processors to enrich traces with Kubernetes and OpenShift metadata, and implement routing connectors to direct traces to the appropriate TempoStack tenants based on namespace.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>To be honest, this was the most challenging part of the entire setup to get right, as you can easily miss a setting, or misconfigure a part of the configuration. But once you understand the configuration, it becomes straightforward to extend and modify.&lt;/p>
&lt;/div></description></item><item><title>The Hitchhiker's Guide to Observability - Example Applications - Part 4</title><link>https://blog.stderr.at/day-2/observability/2025-11-26-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part4/</link><pubDate>Wed, 26 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/observability/2025-11-26-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part4/</guid><description>&lt;div class="paragraph">
&lt;p>With the &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/">architecture defined&lt;/a>, &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/">TempoStack deployed&lt;/a>, and the &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-25-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part3/">Central Collector configured&lt;/a>, we’re now ready to complete the distributed tracing pipeline. It’s time to deploy real applications and see traces flowing through the entire system!&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>In this fourth installment, we’ll focus on the &lt;strong>application layer&lt;/strong> - deploying &lt;strong>Local OpenTelemetry Collectors&lt;/strong> in team namespaces and configuring example applications to generate traces. You’ll see how applications automatically get enriched with Kubernetes metadata, how namespace-based routing directs traces to the correct TempoStack tenants, and how the entire two-tier architecture comes together.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>We’ll deploy an example application (&lt;strong>Mockbin&lt;/strong>) into two different namespaces. Together with the application, we will deploy a local OpenTelemetry Collector. One with sidecar mode, one with deployment mode.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>You’ll learn how to configure local collectors to add namespace attributes, forward traces to the central collector, and verify that traces appear in the UI with full Kubernetes context.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>By the end of this article, you’ll have a fully functional distributed tracing system with applications generating traces, local collectors forwarding them, and the central collector routing everything to the appropriate TempoStack tenants. Let’s bring this architecture to life!&lt;/p>
&lt;/div></description></item><item><title>The Hitchhiker's Guide to Observability - Understanding Traces - Part 5</title><link>https://blog.stderr.at/day-2/observability/2025-11-27-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part5/</link><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/observability/2025-11-27-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part5/</guid><description>&lt;div class="paragraph">
&lt;p>With the &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/">architecture established&lt;/a>, &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/">TempoStack deployed&lt;/a>, &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-25-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part3/">the Central Collector configured&lt;/a>, and &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-26-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part4/">applications generating traces&lt;/a>, it’s time to take a step back and understand what we’re actually building. Before you deploy more applications and start troubleshooting performance issues, you need to &lt;strong>understand how to read and interpret distributed traces&lt;/strong>.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Let’s decode the matrix of distributed tracing!&lt;/p>
&lt;/div></description></item><item><title>The Hitchhiker's Guide to Observability - Adding A New Tenant - Part 6</title><link>https://blog.stderr.at/day-2/observability/2025-11-28-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part6/</link><pubDate>Fri, 28 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/observability/2025-11-28-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part6/</guid><description>&lt;div class="paragraph">
&lt;p>While we have created our distributed tracing infrastructure, we created two tenants as an example. In this article, I will show you how to add a new tenant and which changes must be made in the TempoStack and the OpenTelemetry Collector.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This article was mainly created as a quick reference guide to see which changes must be made when adding new tenants.&lt;/p>
&lt;/div></description></item><item><title>What's new in OpenShift, 4.20 Edition</title><link>https://blog.stderr.at/whatss-new/2025-11-10-whatsnew-420/</link><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/whatss-new/2025-11-10-whatsnew-420/</guid><description>&lt;div class="paragraph">
&lt;p>This article covers news and updates in the OpenShift 4.20 release. We focus on points that got our attention, but this
is &lt;strong>not&lt;/strong> a complete summary of the &lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/release_notes/index">release notes&lt;/a>.&lt;/p>
&lt;/div></description></item><item><title>Red Hat Quay Registry - Integrate Keycloak</title><link>https://blog.stderr.at/quay/2025-09-19-quay-integrate-keycloak/</link><pubDate>Fri, 19 Sep 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/quay/2025-09-19-quay-integrate-keycloak/</guid><description>&lt;div class="paragraph">
&lt;p>This guide shows you how to configure Keycloak as an OpenID Connect (OIDC) provider for Red Hat Quay Registry. It covers what to configure in Keycloak, what to put into Quay’s config.yaml (or Operator config), how to verify the login flow, and how to switch your Quay initial/admin account (stored locally in Quay’s DB) to an admin user that authenticates via Keycloak.&lt;/p>
&lt;/div></description></item><item><title>A second look into the Kubernetes Gateway API on OpenShift</title><link>https://blog.stderr.at/openshift/2025/08/a-second-look-into-the-kubernetes-gateway-api-on-openshift/</link><pubDate>Sun, 31 Aug 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2025/08/a-second-look-into-the-kubernetes-gateway-api-on-openshift/</guid><description>&lt;div class="paragraph">
&lt;p>This is our second look into the Kubernetes Gateway API an it’s
integration into OpenShift. This post covers TLS configuration.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The Kubernetes Gateway API is new implementation of the ingress, load
balancing and service mesh API’s. See
&lt;a href="https://gateway-api.sigs.k8s.io/" target="_blank" rel="noopener">upstream&lt;/a> for more information.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Also the &lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/ingress_and_load_balancing/configuring-ingress-cluster-traffic#nw-ingress-gateway-api-overview_ingress-gateway-api" target="_blank" rel="noopener">OpenShift documentation&lt;/a> provides an overview of the Gateway API and it’s integration.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>We demonstrate how to add TLS to our Nginx deployment, how to
implement a shared Gateway and finally how to implement HTTP to HTTPS
redirection with the Gateway API. Furthermore we cover how &lt;em>HTTPRoute&lt;/em>
objects attach to Gateways and dive into ordering of &lt;em>HTTPRoute&lt;/em>
objects.&lt;/p>
&lt;/div></description></item><item><title>A first look into the Kubernetes Gateway API on OpenShift</title><link>https://blog.stderr.at/openshift/2025/08/a-first-look-into-the-kubernetes-gateway-api-on-openshift/</link><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2025/08/a-first-look-into-the-kubernetes-gateway-api-on-openshift/</guid><description>&lt;div class="paragraph">
&lt;p>This blog post summarizes our first look into the Kubernetes Gateway
API and how it is integrated in OpenShift.&lt;/p>
&lt;/div></description></item><item><title>Reusable Argo CD Application Helm Template</title><link>https://blog.stderr.at/gitopscollection/2025-07-17-common-template-application/</link><pubDate>Thu, 17 Jul 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2025-07-17-common-template-application/</guid><description>&lt;div class="paragraph">
&lt;p>When working with Argo CD at scale, you often find yourself creating similar Application manifests repeatedly. Each application needs the same basic structure but with different configurations for source repositories, destinations, and sync policies. Additionally, managing namespace metadata becomes tricky when you need to conditionally control whether Argo CD should manage namespace metadata based on sync options.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>In this article, I’ll walk you through a reusable Helm template that solves these challenges by providing a flexible, DRY (Don’t Repeat Yourself) approach to creating Argo CD Applications. This template is available in my public Helm Chart library and can easily be used by anyone.&lt;/p>
&lt;/div></description></item><item><title>Cert-Manager Policy Approver in OpenShift</title><link>https://blog.stderr.at/openshift/2025/06/cert-manager-policy-approver-in-openshift/</link><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2025/06/cert-manager-policy-approver-in-openshift/</guid><description>&lt;div class="paragraph">
&lt;p>One of the most commonly deployed operators in OpenShift environments is the &lt;strong>Cert-Manager Operator&lt;/strong>. It automates the management of TLS certificates for applications running within the cluster, including their issuance and renewal.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The tool supports a variety of certificate issuers by default, including ACME, Vault, and self-signed certificates. Whenever a certificate is needed, Cert-Manager will automatically create a CertificateRequest resource that contains the details of the certificate. This resource is then processed by the appropriate issuer to generate the actual TLS certificate. The approval process in this case is usually fully automated, meaning that the certificate is issued without any manual intervention.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>But what if you want to have more control? What if certificate issuance must follow strict organizational policies, such as requiring a specifc country code or organization name?
This is where the &lt;strong>CertificateRequestPolicy&lt;/strong> resource, a resource provided by the Approver Policy, comes into play.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This article walks through configuring the &lt;strong>Cert-Manager Approver Policy&lt;/strong> in OpenShift to enforce granular policies on certificate requests.&lt;/p>
&lt;/div></description></item><item><title>Single log out from Keycloak and OpenShift</title><link>https://blog.stderr.at/openshift/2025/05/single-log-out-from-keycloak-and-openshift/</link><pubDate>Thu, 22 May 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2025/05/single-log-out-from-keycloak-and-openshift/</guid><description>&lt;div class="paragraph">
&lt;p>The following 1-minute article is a follow-up to my &lt;a href="https://blog.stderr.at/openshift/2025/05/step-by-step-using-keycloak-authentication-in-openshift/">previous article&lt;/a> about how to use Keycloak as an authentication provider for OpenShift. In this article, I will show you how to configure Keycloak and OpenShift for Single Log Out (SLO). This means that when you log out from Keycloak, you will also be logged out from OpenShift automatically. This requires some additional configuration in Keycloak and OpenShift, but it is not too complicated.&lt;/p>
&lt;/div></description></item><item><title>Step by Step - Using Keycloak Authentication in OpenShift</title><link>https://blog.stderr.at/openshift/2025/05/step-by-step-using-keycloak-authentication-in-openshift/</link><pubDate>Sat, 17 May 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2025/05/step-by-step-using-keycloak-authentication-in-openshift/</guid><description>&lt;div class="paragraph">
&lt;p>I was recently asked about how to use Keycloak as an authentication provider for OpenShift. How to install Keycloak using the Operator and how to configure Keycloak and OpenShift so that users can log in to OpenShift using OpenID.
I have to admit that the exact steps are not easy to find, so I decided to write a blog post about it, describing each step in detail.
This time I will not use GitOps, but the OpenShift and Keycloak Web Console to show the steps, because before we put it into GitOps, we need to understand what is actually happening.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This article tries to explain every step required so that a user can authenticate to OpenShift using Keycloak as an Identity Provider (IDP) and that Groups from Keycloak are imported into OpenShift. This article does not cover a production grade installation of Keycloak, but only a test installation, so you can see how it works. For production, you might want to consider a proper database (maybe external, but at least with a backup), high availability, etc.).&lt;/p>
&lt;/div></description></item><item><title>Using ApplicationSet with Matrix Generator and define individual Namespaces</title><link>https://blog.stderr.at/gitopscollection/2025-04-17-applicationset-defining-namespaces/</link><pubDate>Thu, 17 Apr 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2025-04-17-applicationset-defining-namespaces/</guid><description>&lt;div class="paragraph">
&lt;p>During my day-to-day business, I am discussing the following setup with many customers: &lt;a href="https://blog.stderr.at/gitopscollection/2024-04-02-configure_app_of_apps/">Configure App-of-Apps&lt;/a>. Here I try to explain how I use an ApplicationSet that watches over a folder in Git and automatically adds a new Argo CD Application whenever a new folder is found. This works great, but there is a catch: The ApplicationSet uses the same Namespace &lt;strong>default&lt;/strong> for all Applications. This is not always desired, especially when you have different teams working on different Applications.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Recently I was asked by the customer if this can be fixed and if it is possible to define different Namespaces for each Application. The answer is yes, and I would like to show you how to do this.&lt;/p>
&lt;/div></description></item><item><title>Introducing AdminNetworkPolicies</title><link>https://blog.stderr.at/openshift/2024/11/introducing-adminnetworkpolicies/</link><pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2024/11/introducing-adminnetworkpolicies/</guid><description>&lt;div class="paragraph">
&lt;p>Classic Kubernetes/OpenShift offer a feature called NetworkPolicy that allows users to control the traffic to and from their assigned Namespace.
NetworkPolicies are designed to give project owners or tenants the ability to protect their own namespace. Sometimes, however, I worked with customers where the
cluster administrators or a dedicated (network) team need to enforce these policies.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Since the NetworkPolicy API is namespace-scoped, it is not possible to enforce policies across namespaces. The only solution was to create custom (project) admin and edit
roles, and remove the ability of creating, modifying or deleting NetworkPolicy objects. Technically, this is possible and easily done. But shifts the whole network security to cluster administrators.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Luckily, this is where &lt;strong>AdminNetworkPolicy&lt;/strong> (ANP) and &lt;strong>BaselineAdminNetworkPolicy&lt;/strong> (BANP) comes into play.&lt;/p>
&lt;/div></description></item><item><title>Using Kustomize to post render a Helm Chart</title><link>https://blog.stderr.at/gitopscollection/2024-10-13-using-post-renderer/</link><pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2024-10-13-using-post-renderer/</guid><description>&lt;div class="paragraph">
&lt;p>Lately I came across several issues where a given Helm Chart must be modified after it has been rendered by Argo CD.
Argo CD does a &lt;strong>helm template&lt;/strong> to render a Chart. Sometimes, especially when you work with Subcharts or when a specific setting is not yet supported by the Chart, you need to modify it later …​ you need to post-render the Chart.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>In this very short article, I would like to demonstrate this on a real-live example I had to do. I would like to inject annotations to a Route objects, so that the certificate can be injected. This is done by the cert-utils operator.
For the post-rendering the Argo CD repo pod will be extended with a sidecar container, that is watching for the repos and patches them if required.&lt;/p>
&lt;/div></description></item><item><title>Managing Certificates using GitOps approach</title><link>https://blog.stderr.at/gitopscollection/2024-07-04-managing-certificates-with-gitops/</link><pubDate>Thu, 04 Jul 2024 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2024-07-04-managing-certificates-with-gitops/</guid><description>&lt;div class="paragraph">
&lt;p>The article &lt;a href="https://blog.stderr.at/openshift/2023/02/ssl-certificate-management-for-openshift-on-aws/">SSL Certificate Management for OpenShift on AWS&lt;/a> explains how to use the &lt;strong>Cert-Manager Operator&lt;/strong> to request and install a new SSL Certificate.
This time, I would like to leverage the GitOps approach using the Helm Chart &lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/cert-manager" target="_blank" rel="noopener">cert-manager&lt;/a> I have prepared to deploy the Operator and order new Certificates.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>I will use an ACME Letsencrypt issuer with a DNS challenge. My domain is hosted at AWS Route 53.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>However, any other integration can be easily used.&lt;/p>
&lt;/div></description></item><item><title>Update Cluster Version using GitOps approach</title><link>https://blog.stderr.at/gitopscollection/2024-06-07-update-cluster-version-with-gitops/</link><pubDate>Fri, 07 Jun 2024 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2024-06-07-update-cluster-version-with-gitops/</guid><description>&lt;div class="paragraph">
&lt;p>During a GitOps journey at one point, the question arises, how to update a cluster? Nowadays it is very easy to update a cluster using CLI or WebUI, so why bother with GitOps in that case? The reason is simple: Using GitOps you can be sure that all clusters are updated to the correct, required version and the version of each cluster is also managed in Git.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>All you need is the &lt;strong>channel&lt;/strong> you want to use and the desired cluster &lt;strong>version&lt;/strong>. Optionally, you can define the exact image SHA. This might be required when you are operating in a restricted environment.&lt;/p>
&lt;/div></description></item><item><title>Multiple Sources for Applications in Argo CD</title><link>https://blog.stderr.at/gitopscollection/2024-06-02-multisources-for-application-in-argocd/</link><pubDate>Sun, 02 Jun 2024 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2024-06-02-multisources-for-application-in-argocd/</guid><description>&lt;div class="paragraph">
&lt;p>Argo CD or OpenShift GitOps uses Applications or ApplicationSets to define the relationship between a source (Git) and a cluster. Typically, this is a 1:1 link, which means one Application is using one source to compare the cluster status. This can be a limitation. For example, if you are working with Helm Charts and a Helm repository, you do not want to re-build (or re-release) the whole chart just because you made a small change in the values file that is packaged into the repository. You want to separate the configuration of the chart with the Helm package.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The most common scenarios for multiple sources are (see: &lt;a href="https://argo-cd.readthedocs.io/en/stable/user-guide/multiple_sources/" target="_blank" rel="noopener">Argo CD documentation&lt;/a>):&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>Your organization wants to use an external/public Helm chart&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You want to override the Helm values with your own local values&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You don’t want to clone the Helm chart locally as well because that would lead to duplication and you would need to monitor it manually for upstream changes.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This small article describes three different ways with a working example and tries to cover the advantages and disadvantages of each of them. They might be opinionated but some of them proved to be easier to use and manage.&lt;/p>
&lt;/div></description></item></channel></rss>