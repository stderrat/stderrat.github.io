<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>YAUB Yet Another Useless Blog on TechBlog about OpenShift/Ansible/Satellite and much more</title><link>https://blog.stderr.at/</link><description>TechBlog about OpenShift/Ansible/Satellite and much more</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Toni Schmidbauer &amp; Thomas Jungbauer</copyright><atom:link href="https://blog.stderr.at/index.xml" rel="self" type="application/rss+xml"/><item><title>GitOps Catalog</title><link>https://blog.stderr.at/whats-new/2025-12-23-gitops-catalog/</link><pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/whats-new/2025-12-23-gitops-catalog/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;The &lt;a href="https://blog.stderr.at/gitops-catalog/"&gt;GitOps Catalog&lt;/a&gt; page provides an interactive visualization of all available ArgoCD applications from the openshift-clusterconfig-gitops repository. Check out the page &lt;strong&gt;&lt;a href="https://blog.stderr.at/gitops-catalog/"&gt;GitOps Catalog&lt;/a&gt;&lt;/strong&gt; for more details.&lt;/p&gt;
&lt;/div&gt;</description></item><item><title>Hosted Control Planes behind a Proxy</title><link>https://blog.stderr.at/openshift-platform/other-topics/2025-12-15-hosted-control-planes-and-proxy/</link><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/other-topics/2025-12-15-hosted-control-planes-and-proxy/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;Recently, I encountered a problem deploying a Hosted Control Plane (HCP) at a customer site. The installation started successfullyâ€”etcd came up fineâ€”but then it just stopped. The virtual machines were created, but they never joined the cluster. No OVN or Multus pods ever started.
The only meaningful message in the cluster-version-operator pod logs was:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;I1204 08:22:35.473783 1 status.go:185] Synchronizing status errs=field.ErrorList(nil) status=&amp;amp;cvo.SyncWorkerStatus{Generation:1, Failure:(*payload.UpdateError)(0xc0006313b0), Done:575, Total:623,&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This message did appear over and over again.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_problem_summary"&gt;Problem Summary&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;etcd started successfully, but the installation stalled&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;API server was running&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VMs started but never joined the cluster&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;No OVN or Multus pods ever started&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The installation was stuck in a loop&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_troubleshooting"&gt;Troubleshooting&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Troubleshooting was not straightforward. The cluster-version-operator logs provided the only clue.
However, the VMs were already running, so we could log in to themâ€”and there it was, the reason for the stalled installation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_connect_to_a_vm"&gt;Connect to a VM&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To connect to a VM, use the &lt;code&gt;virtctl&lt;/code&gt; command. This connects you to the machine as the &lt;code&gt;core&lt;/code&gt; user:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
You can download &lt;code&gt;virtctl&lt;/code&gt; from the OpenShift Downloads page in the OpenShift web console.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock warning"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-warning" title="Warning"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
You need the SSH key for the &lt;code&gt;core&lt;/code&gt; user.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;virtctl ssh -n clusters-my-hosted-cluster core@vmi/my-node-pool-dsj7z-sss8w &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Replace &lt;strong&gt;my-hosted-cluster&lt;/strong&gt; with the name of your hosted cluster and &lt;strong&gt;my-node-pool-dsj7z-sss8w&lt;/strong&gt; with the name of your VM.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_verify_the_problem"&gt;Verify the Problem&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Once logged in, check the &lt;code&gt;journalctl -xf&lt;/code&gt; output. In case of a proxy issue, youâ€™ll see an error like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&amp;gt; Dec 10 06:23:09 my-node-pool2-gwvjh-rwtx8 sh[2143]: time=&amp;#34;2025-12-10T06:23:09Z&amp;#34; level=warning msg=&amp;#34;Failed, retrying in 1s ... (3/3). Error: initializing source docker://quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:500704de3ef374e61417cc14eda99585450c317d72f454dda0dadd5dda1ba57a: pinging container registry quay.io: Get \&amp;#34;https://quay.io/v2/\&amp;#34;: dial tcp 3.209.93.201:443: i/o timeout&amp;#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;So we have a timeout when trying to pull the image from the container registry. This is a classic proxy issue. When you try &lt;code&gt;curl&lt;/code&gt; or manual pulls you will get the same message.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_configuring_the_proxy_for_the_hosted_control_plane"&gt;Configuring the Proxy for the Hosted Control Plane&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The big question is: How do we configure the proxy for the Hosted Control Plane?&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This is not well documented yetâ€”in fact, itâ€™s barely documented at all.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Hosted Control Plane is managed through a custom resource called &lt;code&gt;HostedCluster&lt;/code&gt;, and thatâ€™s exactly where we configure the proxy.
The upstream documentation at &lt;a href="https://hypershift.pages.dev/how-to/configure-ocp-components/#overview" target="_blank" rel="noopener"&gt;HyperShift Documentation&lt;/a&gt; explains that you can add a &lt;code&gt;configuration&lt;/code&gt; section to the resource. Letâ€™s do that:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Update the &lt;code&gt;HostedCluster&lt;/code&gt; resource with the following configuration:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;spec:
configuration:
proxy:
httpProxy: http://proxy.example.com:8080 &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
httpsProxy: https://proxy.example.com:8080 &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
noProxy: .cluster.local,.svc,10.128.0.0/14,127.0.0.1,localhost &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
trustedCA:
name: user-ca-bundle &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;HTTP proxy URL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;HTTPS proxy URL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Comma-separated list of domains, IPs, or CIDRs to exclude from proxy routing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;OPTIONAL: Name of a ConfigMap containing a custom CA certificate bundle&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
If youâ€™re using a custom CA, create the ConfigMap beforehand or alongside the &lt;code&gt;HostedCluster&lt;/code&gt; resource. The ConfigMap must contain a key named &lt;code&gt;ca-bundle.crt&lt;/code&gt; with your CA certificate(s).
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_summary"&gt;Summary&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Thatâ€™s actually everything you need for proxy configuration with Hosted Control Planes. Hopefully, the official OpenShift documentation will be updated soon to include this information.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>Helm Charts Repository Updates</title><link>https://blog.stderr.at/whats-new/2025-12-12-helm-charts-changelog/</link><pubDate>Fri, 12 Dec 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/whats-new/2025-12-12-helm-charts-changelog/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;This page shows the &lt;strong&gt;latest updates&lt;/strong&gt; to the &lt;a href="https://blog.stderr.at/helm-charts"&gt;stderr.at Helm Charts Repository&lt;/a&gt;.
The charts are designed for OpenShift and Kubernetes deployments, with a focus on GitOps workflows using Argo CD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The content below is dynamically loaded from the Helm repository and always shows the most recent changes.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;div id="helm-changelog-widget" class="helm-changelog"&gt;
&lt;div class="helm-changelog-loading"&gt;
&lt;i class="fa fa-spinner fa-spin"&gt;&lt;/i&gt; Loading latest Helm chart updates...
&lt;/div&gt;
&lt;/div&gt;
&lt;style&gt;
&lt;/style&gt;
&lt;script&gt;
(function() {
const container = document.getElementById('helm-changelog-widget');
const maxItems = "10";
const cacheBuster = Math.floor(Date.now() / 60000);
fetch(`https://charts.stderr.at/changelog.json?v=${cacheBuster}`)
.then(response =&gt; {
if (!response.ok) throw new Error('Failed to load changelog');
return response.json();
})
.then(data =&gt; {
if (!data.charts || data.charts.length === 0) {
container.innerHTML = '&lt;p class="helm-changelog-error"&gt;No charts found&lt;/p&gt;';
return;
}
const genDate = new Date(data.generated);
const charts = data.charts
.sort((a, b) =&gt; new Date(b.lastModified) - new Date(a.lastModified))
.slice(0, maxItems);
let html = `
&lt;div class="helm-changelog-header"&gt;
&lt;h3&gt;&lt;i class="fa fa-cubes"&gt;&lt;/i&gt; Latest Helm Chart Updates&lt;/h3&gt;
&lt;span class="helm-changelog-generated"&gt;Updated: ${genDate.toLocaleDateString()}&lt;/span&gt;
&lt;/div&gt;
`;
charts.forEach(chart =&gt; {
const date = new Date(chart.lastModified);
const dateStr = date.toLocaleDateString('en-US', {
month: 'short', day: 'numeric', year: 'numeric'
});
const iconHtml = chart.icon
? `&lt;img src="${chart.icon}" alt="" class="helm-chart-icon" width="60" height="60" data-webp-upgraded="true" onerror="this.outerHTML='&lt;i class=\\'fa fa-cube helm-chart-icon-fallback\\'&gt;&lt;/i&gt;'"&gt;`
: '&lt;i class="fa fa-cube helm-chart-icon-fallback"&gt;&lt;/i&gt;';
let changesHtml = '';
if (chart.changes &amp;&amp; chart.changes.length &gt; 0) {
changesHtml = '&lt;ul class="helm-changes-list"&gt;';
chart.changes.slice(0, 4).forEach(change =&gt; {
const kind = (change.kind || 'changed').toLowerCase();
changesHtml += `&lt;li class="${kind}"&gt;&lt;span class="helm-change-badge ${kind}"&gt;${kind}&lt;/span&gt;${change.description}&lt;/li&gt;`;
});
if (chart.changes.length &gt; 4) {
changesHtml += `&lt;li style="color:#888;border-left-color:#888;"&gt;... and ${chart.changes.length - 4} more changes&lt;/li&gt;`;
}
changesHtml += '&lt;/ul&gt;';
}
const chartUrl = chart.home || 'https://github.com/tjungbauer/helm-charts';
html += `
&lt;div class="helm-chart-item" data-href="${chartUrl}" onclick="window.open('${chartUrl}', '_blank')" role="link" tabindex="0"&gt;
&lt;div class="helm-chart-header"&gt;
${iconHtml}
&lt;span class="helm-chart-name"&gt;&lt;a href="${chartUrl}" target="_blank" rel="noopener noreferrer" class="highlight"&gt;${chart.name}&lt;/a&gt;&lt;/span&gt;
&lt;span class="helm-chart-version"&gt;v${chart.version}&lt;/span&gt;
&lt;span class="helm-chart-date"&gt;ðŸ“… ${dateStr}&lt;/span&gt;
&lt;/div&gt;
&lt;div class="helm-chart-description"&gt;${chart.description}&lt;/div&gt;
${changesHtml}
&lt;/div&gt;
`;
});
html += `
&lt;div class="helm-changelog-footer"&gt;
&lt;a href="https://github.com/tjungbauer/helm-charts" target="_blank" rel="noopener noreferrer"&gt;
&lt;i class="fa fa-github"&gt;&lt;/i&gt; View all ${data.charts.length} charts on GitHub
&lt;/a&gt;
&amp;nbsp;|&amp;nbsp;
&lt;a href="https://charts.stderr.at/" target="_blank" rel="noopener noreferrer"&gt;
&lt;i class="fa fa-external-link"&gt;&lt;/i&gt; Helm Repository
&lt;/a&gt;
&lt;/div&gt;
`;
container.innerHTML = html;
})
.catch(error =&gt; {
console.error('Helm changelog error:', error);
container.innerHTML = `
&lt;p class="helm-changelog-error"&gt;
&lt;i class="fa fa-exclamation-triangle"&gt;&lt;/i&gt;
Could not load Helm chart updates.
&lt;a href="https://github.com/tjungbauer/helm-charts" target="_blank" rel="noopener noreferrer"&gt;View on GitHub&lt;/a&gt;
&lt;/p&gt;
`;
});
})();
&lt;/script&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_quick_links"&gt;Quick Links&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;table class="tableblock frame-all grid-all stretch"&gt;
&lt;colgroup&gt;
&lt;col style="width: 33.3333%;"/&gt;
&lt;col style="width: 66.6667%;"/&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class="tableblock halign-left valign-top"&gt;Resource&lt;/th&gt;
&lt;th class="tableblock halign-left valign-top"&gt;Link&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Helm Repository&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;&lt;a href="https://charts.stderr.at/" class="bare"&gt;https://charts.stderr.at/&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;GitHub Source&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;&lt;a href="https://github.com/tjungbauer/helm-charts" class="bare"&gt;https://github.com/tjungbauer/helm-charts&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;ArtifactHub&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;&lt;a href="https://artifacthub.io/packages/search?repo=tjungbauer" class="bare"&gt;https://artifacthub.io/packages/search?repo=tjungbauer&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Example GitOps Repo&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;&lt;a href="https://github.com/tjungbauer/openshift-clusterconfig-gitops" class="bare"&gt;https://github.com/tjungbauer/openshift-clusterconfig-gitops&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>The Hitchhiker's Guide to Observability - Limit Read Access to Traces - Part 8</title><link>https://blog.stderr.at/openshift-platform/observability/observability/2025-12-06-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part8/</link><pubDate>Sat, 06 Dec 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/observability/observability/2025-12-06-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part8/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;In the previous articles, we deployed a distributed tracing infrastructure with TempoStack and OpenTelemetry Collector. We also deployed a Grafana instance to visualize the traces. The configuration was done in a way that allows everybody to read the traces. Every &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/#_step_3_configure_rbac_for_tempostack_trace_access_readwrite"&gt;&lt;strong&gt;system:authenticated&lt;/strong&gt;&lt;/a&gt; user is able to read &lt;strong&gt;ALL&lt;/strong&gt; traces.
This is usually not what you want. You want to limit trace access to only the appropriate namespace.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In this article, weâ€™ll limit the read access to traces. The users of the &lt;strong&gt;team-a&lt;/strong&gt; namespace will only be able to see their own traces.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Before we begin, make sure you have:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;TempoStack deployed and configured (from &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/"&gt;Part 2&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Team-a namespace with traces flowing (from &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-26-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part4/"&gt;Part 4&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A separate user for the team-a namespace.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_verify_trace_access"&gt;Verify Trace Access&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s verify what a user can see when they are authenticated.
We have &lt;strong&gt;user1&lt;/strong&gt; who is a member of the &lt;strong&gt;team-a&lt;/strong&gt; namespace. Letâ€™s log in as this user and verify what they can see.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Navigate to &lt;strong&gt;Observability &amp;gt; Traces&lt;/strong&gt; and select the tempostack/simplest datasource:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/observability-traces-tenanta.png" alt="Observability Traces Team-a"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;You should see the traces for the &lt;strong&gt;team-a&lt;/strong&gt; namespace. This is fineâ€”thatâ€™s what we want.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;But now letâ€™s change the tenant to &lt;strong&gt;tenantB&lt;/strong&gt; and verify what the user can see.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/observability-traces-tenantb.png" alt="Observability Traces Team-b"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As you can see, the user can see traces from the &lt;strong&gt;tenantB&lt;/strong&gt; namespace, although they are a member of the &lt;strong&gt;team-a&lt;/strong&gt; namespace. This is not what we want. We want to limit trace access to only the appropriate namespace.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_the_original_rbac_configuration"&gt;The Original RBAC Configuration&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/"&gt;Part 2&lt;/a&gt;, we created a ClusterRoleBinding to grant read access to traces for everybody who is authenticated.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
name: tempostack-traces-reader
subjects:
- kind: Group
apiGroup: rbac.authorization.k8s.io
name: &amp;#39;system:authenticated&amp;#39;
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: tempostack-traces-reader&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This ClusterRoleBinding allows &lt;strong&gt;system:authenticated&lt;/strong&gt; users to read all traces from all tenants and is bound to the ClusterRole &lt;strong&gt;tempostack-traces-reader&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
name: tempostack-traces-reader
rules:
- verbs:
- get
apiGroups:
- tempo.grafana.com
resources:
- tenantA
- tenantB
resourceNames:
- traces&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This is the configuration we want to change. We want to limit read access to traces to only the appropriate namespace.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_change_the_rbac_configuration"&gt;Change the RBAC Configuration&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We will change the RBAC configuration to limit read access to traces.
To do so, we will first create a new ClusterRole for the &lt;strong&gt;team-a&lt;/strong&gt; namespace and bind it to users of that namespace.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_create_the_new_clusterrole"&gt;Create the new ClusterRole:&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
name: tempostack-traces-reader-team-a &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
rules:
- verbs:
- get
apiGroups:
- tempo.grafana.com
resources:
- tenantA &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
resourceNames:
- traces&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Name of the ClusterRole, now with the prefix &lt;strong&gt;team-a&lt;/strong&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The Tenant that is allowed to read the traces. This time it is only the &lt;strong&gt;tenantA&lt;/strong&gt; tenant.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_create_the_new_clusterrolebinding"&gt;Create the new ClusterRoleBinding:&lt;/h3&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
name: tempostack-traces-reader-team-a &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
subjects:
- kind: User &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
apiGroup: rbac.authorization.k8s.io
name: user1
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: tempostack-traces-reader-team-a &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Name of the ClusterRoleBinding, now with the prefix &lt;strong&gt;team-a&lt;/strong&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The User that is allowed to read the traces. In this example: &lt;strong&gt;user1&lt;/strong&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The ClusterRole that is allowed to read the traces.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
In this example, we are using a single user. In a real-world scenario, you would most likely have a group of users. In that case, you would use a Group instead of a User.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_modify_the_original_clusterrole"&gt;Modify the Original ClusterRole:&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As a final step, we need to modify the original ClusterRole to remove &lt;strong&gt;tenantB&lt;/strong&gt; from the list of tenants that are allowed to read the traces.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
name: tempostack-traces-reader
rules:
- verbs:
- get
apiGroups:
- tempo.grafana.com
resources:
- tenantB &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
resourceNames:
- traces&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Only &lt;strong&gt;tenantA&lt;/strong&gt; remains. Remove &lt;strong&gt;tenantB&lt;/strong&gt; from the list of resources.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This removes the permission to read traces from the &lt;strong&gt;tenantB&lt;/strong&gt; namespace for the &lt;strong&gt;system:authenticated&lt;/strong&gt; group.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Eventually, the original ClusterRoleBinding might be deleted once every user has been assigned to a separate ClusterRole.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_verify_the_changes"&gt;Verify the Changes&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s see what these changes do.
As &lt;strong&gt;user1&lt;/strong&gt;, you should still be able to see the traces from the &lt;strong&gt;tenantA&lt;/strong&gt; namespace.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/observability-traces-tenanta-user1.png" alt="Observability Traces Team-a for user1"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;But if you change the tenant to &lt;strong&gt;tenantB&lt;/strong&gt;, you should see an error message like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/observability-traces-tenantb-user1.png" alt="Observability Traces Team-b for user1"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The user will still see the list of tenants. Hopefully, this will be fixed in a future version of TempoStack.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>The Hitchhiker's Guide to Observability - Here Comes Grafana - Part 7</title><link>https://blog.stderr.at/openshift-platform/observability/observability/2025-12-04-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part7/</link><pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/observability/observability/2025-12-04-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part7/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;While we have been using the integrated tracing UI in OpenShift, it is time to summon &lt;strong&gt;Grafana&lt;/strong&gt;. Grafana is a visualization powerhouse that allows teams to build custom dashboards, correlate traces with logs and metrics, and gain deep insights into their applications. In this article, weâ€™ll deploy a dedicated Grafana instance for &lt;strong&gt;team-a&lt;/strong&gt; in their namespace, configure a Tempo datasource, and create a dashboard to explore distributed traces.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Before we begin, make sure you have:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The Grafana Operator installed cluster-wide (weâ€™ll cover this first)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TempoStack deployed and configured (from &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/"&gt;Part 2&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Team-a namespace with traces flowing (from &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-26-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part4/"&gt;Part 4&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_the_grafana_operator"&gt;The Grafana Operator&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Grafana Operator provides Custom Resource Definitions (CRDs) for managing Grafana instances, datasources, and dashboards declaratively.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Be sure that the Operator is installed. Typically, it is installed in the &lt;code&gt;openshift-operators&lt;/code&gt; namespace. If you keep that namespace, all you need to do is create a Subscription. Otherwise you will also need to create an OperatorGroup.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
If you select a different namespace, be sure to verify possible RBAC bindings, that you might need to set up.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_install_subscription"&gt;Install Subscription&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Grafana Operator is available from the OperatorHub. Either use the UI to install it, or simply create a Subscription to install it:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
name: grafana-operator
namespace: openshift-operators &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
spec:
channel: v5 &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
installPlanApproval: Automatic
name: grafana-operator
source: community-operators &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
sourceNamespace: openshift-marketplace&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Installing in &lt;code&gt;openshift-operators&lt;/code&gt; makes the operator available cluster-wide&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Use the v5 channel for the operator. This is the only available channel currently.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The source is the OperatorHub catalog. Grafana is a &lt;strong&gt;community&lt;/strong&gt; operator.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Verify the operator is running:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc get pods -n openshift-operators -l app.kubernetes.io/name=grafana-operator&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;You should see output similar to:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;NAME READY STATUS RESTARTS AGE
grafana-operator-7d8f9c6b5-xyz12 1/1 Running 0 2m&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_create_grafana_instance_for_team_a"&gt;Create Grafana Instance for Team-A&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now letâ€™s deploy a Grafana instance in the &lt;strong&gt;team-a namespace&lt;/strong&gt;. This gives the team full control over their dashboards while isolating them from other teams.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
We are logging in as the &lt;strong&gt;project admin&lt;/strong&gt; user of the &lt;strong&gt;team-a namespace&lt;/strong&gt;. So everything we do in this namespace will be done as this user.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_create_the_grafana_admin_secret"&gt;Create the Grafana Admin Secret&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;First, we need to create a secret containing the Grafana admin credentials, since we do not want to store passwords in plain text in our manifests!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Never store passwords in plain text in your manifests and use a secrets management solution like Sealed Secrets or External Secrets Operator.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Create the following example. Replace the password with your own strong password.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: v1
kind: Secret
metadata:
name: grafana-admin-credentials
namespace: team-a
type: Opaque
stringData:
GF_SECURITY_ADMIN_USER: admin &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
GF_SECURITY_ADMIN_PASSWORD: &amp;lt;your-secure-password&amp;gt; &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The admin username for Grafana&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Replace with a strong password&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_deploy_the_grafana_instance"&gt;Deploy the Grafana Instance&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Grafana Operator provides a Custom Resource Definition (CRD) for deploying Grafana instances. We can use this to deploy a Grafana instance in the &lt;strong&gt;team-a namespace&lt;/strong&gt;.
This instance is labelled with &amp;#34;dashboards: &amp;#34;grafana-team-a&amp;#34;&amp;#34; to make it easier to target it with GrafanaDashboard resources.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: grafana.integreatly.org/v1beta1
kind: Grafana
metadata:
name: grafana
namespace: team-a
labels:
dashboards: &amp;#34;grafana-team-a&amp;#34; &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
spec:
config:
log:
mode: &amp;#34;console&amp;#34;
auth:
disable_login_form: &amp;#34;false&amp;#34;
security:
admin_user: ${GF_SECURITY_ADMIN_USER} &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
admin_password: ${GF_SECURITY_ADMIN_PASSWORD}
deployment:
spec:
replicas: 1
template:
spec:
containers:
- name: grafana
envFrom:
- secretRef:
name: grafana-admin-credentials &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
resources:
requests:
cpu: 100m
memory: 256Mi
limits:
cpu: 500m
memory: 512Mi
route:
spec:
tls:
termination: edge &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
disableDefaultAdminSecret: true &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Label used by GrafanaDashboard resources to target this instance. Required so that the datasource can find the instance.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;References environment variables from the secret&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Mounts the secret as environment variables&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Creates an OpenShift Route with TLS edge termination&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Disables the default admin secret created by the Grafana Operator. We will use our own secret and this setting prevents that the operator overwrites it.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;After a few moments, the Grafana pod should be ready.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc get pods -n team-a -l app=grafana -w&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Get the Route URL:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc get route grafana-route -n team-a -o jsonpath=&amp;#39;{.spec.host}&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Use this route and the credentials from the secret to log into Grafana.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/grafana-login.png" alt="Grafana Login"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_configure_tempo_datasource"&gt;Configure Tempo Datasource&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The datasource connects Grafana to TempoStack (or any other datasource like Loki, Prometheus, etc.), allowing you to query and visualize traces. We need to authenticate with a client certificate to TempoStack and be sure the send the tenant ID of tenantA in the header of the queries.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
When you read the previous part of this series, you saw that we created a ClusterRole to grant read access to the traces. The Binding for this role allows &lt;strong&gt;read&lt;/strong&gt; access to everybody who is &lt;strong&gt;authenticated against the system&lt;/strong&gt;. Therefore, we do not need to take care of permissionsâ€¦â€‹ at this point.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_tempostack_client_certificate_authentication"&gt;TempoStack Client Certificate Authentication&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To query the TempoStack instance, we need to authenticate with it. This is done by using a client certificate. Therefore, we need to create a secret with the client certificate and key. In addition, this secret will also contain the tenant ID, which must be sent to the TempoStack instance as a header.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;First, get the client certificate and key from the TempoStack instance:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc get secret -n tempostack | grep gateway-mtls&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;You should find something like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;tempo-simplest-gateway-mtls kubernetes.io/tls 2 19d&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Fetch the client certificate and key and store them in a file locally:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc get secret -n tempostack tempo-simplest-gateway-mtls -o jsonpath=&amp;#39;{.data.tls\.crt}&amp;#39; | base64 -d &amp;gt; tls.crt
oc get secret -n tempostack tempo-simplest-gateway-mtls -o jsonpath=&amp;#39;{.data.tls\.key}&amp;#39; | base64 -d &amp;gt; tls.key&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now, letâ€™s get the tenant ID from the TempoStack instance:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc get tempostack/simplest -n tempostack -o jsonpath=&amp;#39;{.spec.tenants.authentication}&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;You should find something like this (tenantA is the one we are interested in):&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-json" data-lang="json"&gt;[
{
&amp;#34;tenantId&amp;#34;: &amp;#34;1610b0c3-c509-4592-a256-a1871353dbfc&amp;#34;, &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
&amp;#34;tenantName&amp;#34;: &amp;#34;tenantA&amp;#34;
},
{
&amp;#34;tenantId&amp;#34;: &amp;#34;1610b0c3-c509-4592-a256-a1871353dbfd&amp;#34;,
&amp;#34;tenantName&amp;#34;: &amp;#34;tenantB&amp;#34;
}
]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The tenant ID of tenantA&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now, letâ€™s create the secret with the client certificate and key and the tenant ID:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc create secret generic tempo-auth -n team-a --from-file=tls.crt=tls.crt --from-file=tls.key=tls.key --from-literal=tenantA=&amp;#34;1610b0c3-c509-4592-a256-a1871353dbfc&amp;#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Thatâ€™s everything we need to authenticate with TempoStack for tenantA.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
As you prefer, you can create a separate secret for each tenantID. Just be sure to update the &amp;#34;valuesFrom&amp;#34; section of the GrafanaDatasource resource.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_create_the_tempo_datasource"&gt;Create the Tempo Datasource&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now we can create the GrafanaDatasource that connects to TempoStack:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaDatasource
metadata:
name: tempo-tenanta-datasource
namespace: team-a
spec:
allowCrossNamespaceImport: false
datasource:
access: proxy
editable: true
isDefault: true
jsonData:
httpHeaderName1: X-Scope-OrgID &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
timeInterval: 5s
tlsAuth: true
tlsAuthWithCACert: false
tlsSkipVerify: true
name: tempo-tenanta &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
secureJsonData: &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
httpHeaderValue1: &amp;#39;${tenantA}&amp;#39;
tlsClientCert: &amp;#39;${tls.crt}&amp;#39;
tlsClientKey: &amp;#39;${tls.key}&amp;#39;
type: tempo &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
url: &amp;#39;https://tempo-simplest-query-frontend.tempostack.svc.cluster.local:3200&amp;#39; &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
instanceSelector:
matchLabels:
dashboards: grafana-team-a &lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;(6)&lt;/b&gt;
resyncPeriod: 10m0s
valuesFrom: &lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;(7)&lt;/b&gt;
- targetPath: secureJsonData.tlsClientCert
valueFrom:
secretKeyRef:
key: tls.crt
name: tempo-auth
- targetPath: secureJsonData.tlsClientKey
valueFrom:
secretKeyRef:
key: tls.key
name: tempo-auth
- targetPath: secureJsonData.httpHeaderValue1
valueFrom:
secretKeyRef:
key: tenantA
name: tempo-auth&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The header name for the tenant ID.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The name of the Tempo datasource.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The reference to the client certificate, key and tenantID. They are coming from the secret we created earlier.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The type of the datasource. This type must be available in Grafana. For Tempo, the type is &lt;code&gt;tempo&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The URL of the Tempo query frontend. This is the URL of the Tempo query frontend service in the TempoStack namespace.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The label of the Grafana instance to target. This is the label we added to the Grafana instance when we created it.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;7&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The reference to the secrets and the keys inside the secret.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_verify_the_datasource"&gt;Verify the Datasource&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Log into Grafana and navigate to &lt;strong&gt;Connection &amp;gt; Data sources&lt;/strong&gt;. You should see the Tempo datasource listed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/grafana-data-sources-list.png" alt="Grafana Datasource"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Click on the Tempo datasource to see the details:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/grafana-datasource.png" alt="Grafana Datasource Details"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As depicted in the image above, the datasource is configured with TLS Client Authentication and a HTTP Header.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock warning"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-warning" title="Warning"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Any changes in the Grafana UI will not be synced back to the GrafanaDatasource resource. You need to update the resource manually.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To verify the datasource, we can create a test query. Navigate to &lt;strong&gt;Explore&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Enter the query &lt;strong&gt;{}&lt;/strong&gt;, which will simply get all traces from the TempoStack instance from (by default) the last minute.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
If you have multiple data sources configured already, be sure to select the correct one in the dropdown menu.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;You should see the traces in the Grafana Explore UI.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/grafana-explore-metrics.png" alt="Grafana Explore Metrics"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;You can select any trace and see the details in the Grafana Explore UI.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/grafana-explore-trace.png" alt="Grafana Explore Trace"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_create_a_traces_dashboard"&gt;Create a Traces Dashboard&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now letâ€™s create a dashboard to visualize and explore traces. The Grafana Operator allows us to define dashboards as Kubernetes resources.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_create_the_dashboard_configmap"&gt;Create the Dashboard ConfigMap&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To have the first showable data, we need to create a dashboard. The Grafana Operator provides a Custom Resource Definition called GrafanaDashboard for deploying such dashboards.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now, dashboards are their own beast. There are so many things to configure and set that probably a whole separate blog series is needed to cover all of it.
I am by far not an expert in this field and I am happy that I got this one up and running, so I will just create a simple dashboard with a few panels.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Honestly, I am not sure if it is a good way to create dashboards using the CRD. It is probably better to use the Grafana UI and then export the JSON data if you want to keep it declarative.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This dashboard provides:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A trace search panel&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Latency histogram -â†’ I have set this to &amp;#34;higher than 3ms&amp;#34; to at least see something.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recent traces table&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaDashboard
metadata:
name: grafana-team-a-traces-dashboard
namespace: team-a
spec:
instanceSelector:
matchLabels:
dashboards: &amp;#34;grafana-team-a&amp;#34; &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
json: | &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
{
&amp;#34;annotations&amp;#34;: {
&amp;#34;list&amp;#34;: [
{
&amp;#34;builtIn&amp;#34;: 1,
&amp;#34;datasource&amp;#34;: {
&amp;#34;type&amp;#34;: &amp;#34;grafana&amp;#34;,
&amp;#34;uid&amp;#34;: &amp;#34;-- Grafana --&amp;#34;
},
&amp;#34;enable&amp;#34;: true,
&amp;#34;hide&amp;#34;: true,
&amp;#34;iconColor&amp;#34;: &amp;#34;rgba(0, 211, 255, 1)&amp;#34;,
&amp;#34;name&amp;#34;: &amp;#34;Annotations &amp;amp; Alerts&amp;#34;,
&amp;#34;type&amp;#34;: &amp;#34;dashboard&amp;#34;
}
]
},
&amp;#34;editable&amp;#34;: true,
&amp;#34;fiscalYearStartMonth&amp;#34;: 0,
&amp;#34;graphTooltip&amp;#34;: 0,
&amp;#34;id&amp;#34;: 1,
&amp;#34;links&amp;#34;: [],
&amp;#34;panels&amp;#34;: [
{
&amp;#34;fieldConfig&amp;#34;: {
&amp;#34;defaults&amp;#34;: {},
&amp;#34;overrides&amp;#34;: []
},
&amp;#34;gridPos&amp;#34;: {
&amp;#34;h&amp;#34;: 4,
&amp;#34;w&amp;#34;: 24,
&amp;#34;x&amp;#34;: 0,
&amp;#34;y&amp;#34;: 0
},
&amp;#34;id&amp;#34;: 1,
&amp;#34;options&amp;#34;: {
&amp;#34;code&amp;#34;: {
&amp;#34;language&amp;#34;: &amp;#34;plaintext&amp;#34;,
&amp;#34;showLineNumbers&amp;#34;: false,
&amp;#34;showMiniMap&amp;#34;: false
},
&amp;#34;content&amp;#34;: &amp;#34;# Team-A Distributed Traces\n\nThis dashboard allows you to explore distributed traces from your applications.\n\n&amp;#34;,
&amp;#34;mode&amp;#34;: &amp;#34;markdown&amp;#34;
},
&amp;#34;pluginVersion&amp;#34;: &amp;#34;12.1.0&amp;#34;,
&amp;#34;title&amp;#34;: &amp;#34;Welcome&amp;#34;,
&amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;
},
{
&amp;#34;datasource&amp;#34;: {
&amp;#34;type&amp;#34;: &amp;#34;tempo&amp;#34;,
&amp;#34;uid&amp;#34;: &amp;#34;${datasource}&amp;#34;
},
&amp;#34;fieldConfig&amp;#34;: {
&amp;#34;defaults&amp;#34;: {
&amp;#34;color&amp;#34;: {
&amp;#34;mode&amp;#34;: &amp;#34;thresholds&amp;#34;
},
&amp;#34;custom&amp;#34;: {
&amp;#34;align&amp;#34;: &amp;#34;auto&amp;#34;,
&amp;#34;cellOptions&amp;#34;: {
&amp;#34;type&amp;#34;: &amp;#34;auto&amp;#34;
},
&amp;#34;inspect&amp;#34;: false
},
&amp;#34;mappings&amp;#34;: [],
&amp;#34;thresholds&amp;#34;: {
&amp;#34;mode&amp;#34;: &amp;#34;absolute&amp;#34;,
&amp;#34;steps&amp;#34;: [
{
&amp;#34;color&amp;#34;: &amp;#34;green&amp;#34;,
&amp;#34;value&amp;#34;: 0
},
{
&amp;#34;color&amp;#34;: &amp;#34;red&amp;#34;,
&amp;#34;value&amp;#34;: 80
}
]
}
},
&amp;#34;overrides&amp;#34;: []
},
&amp;#34;gridPos&amp;#34;: {
&amp;#34;h&amp;#34;: 8,
&amp;#34;w&amp;#34;: 12,
&amp;#34;x&amp;#34;: 0,
&amp;#34;y&amp;#34;: 4
},
&amp;#34;id&amp;#34;: 2,
&amp;#34;options&amp;#34;: {
&amp;#34;cellHeight&amp;#34;: &amp;#34;sm&amp;#34;,
&amp;#34;footer&amp;#34;: {
&amp;#34;countRows&amp;#34;: false,
&amp;#34;fields&amp;#34;: &amp;#34;&amp;#34;,
&amp;#34;reducer&amp;#34;: [
&amp;#34;sum&amp;#34;
],
&amp;#34;show&amp;#34;: false
},
&amp;#34;showHeader&amp;#34;: true
},
&amp;#34;pluginVersion&amp;#34;: &amp;#34;12.1.0&amp;#34;,
&amp;#34;targets&amp;#34;: [
{
&amp;#34;datasource&amp;#34;: {
&amp;#34;type&amp;#34;: &amp;#34;tempo&amp;#34;,
&amp;#34;uid&amp;#34;: &amp;#34;${datasource}&amp;#34;
},
&amp;#34;filters&amp;#34;: [
{
&amp;#34;id&amp;#34;: &amp;#34;81ea3f00&amp;#34;,
&amp;#34;operator&amp;#34;: &amp;#34;=&amp;#34;,
&amp;#34;scope&amp;#34;: &amp;#34;span&amp;#34;
}
],
&amp;#34;limit&amp;#34;: 20,
&amp;#34;metricsQueryType&amp;#34;: &amp;#34;range&amp;#34;,
&amp;#34;queryType&amp;#34;: &amp;#34;traceqlSearch&amp;#34;,
&amp;#34;refId&amp;#34;: &amp;#34;A&amp;#34;,
&amp;#34;tableType&amp;#34;: &amp;#34;traces&amp;#34;
}
],
&amp;#34;title&amp;#34;: &amp;#34;Trace Search&amp;#34;,
&amp;#34;type&amp;#34;: &amp;#34;table&amp;#34;
},
{
&amp;#34;datasource&amp;#34;: {
&amp;#34;type&amp;#34;: &amp;#34;tempo&amp;#34;,
&amp;#34;uid&amp;#34;: &amp;#34;${datasource}&amp;#34;
},
&amp;#34;fieldConfig&amp;#34;: {
&amp;#34;defaults&amp;#34;: {
&amp;#34;color&amp;#34;: {
&amp;#34;mode&amp;#34;: &amp;#34;palette-classic&amp;#34;
},
&amp;#34;custom&amp;#34;: {
&amp;#34;axisBorderShow&amp;#34;: false,
&amp;#34;axisCenteredZero&amp;#34;: false,
&amp;#34;axisColorMode&amp;#34;: &amp;#34;text&amp;#34;,
&amp;#34;axisLabel&amp;#34;: &amp;#34;&amp;#34;,
&amp;#34;axisPlacement&amp;#34;: &amp;#34;auto&amp;#34;,
&amp;#34;barAlignment&amp;#34;: 0,
&amp;#34;barWidthFactor&amp;#34;: 0.6,
&amp;#34;drawStyle&amp;#34;: &amp;#34;bars&amp;#34;,
&amp;#34;fillOpacity&amp;#34;: 100,
&amp;#34;gradientMode&amp;#34;: &amp;#34;none&amp;#34;,
&amp;#34;hideFrom&amp;#34;: {
&amp;#34;legend&amp;#34;: false,
&amp;#34;tooltip&amp;#34;: false,
&amp;#34;viz&amp;#34;: false
},
&amp;#34;insertNulls&amp;#34;: false,
&amp;#34;lineInterpolation&amp;#34;: &amp;#34;linear&amp;#34;,
&amp;#34;lineWidth&amp;#34;: 1,
&amp;#34;pointSize&amp;#34;: 5,
&amp;#34;scaleDistribution&amp;#34;: {
&amp;#34;type&amp;#34;: &amp;#34;linear&amp;#34;
},
&amp;#34;showPoints&amp;#34;: &amp;#34;auto&amp;#34;,
&amp;#34;spanNulls&amp;#34;: false,
&amp;#34;stacking&amp;#34;: {
&amp;#34;group&amp;#34;: &amp;#34;A&amp;#34;,
&amp;#34;mode&amp;#34;: &amp;#34;none&amp;#34;
},
&amp;#34;thresholdsStyle&amp;#34;: {
&amp;#34;mode&amp;#34;: &amp;#34;off&amp;#34;
}
},
&amp;#34;mappings&amp;#34;: [],
&amp;#34;thresholds&amp;#34;: {
&amp;#34;mode&amp;#34;: &amp;#34;absolute&amp;#34;,
&amp;#34;steps&amp;#34;: [
{
&amp;#34;color&amp;#34;: &amp;#34;green&amp;#34;,
&amp;#34;value&amp;#34;: 0
},
{
&amp;#34;color&amp;#34;: &amp;#34;red&amp;#34;,
&amp;#34;value&amp;#34;: 80
}
]
},
&amp;#34;unit&amp;#34;: &amp;#34;ms&amp;#34;
},
&amp;#34;overrides&amp;#34;: []
},
&amp;#34;gridPos&amp;#34;: {
&amp;#34;h&amp;#34;: 8,
&amp;#34;w&amp;#34;: 12,
&amp;#34;x&amp;#34;: 12,
&amp;#34;y&amp;#34;: 4
},
&amp;#34;id&amp;#34;: 3,
&amp;#34;options&amp;#34;: {
&amp;#34;legend&amp;#34;: {
&amp;#34;calcs&amp;#34;: [],
&amp;#34;displayMode&amp;#34;: &amp;#34;list&amp;#34;,
&amp;#34;placement&amp;#34;: &amp;#34;bottom&amp;#34;,
&amp;#34;showLegend&amp;#34;: true
},
&amp;#34;tooltip&amp;#34;: {
&amp;#34;hideZeros&amp;#34;: false,
&amp;#34;mode&amp;#34;: &amp;#34;single&amp;#34;,
&amp;#34;sort&amp;#34;: &amp;#34;none&amp;#34;
}
},
&amp;#34;pluginVersion&amp;#34;: &amp;#34;12.1.0&amp;#34;,
&amp;#34;targets&amp;#34;: [
{
&amp;#34;datasource&amp;#34;: {
&amp;#34;type&amp;#34;: &amp;#34;tempo&amp;#34;,
&amp;#34;uid&amp;#34;: &amp;#34;${datasource}&amp;#34;
},
&amp;#34;filters&amp;#34;: [
{
&amp;#34;id&amp;#34;: &amp;#34;6ff20d0d&amp;#34;,
&amp;#34;operator&amp;#34;: &amp;#34;=&amp;#34;,
&amp;#34;scope&amp;#34;: &amp;#34;span&amp;#34;
},
{
&amp;#34;id&amp;#34;: &amp;#34;min-duration&amp;#34;,
&amp;#34;operator&amp;#34;: &amp;#34;&amp;gt;&amp;#34;,
&amp;#34;tag&amp;#34;: &amp;#34;duration&amp;#34;,
&amp;#34;value&amp;#34;: &amp;#34;0.3ms&amp;#34;,
&amp;#34;valueType&amp;#34;: &amp;#34;duration&amp;#34;
}
],
&amp;#34;limit&amp;#34;: 20,
&amp;#34;metricsQueryType&amp;#34;: &amp;#34;range&amp;#34;,
&amp;#34;queryType&amp;#34;: &amp;#34;traceqlSearch&amp;#34;,
&amp;#34;refId&amp;#34;: &amp;#34;A&amp;#34;,
&amp;#34;tableType&amp;#34;: &amp;#34;spans&amp;#34;
}
],
&amp;#34;title&amp;#34;: &amp;#34;Span Duration Distribution&amp;#34;,
&amp;#34;type&amp;#34;: &amp;#34;timeseries&amp;#34;
},
{
&amp;#34;datasource&amp;#34;: {
&amp;#34;type&amp;#34;: &amp;#34;tempo&amp;#34;,
&amp;#34;uid&amp;#34;: &amp;#34;${datasource}&amp;#34;
},
&amp;#34;fieldConfig&amp;#34;: {
&amp;#34;defaults&amp;#34;: {
&amp;#34;color&amp;#34;: {
&amp;#34;mode&amp;#34;: &amp;#34;thresholds&amp;#34;
},
&amp;#34;custom&amp;#34;: {
&amp;#34;align&amp;#34;: &amp;#34;auto&amp;#34;,
&amp;#34;cellOptions&amp;#34;: {
&amp;#34;type&amp;#34;: &amp;#34;auto&amp;#34;
},
&amp;#34;inspect&amp;#34;: false
},
&amp;#34;mappings&amp;#34;: [],
&amp;#34;thresholds&amp;#34;: {
&amp;#34;mode&amp;#34;: &amp;#34;absolute&amp;#34;,
&amp;#34;steps&amp;#34;: [
{
&amp;#34;color&amp;#34;: &amp;#34;green&amp;#34;,
&amp;#34;value&amp;#34;: 0
}
]
}
},
&amp;#34;overrides&amp;#34;: [
{
&amp;#34;matcher&amp;#34;: {
&amp;#34;id&amp;#34;: &amp;#34;byName&amp;#34;,
&amp;#34;options&amp;#34;: &amp;#34;traceID&amp;#34;
},
&amp;#34;properties&amp;#34;: [
{
&amp;#34;id&amp;#34;: &amp;#34;links&amp;#34;,
&amp;#34;value&amp;#34;: [
{
&amp;#34;title&amp;#34;: &amp;#34;View Trace&amp;#34;,
&amp;#34;url&amp;#34;: &amp;#34;/explore?orgId=1&amp;amp;left=%7B%22datasource%22:%22${datasource}%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22datasource%22:%7B%22type%22:%22tempo%22,%22uid%22:%22${datasource}%22%7D,%22queryType%22:%22traceql%22,%22limit%22:20,%22query%22:%22${__value.raw}%22%7D%5D,%22range%22:%7B%22from%22:%22now-1h%22,%22to%22:%22now%22%7D%7D&amp;#34;
}
]
}
]
},
{
&amp;#34;matcher&amp;#34;: {
&amp;#34;id&amp;#34;: &amp;#34;byName&amp;#34;,
&amp;#34;options&amp;#34;: &amp;#34;duration&amp;#34;
},
&amp;#34;properties&amp;#34;: [
{
&amp;#34;id&amp;#34;: &amp;#34;unit&amp;#34;,
&amp;#34;value&amp;#34;: &amp;#34;ns&amp;#34;
}
]
}
]
},
&amp;#34;gridPos&amp;#34;: {
&amp;#34;h&amp;#34;: 10,
&amp;#34;w&amp;#34;: 24,
&amp;#34;x&amp;#34;: 0,
&amp;#34;y&amp;#34;: 12
},
&amp;#34;id&amp;#34;: 4,
&amp;#34;options&amp;#34;: {
&amp;#34;cellHeight&amp;#34;: &amp;#34;sm&amp;#34;,
&amp;#34;footer&amp;#34;: {
&amp;#34;countRows&amp;#34;: false,
&amp;#34;fields&amp;#34;: &amp;#34;&amp;#34;,
&amp;#34;reducer&amp;#34;: [
&amp;#34;sum&amp;#34;
],
&amp;#34;show&amp;#34;: false
},
&amp;#34;showHeader&amp;#34;: true,
&amp;#34;sortBy&amp;#34;: [
{
&amp;#34;desc&amp;#34;: true,
&amp;#34;displayName&amp;#34;: &amp;#34;startTime&amp;#34;
}
]
},
&amp;#34;pluginVersion&amp;#34;: &amp;#34;12.1.0&amp;#34;,
&amp;#34;targets&amp;#34;: [
{
&amp;#34;datasource&amp;#34;: {
&amp;#34;type&amp;#34;: &amp;#34;tempo&amp;#34;,
&amp;#34;uid&amp;#34;: &amp;#34;${datasource}&amp;#34;
},
&amp;#34;limit&amp;#34;: 50,
&amp;#34;queryType&amp;#34;: &amp;#34;traceqlSearch&amp;#34;,
&amp;#34;refId&amp;#34;: &amp;#34;A&amp;#34;,
&amp;#34;tableType&amp;#34;: &amp;#34;traces&amp;#34;
}
],
&amp;#34;title&amp;#34;: &amp;#34;Recent Traces&amp;#34;,
&amp;#34;type&amp;#34;: &amp;#34;table&amp;#34;
}
],
&amp;#34;preload&amp;#34;: false,
&amp;#34;refresh&amp;#34;: &amp;#34;30s&amp;#34;,
&amp;#34;schemaVersion&amp;#34;: 41,
&amp;#34;tags&amp;#34;: [
&amp;#34;tracing&amp;#34;,
&amp;#34;tempo&amp;#34;,
&amp;#34;team-a&amp;#34;
],
&amp;#34;templating&amp;#34;: {
&amp;#34;list&amp;#34;: [
{
&amp;#34;current&amp;#34;: {
&amp;#34;text&amp;#34;: &amp;#34;tempo-tenanta&amp;#34;,
&amp;#34;value&amp;#34;: &amp;#34;90b71ee3-693a-4c41-8cdf-624a3bb78e7a&amp;#34;
},
&amp;#34;includeAll&amp;#34;: false,
&amp;#34;label&amp;#34;: &amp;#34;Datasource&amp;#34;,
&amp;#34;name&amp;#34;: &amp;#34;datasource&amp;#34;,
&amp;#34;options&amp;#34;: [],
&amp;#34;query&amp;#34;: &amp;#34;tempo&amp;#34;,
&amp;#34;refresh&amp;#34;: 1,
&amp;#34;regex&amp;#34;: &amp;#34;&amp;#34;,
&amp;#34;type&amp;#34;: &amp;#34;datasource&amp;#34;
}
]
},
&amp;#34;time&amp;#34;: {
&amp;#34;from&amp;#34;: &amp;#34;now-1h&amp;#34;,
&amp;#34;to&amp;#34;: &amp;#34;now&amp;#34;
},
&amp;#34;timepicker&amp;#34;: {},
&amp;#34;timezone&amp;#34;: &amp;#34;&amp;#34;,
&amp;#34;title&amp;#34;: &amp;#34;Team-A Distributed Traces&amp;#34;,
&amp;#34;uid&amp;#34;: &amp;#34;team-a-traces&amp;#34;,
&amp;#34;version&amp;#34;: 3
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The label of the Grafana instance to target. This is the label we added to the Grafana instance when we created it.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The JSON data of the dashboard (yes, itâ€™s a monster!).&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now you will see a new dashboard in the Grafana UI. Navigate to &lt;strong&gt;Dashboards&lt;/strong&gt; and find &lt;strong&gt;Team-A Distributed Traces&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/grafana-dashboard.webp" alt="Grafana Dashboard"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_next_steps"&gt;Next Steps&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Congratulations! &lt;strong&gt;The Cat has summoned Grafana&lt;/strong&gt; .&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/cat-wizard-done-its-job.png?width=400px" alt="Cat Wizard"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;You now have a functional Grafana instance with Tempo integration for team-a. Here are some ideas for extending this setup:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Add Loki datasource&lt;/strong&gt;: Correlate traces with logs using derived fields&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Add Prometheus datasource&lt;/strong&gt;: Link metrics to traces for full observability&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Create alerting rules&lt;/strong&gt;: Set up alerts for high latency or error rates&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Build more dashboards&lt;/strong&gt;: Create service-specific dashboards for different applications&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As mentioned above, Grafana and its dashboards are powerful tools for visualizing and exploring traces. There are tons of things to configure and set up.
Maybe I will cover some of these things in a future article.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Happy tracing! ðŸš€&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>The Hitchhiker's Guide to Observability Introduction - Part 1</title><link>https://blog.stderr.at/openshift-platform/observability/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/</link><pubDate>Sun, 23 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/observability/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;With this article I would like to summarize and, especially, remember my setup. This is Part 1 of a series of articles that I split up so it is easier to read and understand and not too long. Initially, there will be 6 parts, but I will add more as needed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_introduction"&gt;Introduction&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In modern microservices architectures, understanding how requests flow through your distributed system is crucial for debugging, performance optimization, and maintaining system health. &lt;strong&gt;Distributed tracing&lt;/strong&gt; provides visibility into these complex interactions by tracking requests as they traverse multiple services.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This whole guide demonstrates how to set up a distributed tracing infrastructure using&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OpenShift&lt;/strong&gt; (4.16+) as base platform&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Red Hat Build of OpenTelemetry&lt;/strong&gt; - The observability framework based on &lt;a href="https://opentelemetry.io/" target="_blank" rel="noopener"&gt;OpenTelemetry&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TempoStack&lt;/strong&gt; - &lt;a href="https://grafana.com/docs/tempo/latest/" target="_blank" rel="noopener"&gt;Grafanaâ€™s distributed&lt;/a&gt; tracing backend for Kubernetes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-tenant architecture&lt;/strong&gt; - Isolating traces by team or environment&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cluster Observability Operator&lt;/strong&gt; - For now this Operator is only used to extend the OpenShift UI with the tracing UI.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_thanks_to"&gt;Thanks to&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This article would not have been possible without the help of &lt;a href="https://www.linkedin.com/in/michaela-lang-900603b9/" target="_blank" rel="noopener"&gt;Michaela Lang&lt;/a&gt;. Check out her articles on LinkedIn mainly discussing Tracing and Service Mesh.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_what_is_opentelemetry"&gt;What is OpenTelemetry?&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://opentelemetry.io/" target="_blank" rel="noopener"&gt;OpenTelemetry&lt;/a&gt; is an observability framework and toolkit which aims to provide unified, standardized, and vendor-neutral telemetry data collection for traces, metrics and logs for cloud-native software.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
In this article we will focus on traces only.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;When it comes to Red Hat and OpenShift, the supported installation is based on the Operator &lt;strong&gt;Red Hat Build of OpenTelemetry&lt;/strong&gt;, which is based on the open source OpenTelemetry project and adds supportability.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_core_features"&gt;Core Features:&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;em&gt;Source: &lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/red_hat_build_of_opentelemetry/index#otel-product-overview_otel-architecture" target="_blank" rel="noopener"&gt;OpenShift OTEL Product Overview&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The &lt;strong&gt;OpenTelemetry Collector&lt;/strong&gt; can receive, process, and forward telemetry data in multiple formats, making it the ideal component for telemetry processing and interoperability between telemetry systems. The Collector provides a unified solution for collecting and processing metrics, traces, and logs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The core features of the OpenTelemetry Collector include:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Collection and Processing Hub&lt;/strong&gt;
It acts as a central component that gathers telemetry data like metrics and traces from various sources. This data can be created from instrumented applications and infrastructure.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Customizable telemetry data pipeline&lt;/strong&gt;
The OpenTelemetry Collector is customizable and supports various processors, exporters, and receivers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Auto-instrumentation features&lt;/strong&gt;
Automatic instrumentation simplifies the process of adding observability to applications. If used, developers do not need to manually instrument their code for basic telemetry data. (This depends a bit on the used coding language and framework, maybe this is worth a separate article)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Here are some of the use cases for the OpenTelemetry Collector:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Centralized data collection&lt;/strong&gt;
In a microservices architecture, the Collector can be deployed to aggregate data from multiple services.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data enrichment and processing&lt;/strong&gt;
Before forwarding data to analysis tools, the Collector can enrich, filter, and process this data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-backend receiving and exporting&lt;/strong&gt;
The Collector can receive and send data to multiple monitoring and analysis platforms simultaneously.
You can use Red Hat build of OpenTelemetry in combination with Red Hat OpenShift Distributed Tracing Platform.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_what_is_grafana_tempo"&gt;What is Grafana Tempo?&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://grafana.com/oss/tempo/" target="_blank" rel="noopener"&gt;Grafana Tempo&lt;/a&gt; is an open-source, easy-to-use, and high-scale distributed tracing backend. Tempo lets you search for traces, generate metrics from spans, and link your tracing data with logs and metrics.
It is deeply integrated with Grafana, Prometheus and Loki and can ingest traces from various sources, such as OpenTelemetry, Jaeger, Zipkin and more.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_core_features_2"&gt;Core Features:&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;em&gt;Source: &lt;a href="https://grafana.com/oss/tempo/" target="_blank" rel="noopener"&gt;Grafana Tempo&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Built for massive scale&lt;/strong&gt;
The only dependency is object storage which provides affordable long-term storage of traces.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cost-effective&lt;/strong&gt;
Not indexing the traces makes it possible to store orders of magnitude more trace data for the same cost.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Strong integration with open source tools&lt;/strong&gt;
Compatible with open source tracing protocols.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In addition, it is deeply integrated with &lt;strong&gt;Grafana&lt;/strong&gt;, allowing you to visualize the traces in a Grafana dashboard and link logs, metrics and traces together.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_use_case_for_this_article"&gt;Use Case for this Article&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The use case that was tested in this article was the following:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Several applications (team-a, team-b, â€¦â€‹) are hosted on separate OpenShift namespaces.
On each application namespace, a &lt;strong&gt;local&lt;/strong&gt; OpenTelemetry Collector (OTC) is configured to collect the traces from the application. These local OpenTelemetry Collectors will export the traces to a &lt;strong&gt;central&lt;/strong&gt; OpenTelemetry Collector (hosted in the namespace &lt;strong&gt;tempostack&lt;/strong&gt;).
The central Collector will then export the data to a TempoStack instance (also hosted in the namespace &lt;strong&gt;tempostack&lt;/strong&gt;), which will store the traces in object storage. The storage itself is provided by S3-compatible storage, in this example OpenShift Data Foundation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;For a more detailed view see the next section.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_architecture_overview"&gt;Architecture Overview&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As described the implementation follows a &lt;strong&gt;two-tier collector architecture&lt;/strong&gt; with multi-tenancy support:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-mermaid {align=" center"="" zoom="true" }"="" data-lang="mermaid {align=" center"="" zoom="true" }"=""&gt;---
title: &amp;#34;Architecture Overview&amp;#34;
config:
theme: &amp;#39;dark&amp;#39;
---
graph TB
subgraph app[&amp;#34;Application&amp;#34;]
mockbin1[&amp;#34;Mockbin #1&amp;lt;br/&amp;gt;(team-a namespace)&amp;lt;br/&amp;gt;(tenantA)&amp;#34;]
mockbin2[&amp;#34;Mockbin #2&amp;lt;br/&amp;gt;(team-b namespace)&amp;lt;br/&amp;gt;(tenantB)&amp;#34;]
end
subgraph local[&amp;#34;Local OTC&amp;#34;]
otc_a[&amp;#34;OTC-team-a&amp;lt;br/&amp;gt;â€¢ Add namespace&amp;lt;br/&amp;gt;â€¢ Batch processing&amp;lt;br/&amp;gt;â€¢ Forward to central&amp;#34;]
otc_b[&amp;#34;OTC-team-b&amp;lt;br/&amp;gt;â€¢ Add namespace&amp;lt;br/&amp;gt;â€¢ Batch processing&amp;lt;br/&amp;gt;â€¢ Forward to central&amp;#34;]
end
subgraph central[&amp;#34;Central OTC (tempostack namespace)&amp;#34;]
otc_central[&amp;#34;OTC-central&amp;lt;br/&amp;gt;â€¢ Receive from local collectors&amp;lt;br/&amp;gt;â€¢ Add K8s metadata (k8sattributes)&amp;lt;br/&amp;gt;â€¢ Route by namespace (routing connector)&amp;lt;br/&amp;gt;â€¢ Authenticate with bearer token&amp;lt;br/&amp;gt;â€¢ Forward to TempoStack with tenant ID&amp;#34;]
end
subgraph tempo[&amp;#34;TempoStack (tempostack namespace)&amp;#34;]
tempostack[&amp;#34;Multi-tenant Trace Storage&amp;lt;br/&amp;gt;â€¢ tenantA, tenantB, ...&amp;lt;br/&amp;gt;â€¢ S3 backend storage&amp;lt;br/&amp;gt;â€¢ 48-hour retention&amp;#34;]
end
mockbin1 --&amp;gt;|&amp;#34;OTLP&amp;#34;| otc_a
mockbin2 --&amp;gt;|&amp;#34;OTLP&amp;#34;| otc_b
otc_a --&amp;gt;|&amp;#34;OTLP(with namespace)&amp;#34;| otc_central
otc_b --&amp;gt;|&amp;#34;OTLP(with namespace)&amp;#34;| otc_central
otc_central --&amp;gt;|&amp;#34;OTLP&amp;lt;br/&amp;gt;(X-Scope-OrgID header)&amp;#34;| tempostack
classDef appStyle fill:#2f652a,stroke:#2f652a,stroke-width:2px
classDef localStyle fill:#425cc6,stroke:#425cc6,stroke-width:2px;
classDef centralStyle fill:#425cc6,stroke:#425cc6,stroke-width:2px;
classDef tempoStyle fill:#906403,stroke:#906403,stroke-width:2px
class mockbin1,mockbin2 appStyle
class otc_a,otc_b localStyle
class otc_central centralStyle
class tempostack tempoStyle&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;As quick summary&lt;/strong&gt;
Traces from the application &lt;strong&gt;Mockbin #1&lt;/strong&gt; are collected by the &amp;#34;OTC-team-a&amp;#34; and forwarded to the &amp;#34;Central OTC&amp;#34;. From there the traces are further forwarded to Tempo.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_why_2_tier_architecture"&gt;Why 2-Tier Architecture?&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;You may ask yourself why there are two OpenTelemetry Collectors and if the application could not send directly to the Central OTC or if the Local OTC could not write directly into the Tempo storage. Both options are fine and will work, however, I tried to make it more secure. Only one OTC is allowed to perform write actions and applications can only send to the Local OTC, which forwards to the Central OTC where the traces are routed based on the source namespace. This way, nobody can interfere with other namespaces.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Therefore:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Separation of Concerns&lt;/strong&gt;: Application namespaces handle local processing; central namespace handles routing and storage. The Central decides where and how to store. Application owners cannot overwrite this.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Resource Efficiency&lt;/strong&gt;: Lightweight collectors in app namespaces, heavy processing centralized&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Security&lt;/strong&gt;: Applications donâ€™t need direct access to TempoStack&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Each tier can scale independently&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-tenancy&lt;/strong&gt;: Central collector routes traces to appropriate tenants&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_what_now"&gt;What now?&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The next articles will cover the actual implementation. We will first deploy Tempo and the Central Collector. Then we will deploy example applications and the Local Collector.
If everything works as planned, we will be able to see traces on the OpenShift UI.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>The Hitchhiker's Guide to Observability - Grafana Tempo - Part 2</title><link>https://blog.stderr.at/openshift-platform/observability/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/</link><pubDate>Mon, 24 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/observability/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;After covering the fundamentals and architecture in Part 1, itâ€™s time to get our hands dirty! This article walks through the complete implementation of a distributed tracing infrastructure on OpenShift.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Weâ€™ll deploy and configure the &lt;strong&gt;Tempo Operator&lt;/strong&gt; and a multi-tenant &lt;strong&gt;TempoStack&lt;/strong&gt; instance. For S3 storage we will use the integrated OpenShift Data Foundation. However, you can use whatever S3-compatible storage you have available.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_grafana_tempo_step_by_step_implementation"&gt;Grafana Tempo - Step-by-Step Implementation&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_prerequisites"&gt;Prerequisites&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Before starting, ensure you have the following Systems or Operators installed (used Operator versions in this article):&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;OpenShift or Kubernetes cluster (OpenShift v4.20)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Red Hat build of OpenTelemetry installed (v0.135.0-1)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tempo Operator installed (v0.18.0-2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;S3-compatible storage (for TempoStack, based on OpenShift Data Foundation)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cluster Observability Operator (v1.3.0) - for now this Operator is only used to extend the OpenShift UI with the tracing UI.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
For all configurations I also created a proper GitOps implementation (of course :)). However, first I would like to show the actual configuration. The GitOps implementation can be found at the section &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/#_gitops_deployment"&gt;GitOps Deployment&lt;/a&gt;.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_1_verifydeploy_tempo_operator"&gt;Step 1: Verify/Deploy Tempo Operator&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s first verify if the Tempo Operator is installed and ready to use. If everything is fine, then the Operator will be deployed in the namespace &lt;strong&gt;openshift-tempo-operator&lt;/strong&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/tempo-operator-installation.png?width=640px" alt="Tempo Operator"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_2_deploy_tempostack_resource"&gt;Step 2: Deploy TempoStack Resource&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;TempoStack&lt;/strong&gt; is the central trace storage backend. We are using the Tempo Operator here, which provides the TempoStack resource and multi-tenancy capability. During my tests I deployed the TempoStack resource and also created a &lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/tempo-tracing" target="_blank" rel="noopener"&gt;Helm Chart&lt;/a&gt; that is able to render this resource.
However, the Operator itself also provides a TempoMonolith resource. This will put everything into one Pod, while the TempoStack rolls out the stack in separate containers (like ingester, gateway, etc.).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
My Helm Chart currently does not support the TempoMonolith resource yet. If you require this, please ping me and I will try to add it.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Be sure that the S3 bucket is available and a Secret (here called tempo-s3) with the following keys exists (valid for OpenShift Data Foundation): &lt;strong&gt;access_key_id, access_key_secret, bucket, endpoint&lt;/strong&gt;. The layout of the Secret will look slightly different depending on the S3 storage backend you are using.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Create the TempoStack instance:&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following TempoStack resource has been used&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: tempo.grafana.com/v1alpha1
kind: TempoStack
metadata:
name: simplest &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
namespace: tempostack &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
spec:
managementState: Managed
replicationFactor: 1 &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
# Resource limits
resources: &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
total:
limits:
cpu: &amp;#34;2&amp;#34;
memory: 2Gi
# Trace retention
retention: &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
global:
traces: 48h0m0s
# S3 storage configuration
storage:
secret:
credentialMode: static &lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;(6)&lt;/b&gt;
name: tempo-s3 &lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;(7)&lt;/b&gt;
type: s3
tls:
enabled: false
storageSize: 500Gi
# Multi-tenancy configuration
tenants:
mode: openshift
authentication: &lt;i class="conum" data-value="8"&gt;&lt;/i&gt;&lt;b&gt;(8)&lt;/b&gt;
- tenantId: 1610b0c3-c509-4592-a256-a1871353dbfa
tenantName: tenantA
- tenantId: 1610b0c3-c509-4592-a256-a1871353dbfb
tenantName: tenantB
- tenantId: 1610b0c3-c509-4592-a256-a1871353dbfc
tenantName: tenantC
# Gateway and UI
template: &lt;i class="conum" data-value="9"&gt;&lt;/i&gt;&lt;b&gt;(9)&lt;/b&gt;
gateway:
enabled: true
component:
replicas: 1
ingress:
type: route
route:
termination: reencrypt
queryFrontend:
component:
replicas: 1
jaegerQuery:
enabled: true
servicesQueryDuration: 72h0m0s&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Name of the TempoStack instance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Namespace of the TempoStack instance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Integer value for the number of ingesters that must acknowledge the data from the distributors before accepting a span.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Defines resources for the TempoStack instance. Default is (limit only) 2 CPU and 2Gi memory.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Configuration options for retention of traces. The default value is 48h.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Credential mode for the S3 storage. Depends how the storage will be integrated. Default is static.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;7&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Secret name for the S3 storage.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="8"&gt;&lt;/i&gt;&lt;b&gt;8&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Configuration options for the tenants. In this example: tenantA, tenantB, tenantC. Consists of tenantName and tenantId, both can be defined by the user.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="9"&gt;&lt;/i&gt;&lt;b&gt;9&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Configuration options for the different Tempo components. In this example: gateway, query-frontend. Other components could be: distributor, ingester, compactor or querier.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Key Configuration Points:&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Multi-tenancy&lt;/strong&gt;: Supports 3 tenants currently (tenantA, tenantB, tenantC). This part must be modified when a new tenant enters the realm.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Retention&lt;/strong&gt;: Traces stored for 48 hours.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Uses S3-compatible backend (requires separate secret).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gateway&lt;/strong&gt;: Exposes OTLP endpoint with TLS.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_3_configure_rbac_for_tempostack_trace_access_readwrite"&gt;Step 3: Configure RBAC for TempoStack Trace Access read/write&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Set up ClusterRoles to control who can read and write traces.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The ClusterRoles must be updated whenever a new tenant is configured in TempoStack. The name of the tenant must be added in the &lt;strong&gt;resources&lt;/strong&gt; array.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Traces Reader Role:&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
name: tempostack-traces-reader &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
rules:
- verbs: &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
- get
apiGroups:
- tempo.grafana.com
resources:
- tenantA &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
- tenantB
- tenantC
resourceNames:
- traces&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Name of the ClusterRole&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Verbs for the ClusterRole.
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;apiGroups: tempo.grafana.com&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;resources:&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;List of Tenants&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;resourceNames:&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;traces&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;verbs:&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;get&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;List of tenants that are allowed to read the traces.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Bind Reader Role to Authenticated Users:&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
name: tempostack-traces-reader
subjects:
- kind: Group
apiGroup: rbac.authorization.k8s.io
name: &amp;#39;system:authenticated&amp;#39; &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: tempostack-traces-reader&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The Group that is allowed to read the traces. In this example: &lt;strong&gt;system:authenticated&lt;/strong&gt;. This means ALL authenticated users will be able to read all the traces (see warning below).&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock warning"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-warning" title="Warning"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
With this ClusterRoleBinding, anybody who is authenticated (system:authenticated) will be able to see the traces for the defined tenants (A, B, C). This is for an easy showcase in this article. For production environments, you should implement more granular RBAC controls per tenant.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Traces Writer Role:&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This ClusterRole is used to write traces into TempoStack. Typically you will use this ClusterRole for the OpenTelemetry Collector, so it can write into TempoStack.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
name: tempostack-traces-write &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
rules:
- verbs:
- create &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
apiGroups:
- tempo.grafana.com
resources: &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
- tenantB
- tenantA
- tenantC
resourceNames:
- traces&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Name of the ClusterRoleBinding&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;This time the verb is &lt;strong&gt;create&lt;/strong&gt;. This means the user will be able to write new traces into TempoStack.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;List of tenants.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Bind Writer Role to Central Collector:&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
name: tempostack-traces
subjects:
- kind: ServiceAccount
name: otel-collector &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
namespace: tempostack &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: tempostack-traces-write&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The ServiceAccount that is allowed to write the traces. In this example: &lt;strong&gt;otel-collector&lt;/strong&gt;. We will create this Service Account in the next article.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The namespace of the ServiceAccount. In this example: &lt;strong&gt;tempostack&lt;/strong&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;hr/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_gitops_deployment"&gt;GitOps Deployment&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;While the above is good for quick tests, it always makes sense to have a proper GitOps deployment. I have created a Chart and GitOps configuration that will:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Deploy the Tempo Operator&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure the TempoStack instance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure the RBAC configurations for the TempoStack instance&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following sources will be used:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://charts.stderr.at/" target="_blank" rel="noopener"&gt;Helm Repository&lt;/a&gt; - to fetch the Helm Chart for the Tempo Operator including required Sub-Charts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tjungbauer/openshift-clusterconfig-gitops/tree/main/clusters/management-cluster/setup-tempo-operator" target="_blank" rel="noopener"&gt;Setup Tempo Operator&lt;/a&gt; - To deploy and configure the Tempo Operator, configure object storage, magically create a Secret with the required keys, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Feel free to clone or use whatever you need.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following Sub-Charts are used:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/helper-objectstore" target="_blank" rel="noopener"&gt;helper-objectstore&lt;/a&gt; (version ~1.0.0) - Creates S3 Bucket&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/helper-odf-bucket-secret" target="_blank" rel="noopener"&gt;helper-odf-bucket-secret&lt;/a&gt; (version ~1.0.0) - Creates the Secret usable by the TempoStack instance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/helper-operator" target="_blank" rel="noopener"&gt;helper-operator&lt;/a&gt; (version ~1.0.18) - Installs the Tempo Operator&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/helper-status-checker" target="_blank" rel="noopener"&gt;helper-status-checker&lt;/a&gt; (version ~4.0.0) - Verifies the status of the Tempo Operator&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/tempo-tracing" target="_blank" rel="noopener"&gt;tempo-tracing&lt;/a&gt; (version ~1.0.0) - Installs the TempoStack instance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/tpl" target="_blank" rel="noopener"&gt;tpl&lt;/a&gt; (version ~1.0.0) - Template Library&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The this Argo CD Application we can deploy the Tempo Operator:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
name: setup-tempo-operator
namespace: openshift-gitops
spec:
destination:
name: in-cluster &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
namespace: openshift-tempo-operator &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
info:
- name: Description
value: ApplicationSet that Deploys on Management Cluster Configuration (using Git Generator)
project: in-cluster &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
source:
path: clusters/management-cluster/setup-tempo-operator &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
repoURL: &amp;#39;https://github.com/tjungbauer/openshift-clusterconfig-gitops&amp;#39; &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
targetRevision: main
syncPolicy:
retry:
backoff:
duration: 5s
factor: 2
maxDuration: 3m
limit: 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Target cluster, here the local cluster&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Namespace of the target cluster, here the Operator will be installed.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Project of the target cluster&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Path to the Git repository&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;URL to the Git repository&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock warning"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-warning" title="Warning"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Since many things are moving in the background, it will take a while until the Argo CD Application is synced (i.e. Creation of S3 Bucket)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This will create the Argo CD Application that can be synchronized with the cluster:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/tempo-argocd.png?width=640px" alt="Tempo Deployment via Argo CD"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As seen above, many resources are created using a single Helm Chart (ok, with some Sub-Charts). The actual configuration is done in the values.yaml file. The full values file is quite long, so I will break it down to the different Sub-Charts and add the complete file &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/#_complete_values_file"&gt;at the end&lt;/a&gt; that latest file I am using for testing can be found at &lt;a href="https://github.com/tjungbauer/openshift-clusterconfig-gitops/blob/main/clusters/management-cluster/setup-tempo-operator/values.yaml" target="_blank" rel="noopener"&gt;values.yaml&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_values_file_snippets_for_gitops"&gt;Values File Snippets for GitOps&lt;/h3&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_tempstack_settings"&gt;TempStack Settings&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following will configure the TempoStack resource. Verify the upstream Helm Chart &lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/tempo-tracing" target="_blank" rel="noopener"&gt;tempo-tracing&lt;/a&gt; for detailed and additional information about the settings.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;tempo-tracing:
tempostack: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
enabled: true
name: simplest
managementState: Managed
namespace: &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
create: true
name: tempostack
descr: &amp;#34;Namespace for the TempoStack&amp;#34;
display: &amp;#34;TempoStack&amp;#34;
additionalAnnotations: {}
additionalLabels: {}
storage: &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
secret:
name: tempo-s3
type: s3
credentialMode: static
storageSize: 500Gi
replicationFactor: 1
serviceAccount: tempo-simplest &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
tenants: &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
mode: openshift
enabled: true
authentication: &lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;(6)&lt;/b&gt;
- tenantName: &amp;#39;tenantA&amp;#39;
tenantId: &amp;#39;1610b0c3-c509-4592-a256-a1871353dbfc&amp;#39;
permissions: &lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;(7)&lt;/b&gt;
- write
- read
- tenantName: &amp;#39;tenantB&amp;#39;
tenantId: &amp;#39;1610b0c3-c509-4592-a256-a1871353dbfd&amp;#39;
permissions:
- write
- read
observability:
enabled: true
tracing:
jaeger_agent_endpoint: &amp;#39;localhost:6831&amp;#39;
otlp_http_endpoint: &amp;#39;http://localhost:4320&amp;#39;
template:
gateway:
enabled: true
rbac: false
ingress:
type: &amp;#39;route&amp;#39;
termination: &amp;#39;reencrypt&amp;#39;
component:
replicas: 1
queryFrontend:
jaegerQuery:
enabled: true
component:
replicas: 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Basic settings, like name of the instance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Namespace for the TempoStack instance.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Storage configuration for the TempoStack instance. Here we use type s3 and the Secret called &amp;#34;tempo-s3&amp;#34; which will be generated.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;ServiceAccount for the TempoStack instance.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Tenant configuration for the TempoStack instance.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;List of tenants. This list must be extended when new tenants shall be configured.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;7&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;RBAC permissions for this tenant. This will add the tenant as &lt;strong&gt;resource&lt;/strong&gt; to the READ and/or WRITE ClusterRole&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_tempo_operator_deployment"&gt;Tempo Operator Deployment&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following settings will deploy the Operator and verify the status of the Operator installation. Only when the Operator has been installed successfully will Argo CD continue with the synchronization.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;helper-operator:
operators:
tempo-operator: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
enabled: true
namespace: &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
name: openshift-tempo-operator
create: true
subscription: &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
channel: stable
approval: Automatic
operatorName: tempo-product
source: redhat-operators
sourceNamespace: openshift-marketplace
operatorgroup:
create: true
notownnamespace: true
########################################
# SUBCHART: helper-status-checker
# Verify the status of a given operator.
########################################
helper-status-checker:
enabled: true
approver: false &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
checks:
- operatorName: tempo-operator &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
namespace:
name: openshift-tempo-operator
syncwave: 1
serviceAccount:
name: &amp;#34;status-checker-tempo&amp;#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Install the Operator&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Namespace settings of the Operator&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Settings of the Operator itself, like name, channel and approval strategy.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Approver settings for the status checker. Here disabled, because we are using automatic approval.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Operator name to be verified.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_s3_bucket_deployment"&gt;S3 Bucket Deployment&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following Sub-Chart can be used to automatically create a Bucket in OpenShift Data Foundation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;########################################
# SUBCHART: helper-objectstore
# A helper chart that simply creates another backingstore for logging.
# This is a chart in a very early state, and not everything can be customized for now.
# It will create the objects:
# - BackingStore
# - BackingClass
# - StorageClass
# NOTE: Currently only PV type is supported
########################################
helper-objectstore:
# -- Enable objectstore configuration
# @default -- false
enabled: true
# -- Syncwave for Argo CD
# @default - 1
syncwave: 1
# -- Name of the BackingStore
backingstore_name: tempo-backingstore
# -- Size of the BackingStore that each volume shall have.
backingstore_size: 400Gi &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
# -- CPU Limit for the Noobaa Pod
# @default -- 500m
limits_cpu: 500m
# -- Memory Limit for the Noobaa Pod.
# @default -- 2Gi
limits_memory: 2Gi
pvPool:
# -- Number of volumes that shall be used
# @default -- 1
numOfVolumes: 1
# Type of BackingStore. Currently pv-pool is the only one supported by this Helm Chart.
# @default -- pv-pool
type: pv-pool
# -- The StorageClass the BackingStore is based on
baseStorageClass: gp3-csi &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
# -- Name of the StorageClass that shall be created for the bucket.
storageclass_name: tempo-bucket-storage-class &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
# Bucket that shall be created
bucket:
# -- Shall a new bucket be enabled?
# @default -- false
enabled: true
# -- Name of the bucket that shall be created
name: tempo-bucket &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
# -- Target Namespace for that bucket.
namespace: tempostack &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
# -- Syncwave for bucketclaim creation. This should be done very early, but it depends on ODF.
# @default -- 2
syncwave: 2
# -- Name of the storageclass for our bucket
# @default -- openshift-storage.noobaa.io
storageclass: tempo-bucket-storage-class &lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;(6)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Size of the bucket.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;StorageClass the bucket is based on.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Name of the StorageClass that shall be created for the bucket.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Name of the bucket that shall be created.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Namespace for the bucket.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;StorageClass for the bucket.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_automatic_secret_creation_for_tempostack"&gt;Automatic Secret Creation for TempoStack&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;TempoStack is expecting a Secret with the following keys:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;access_key_id&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;access_key_secret&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bucket&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;endpoint&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;region (only for specific settings)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;OpenShift Data Foundation creates a secret with access_key_id and access_key_secret and a ConfigMap with endpoint, region and the name of the bucket.
Unfortunately, this does not work for TempoStack. Therefore, we are using a &lt;strong&gt;helper-odf-bucket-secret&lt;/strong&gt; chart that will create a new Secret with the required keys.
This chart creates a Job that reads the required information and creates a new Secret with the required keys.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;##############################################
# SUBCHART: helper-odf-bucket-secret
# Creates a Secret that Tempo requires
#
# A Kubernetes Job is created, that reads the
# data from the Secret and ConfigMap and
# creates a new secret for Tempo.
##############################################
helper-odf-bucket-secret:
# -- Enable Job to create a Secret for TempoStack.
# @default -- false
enabled: true
# -- Syncwave for Argo CD.
# @default -- 3
syncwave: 3
# -- Namespace where TempoStack is deployed and where the Secret shall be created.
namespace: tempostack
# -- Name of Secret that shall be created.
secretname: tempo-s3 &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
# Bucket Configuration
bucket:
# -- Name of the Bucket shall has been created.
name: tempo-bucket &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
# -- Keys that shall be used to create the Secret.
keys: &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
# -- Overwrite access_key_id key.
# @default -- access_key_id
access_key_id: access_key_id
# -- Overwrite access_key_secret key.
# @default -- access_key_secret
access_key_secret: access_key_secret
# -- Overwrite bucket key.
# @default -- bucket
bucket: bucket
# -- Overwrite endpoint key.
# @default -- endpoint
endpoint: endpoint
# -- Overwrite region key. Region is only set if set_region is true.
# @default -- region
region: region
# -- Set region key.
# @default -- false
set_region: false &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Name of the Secret that shall be created.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Name of the bucket that shall be used.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Keys that shall be used to create the Secret.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Set region key. Here disabled, because we are not using a specific region.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Using OpenShift Data Foundation with TempoStack requires you to NOT set a region. Therefore, it is disabled above.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_complete_values_file"&gt;Complete values file&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To see the whole file expand the code:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;div class="expand"&gt;
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});"&gt;
&lt;i style="font-size:x-small;" class="fas fa-chevron-right"&gt;&lt;/i&gt;
&lt;span&gt;
&lt;a&gt;Expand me...&lt;/a&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="expand-content" style="display: none;"&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;tempo: &amp;amp;channel-tempo stable
tempo-namespace: &amp;amp;tempo-namespace openshift-tempo-operator
bucketname: &amp;amp;bucketname tempo-bucket
tempo-secret: &amp;amp;tempo-secret-name tempo-s3
storageclassname: &amp;amp;storageclassname tempo-bucket-storage-class
tempostack-namespace: &amp;amp;tempostack-namespace tempostack
tempo-tracing:
tempostack:
enabled: true
name: simplest
managementState: Managed
namespace:
create: true
name: *tempostack-namespace
descr: &amp;#34;Namespace for the TempoStack&amp;#34;
display: &amp;#34;TempoStack&amp;#34;
additionalAnnotations: {}
additionalLabels: {}
storage:
secret:
name: tempo-s3
type: s3
credentialMode: static
storageSize: 500Gi
replicationFactor: 1
serviceAccount: tempo-simplest
tenants:
mode: openshift
enabled: true
authentication:
- tenantName: &amp;#39;tenantA&amp;#39;
tenantId: &amp;#39;1610b0c3-c509-4592-a256-a1871353dbfc&amp;#39;
permissions:
- write
- read
- tenantName: &amp;#39;tenantB&amp;#39;
tenantId: &amp;#39;1610b0c3-c509-4592-a256-a1871353dbfd&amp;#39;
permissions:
- write
- read
observability:
enabled: true
tracing:
jaeger_agent_endpoint: &amp;#39;localhost:6831&amp;#39;
otlp_http_endpoint: &amp;#39;http://localhost:4320&amp;#39;
template:
gateway:
enabled: true
rbac: false
ingress:
type: &amp;#39;route&amp;#39;
termination: &amp;#39;reencrypt&amp;#39;
component:
replicas: 1
queryFrontend:
jaegerQuery:
enabled: true
component:
replicas: 1
######################################
# SUBCHART: helper-operator
# Operators that shall be installed.
######################################
helper-operator:
operators:
tempo-operator:
enabled: true
namespace:
name: *tempo-namespace
create: true
subscription:
channel: *channel-tempo
approval: Automatic
operatorName: tempo-product
source: redhat-operators
sourceNamespace: openshift-marketplace
operatorgroup:
create: true
notownnamespace: true
########################################
# SUBCHART: helper-status-checker
# Verify the status of a given operator.
########################################
helper-status-checker:
enabled: true
approver: false
checks:
- operatorName: tempo-operator
namespace:
name: *tempo-namespace
syncwave: 1
serviceAccount:
name: &amp;#34;status-checker-tempo&amp;#34;
########################################
# SUBCHART: helper-objectstore
# A helper chart that simply creates another backingstore for logging.
# This is a chart in a very early state, and not everything can be customized for now.
# It will create the objects:
# - BackingStore
# - BackingClass
# - StorageClass
# NOTE: Currently only PV type is supported
########################################
helper-objectstore:
# -- Enable objectstore configuration
# @default -- false
enabled: true
# -- Syncwave for Argo CD
# @default - 1
syncwave: 1
# -- Name of the BackingStore
backingstore_name: tempo-backingstore
# -- Size of the BackingStore that each volume shall have.
backingstore_size: 400Gi
# -- CPU Limit for the Noobaa Pod
# @default -- 500m
limits_cpu: 500m
# -- Memory Limit for the Noobaa Pod.
# @default -- 2Gi
limits_memory: 2Gi
pvPool:
# -- Number of volumes that shall be used
# @default -- 1
numOfVolumes: 1
# Type of BackingStore. Currently pv-pool is the only one supported by this Helm Chart.
# @default -- pv-pool
type: pv-pool
# -- The StorageClass the BackingStore is based on
baseStorageClass: gp3-csi
# -- Name of the StorageClass that shall be created for the bucket.
storageclass_name: *storageclassname
# Bucket that shall be created
bucket:
# -- Shall a new bucket be enabled?
# @default -- false
enabled: true
# -- Name of the bucket that shall be created
name: *bucketname
# -- Target Namespace for that bucket.
namespace: *tempo-namespace
# -- Syncwave for bucketclaim creation. This should be done very early, but it depends on ODF.
# @default -- 2
syncwave: 2
# -- Name of the storageclass for our bucket
# @default -- openshift-storage.noobaa.io
storageclass: *storageclassname
##############################################
# SUBCHART: helper-odf-bucket-secret
# Creates a Secret that Tempo requires
#
# A Kubernetes Job is created, that reads the
# data from the Secret and ConfigMap and
# creates a new secret for Tempo.
##############################################
helper-odf-bucket-secret:
# -- Enable Job to create a Secret for TempoStack.
# @default -- false
enabled: true
# -- Syncwave for Argo CD.
# @default -- 3
syncwave: 3
# -- Namespace where TempoStack is deployed and where the Secret shall be created.
namespace: *tempostack-namespace
# -- Name of Secret that shall be created.
secretname: *tempo-secret-name
# Bucket Configuration
bucket:
# -- Name of the Bucket shall has been created.
name: *bucketname
# -- Keys that shall be used to create the Secret.
keys:
# -- Overwrite access_key_id key.
# @default -- access_key_id
access_key_id: access_key_id
# -- Overwrite access_key_secret key.
# @default -- access_key_secret
access_key_secret: access_key_secret
# -- Overwrite bucket key.
# @default -- bucket
bucket: bucket
# -- Overwrite endpoint key.
# @default -- endpoint
endpoint: endpoint
# -- Overwrite region key. Region is only set if set_region is true.
# @default -- region
region: region
# -- Set region key.
# @default -- false
set_region: false&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_the_tempostack"&gt;The TempoStack&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s imagine all of the above works and we have a TempoStack instance running in the namespace &lt;strong&gt;tempostack&lt;/strong&gt; with the name &lt;strong&gt;simplest&lt;/strong&gt;. Several Pods are running, like the distributor, ingester, gateway, query-frontend, compactor and querier.
I admit it is not highly available, but this can be easily changed in the values file above (replica count).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc get pods -n tempostack | grep simplest
tempo-simplest-compactor-584689c78f-t7pxb 1/1 Running 0 3d2h
tempo-simplest-distributor-6fb5d7dc9d-wrzt4 1/1 Running 0 3d2h
tempo-simplest-gateway-bbcb774b9-p44lq 2/2 Running 0 11h
tempo-simplest-ingester-0 1/1 Running 0 3d2h
tempo-simplest-querier-6cf9d7b6d8-mvc9d 1/1 Running 0 3d2h
tempo-simplest-query-frontend-7d859f9f9f-xzj97 3/3 Running 0 3d2h&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now we can continue with the OpenTelemetry Collector deployment.
But before we do this, letâ€™s first discuss how to:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Add Tracing UI to OpenShift&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;hr/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_extend_the_openshift_ui"&gt;Extend the OpenShift UI&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;While we have Tempo installed, it will not be visible in the OpenShift UI by default.
Here a separate Operator has been created by Red Hat, that will take care of this extension: &lt;strong&gt;Cluster Observability Operator&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This Operator be installed:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/coo.png?width=640px" alt="Cluster Observability Operator"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The Operator can be deployed with the Chart &lt;strong&gt;helper-operator&lt;/strong&gt; as well. However, I did not merge this together with the TempoStack deployment, because this Operator also serves different purposes.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We only require a very small bit of this Operator. Amongst other resources, it also provides a resource called &lt;strong&gt;UIPlugin&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The resource must be configured as follows:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: observability.openshift.io/v1alpha1
kind: UIPlugincd
metadata:
name: distributed-tracing &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
spec:
type: DistributedTracing &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The name MUST be &lt;strong&gt;distributed-tracing&lt;/strong&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The type MUST be &lt;strong&gt;DistributedTracing&lt;/strong&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This will extend the OpenShift UI with a new navigation link &lt;strong&gt;&amp;#34;Observe&amp;#34; &amp;gt; &amp;#34;Traces&amp;#34;&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/tempo-UI.png" alt="Tempo UI"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_on_the_next_episode"&gt;On the next Episode&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The next article will cover the deployment of the Central OpenTelemetry Collector. We will configure the RBAC permissions required for the Central Collector to enrich traces with Kubernetes metadata and deploy the Central OpenTelemetry Collector with its complete configuration.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>The Hitchhiker's Guide to Observability - Central Collector - Part 3</title><link>https://blog.stderr.at/openshift-platform/observability/observability/2025-11-25-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part3/</link><pubDate>Tue, 25 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/observability/observability/2025-11-25-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part3/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;With the architecture defined in &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/"&gt;Part 1&lt;/a&gt; and TempoStack deployed in &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/"&gt;Part 2&lt;/a&gt;, itâ€™s time to tackle the heart of our distributed tracing system: the &lt;strong&gt;Central OpenTelemetry Collector&lt;/strong&gt;. This is the critical component that sits between your application namespaces and TempoStack, orchestrating trace flow, metadata enrichment, and tenant routing.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In this article, weâ€™ll configure the RBAC permissions required for the Central Collector to enrich traces with Kubernetes metadata and deploy the Central OpenTelemetry Collector with its complete configuration. Youâ€™ll learn how to set up receivers for accepting traces from local collectors, configure processors to enrich traces with Kubernetes and OpenShift metadata, and implement routing connectors to direct traces to the appropriate TempoStack tenants based on namespace.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To be honest, this was the most challenging part of the entire setup to get right, as you can easily miss a setting, or misconfigure a part of the configuration. But once you understand the configuration, it becomes straightforward to extend and modify.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_central_opentelemetry_collector_step_by_step_implementation"&gt;Central OpenTelemetry Collector - Step-by-Step Implementation&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_prerequisites"&gt;Prerequisites&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Before starting, ensure you have completed the previous parts and have:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Part 1 completed&lt;/strong&gt;: &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/"&gt;Hitchhikerâ€™s Guide to Observability with OpenTelemetry and Tempo&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Part 2 completed&lt;/strong&gt;: &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/"&gt;Deploy Grafana Tempo and TempoStack&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OpenShift cluster&lt;/strong&gt; (v4.20) with Red Hat build of OpenTelemetry Operator installed (v0.135.0-1)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Access&lt;/strong&gt; Cluster-admin access&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
For all configurations I also created a proper GitOps implementation (of course :)). However, first I would like to show the actual configuration. The GitOps implementation can be found at the end of this article.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_1_verifydeploy_red_hat_build_of_opentelemetry_operator"&gt;Step 1: Verify/Deploy Red Hat Build of OpenTelemetry Operator&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Verify if the Operator &lt;strong&gt;Red Hat Build of OpenTelemetry&lt;/strong&gt; is installed and ready. The Operator itself is deployed in the namespace &lt;strong&gt;openshift-opentelemetry-operator&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/otel-operator.png" alt="OpenTelemetry Operator"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_2_configure_rbac_for_central_collector"&gt;Step 2: Configure RBAC for Central Collector&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;For the OpenTelemetry Collector to function properly with processors like k8sattributes and resourcedetection, it requires cluster-wide read access to Kubernetes resources.
Depending on the configuration of the central collector, you might need to configure different RBAC settings to allow the collector to perform specific things. The central collector in our example uses the &lt;strong&gt;k8sattributes processor&lt;/strong&gt; and &lt;strong&gt;resourcedetection processor&lt;/strong&gt; to enrich traces with Kubernetes and OpenShift metadata. These processors require read access to cluster resources.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_why_these_permissions_are_needed"&gt;Why These Permissions Are Needed&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The k8sattributes processor enriches telemetry data by querying the Kubernetes API to add metadata such as:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pod information&lt;/strong&gt;: Pod name, UID, start time&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Namespace details&lt;/strong&gt;: Namespace name and labels&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deployment context&lt;/strong&gt;: ReplicaSet and Deployment names&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Node information&lt;/strong&gt;: Node name where the pod is running&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The resourcedetection processor detects the OpenShift environment by querying:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Infrastructure resources&lt;/strong&gt;: Cluster name, platform type, region (from &lt;code&gt;config.openshift.io/infrastructures&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Without these permissions, traces would lack critical context needed for debugging and analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock warning"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-warning" title="Warning"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Always check the latest &lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/red_hat_build_of_opentelemetry/index#otel-collector-receivers_otel-configuration-of-otel-collector" target="_blank" rel="noopener"&gt;documentation&lt;/a&gt; of the appropriate component to get the most up to date information. I prefer to use separate ClusterRoles for each processor to keep the permissions as granular as possible. However, that causes some overhead, so you might want to combine the permissions into a single ClusterRole.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_create_clusterrole_for_kubernetes_attributes"&gt;Create ClusterRole for Kubernetes Attributes&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following ClusterRole has been created:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
name: k8sattributes-otel-collector
rules:
# Permissions for k8sattributes processor
# Allows the collector to read pod, namespace, and replicaset information
# to enrich traces with Kubernetes metadata
- verbs:
- get
- watch
- list
apiGroups:
- &amp;#39;*&amp;#39;
resources:
- pods
- namespaces
- replicasets
# Permissions for resourcedetection processor (OpenShift)
# Allows detection of OpenShift cluster information
# such as cluster name, platform type, and region
- verbs:
- get
- watch
- list
apiGroups:
- config.openshift.io
resources:
- infrastructures
- infrastructures/status&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Read-Only Access&lt;/strong&gt;: The collector only needs &lt;code&gt;get&lt;/code&gt;, &lt;code&gt;watch&lt;/code&gt;, and &lt;code&gt;list&lt;/code&gt; verbs (no write permissions)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cluster-Wide Scope&lt;/strong&gt;: ClusterRole grants permissions across all namespaces, necessary for monitoring multi-tenant environments&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Essential Resources&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;pods&lt;/code&gt;: Source of trace context (which pod generated the span)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;namespaces&lt;/code&gt;: Namespace metadata and labels for routing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;replicasets&lt;/code&gt;: Determine the owning Deployment for better trace attribution&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OpenShift Infrastructure&lt;/strong&gt;: Access to &lt;code&gt;config.openshift.io/infrastructures&lt;/code&gt; allows detection of cluster-level properties&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_create_clusterrolebinding"&gt;Create ClusterRoleBinding&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Our OpenTelemetry Collector will use the ServiceAccount &lt;strong&gt;otel-collector&lt;/strong&gt; (in the namespace tempostack) to read the Kubernetes resources. Thus, we need to create a ClusterRoleBinding to grant the necessary permissions to the ServiceAccount.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
name: k8sattributes-collector-tempo
subjects:
- kind: ServiceAccount
name: otel-collector &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
namespace: tempostack &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: k8sattributes-otel-collector &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The ServiceAccount that is allowed to write the traces. In this example: &lt;strong&gt;otel-collector&lt;/strong&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The namespace of the ServiceAccount. In this example: &lt;strong&gt;tempostack&lt;/strong&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The reference to the ClusterRole&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_security_note"&gt;Security Note&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The above permissions in the ClusterRole follow the &lt;strong&gt;principle of least privilege&lt;/strong&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Only read operations are granted (no create, update, or delete)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Access is limited to specific resource types needed for metadata enrichment&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The ServiceAccount &lt;strong&gt;otel-collector&lt;/strong&gt; is dedicated to the collector and not shared with other applications&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_3_create_serviceaccount_for_central_opentelemetry_collector"&gt;Step 3: Create ServiceAccount for Central OpenTelemetry Collector&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The ServiceAccount used by the OpenTelemetry Collector. This is the service account that will be used to authenticate to the TempoStack instance. Thus, the Bindings created earlier to write into TempoStack and to read from Kubernetes will be required.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
In this article we are installing the central Collector into the same namespace as the TempoStack instance. However, you might want to install it in a different namespace to keep the namespaces separated. Keep an eye on possible Network Policies that might be required to allow the communication between the namespaces.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: v1
kind: ServiceAccount
metadata:
name: otel-collector
namespace: tempostack&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_4_deploy_central_collector"&gt;Step 4: Deploy Central Collector&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The central collector receives traces from local collectors, enriches them with Kubernetes metadata, and routes them to appropriate TempoStack tenants.
For the sake of simplicity, I have taken snippets from the whole Configuration manifest. At the end of this section you will find the &lt;a href="#_complete_opentelemetry_collector_manifest"&gt;whole manifest&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_basic_configuration"&gt;Basic Configuration&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The very basic settings of the OpenTelemetry Collector, are the amount of replicas, the ServiceAccount to use and the deployment mode.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Collector can be deployed in one of the following modes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deployment&lt;/strong&gt; (default) - Creates a Deployment with the given numbers of replicas.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;StatefulSet&lt;/strong&gt; - Creates a StatefulSet with the given numbers of replicas. Useful for stateful workloads, for example when using the Collectorâ€™s File Storage Extension or Tail Sampling Processor.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DaemonSet&lt;/strong&gt; - Creates a DaemonSet with the given numbers of replicas. Useful for scraping telemetry data from every node, for example by using the Collectorâ€™s Filelog Receiver to read container logs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sidecar&lt;/strong&gt; - Injects the Collector as a sidecar into the pod. Useful for accessing log files inside a container, for example by using the Collectorâ€™s Filelog Receiver and a shared volume such as emptyDir.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In the examples in this series of articles we will use the modes: &lt;strong&gt;deployment&lt;/strong&gt; and &lt;strong&gt;sidecar&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
name: otel
namespace: tempostack
spec:
mode: deployment &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
replicas: 1 &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
serviceAccount: otel-collector &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
[...]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The deployment mode to use.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The number of replicas to use.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The ServiceAccount to use, created in the previous step.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_receivers"&gt;Receivers&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Receivers are the components that &lt;strong&gt;receive&lt;/strong&gt; the traces from the local collectors. Receivers accept data in a specified format and translate it into the internal format. In our example we want to receive the traces from the local collectors. For our tests we are using the &lt;strong&gt;oltp&lt;/strong&gt; Receiver, which is collecting traces, metrics and logs by using the OpenTelemetry Protocol (OTLP).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The easiest configuration is:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt; receivers:
otlp: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
protocols:
grpc:
endpoint: 0.0.0.0:4317 &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
http:
endpoint: 0.0.0.0:4318 &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The name of the receiver.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The gRPC endpoint to receive.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The http endpoint to receive.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Besides the otlp receiver, there are other receivers available. For example:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Jaeger&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kubernetes Object Receiver&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kubelet Stats Receiver&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prometheus Receiver&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Filelog Receiver&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Journald Receiver&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kubernetes Events Receiver&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kubernetes Cluster Receiver&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OpenCensus Receiver&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zipkin Receiver&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kafka Receiver.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Please check the &lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/red_hat_build_of_opentelemetry/index#otel-collector-receivers_otel-configuration-of-otel-collector" target="_blank" rel="noopener"&gt;OpenShift OTEL Receivers&lt;/a&gt; documentation for more details.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_processors"&gt;Processors&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Processors are the components that &lt;strong&gt;process&lt;/strong&gt; the data after it is received and before it is exported. Processors are completely optional, but they are useful to transform, enrich, or filter traces.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock warning"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-warning" title="Warning"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The order of processors matters.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The example configuration is using the following processors:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;k8sattributes&lt;/strong&gt;: Can add Kubernetes metadata to the traces.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;resourcedetection&lt;/strong&gt;: Can detect OpenShift/K8s environment info.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;memory_limiter&lt;/strong&gt;: Periodically checks the Collectorâ€™s memory usage and pauses data processing when the soft memory limit is reached.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;batch&lt;/strong&gt;: Batches the traces for efficiency. This is a very important processor to improve the performance of the Collector.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Additional processors are available. Please check the &lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/red_hat_build_of_opentelemetry/index#otel-collector-processors" target="_blank" rel="noopener"&gt;OpenShift OTEL Processors&lt;/a&gt; documentation for more details.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock warning"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-warning" title="Warning"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Some processors will requires additional ClusterRole configuration.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt; # Processors - enrich and batch traces
processors:
# Add Kubernetes metadata
k8sattributes: {} &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
# Detect OpenShift/K8s environment info
resourcedetection: &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
detectors:
- openshift
timeout: 2s
# Memory protection
memory_limiter: &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
check_interval: 1s
limit_percentage: 75
spike_limit_percentage: 15
# Batch for efficiency
batch: &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
send_batch_size: 10000
timeout: 10s&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The k8sattributes processor to add Kubernetes metadata to the traces.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The resourcedetection processor to detect OpenShift/K8s environment info.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The memory_limiter processor to protect the Collectorâ€™s memory usage.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The batch processor to batch the traces for efficiency.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_connectors"&gt;Connectors&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;A Connector joins two pipelines together. It consumes data as an exporter at the end of one pipeline and emits data as a receiver at the start of another pipeline. It can consume and emit data of the same or different data type. It can generate and emit data to summarize the consumed data, or it can merely replicate or route data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Several Connectors are available, for example:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Count Connector&lt;/strong&gt;: Counts traces spans, trace span events, metrics, metric data points, and log records.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Routing Connector&lt;/strong&gt;: Routes the traces to different pipelines.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Forward Connector&lt;/strong&gt;: Merges two pipelines of the same type.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spanmetrics Connector&lt;/strong&gt;: Aggregates Request, Error, and Duration (R.E.D) OpenTelemetry metrics from span data.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The &lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/red_hat_build_of_opentelemetry/index#otel-collector-connectors" target="_blank" rel="noopener"&gt;OpenShift OTEL Connectors&lt;/a&gt; documentation lists all available Connectors and their configuration options.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In our example we are using the &lt;strong&gt;routing&lt;/strong&gt; Connector, which is able to route the traces to different pipelines based on the namespace.
This helps to route the traces to the correct tenant, without the need to configure this in the local OpenTelemetry Collector. (In other words, the project cannot change this setting, because it is configured in the central OpenTelemetry Collector.)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In this example traces from the namespace &amp;#34;team-a&amp;#34; will be routed to the pipeline &amp;#34;tenantA&amp;#34;, traces from the namespace &amp;#34;team-b&amp;#34; will be routed to the pipeline &amp;#34;tenantB&amp;#34;, and so on.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt; # Connectors - route traces to different pipelines
connectors:
routing/traces:
default_pipelines: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
- traces/Default
error_mode: ignore &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
table:
# Route team-a namespace to tenantA
- statement: route() where attributes[&amp;#34;k8s.namespace.name&amp;#34;] == &amp;#34;team-a&amp;#34; &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
pipelines:
- traces/tenantA
# Route team-b namespace to tenantB
- statement: route() where attributes[&amp;#34;k8s.namespace.name&amp;#34;] == &amp;#34;team-b&amp;#34;
pipelines:
- traces/tenantB
# Route team-c namespace to tenantC
- statement: route() where attributes[&amp;#34;k8s.namespace.name&amp;#34;] == &amp;#34;team-c&amp;#34;
pipelines:
- traces/tenantC&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Destination pipelines for routing the telemetry data for which no routing condition is satisfied.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Error-handling mode&lt;/strong&gt;: Defines how the connector handles routing errors:
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;propagate&lt;/code&gt;: Logs an error and drops the payload (stops processing)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;ignore&lt;/code&gt;: Logs the error but continues attempting to match subsequent routing rules&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;silent&lt;/code&gt;: Same as &lt;code&gt;ignore&lt;/code&gt; but without logging&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Default: &lt;code&gt;propagate&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Route traces from namespace &lt;strong&gt;team-a&lt;/strong&gt; to the pipeline &lt;strong&gt;tenantA&lt;/strong&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_exporters"&gt;Exporters&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Exporters are the components that &lt;strong&gt;export&lt;/strong&gt; the traces to a destination. Exporters accept data in a specified format and translate it into the destination format. In our example we want to export the traces to the TempoStack instance. The X-Scope-OrgID header is used to identify the tenant and is sent to the TempoStack instance.
The authentication is done by using the ServiceAccount token.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Many different Exporters are available. The &lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/red_hat_build_of_opentelemetry/index#otel-collector-exporters" target="_blank" rel="noopener"&gt;OpenShift OTEL Exporters&lt;/a&gt; documentation lists all available Exporters and their configuration options.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;For our tests we are using the &lt;strong&gt;otlp&lt;/strong&gt; Exporter, which will export using the OpenTelemetry Protocol (OTLP):&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt; exporters:
# Tenant A exporter
otlp/tenantA: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
endpoint: tempo-simplest-gateway:8090 &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
auth:
authenticator: bearertokenauth
headers:
X-Scope-OrgID: tenantA &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
tls:
ca_file: /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
insecure_skip_verify: true &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
server_name_override: tempo-simplest-gateway.tempostack.svc.cluster.local &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Protocol/name of the exporter.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The endpoint to export to. Here, the endpoint is the address of the TempoStack instance.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The scope org ID to use.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Whether to skip certificate verification. I did not bother with certificates in this example.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The server name override to use. This was just for testing purposes and can be omitted.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_extensions"&gt;Extensions&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Extensions extend the Collector capabilities. In our example we are using the &lt;strong&gt;bearertokenauth&lt;/strong&gt; Extension, which is used to authenticate to the TempoStack instance.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt; # Extensions - authentication
extensions:
bearertokenauth:
filename: /var/run/secrets/kubernetes.io/serviceaccount/token&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_service"&gt;Service:&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Components are enabled by adding them into a &lt;strong&gt;Pipeline&lt;/strong&gt;. If a component is not configured in a pipeline, it is not enabled.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In this example we are using the following pipelines (snippets):&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;traces/in&lt;/strong&gt;: The incoming traces pipeline.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;traces/tenantA&lt;/strong&gt;: The tenant A pipeline.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The &lt;strong&gt;traces/in&lt;/strong&gt; pipeline is the incoming traces pipeline. It is used to receive the traces from the local collectors. It leverages the &lt;strong&gt;otlp&lt;/strong&gt; Receiver to receive the traces. It uses the &lt;strong&gt;resourcedetection&lt;/strong&gt;, &lt;strong&gt;k8sattributes&lt;/strong&gt;, &lt;strong&gt;memory_limiter&lt;/strong&gt;, and &lt;strong&gt;batch&lt;/strong&gt; processors to process the traces. And finally, it uses the &lt;strong&gt;routing/traces&lt;/strong&gt; Connector to route the traces.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The &lt;strong&gt;routing/traces&lt;/strong&gt; Connector is used to route the traces to the correct tenant based on the namespace.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The &lt;strong&gt;traces/tenantA&lt;/strong&gt; Pipeline will receive the traces from &lt;strong&gt;routing/traces&lt;/strong&gt; and export the traces to &lt;strong&gt;otlp/tenantA&lt;/strong&gt; which then sends everything to TempoStack to store the traces using the header &lt;strong&gt;X-Scope-OrgID: tenantA&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt; # Service pipelines
service:
extensions:
- bearertokenauth
pipelines:
# Incoming traces pipeline
traces/in: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
receivers: &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
- otlp
processors: &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
- resourcedetection
- k8sattributes
- memory_limiter
- batch
exporters: &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
- routing/traces
# Tenant A pipeline
traces/tenantA: &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
receivers: &lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;(6)&lt;/b&gt;
- routing/traces
exporters: &lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;(7)&lt;/b&gt;
- otlp/tenantA&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The incoming traces pipeline. It takes the traces from the otlp receiver.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The receivers to use.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The processors to use.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The exporters to use. It is exporting the data to the routing connector.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The tenant A pipeline.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The receivers to use.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;7&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The exporters to use. It is exporting to the exporter, that will send the data to TempoStack.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The service automatically creates Kubernetes Services for the receivers. The Central Collector will be accessible at &lt;code&gt;otel-collector.tempostack.svc.cluster.local:4317&lt;/code&gt; (gRPC) and &lt;code&gt;:4318&lt;/code&gt; (HTTP) for local collectors to send traces.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_data_flow_visualization"&gt;Data Flow Visualization&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To better understand how traces flow through the Central Collector, the following Mermaid diagram visualizes the complete journey from application to storage using &lt;strong&gt;team-a&lt;/strong&gt; as an example:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-mermaid {align=" center"="" zoom="true" }"="" data-lang="mermaid {align=" center"="" zoom="true" }"=""&gt;---
title: &amp;#34;Central Collector Data Flow (Wrapped Layout)&amp;#34;
config:
theme: &amp;#39;dark&amp;#39;
---
flowchart LR
%% --------------------------
%% Column 1: Local Collector
%% --------------------------
subgraph local[&amp;#34;(team-a namespace)&amp;#34;]
app[&amp;#34;Application&amp;lt;br/&amp;gt;(team-a)&amp;#34;]
end
%% --------------------------
%% Column 2: Central Collector
%% --------------------------
subgraph central[&amp;#34;Central Collector (tempostack namespace)&amp;#34;]
%% We force this specific column to stack vertically (Top-Bottom)
direction TB
%% --- ROW 1: Receiver + Processors ---
subgraph row1[&amp;#34; &amp;#34;]
direction LR
receiver[&amp;#34;OTLP Receiver&amp;lt;br/&amp;gt;Port: 4317/4318&amp;#34;]
subgraph pipeline_in[&amp;#34;traces/in Processors&amp;#34;]
proc1[&amp;#34;resourcedetection&amp;lt;br/&amp;gt;(Detect OpenShift)&amp;#34;]
proc2[&amp;#34;k8sattributes&amp;lt;br/&amp;gt;(Add K8s metadata)&amp;#34;]
proc3[&amp;#34;memory_limiter&amp;lt;br/&amp;gt;(Protect memory)&amp;#34;]
proc4[&amp;#34;batch&amp;lt;br/&amp;gt;(Batch traces)&amp;#34;]
end
end
%% --- ROW 2: Connector + Tenant Pipeline ---
subgraph row2[&amp;#34; &amp;#34;]
direction LR
exporter_routing[&amp;#34;Exporter: to connector&amp;#34;]
connector[&amp;#34;Export to routing/traces Connector&amp;lt;br/&amp;gt;(Route by namespace)&amp;#34;]
subgraph pipeline_tenant[&amp;#34;Pipeline: traces/tenantA&amp;#34;]
receiver_tenant[&amp;#34;Receiver:&amp;lt;br/&amp;gt;routing/traces&amp;#34;]
exporter_tenant[&amp;#34;Exporter:&amp;lt;br/&amp;gt;otlp/tenantA&amp;#34;]
end
end
end
%% --------------------------
%% Column 3: TempoStack
%% --------------------------
subgraph tempo[&amp;#34;TempoStack&amp;#34;]
gateway[&amp;#34;Tempo Gateway&amp;lt;br/&amp;gt;(X-Scope-OrgID: tenantA)&amp;#34;]
storage[&amp;#34;S3 Storage&amp;lt;br/&amp;gt;(tenantA traces)&amp;#34;]
end
%% --------------------------
%% Connections
%% --------------------------
app --&amp;gt;|&amp;#34;OTLP traces&amp;#34;| receiver
receiver --&amp;gt; proc1 --&amp;gt; proc2 --&amp;gt; proc3 --&amp;gt; proc4
%% The Wrap: Connect end of Row 1 to start of Row 2
proc4 --&amp;gt; exporter_routing
exporter_routing --&amp;gt; connector
connector --&amp;gt;|&amp;#34;Route by namespace=team-a&amp;lt;br/&amp;gt;â†’ tenantA&amp;#34;| receiver_tenant
receiver_tenant --&amp;gt; exporter_tenant
exporter_tenant --&amp;gt;|&amp;#34;OTLP + bearer token&amp;lt;br/&amp;gt;Header: X-Scope-OrgID&amp;#34;| gateway
gateway --&amp;gt; storage
%% --------------------------
%% Styles
%% --------------------------
classDef appStyle fill:#2f652a,stroke:#2f652a,stroke-width:2px
classDef receiverStyle fill:#425cc6,stroke:#425cc6,stroke-width:2px
classDef processorStyle fill:#4a90e2,stroke:#4a90e2,stroke-width:2px
classDef connectorStyle fill:#906403,stroke:#906403,stroke-width:2px
classDef exporterStyle fill:#7b4397,stroke:#7b4397,stroke-width:2px
classDef tempoStyle fill:#d35400,stroke:#d35400,stroke-width:2px
class app appStyle
class receiver,receiver_tenant receiverStyle
class proc1,proc2,proc3,proc4 processorStyle
class connector connectorStyle
class exporter_tenant exporterStyle
class exporter_routing exporterStyle
class gateway,storage tempoStyle
%% Hide the structural boxes for the rows so they look seamless
style row1 fill:none,stroke:none
style row2 fill:none,stroke:none&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Key Points in the Flow:&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Application&lt;/strong&gt; in team-a namespace sends traces to local collector&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Local Collector&lt;/strong&gt; forwards to &lt;strong&gt;Central Collectorâ€™s OTLP receiver&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pipeline traces/in&lt;/strong&gt; processes the traces sequentially:&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Detects OpenShift environment info&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Adds Kubernetes metadata (namespace, pod, labels)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Applies memory limits&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Batches traces for efficiency&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Routing Connector&lt;/strong&gt; examines the namespace attribute and routes to the correct tenant pipeline&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Pipeline traces/tenantA&lt;/strong&gt; receives from connector and exports to TempoStack&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Exporter&lt;/strong&gt; adds authentication (bearer token) and tenant ID header (X-Scope-OrgID: tenantA)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TempoStack&lt;/strong&gt; receives and stores traces in the appropriate tenant storage&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This architecture ensures complete isolation between tenants while maintaining a single, centralized collection point.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_complete_opentelemetry_collector_manifest"&gt;Complete OpenTelemetry Collector Manifest&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s put everything together in the complete OpenTelemetry Collector Manifest. The following defines the Central OpenTelemetry Collector:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;mode&lt;/strong&gt;: The deployment mode to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;replicas&lt;/strong&gt;: The number of replicas to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;serviceAccount&lt;/strong&gt;: The ServiceAccount to use, created in the previous step.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;config&lt;/strong&gt;: The configuration of the OpenTelemetry Collector.&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;receivers&lt;/strong&gt;: The receivers to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;processors&lt;/strong&gt;: The processors to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;connectors&lt;/strong&gt;: The connectors to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;exporters&lt;/strong&gt;: The exporters to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;extensions&lt;/strong&gt;: The extensions to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;service&lt;/strong&gt;: The service to use.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
name: otel
namespace: tempostack
spec:
mode: deployment
replicas: 1
serviceAccount: otel-collector
config:
# Receivers - accept traces from local collectors
receivers:
otlp:
protocols:
grpc:
endpoint: 0.0.0.0:4317
http:
endpoint: 0.0.0.0:4318
# Processors - enrich and batch traces
processors:
# Add Kubernetes metadata
k8sattributes: {}
# Detect OpenShift/K8s environment info
resourcedetection:
detectors:
- openshift
timeout: 2s
# Memory protection
memory_limiter:
check_interval: 1s
limit_percentage: 75
spike_limit_percentage: 15
# Batch for efficiency
batch:
send_batch_size: 10000
timeout: 10s
# Connectors - route traces to different pipelines
connectors:
routing/traces:
default_pipelines:
- traces/Default
error_mode: ignore
table:
# Route team-a namespace to tenantA
- statement: route() where attributes[&amp;#34;k8s.namespace.name&amp;#34;] == &amp;#34;team-a&amp;#34;
pipelines:
- traces/tenantA
# Route team-b namespace to tenantB
- statement: route() where attributes[&amp;#34;k8s.namespace.name&amp;#34;] == &amp;#34;team-b&amp;#34;
pipelines:
- traces/tenantB
# Exporters - send to TempoStack
exporters:
# Default tenant exporter
otlp/Default:
endpoint: tempo-simplest-gateway:8090
auth:
authenticator: bearertokenauth
headers:
X-Scope-OrgID: dev
tls:
ca_file: /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
insecure_skip_verify: true
server_name_override: tempo-simplest-gateway.tempostack.svc.cluster.local
# Tenant A exporter
otlp/tenantA:
endpoint: tempo-simplest-gateway:8090
auth:
authenticator: bearertokenauth
headers:
X-Scope-OrgID: tenantA
tls:
ca_file: /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
insecure_skip_verify: true
server_name_override: tempo-simplest-gateway.tempostack.svc.cluster.local
# Tenant B exporter
otlp/tenantB:
endpoint: tempo-simplest-gateway:8090
auth:
authenticator: bearertokenauth
headers:
X-Scope-OrgID: tenantB
tls:
ca_file: /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
insecure_skip_verify: true
server_name_override: tempo-simplest-gateway.tempostack.svc.cluster.local
# Extensions - authentication
extensions:
bearertokenauth:
filename: /var/run/secrets/kubernetes.io/serviceaccount/token
# Service pipelines
service:
extensions:
- bearertokenauth
pipelines:
# Incoming traces pipeline
traces/in:
receivers:
- otlp
processors:
- resourcedetection
- k8sattributes
- memory_limiter
- batch
exporters:
- routing/traces
# Default tenant pipeline
traces/Default:
receivers:
- routing/traces
exporters:
- otlp/Default
# Tenant A pipeline
traces/tenantA:
receivers:
- routing/traces
exporters:
- otlp/tenantA
# Tenant B pipeline
traces/tenantB:
receivers:
- routing/traces
exporters:
- otlp/tenantB&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_gitops_deployment"&gt;GitOps Deployment&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;While the above is good for quick tests, it always makes sense to have a proper GitOps deployment. I have created a Chart and GitOps configuration that will:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Deploy the OTEL Operator&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure the the Central Collector instance&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following sources will be used:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://charts.stderr.at/" target="_blank" rel="noopener"&gt;Helm Repository&lt;/a&gt; - to fetch the Helm Chart for the OTEL Operator including required Sub-Charts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tjungbauer/openshift-clusterconfig-gitops/tree/main/clusters/management-cluster/setup-otel-operator" target="_blank" rel="noopener"&gt;Setup OTEL Operator&lt;/a&gt; - To deploy and configure the OpenTelemetry Operator.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Feel free to clone or use whatever you need.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following Sub-Charts are used:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/helper-operator" target="_blank" rel="noopener"&gt;helper-operator&lt;/a&gt; (version ~1.0.18) - Installs the OpenTelemetry Operator&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/helper-status-checker" target="_blank" rel="noopener"&gt;helper-status-checker&lt;/a&gt; (version ~4.0.0) - Verifies the status of the OpenTelemetry Operator&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/rh-build-of-opentelemetry" target="_blank" rel="noopener"&gt;opentelemetry-collector&lt;/a&gt; (version ~1.0.0) - Creates the Central OpenTelemetry Collector instance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/tpl" target="_blank" rel="noopener"&gt;tpl&lt;/a&gt; (version ~1.0.0) - Template Library&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following Argo CD Application will deploy the OTEL Operator:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
name: setup-otel-operator
namespace: openshift-gitops
spec:
destination:
name: in-cluster &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
namespace: openshift-opentelemetry-operator &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
info:
- name: Description
value: ApplicationSet that Deploys on Management Cluster Configuration (using Git Generator)
project: in-cluster &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
source:
path: clusters/management-cluster/setup-otel-operator &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
repoURL: &amp;#39;https://github.com/tjungbauer/openshift-clusterconfig-gitops&amp;#39; &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
targetRevision: main
syncPolicy:
retry:
backoff:
duration: 5s
factor: 2
maxDuration: 3m
limit: 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Target cluster, here the local cluster&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Namespace of the target cluster, here the Operator will be installed.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Project of the target cluster&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Path to the Git repository&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;URL to the Git repository&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This will create the Argo CD Application that can be synchronized with the cluster:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/otel-argocd.png?width=640px" alt="OTEL Deployment via Argo CD"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_complete_values_file"&gt;Complete values file&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To see the whole file expand the code:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
otel: &amp;amp;channel-otel stable
otel-namespace: &amp;amp;otel-namespace openshift-opentelemetry-operator
######################################
# SUBCHART: helper-operator
# Operators that shall be installed.
######################################
helper-operator:
operators:
opentelemetry-product:
enabled: true
namespace:
name: *otel-namespace
create: true
subscription:
channel: *channel-otel
approval: Automatic
operatorName: opentelemetry-product
source: redhat-operators
sourceNamespace: openshift-marketplace
operatorgroup:
create: true
notownnamespace: true
########################################
# SUBCHART: helper-status-checker
# Verify the status of a given operator.
########################################
helper-status-checker:
enabled: true
approver: false
checks:
- operatorName: opentelemetry-product
namespace:
name: *otel-namespace
syncwave: 1
serviceAccount:
name: &amp;#34;status-checker-otel&amp;#34;
rh-build-of-opentelemetry:
#########################################################################################
# namespace ... disabled here, since we deployed it via Tempo already
#########################################################################################
namespace:
name: tempostack
create: false
#########################################################################################
# OPENTELEMETRY COLLECTOR - Production Configuration
#########################################################################################
collector:
enabled: true
name: otel
mode: deployment
replicas: 1
serviceAccount: otel-collector
managementState: managed
resources: {}
tolerations: []
config:
connectors:
routing/traces:
default_pipelines:
- traces/Default
error_mode: ignore
table:
- pipelines:
- traces/tenantA
statement: &amp;#39;route() where attributes[&amp;#34;k8s.namespace.name&amp;#34;] == &amp;#34;team-a&amp;#34;&amp;#39;
- pipelines:
- traces/tenantB
statement: &amp;#39;route() where attributes[&amp;#34;k8s.namespace.name&amp;#34;] == &amp;#34;team-b&amp;#34;&amp;#39;
- pipelines:
- traces/tenantC
statement: &amp;#39;route() where attributes[&amp;#34;k8s.namespace.name&amp;#34;] == &amp;#34;team-c&amp;#34;&amp;#39;
- pipelines:
- traces/tenantD
statement: &amp;#39;route() where attributes[&amp;#34;k8s.namespace.name&amp;#34;] == &amp;#34;team-d&amp;#34;&amp;#39;
- pipelines:
- traces/tenantX
statement: &amp;#39;route() where attributes[&amp;#34;k8s.namespace.name&amp;#34;] == &amp;#34;mockbin-1&amp;#34;&amp;#39;
receivers:
# OTLP receivers for traces, metrics, and logs
otlp:
protocols:
grpc:
endpoint: 0.0.0.0:4317
http:
endpoint: 0.0.0.0:4318
# Jaeger receiver (if migrating from Jaeger)
jaeger:
protocols:
grpc:
endpoint: 0.0.0.0:14250
thrift_http:
endpoint: 0.0.0.0:14268
processors:
batch:
send_batch_size: 10000
timeout: 10s
k8sattributes: {}
memory_limiter:
check_interval: 1s
limit_percentage: 75
spike_limit_percentage: 15
resourcedetection:
detectors:
- openshift
timeout: 2s
exporters:
otlp/tenantX:
auth:
authenticator: bearertokenauth
endpoint: &amp;#39;tempo-simplest-gateway:8090&amp;#39;
headers:
X-Scope-OrgID: tenantX
tls:
ca_file: /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
insecure: true
insecure_skip_verify: true
server_name_override: tempo-simplest-gateway.tempostack.svc.cluster.local
otlp:
auth:
authenticator: bearertokenauth
endpoint: &amp;#39;tempo-simplest-gateway:8090&amp;#39;
headers:
X-Scope-OrgID: prod
tls:
ca_file: /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
insecure: true
insecure_skip_verify: true
server_name_override: tempo-simplest-gateway.tempostack.svc.cluster.local
otlp/tenantA:
auth:
authenticator: bearertokenauth
endpoint: &amp;#39;tempo-simplest-gateway:8090&amp;#39;
headers:
X-Scope-OrgID: tenantA
tls:
ca_file: /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
insecure: true
insecure_skip_verify: true
server_name_override: tempo-simplest-gateway.tempostack.svc.cluster.local
otlp/tenantB:
auth:
authenticator: bearertokenauth
endpoint: &amp;#39;tempo-simplest-gateway:8090&amp;#39;
headers:
X-Scope-OrgID: tenantB
tls:
ca_file: /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
insecure: true
insecure_skip_verify: true
server_name_override: tempo-simplest-gateway.tempostack.svc.cluster.local
otlp/Default:
auth:
authenticator: bearertokenauth
endpoint: &amp;#39;tempo-simplest-gateway:8090&amp;#39;
headers:
X-Scope-OrgID: dev
tls:
ca_file: /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
insecure: true
insecure_skip_verify: true
server_name_override: tempo-simplest-gateway.tempostack.svc.cluster.local
extensions:
bearertokenauth:
filename: /var/run/secrets/kubernetes.io/serviceaccount/token
service:
extensions:
- bearertokenauth
pipelines:
traces/Default:
exporters:
- otlp/Default
receivers:
- routing/traces
traces/in:
exporters:
- routing/traces
processors:
- resourcedetection
- k8sattributes
- memory_limiter
- batch
receivers:
- otlp
traces/tenantA:
exporters:
- otlp/tenantA
receivers:
- routing/traces
traces/tenantB:
exporters:
- otlp/tenantB
receivers:
- routing/traces&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_the_central_opentelemetry_collector"&gt;The Central OpenTelemetry Collector&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;With this deployment a single Pod (because we have set the replica count to 1) is running in the namespace &lt;strong&gt;tempostack&lt;/strong&gt; with the name &lt;strong&gt;otel-collector&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc get pods -n tempostack | grep otel-col
otel-collector-75f8794dc6-rbq26 1/1 Running 0 52m&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_stay_tuned"&gt;Stay Tuned&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The next article will cover deploying an example application (Mockbin) and configuring &lt;strong&gt;Local OpenTelemetry Collectors&lt;/strong&gt; in application namespaces to send traces to this Central Collector. Weâ€™ll also demonstrate how the namespace-based routing automatically directs traces to the correct TempoStack tenants.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>The Hitchhiker's Guide to Observability - Example Applications - Part 4</title><link>https://blog.stderr.at/openshift-platform/observability/observability/2025-11-26-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part4/</link><pubDate>Wed, 26 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/observability/observability/2025-11-26-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part4/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;With the &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/"&gt;architecture defined&lt;/a&gt;, &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/"&gt;TempoStack deployed&lt;/a&gt;, and the &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-25-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part3/"&gt;Central Collector configured&lt;/a&gt;, weâ€™re now ready to complete the distributed tracing pipeline. Itâ€™s time to deploy real applications and see traces flowing through the entire system!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In this fourth installment, weâ€™ll focus on the &lt;strong&gt;application layer&lt;/strong&gt; - deploying &lt;strong&gt;Local OpenTelemetry Collectors&lt;/strong&gt; in team namespaces and configuring example applications to generate traces. Youâ€™ll see how applications automatically get enriched with Kubernetes metadata, how namespace-based routing directs traces to the correct TempoStack tenants, and how the entire two-tier architecture comes together.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Weâ€™ll deploy an example application (&lt;strong&gt;Mockbin&lt;/strong&gt;) into two different namespaces. Together with the application, we will deploy a local OpenTelemetry Collector. One with sidecar mode, one with deployment mode.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Youâ€™ll learn how to configure local collectors to add namespace attributes, forward traces to the central collector, and verify that traces appear in the UI with full Kubernetes context.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;By the end of this article, youâ€™ll have a fully functional distributed tracing system with applications generating traces, local collectors forwarding them, and the central collector routing everything to the appropriate TempoStack tenants. Letâ€™s bring this architecture to life!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_application_mockbin_step_by_step_implementation"&gt;Application Mockbin - Step-by-Step Implementation&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_prerequisites"&gt;Prerequisites&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Before starting, ensure you have the following Systems or Operators installed (used Operator versions in this article):&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;OpenShift or Kubernetes cluster (OpenShift v4.20)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Red Hat build of OpenTelemetry installed (v0.135.0-1)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tempo Operator installed (v0.18.0-2)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;S3-compatible storage (for TempoStack, based on OpenShift Data Foundation)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cluster Observability Operator (v1.3.0) for now this Operator is only used to extend the OpenShift UI with the tracing UI)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_mockbin_application_introduction"&gt;Mockbin Application Introduction&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Mockbin Application was created and provided by &lt;a href="https://www.linkedin.com/in/michaela-lang-900603b9/" target="_blank" rel="noopener"&gt;Michaela Lang&lt;/a&gt;. She is doing a lot of testings with Service Mesh and Observability. I forked the source code for the Mockbin application to &lt;a href="https://github.com/tjungbauer/mockbin" target="_blank" rel="noopener"&gt;Mockbin&lt;/a&gt; and created an images to test everything at: &lt;a href="https://quay.io/repository/tjungbau/mockbin?tab=tags" target="_blank" rel="noopener"&gt;Mockbin Image&lt;/a&gt;. Mockbin is a simple, OpenTelemetry-instrumented HTTP testing service built with Pythonâ€™s &lt;code&gt;aiohttp&lt;/code&gt; async framework. It serves as both a demonstration tool and a practical testing service for distributed tracing infrastructure.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_why_mockbin"&gt;Why Mockbin?&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Unlike simple &amp;#34;hello world&amp;#34; examples, Mockbin demonstrates &lt;strong&gt;real-world OpenTelemetry implementation patterns&lt;/strong&gt; that you can apply to your own Python applications. It shows how to properly instrument an async Python web service with full observability capabilities.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_key_features"&gt;Key Features&lt;/h3&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Full OpenTelemetry Integration&lt;/strong&gt;: Implements the complete observability stack - traces, metrics, and logs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Automatic Instrumentation&lt;/strong&gt;: Uses OpenTelemetryâ€™s auto-instrumentation for &lt;code&gt;aiohttp&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Manual Span Creation&lt;/strong&gt;: Demonstrates how to create custom spans with attributes and events&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Trace Context Propagation&lt;/strong&gt;: Supports both W3C Trace Context (&lt;code&gt;traceparent&lt;/code&gt;) and Zipkin B3 (&lt;code&gt;x-b3-*&lt;/code&gt;) formats&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OTLP Export&lt;/strong&gt;: Sends all telemetry data via OpenTelemetry Protocol (OTLP) to collectors&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prometheus Metrics with Exemplars&lt;/strong&gt;: Links high-cardinality traces to aggregated metrics&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Structured Logging via OTLP&lt;/strong&gt;: Logs are automatically correlated with traces&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Nested Spans&lt;/strong&gt;: Creates parent-child span relationships for complex operations&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Exception Recording&lt;/strong&gt;: Captures and records exceptions with full stack traces&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_technical_architecture"&gt;Technical Architecture&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The application consists of several Python modules:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;proxy.py&lt;/code&gt;&lt;/strong&gt;: Main application with HTTP endpoint handlers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;tracing.py&lt;/code&gt;&lt;/strong&gt;: OpenTelemetry configuration and trace context management&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;promstats.py&lt;/code&gt;&lt;/strong&gt;: Prometheus metrics collection with trace exemplars&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;logfilter.py&lt;/code&gt;&lt;/strong&gt;: OTLP logging with automatic trace correlation&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_http_endpoints_for_testing"&gt;HTTP Endpoints for Testing&lt;/h3&gt;
&lt;table class="tableblock frame-all grid-all fit-content"&gt;
&lt;colgroup&gt;
&lt;col/&gt;
&lt;col/&gt;
&lt;col/&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class="tableblock halign-left valign-top"&gt;Endpoint&lt;/th&gt;
&lt;th class="tableblock halign-left valign-top"&gt;Purpose&lt;/th&gt;
&lt;th class="tableblock halign-left valign-top"&gt;Use Case&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;/&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Echo service - returns headers and environment variables&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Basic trace generation&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;/logging/*&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Generates multiple log entries linked to trace&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Demonstrate trace-to-log correlation&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;/proxy/*&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Chains multiple HTTP requests with nested spans&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Test distributed trace propagation&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;/exception/{status}&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Intentionally raises exception and returns custom status code&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Test error trace capture&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;/webhook/alert-receiver&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Converts AlertManager webhooks to traces&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Alert-to-trace correlation&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;/outlier&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Enables/disables failure mode (PUT/DELETE)&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Test Service Mesh outlier detection&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;/metrics&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Exposes Prometheus metrics&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Metrics scraping&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;/health&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Health check endpoint&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Kubernetes liveness/readiness probes&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_opentelemetry_implementation_highlights"&gt;OpenTelemetry Implementation Highlights&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The application demonstrates several critical OpenTelemetry patterns:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Middleware-Based Instrumentation:&lt;/strong&gt;
Every HTTP request is automatically wrapped in a trace span via &lt;code&gt;aiohttp&lt;/code&gt; middleware, capturing request metadata (method, URL, headers, etc.) as span attributes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Context Propagation:&lt;/strong&gt;
Incoming requests have their trace context extracted from HTTP headers, and outgoing requests inject the context to maintain trace continuity across service boundaries.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Resource Attributes:&lt;/strong&gt;
Service metadata (name, namespace, version) is attached to all telemetry data for proper identification in the backend.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Manual Span Creation:&lt;/strong&gt;
Shows how to create nested spans for complex operations, add custom attributes, record events, and set span status (OK/ERROR).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_configuration_via_environment_variables"&gt;Configuration via Environment Variables&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The application is configured entirely through environment variables, making it easy to adapt to different environments, for example:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;# OpenTelemetry Configuration
OTEL_EXPORTER_OTLP_ENDPOINT=http://otelcol-collector:4317
OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=http://otelcol-collector:4317
OTEL_SPAN_SERVICE=mockbin&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
We will test OpenTelemetry Collector in deployment mode and in sidecar mode. When using sidecar mode, then the sidecar will try to automatically inject the OpenTelemetry Collector into the pod.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;hr/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_deploy_mockbin_application_opentelemetry_collector_team_a"&gt;Deploy Mockbin Application &amp;amp; OpenTelemetry Collector - team-a&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s start with the deployment of the Mockbin Application and the OpenTelemetry Collector in the team-a namespace. Here we will use the OpenTelemetry Collector in deployment mode.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The deployment in sidecar mode will be done in the team-b namespace and is almost the same, just with the difference that we will use a different mode in the OTC resource and that we do not need to take care of the environment variables for the OTC.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_1a_use_kustomize_manually"&gt;Step 1a: Use Kustomize Manually&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;You can deploy the Mockbin Application and the OpenTelemetry Collector manually using Kustomize or by using a GitOps tool like ArgoCD (preferred).
In the repository &lt;a href="https://github.com/tjungbauer/mockbin" target="_blank" rel="noopener"&gt;Mockbin&lt;/a&gt; your will find the sub-folder &lt;code&gt;k8s&lt;/code&gt; with the Kustomize files for the Mockbin Application and the OpenTelemetry Collector.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;You can manually deploy the application:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_step_1a_1_create_namespace_team_a"&gt;Step 1a.1: Create Namespace team-a&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;First create the namespace team-a.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: v1
kind: Namespace
metadata:
name: team-a &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Namespace name&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_step_1a_2_deploy_the_mockbin_application"&gt;Step 1a.2: Deploy the Mockbin Application&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Use the following command to deploy the application (assuming you cloned the repository to your local machine):&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;kustomize build . | kubectl apply -f -&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This will deploy the required resources:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ServiceAccount&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Service&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Route&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deployment using the image: quay.io/tjungbau/mockbin:1.8.1&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
It is possible to kustomize the deployment by modifying the &lt;code&gt;kustomization.yaml&lt;/code&gt; file. For example, you can change the image, or change from Route to Ingress object.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_1b_use_argocd"&gt;Step 1b: Use ArgoCD&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Above can be achieved by using ArgoCD, which allows a more declarative approach. Create the following ArgoCD Application:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
name: mockbin-team-a &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
namespace: openshift-gitops &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
spec:
destination:
namespace: team-a &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
server: &amp;#39;https://kubernetes.default.svc&amp;#39; &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
info:
- name: Description
value: Deploy Mockbin in team-a namespace
project: in-cluster
source:
path: k8s/ &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
repoURL: &amp;#39;https://github.com/tjungbauer/mockbin&amp;#39; &lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;(6)&lt;/b&gt;
targetRevision: main &lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;(7)&lt;/b&gt;
syncPolicy:
syncOptions:
- CreateNamespace=true &lt;i class="conum" data-value="8"&gt;&lt;/i&gt;&lt;b&gt;(8)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Name of the Argo CD Application resource.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Namespace of the ArgoCD Application resource, here it is &lt;strong&gt;openshift-gitops&lt;/strong&gt;. In your environment it might be different.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Namespace of the target application&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes API server URL. Here it is the local cluster where Argo CD is hosted.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Path to the Kustomize files&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;URL to the Git repository&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;7&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Target revision&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="8"&gt;&lt;/i&gt;&lt;b&gt;8&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Create the namespace if it does not exist&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Argo CD will leverage Kustomize to render the resources. It is the same as you would do manually, with the exception that the Namespace will be created automatically. The approach above will deploy the required resources:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Namespace&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ServiceAccount&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Service&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Route&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deployment&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In the Argo CD UI you can see the application and the resources that have been deployed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/mockbin-argocd.png?width=640px" alt="Argo CD Application for Mockbin"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_2_verify_the_deployment"&gt;Step 2: Verify the Deployment&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Verify the Pods, Service and Route of the Mockbin Application:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;kubectl get pods -n team-a
NAME READY STATUS RESTARTS AGE
pod/mockbin-c4587558b-ftvsd 2/2 Running 0 3d15h
pod/mockbin-c4587558b-zrxgc 2/2 Running 0 3d15h
kubectl get services -n team-a
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/mockbin ClusterIP 172.30.202.223 &amp;lt;none&amp;gt; 8080/TCP 3d16h
kubectl get routes -n team-a
NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD
route.route.openshift.io/mockbin mockbin-team-a.apps.ocp.aws.ispworld.at mockbin http edge/Redirect None&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Access the route URL to see the Mockbin Application:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;curl -k https://mockbin-team-a.apps.ocp.aws.ispworld.at&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This will return a response looking like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-json" data-lang="json"&gt;{&amp;#34;headers&amp;#34;: {&amp;#34;User-Agent&amp;#34;: &amp;#34;curl/8.7.1&amp;#34;, &amp;#34;Accept&amp;#34;: &amp;#34;*/*&amp;#34;, &amp;#34;Host&amp;#34;: &amp;#34;mockbin-team-a.apps.ocp.aws.ispworld.at&amp;#34;, .... lot&amp;#39;s of header stuff&amp;#34;}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_3_deploy_the_local_opentelemetry_collector_mode_deployment"&gt;Step 3: Deploy the Local OpenTelemetry Collector - Mode Deployment&lt;/h3&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
All steps below can be applied using GitOps again, using the Chart &lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/rh-build-of-opentelemetry" target="_blank" rel="noopener"&gt;rh-build-of-opentelemetry&lt;/a&gt;. An example of a GitOps configuration (for the Central Collector) can be found at &lt;a href="https://github.com/tjungbauer/openshift-clusterconfig-gitops/tree/main/clusters/management-cluster/setup-otel-operator" target="_blank" rel="noopener"&gt;Setup OTEL Operator&lt;/a&gt;.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_create_a_serviceaccount_for_the_opentelemetry_collector"&gt;Create a ServiceAccount for the OpenTelemetry Collector&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Let us first create a ServiceAccount for the OpenTelemetry Collector.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: v1
kind: ServiceAccount
metadata:
name: otelcol-agent
namespace: team-a&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_create_the_opentelemetry_collector_resource"&gt;Create the OpenTelemetry Collector Resource&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Create the following OpenTelemetry Collector Resource. The mode shall be &amp;#34;deployment&amp;#34; The serviceAccount shall be the one we created in the previous step, and the namespace is &amp;#34;team-a&amp;#34;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
name: otelcol-agent &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
namespace: team-a &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
spec:
mode: deployment &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
replicas: 1 &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
serviceAccount: otelcol-agent &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
config:
# Receivers - accept traces from applications
receivers: &lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;(6)&lt;/b&gt;
otlp:
protocols:
grpc:
endpoint: 0.0.0.0:4317
http:
endpoint: 0.0.0.0:4318
# Processors
processors: &lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;(7)&lt;/b&gt;
# Add namespace identifier
attributes:
actions:
- action: insert
key: k8s.namespace.name
value: team-a
# Batch for efficiency
batch: {}
# Exporters - forward to central collector
exporters:
otlp/central: &lt;i class="conum" data-value="8"&gt;&lt;/i&gt;&lt;b&gt;(8)&lt;/b&gt;
endpoint: otel-collector.tempostack.svc.cluster.local:4317
tls:
insecure: true
# Service pipeline
service:
pipelines: &lt;i class="conum" data-value="9"&gt;&lt;/i&gt;&lt;b&gt;(9)&lt;/b&gt;
traces:
receivers:
- otlp
processors:
- batch
- attributes
exporters:
- otlp/central&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Name of the OpenTelemetry Collector Resource.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Namespace *team-a*of the OpenTelemetry Collector Resource.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Mode &lt;strong&gt;deployment&lt;/strong&gt; of the OpenTelemetry Collector Resource.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Number of replicas of the OpenTelemetry Collector Resource. For HA setup increase the number of replicas.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;ServiceAccount of the OpenTelemetry Collector Resource, that was created in the previous step.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Receivers of the OpenTelemetry Collector Resource. We will receive traces using the OTLP protocol on gRPC and HTTP.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;7&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Processors of the OpenTelemetry Collector Resource. We will add the namespace identifier as an attribute and batch the traces for efficiency.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="8"&gt;&lt;/i&gt;&lt;b&gt;8&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Exporters of the OpenTelemetry Collector Resource. We will forward the traces to the Central Collector.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="9"&gt;&lt;/i&gt;&lt;b&gt;9&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Service pipeline of the OpenTelemetry Collector Resource.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Since we choose the &lt;strong&gt;deployment&lt;/strong&gt; mode, the OpenTelemetry Collector will be deployed as a Deployment resource.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;oc deployment all -n team-a
NAME READY UP-TO-DATE AVAILABLE AGE
deployment.apps/otelcol-agent-collector 1/1 1 1 3d2h&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_4_cool_and_now_what_environment_variables"&gt;Step 4: Cool, and now what? - Environment Variables&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;If you came that far, then you have deployment TempoStack, the Central Collector, the Mockbin Application and the Local Collector.
However, you will not receive any traces yet. We need to tell the Mockbin Application WHERE to send the traces.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To do this, we need to set the environment variables for the Mockbin Deployment resource. Edit the Deployment resource and add the following environment variables:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt; env:
- name: OTEL_EXPORTER_OTLP_ENDPOINT
value: &amp;#39;http://otelcol-agent-collector:4317&amp;#39; &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
- name: OTEL_EXPORTER_OTLP_PROTOCOL
value: grpc &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
- name: OTEL_SERVICE_NAME
value: mockbin &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The endpoint (service) of the Local Collector.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The protocol we would like to use.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The service name of the Mockbin Application.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This will trigger a restart of the Deployment. Once done, the environment variables should show up:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;curl -k https://mockbin-team-a.apps.ocp.aws.ispworld.at
{&amp;#34;headers&amp;#34;: {&amp;#34;...., &amp;#34;OTEL_EXPORTER_OTLP_ENDPOINT&amp;#34;: &amp;#34;http://otelcol-agent-collector:4317&amp;#34;, &amp;#34;OTEL_EXPORTER_OTLP_PROTOCOL&amp;#34;: &amp;#34;grpc&amp;#34;, ....&amp;#34;}}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This will now start sending traces to the Local Collector and from there to the Central Collector.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_traces_traces_traces"&gt;Traces, Traces, Traces&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now we have a working setup. We have a Central Collector, a Local Collector and a Mockbin Application.
Our application is permanently sending requests to the Local Collector.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We can review the traces in the OpenShift UI: &lt;strong&gt;Observe &amp;gt; Traces&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Select the tempostack instance and select the tenant &lt;strong&gt;tenantA&lt;/strong&gt;. If everything is working, you should see traces from the Mockbin Application.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock warning"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-warning" title="Warning"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
If no traces show up, then most probably the RBAC rules are not configured correctly or the environment variables are not set correctly. You can check the logs of the OTC pods for errors.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/mockbin-1-traces.png?width=1200px" alt="Mock Team A - Traces"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_deploy_mockbin_application_opentelemetry_collector_team_b"&gt;Deploy Mockbin Application &amp;amp; OpenTelemetry Collector - team-b&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Let us now deploy the Mockbin Application and the OpenTelemetry Collector in the team-b namespace. Here we will use the OpenTelemetry Collector in sidecar mode.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_1_deploy_the_application_using_argo_cd"&gt;Step 1: Deploy the Application using Argo CD&lt;/h3&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_1b_use_argocd_2"&gt;Step 1b: Use ArgoCD&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Above can be achieved by using ArgoCD, which allows a more declarative approach. Create the following ArgoCD Application (This is the same approach as above)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
name: mockbin-team-b &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
namespace: openshift-gitops &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
spec:
destination:
namespace: team-b &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
server: &amp;#39;https://kubernetes.default.svc&amp;#39; &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
info:
- name: Description
value: Deploy Mockbin in team-b namespace
project: in-cluster
source:
path: k8s/ &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
repoURL: &amp;#39;https://github.com/tjungbauer/mockbin&amp;#39; &lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;(6)&lt;/b&gt;
targetRevision: main &lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;(7)&lt;/b&gt;
syncPolicy:
syncOptions:
- CreateNamespace=true &lt;i class="conum" data-value="8"&gt;&lt;/i&gt;&lt;b&gt;(8)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Name of the Argo CD Application resource.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Namespace of the ArgoCD Application resource, here it is &lt;strong&gt;openshift-gitops&lt;/strong&gt;. In your environment it might be different.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Namespace of the target application&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Kubernetes API server URL. Here it is the local cluster where Argo CD is hosted.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Path to the Kustomize files&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;URL to the Git repository&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;7&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Target revision&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="8"&gt;&lt;/i&gt;&lt;b&gt;8&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Create the namespace if it does not exist&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_2_deploy_the_local_opentelemetry_collector_sidecar_deployment"&gt;Step 2: Deploy the Local OpenTelemetry Collector - Sidecar Deployment&lt;/h3&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
All steps below can be applied using GitOps again, using the Chart &lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/rh-build-of-opentelemetry" target="_blank" rel="noopener"&gt;rh-build-of-opentelemetry&lt;/a&gt;. An example of a GitOps configuration (for the Central Collector) can be found at &lt;a href="https://github.com/tjungbauer/openshift-clusterconfig-gitops/tree/main/clusters/management-cluster/setup-otel-operator" target="_blank" rel="noopener"&gt;Setup OTEL Operator&lt;/a&gt;.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_create_a_serviceaccount_for_the_opentelemetry_collector_2"&gt;Create a ServiceAccount for the OpenTelemetry Collector&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Let us first create a ServiceAccount for the OpenTelemetry Collector.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: v1
kind: ServiceAccount
metadata:
name: otelcol-agent
namespace: team-b&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_create_the_opentelemetry_collector_resource_2"&gt;Create the OpenTelemetry Collector Resource&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Create the following OpenTelemetry Collector Resource. The mode shall be &amp;#34;sidecar&amp;#34; The serviceAccount shall be the one we created in the previous step, and the namespace is &amp;#34;team-b&amp;#34;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
name: otelcol-agent &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
namespace: team-b &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
spec:
mode: sidecar &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
replicas: 1 &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
serviceAccount: otelcol-agent &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
config:
# Receivers - accept traces from applications
receivers: &lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;(6)&lt;/b&gt;
otlp:
protocols:
grpc:
endpoint: 0.0.0.0:4317
http:
endpoint: 0.0.0.0:4318
# Processors
processors: &lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;(7)&lt;/b&gt;
# Add namespace identifier
attributes:
actions:
- action: insert
key: k8s.namespace.name
value: team-b
# Batch for efficiency
batch: {}
# Exporters - forward to central collector
exporters:
otlp/central: &lt;i class="conum" data-value="8"&gt;&lt;/i&gt;&lt;b&gt;(8)&lt;/b&gt;
endpoint: otel-collector.tempostack.svc.cluster.local:4317
tls:
insecure: true
# Service pipeline
service:
pipelines: &lt;i class="conum" data-value="9"&gt;&lt;/i&gt;&lt;b&gt;(9)&lt;/b&gt;
traces:
receivers:
- otlp
processors:
- batch
- attributes
exporters:
- otlp/central&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Name of the OpenTelemetry Collector Resource.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Namespace *team-a*of the OpenTelemetry Collector Resource.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Mode &lt;strong&gt;sidecar&lt;/strong&gt; of the OpenTelemetry Collector Resource.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Number of replicas of the OpenTelemetry Collector Resource. For HA setup increase the number of replicas.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;ServiceAccount of the OpenTelemetry Collector Resource, that was created in the previous step.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Receivers of the OpenTelemetry Collector Resource. We will receive traces using the OTLP protocol on gRPC and HTTP.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;7&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Processors of the OpenTelemetry Collector Resource. We will add the namespace identifier as an attribute and batch the traces for efficiency.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="8"&gt;&lt;/i&gt;&lt;b&gt;8&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Exporters of the OpenTelemetry Collector Resource. We will forward the traces to the Central Collector.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="9"&gt;&lt;/i&gt;&lt;b&gt;9&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Service pipeline of the OpenTelemetry Collector Resource.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_3_enable_sidecar_injection_for_team_b"&gt;Step 3: Enable Sidecar Injection for Team-B&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The big advantage of the &lt;strong&gt;sidecar&lt;/strong&gt; mode is that the OpenTelemetry Collector is deployed as a sidecar container in the same pod as the application. This happens fully automatically, means you do not need to set any extra environment variables or anything like that.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To enable the sidecar injection, we need to add the following annotation to the namespace. This will automatically inject the OpenTelemetry Collector as a sidecar container into the ALL pods of the namespace:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Do not forget this to add it into GitOps process :)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;First verify the current state of the deployment:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc deployment all -n team-b
NAME READY UP-TO-DATE AVAILABLE AGE
deployment.apps/otelcol-agent-collector 1/1 1 1 3d2h&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As you can see one container out of 1 is running (Read 1/1)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now lets modify the namespace to enable the sidecar injection:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: v1
kind: Namespace
metadata:
name: team-b
annotations:
sidecar.opentelemetry.io/inject: &amp;#34;true&amp;#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
This can also be done by annotate the Deployment resource. In this case, the OpenTelemetry Collector will be injected into the specific pod.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Restart the deployment to apply the changes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc rollout restart deployment/mockbin -n team-b
deployment.apps/mockbin restarted&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now, when you check again, the sidecar container should be injected into the pods. The Deployment has now 2/2 containers ready:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;NAME READY UP-TO-DATE AVAILABLE AGE
deployment.apps/mockbin 2/2 1 2 3m53s&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_traces_traces_traces_again"&gt;Traces, Traces, Traces - Again&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Again, we can review the traces in the OpenShift UI: &lt;strong&gt;Observe &amp;gt; Traces&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Select the tempostack instance and select the tenant &lt;strong&gt;tenantB&lt;/strong&gt; this time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/mockbin-2-traces.png" alt="Mock Team B - Traces"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_argo_cd_stays_in_progressing_state"&gt;Argo CD Stays in Progressing State&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;If you have deployed the Mockbin Application and the OpenTelemetry Collector &lt;strong&gt;as a sidecar&lt;/strong&gt; deployment in the team-b namespace, you might notice that the Argo CD Application stays in the &lt;strong&gt;Progressing&lt;/strong&gt; state.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This is because the OpenTelemetry Collector does not really return a state. The status is simply:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;status:
version: 0.140.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Nothing else â€¦â€‹ thats too less for Argo CD to work with.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In order to fix this, we need to add a custom health check to the Argo CD instance.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Argo CD instance knows the following part:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: argoproj.io/v1beta1
kind: ArgoCD
metadata:
name: openshift-gitops
namespace: openshift-gitops
spec:
[...]
resourceHealthChecks: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
- check: |
hs = {}
if obj.status ~= nil then
if obj.status.version ~= nil and obj.status.version ~= &amp;#34;&amp;#34; then
hs.status = &amp;#34;Healthy&amp;#34;
hs.message = &amp;#34;Running version &amp;#34; .. obj.status.version
else
hs.status = &amp;#34;Progressing&amp;#34;
hs.message = &amp;#34;Waiting for version report&amp;#34;
end
else
hs.status = &amp;#34;Progressing&amp;#34;
hs.message = &amp;#34;Status not available&amp;#34;
end
return hs
group: opentelemetry.io
kind: OpenTelemetryCollector
[...]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Custom health check for the OpenTelemetry Collector.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Above will end up in a ConfigMap that is managed by the OpenShift GitOps operator.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This will make Argo CD happy and the Application will be in the &lt;strong&gt;Progressing&lt;/strong&gt; state.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>The Hitchhiker's Guide to Observability - Understanding Traces - Part 5</title><link>https://blog.stderr.at/openshift-platform/observability/observability/2025-11-27-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part5/</link><pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/observability/observability/2025-11-27-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part5/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;With the &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-23-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part1/"&gt;architecture established&lt;/a&gt;, &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-24-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part2/"&gt;TempoStack deployed&lt;/a&gt;, &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-25-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part3/"&gt;the Central Collector configured&lt;/a&gt;, and &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-26-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part4/"&gt;applications generating traces&lt;/a&gt;, itâ€™s time to take a step back and understand what weâ€™re actually building. Before you deploy more applications and start troubleshooting performance issues, you need to &lt;strong&gt;understand how to read and interpret distributed traces&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s decode the matrix of distributed tracing!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_understanding_distributed_traces"&gt;Understanding Distributed Traces&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="admonitionblock warning"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-warning" title="Warning"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
This article is not a comprehensive guide to distributed tracing. It is a quick overview to understand the building blocks of a trace.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We need to understand the building blocks of a trace to be able to interpret them in the UI.
As the UI, we will use the integrated tracing interface inside OpenShift.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Also compare with: &lt;a href="https://grafana.com/docs/tempo/latest/introduction/trace-structure/" target="_blank" rel="noopener"&gt;Trace Structure in Tempo Documentation&lt;/a&gt;.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_what_you_can_do_with_traces"&gt;What You Can Do With Traces&lt;/h3&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Performance Optimization&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Identify slow operations (database queries, API calls)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Find bottlenecks in the critical path&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compare performance across versions/deployments&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Root Cause Analysis&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Trace errors back to their origin&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;See the complete context of a failure&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Understand cascading failures&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Service Dependencies&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Visualize your service architecture&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Identify tightly coupled services&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plan capacity and scaling&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;User Experience Monitoring&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Track end-to-end latency for user actions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Identify outliers and edge cases&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Correlate user complaints with actual traces&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Capacity Planning&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Understand resource usage patterns&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Identify underutilized or overloaded services&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Plan infrastructure scaling&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A/B Testing and Rollouts&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Compare performance between feature flags&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Verify canary deployments&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Measure impact of code changes&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_what_is_a_trace"&gt;What is a Trace?&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;A &lt;strong&gt;trace&lt;/strong&gt; represents the complete journey of a request as it flows through your system. Every service, database call, and external API interaction along the way will be tracked.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Key Characteristics:&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unique Trace ID&lt;/strong&gt;: Every trace has a globally unique identifier (128-bit or 64-bit)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Timeline&lt;/strong&gt;: Traces capture the temporal relationship between operations&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distributed Context&lt;/strong&gt;: Maintains continuity across service boundaries&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hierarchical Structure&lt;/strong&gt;: Organized as a tree of spans&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_what_is_a_span"&gt;What is a Span?&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;A &lt;strong&gt;span&lt;/strong&gt; is the fundamental unit of work in distributed tracing. It represents a single operation within a trace - such as handling an HTTP request, executing a database query, or calling an external service.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_span_components"&gt;Span Components&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following components of a span can be considered (among others):&lt;/p&gt;
&lt;/div&gt;
&lt;table class="tableblock frame-all grid-all stretch"&gt;
&lt;colgroup&gt;
&lt;col style="width: 25%;"/&gt;
&lt;col style="width: 75%;"/&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class="tableblock halign-left valign-top"&gt;Component&lt;/th&gt;
&lt;th class="tableblock halign-left valign-top"&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;&lt;strong&gt;Name&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Human-readable description of the operation (e.g., &amp;#34;GET /api/users&amp;#34;, &amp;#34;SELECT FROM orders&amp;#34;)&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;&lt;strong&gt;Trace ID&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Links this span to its parent trace&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;&lt;strong&gt;Span ID&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Unique identifier for this specific span&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;&lt;strong&gt;Start Time&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;When the operation began (nanosecond precision)&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;&lt;strong&gt;Duration&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;How long the operation took&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;&lt;strong&gt;Span Kind&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Type of span: SERVER, CLIENT, PRODUCER, CONSUMER, INTERNAL&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;&lt;strong&gt;Status&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class="tableblock halign-left valign-top"&gt;&lt;p class="tableblock"&gt;Operation outcome: OK, ERROR, UNSET&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_trace_tree_structure"&gt;Trace Tree Structure&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;A trace forms a &lt;strong&gt;directed acyclic graph (DAG)&lt;/strong&gt; - typically a tree structure where each span can have multiple children but only one parent.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Visual Example: Basic Trace Structure&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-mermaid" data-lang="mermaid"&gt;---
config:
theme: &amp;#39;neutral&amp;#39;
---
graph TD
%% Grouping everything as a Trace
subgraph Trace
direction TB
SpanA[Span A]
SpanB[Span B]
SpanC[Span C]
SpanD[Span D]
SpanE[Span E]
end
%% Quotes added to handle the curly braces
SpanA --&amp;gt;|&amp;#34;{Span context}&amp;#34;| SpanB
SpanA --&amp;gt;|&amp;#34;{Span context}&amp;#34;| SpanC
SpanC --&amp;gt;|&amp;#34;{Span context}&amp;#34;| SpanD
SpanC --&amp;gt;|&amp;#34;{Span context}&amp;#34;| SpanE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;What This Trace Structure Shows:&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Span A&lt;/strong&gt; is the &lt;strong&gt;root span&lt;/strong&gt; (parent of all other spans)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Span B&lt;/strong&gt; and &lt;strong&gt;Span C&lt;/strong&gt; are direct children of Span A (parallel operations)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Span D&lt;/strong&gt; and &lt;strong&gt;Span E&lt;/strong&gt; are children of Span C (sequential or parallel sub-operations)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;span context&lt;/strong&gt; is propagated from parent to child, maintaining trace continuity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This hierarchical structure allows you to understand the complete request flow&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s have a look at a real trace in the OpenShift UI under &lt;strong&gt;Observe &amp;gt; Traces&lt;/strong&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/trace-example.png" alt="Trace Example"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_span_attributes_adding_context"&gt;Span Attributes: Adding Context&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Attributes are key-value pairs that add semantic meaning to spans. OpenTelemetry defines &lt;strong&gt;semantic conventions&lt;/strong&gt; - standardized attribute names for common scenarios.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;HTTP Attributes:&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code&gt;http.method: &amp;#34;POST&amp;#34;
http.url: &amp;#34;https://api.example.com/checkout&amp;#34;
http.status_code: 200
http.user_agent: &amp;#34;Mozilla/5.0...&amp;#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Database Attributes:&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code&gt;db.system: &amp;#34;postgresql&amp;#34;
db.name: &amp;#34;orders&amp;#34;
db.statement: &amp;#34;SELECT * FROM orders WHERE user_id = $1&amp;#34;
db.connection_string: &amp;#34;postgresql://db.example.com:5432&amp;#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Kubernetes Attributes (added by k8sattributes processor):&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code&gt;k8s.namespace.name: &amp;#34;team-a&amp;#34;
k8s.pod.name: &amp;#34;checkout-service-7d8f9c-xyz12&amp;#34;
k8s.deployment.name: &amp;#34;checkout-service&amp;#34;
k8s.node.name: &amp;#34;worker-node-2&amp;#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Custom Business Attributes:&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code&gt;user.id: &amp;#34;12345&amp;#34;
order.id: &amp;#34;ORD-98765&amp;#34;
order.total: 149.99
payment.method: &amp;#34;credit_card&amp;#34;
inventory.items_count: 3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_span_events_timestamped_logs"&gt;Span Events: Timestamped Logs&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Events&lt;/strong&gt; are timestamped messages within a span that mark significant moments:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code&gt;Span: Process Payment
â”œâ”€ Event @ 10ms: &amp;#34;Payment request validated&amp;#34;
â”œâ”€ Event @ 50ms: &amp;#34;Calling payment gateway&amp;#34;
â”œâ”€ Event @ 750ms: &amp;#34;Payment gateway responded&amp;#34;
â””â”€ Event @ 760ms: &amp;#34;Payment confirmed&amp;#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Use Cases for Span Events:&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Debug checkpoints&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Exception details&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;State transitions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;External API interactions&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_span_status_and_error_handling"&gt;Span Status and Error Handling&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Spans have three status codes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;UNSET&lt;/strong&gt;: Default, operation completed (not necessarily successful)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OK&lt;/strong&gt;: Explicitly marked as successful&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ERROR&lt;/strong&gt;: Operation failed&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s call one of your application endpoints on the path &lt;strong&gt;/exception/500&lt;/strong&gt;. This will return a 500 status code and the span will be marked as ERROR.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;curl -X GET https://&amp;lt;YOUR-APPLICATION-URL&amp;gt;/exception/500&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now we can see the span in the trace with the error status. Note how the span is highlighted in red, indicating an error occurred:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/observability/images/observability/trace-error-handling.png" alt="Span Status"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>The Hitchhiker's Guide to Observability - Adding A New Tenant - Part 6</title><link>https://blog.stderr.at/openshift-platform/observability/observability/2025-11-28-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part6/</link><pubDate>Fri, 28 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/observability/observability/2025-11-28-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part6/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;While we have created our distributed tracing infrastructure, we created two tenants as an example. In this article, I will show you how to add a new tenant and which changes must be made in the TempoStack and the OpenTelemetry Collector.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This article was mainly created as a quick reference guide to see which changes must be made when adding new tenants.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_quick_checklist_for_adding_a_new_tenant"&gt;Quick Checklist for Adding a New Tenant&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Use this checklist to ensure you havenâ€™t missed any steps:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist checklist"&gt;
&lt;ul class="checklist"&gt;
&lt;li&gt;
&lt;p&gt;&lt;i class="fa fa-square-o"&gt;&lt;/i&gt; &lt;strong&gt;TempoStack&lt;/strong&gt;: Add tenant to &lt;code&gt;spec.tenants.authentication[]&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;i class="fa fa-square-o"&gt;&lt;/i&gt; &lt;strong&gt;ClusterRole (write)&lt;/strong&gt;: Add tenant name to &lt;code&gt;tempostack-traces-write&lt;/code&gt; resources&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;i class="fa fa-square-o"&gt;&lt;/i&gt; &lt;strong&gt;ClusterRole (read)&lt;/strong&gt;: Add tenant name to &lt;code&gt;tempostack-traces-reader&lt;/code&gt; resources&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;i class="fa fa-square-o"&gt;&lt;/i&gt; &lt;strong&gt;Central OTC Routing&lt;/strong&gt;: Add routing rule in &lt;code&gt;connectors.routing/traces.table[]&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;i class="fa fa-square-o"&gt;&lt;/i&gt; &lt;strong&gt;Central OTC Exporter&lt;/strong&gt;: Add &lt;code&gt;otlp/[tenant-name]&lt;/code&gt; exporter with correct &lt;code&gt;X-Scope-OrgID&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;i class="fa fa-square-o"&gt;&lt;/i&gt; &lt;strong&gt;Central OTC Pipeline&lt;/strong&gt;: Add &lt;code&gt;traces/[tenant-name]&lt;/code&gt; pipeline&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;i class="fa fa-square-o"&gt;&lt;/i&gt; &lt;strong&gt;Namespace&lt;/strong&gt;: Create namespace for the team&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;i class="fa fa-square-o"&gt;&lt;/i&gt; &lt;strong&gt;ServiceAccount&lt;/strong&gt;: Create ServiceAccount for local collector&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;i class="fa fa-square-o"&gt;&lt;/i&gt; &lt;strong&gt;Local OTC&lt;/strong&gt;: Deploy OpenTelemetryCollector in team namespace&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;i class="fa fa-square-o"&gt;&lt;/i&gt; &lt;strong&gt;Application&lt;/strong&gt;: Deploy application with OTEL configuration&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;i class="fa fa-square-o"&gt;&lt;/i&gt; &lt;strong&gt;Verify&lt;/strong&gt;: Check logs, generate traces, view in Jaeger UI&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
You do NOT need to create new ClusterRoles for each tenant. The &lt;code&gt;k8sattributes-otel-collector&lt;/code&gt; ClusterRole already grants the central collector cluster-wide read access to pods, namespaces, and replicasets across all namespaces. This single ClusterRole serves all tenants.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_common_mistakes_to_avoid"&gt;Common Mistakes to Avoid&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Mismatched Tenant Names&lt;/strong&gt;: Ensure the tenant name in TempoStack matches the &lt;code&gt;X-Scope-OrgID&lt;/code&gt; header in the exporter&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Wrong Namespace Attribute&lt;/strong&gt;: The routing rule matches on &lt;code&gt;k8s.namespace.name&lt;/code&gt; - ensure the local collector sets this correctly&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Forgot RBAC Update&lt;/strong&gt;: Without updating ClusterRoles, traces will be rejected&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Typo in Pipeline Names&lt;/strong&gt;: Pipeline names in the routing connector must match the actual pipeline definitions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Missing Exporter in Pipeline&lt;/strong&gt;: Each new pipeline must reference the correct exporter (e.g., &lt;code&gt;otlp/team-d&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_adding_a_new_tenant_team_d"&gt;Adding a New Tenant - team-d&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s add a new tenant called &amp;#34;team-d&amp;#34; with their own namespace, local collector, and route to TempoStack tenant &amp;#34;tenantD&amp;#34;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_1_update_tempostack_configuration"&gt;Step 1: Update TempoStack Configuration&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;If the tenant &amp;#34;tenantD&amp;#34; does not already exist in TempoStack, we need to add it to the tenants list.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Edit the TempoStack resource in the tempostack namespace and add the new tenant to the authentication list:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;spec:
tenants:
mode: openshift
authentication:
- tenantId: 1610b0c3-c509-4592-a256-a1871353dbfc
tenantName: tenantA
- tenantId: 1610b0c3-c509-4592-a256-a1871353dbfd
tenantName: tenantB
- tenantId: 1610b0c3-c509-4592-a256-a1871353dbfe
tenantName: tenantC
# Add new tenant
- tenantId: 1610b0c3-c509-4592-a256-a1871353dbff &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
tenantName: tenantD&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Our new tenantID with the tenantName &amp;#34;tenantD&amp;#34;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_2_update_rbac_permissions_write"&gt;Step 2: Update RBAC Permissions - write&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Update the ClusterRoles to grant write permissions for the new tenant. Edit the ClusterRole &lt;strong&gt;tempostack-traces-write&lt;/strong&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc edit clusterrole tempostack-traces-write&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;rules:
- verbs:
- create
apiGroups:
- tempo.grafana.com
resources:
- tenantA
- tenantB
- tenantC
- tenantD &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
resourceNames:
- traces&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Our new tenant with the tenantName &amp;#34;tenantD&amp;#34;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_3_update_rbac_permissions_read"&gt;Step 3: Update RBAC Permissions - read&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Update the ClusterRoles to grant read permissions for the new tenant. Edit the ClusterRole &lt;strong&gt;tempostack-traces-reader&lt;/strong&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc edit clusterrole tempostack-traces-reader&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;rules:
- verbs:
- get
apiGroups:
- tempo.grafana.com
resources:
- dev
- tenantA
- tenantB
- tenantC
- tenantD &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
resourceNames:
- traces&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Our new tenant with the tenantName &amp;#34;tenantD&amp;#34;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_4_update_central_opentelemetry_collector"&gt;Step 4: Update Central OpenTelemetry Collector&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The central collector needs configuration changes to route traces from the new namespace to the appropriate TempoStack tenant. Edit the OpenTelemetry Collector in the &lt;code&gt;tempostack&lt;/code&gt; namespace and add the new routing rule:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc edit otelcol otel -n tempostack&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_add_the_new_routing_rule"&gt;Add the New Routing Rule&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Add the new routing rule to the &lt;code&gt;connectors&lt;/code&gt; section:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;spec:
config:
connectors:
routing/traces:
default_pipelines:
- traces/Default
error_mode: ignore
table:
- statement: route() where attributes[&amp;#34;k8s.namespace.name&amp;#34;] == &amp;#34;team-a&amp;#34;
pipelines:
- traces/tenantA
- statement: route() where attributes[&amp;#34;k8s.namespace.name&amp;#34;] == &amp;#34;team-b&amp;#34;
pipelines:
- traces/tenantB
- statement: route() where attributes[&amp;#34;k8s.namespace.name&amp;#34;] == &amp;#34;team-c&amp;#34;
pipelines:
- traces/tenantC
# Add new routing rule
- statement: route() where attributes[&amp;#34;k8s.namespace.name&amp;#34;] == &amp;#34;team-d&amp;#34; &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
pipelines:
- traces/tenantD&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Our new tenant with the tenantName &amp;#34;tenantD&amp;#34;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_add_exporter_for_the_new_tenant"&gt;Add Exporter for the New Tenant&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Add the new exporter to the &lt;code&gt;exporters&lt;/code&gt; section:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;spec:
config:
exporters:
# ... existing exporters ...
# New tenant exporter
otlp/tenantD: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
endpoint: tempo-simplest-gateway:8090
auth:
authenticator: bearertokenauth
headers:
X-Scope-OrgID: tenantD &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
tls:
ca_file: /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt
insecure_skip_verify: true
server_name_override: tempo-simplest-gateway.tempostack.svc.cluster.local&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Our new exporter with the name &amp;#34;otlp/tenantD&amp;#34;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The &lt;code&gt;X-Scope-OrgID&lt;/code&gt; header value that identifies the tenant&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect3"&gt;
&lt;h4 id="_add_pipeline_for_the_new_tenant"&gt;Add Pipeline for the New Tenant&lt;/h4&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Add the new pipeline to the &lt;code&gt;pipelines&lt;/code&gt; section:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;spec:
config:
service:
pipelines:
# ... existing pipelines ...
# New tenant pipeline
traces/tenantD: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
receivers:
- routing/traces
exporters:
- otlp/tenantD&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Our new tenant with the tenantName &amp;#34;tenantD&amp;#34;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_step_5_install_the_application_and_local_opentelemetry_collector"&gt;Step 5: Install the Application and Local OpenTelemetry Collector&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Follow the steps from the &lt;a href="https://blog.stderr.at/day-2/observability/2025-11-26-hitchhikers-guide-to-distributed-tracing-with-opentelemetry-and-tempostack-part4/"&gt;previous article&lt;/a&gt; to install the application and local OpenTelemetry Collector in the new namespace.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>What's new in OpenShift, 4.20 Edition</title><link>https://blog.stderr.at/whats-new/2025-11-10-whatsnew-420/</link><pubDate>Mon, 10 Nov 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/whats-new/2025-11-10-whatsnew-420/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;This article covers news and updates in the OpenShift 4.20 release. We focus on points that got our attention, but this
is &lt;strong&gt;not&lt;/strong&gt; a complete summary of the &lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/release_notes/index"&gt;release notes&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_configuring_a_local_arbiter_node"&gt;Configuring a local arbiter node&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/installing_an_on-premise_cluster_with_the_agent-based_installer/index#installing-ocp-agent-local-arbiter-node_installing-with-agent-based-installer"&gt;Configuring a local arbiter node&lt;/a&gt; describes how to configure an OCP cluster with only two control plane (ETCD) nodes. Might be useful in a pure bare metal environment where three bare metal control plane nodes might be overkill.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_two_node_clusters_with_fencing"&gt;Two node clusters with fencing&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/installing_on_bare_metal/index#bmo-about-the-hostfirmwarecomponents-resource_bare-metal-postinstallation-configuration"&gt;Two-node with Fencing&lt;/a&gt; still in tech preview. Useful for environments with only two active datacenters. Especially if you have bare metal control plane nodes. We got quite a few customers with only to data centers available.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_about_the_hostfirmwarecomponents_resource"&gt;About the HostFirmwareComponents resource&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/installing_on_bare_metal/index#bmo-about-the-hostfirmwarecomponents-resource_bare-metal-postinstallation-configuration"&gt;About the HostFirmwareComponents&lt;/a&gt;. When Metal3 is used itâ€™s now possible to update the NIC (Network interface card) firmware.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_boot_image_updates_on_vmware"&gt;Boot image updates on Vmware&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/machine_configuration/index#mco-update-boot-images"&gt;Boot image management&lt;/a&gt; can be used to update the boot image for new nodes. Up until now
when you installed you cluster on VMware with a certain boot image (letâ€™s say 4.15) it never got updates. New nodes were always booted from this old image and later updated to the current cluster release.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_about_on_cluster_image_mode"&gt;About on-cluster image mode&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/machine_configuration/index#coreos-layering-configuring-on_mco-coreos-layering"&gt;About on-cluster image mode&lt;/a&gt;. Image mode allows you to customize the node operating system image. You basically create a Containerfile, install custom RPMâ€™s or deploy custom configuration files in the Containerfile and OCP creates a layered image for you cluster. The image is automatically rolled out to the cluster with the Machine Config Operator. Fancy stuffâ€¦â€‹.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_pinning_images"&gt;Pinning images&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/machine_configuration/index#machine-config-pin-preload-images_machine-config-operator"&gt;Pinning images&lt;/a&gt;. If your internet connection is flaky, or you have reasons to not trust the availablity of Red Hat registries like quay.io or registry.redhat.io you can now pin images those images. They are pulled down immediately and will not be garbage collected (hopefullyâ€¦â€‹).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_bgp_routing"&gt;BGP routing&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/advanced_networking/index#about-bgp-routing"&gt;About BGP routing&lt;/a&gt;. MetalLB had support for announcing IPâ€™s via BGP for a long time, but now itâ€™s also possible to announce UDNâ€™s or EgressIP. IMHO this is especially interesting for EgressIP because there were some nasty &lt;a href="https://issues.redhat.com/browse/OCPBUGS-42303"&gt;bugs&lt;/a&gt; around using &lt;a href="https://wiki.wireshark.org/Gratuitous_ARP"&gt;GRAP (Gratuitous ARP)&lt;/a&gt; for announcing IPâ€™s to switches.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_migrating_a_configured_br_ex_bridge_to_nmstate"&gt;Migrating a configured br-ex bridge to NMState&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/installing_on_bare_metal/index#migrating-br-ex-bridge-nmstate_bare-metal-postinstallation-configuration"&gt;Migrating a configured br-ex bridge to NMState&lt;/a&gt;. Thereâ€™s this ominous &lt;em&gt;configure-ovs.sh&lt;/em&gt; that reconfigures the public interface of a node an brings up br-ex. There was support for deploying an NMState-based configuration during cluster installation and not using &lt;em&gt;configure-ovs.sh&lt;/em&gt;. Now itâ€™s also possible to get rid of the shell script &lt;strong&gt;after&lt;/strong&gt; installation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_configuration_for_a_bond_cni_secondary_network"&gt;Configuration for a Bond CNI secondary network&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/multiple_networks/index#nw-multus-bond-cni-object_configuring-additional-network-cni"&gt;Configuration for a Bond CNI secondary network&lt;/a&gt;. Bond interface can now be created for interface in containers. This currently only supports SR-IOV virtual functions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_manage_secure_signatures_with_sigstore"&gt;Manage secure signatures with sigstore&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/nodes/index#nodes-sigstore-using"&gt;Manage secure signatures with sigstore&lt;/a&gt;. Sigstore support can now be enabled on a cluster level or on an individual namespace level.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_running_pods_in_linux_user_namespaces"&gt;Running pods in Linux user namespaces&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/nodes/index#nodes-pods-user-namespaces"&gt;Running pods in Linux user namespaces&lt;/a&gt;. This is IMHO a big one. Linux user namespaces are finally supported in OpenShift! So itâ€™s possible to grant root inside a container and map the root UID to an unprivileged ID outside the container.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_adjust_pod_resource_levels_without_pod_disruption"&gt;Adjust pod resource levels without pod disruption&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/nodes/index#nodes-pods-adjust-resources-in-place"&gt;Adjust pod resource levels without pod disruption&lt;/a&gt;. Resize CPU and memory resource without restarting a Pod!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_introducing_the_oc_adm_upgrade_recommend_command_general_availability"&gt;Introducing the &lt;em&gt;oc adm upgrade recommend&lt;/em&gt; command (General Availability)&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/updating_clusters/index#understanding-openshift-updates"&gt;Understanding OpenShift upgrades&lt;/a&gt;. &lt;em&gt;oc&lt;/em&gt; know officially supports cluster upgrade from the command line. &lt;em&gt;oc adm upgrade recommended&lt;/em&gt; performs some pre-flight checks before upgrading a cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_additional_cluster_latency_requirements_for_etcd"&gt;Additional cluster latency requirements for etcd&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html-single/etcd/index#recommended-cluster-latency-etcd_etcd-practices"&gt;Cluster latency requirements for etcd&lt;/a&gt;. ETCD latency requirements were updated in the documentation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_sunset_of_the_red_hat_marketplace_operated_by_ibm"&gt;Sunset of the Red Hat Marketplace, operated by IBM&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://access.redhat.com/articles/7130828"&gt;Sunset of the Red Hat Marketplace, operated by IBM&lt;/a&gt;. It seems Red Hat Marktplace is no moreâ€¦â€‹&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_how_to_use_changed_block_tracking_dev_preview_in_openshift_4_20"&gt;How to use Changed Block Tracking (Dev Preview) in OpenShift 4.20&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;a href="https://access.redhat.com/solutions/7131061"&gt;How to use Changed Block Tracking (Dev Preview) in OpenShift 4.20&lt;/a&gt;. Change block tracking for PVâ€™s is now in dev preview. Whatever this means exactly, needs more investigation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>Red Hat Quay Registry - Integrate Keycloak</title><link>https://blog.stderr.at/openshift-platform/quay/2025-09-19-quay-integrate-keycloak/</link><pubDate>Fri, 19 Sep 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/quay/2025-09-19-quay-integrate-keycloak/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;This guide shows you how to configure Keycloak as an OpenID Connect (OIDC) provider for Red Hat Quay Registry. It covers what to configure in Keycloak, what to put into Quayâ€™s config.yaml (or Operator config), how to verify the login flow, and how to switch your Quay initial/admin account (stored locally in Quayâ€™s DB) to an admin user that authenticates via Keycloak.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Before starting this integration, ensure you have:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;OpenShift cluster with admin access&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Red Hat Quay Registry 3.15+ installed (via Operator)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Red Hat build of Keycloak stable-v22+ installed and configured (via Operator)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Valid SSL certificates for both Quay and Keycloak&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Admin access to both Keycloak Admin Console and Quay configuration&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Basic understanding of OIDC/OAuth2 concepts&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Here we are using the Quay Operator and the Keycloak Operator on OpenShift.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_quick_overview_what_happens"&gt;Quick overview (what happens)&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Create a Keycloak client for Quay (OIDC).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure Quayâ€™s OIDC provider section in config.yaml to point to Keycloak (client id/secret, endpoints, scopes, claim names).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Restart Quay, test login via Keycloak. The first time an OIDC user logs in, Quay creates a local user record that is linked to the OIDC identity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Promote that Quay user to superuser by adding their Quay username to SUPER_USERS (or use the UI/API).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_keycloak_create_a_client_for_quay"&gt;Keycloak: create a client for Quay&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Steps in Keycloak Admin console:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Log in to the Keycloak Admin Console and select the realm you will use for Quay or create a new realm.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Select that realm and go to clients&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Press &lt;strong&gt;Create client&lt;/strong&gt; to create and configure a new client for Quay. Not much must be configured here, but at least the following:&lt;/p&gt;
&lt;div class="olist loweralpha"&gt;
&lt;ol class="loweralpha" type="a"&gt;
&lt;li&gt;
&lt;p&gt;Client type: OpenID Connect&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Client ID: quay-enterprise (The ID we will use in the Quay configuration as well)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Client authentication: ON&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Valid redirects URIs:&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://&amp;lt;quay-hostname&amp;gt;/oauth2/&amp;lt;service-id&amp;gt;/callback" class="bare"&gt;https://&amp;lt;quay-hostname&amp;gt;/oauth2/&amp;lt;service-id&amp;gt;/callback&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Optional) &lt;a href="https://&amp;lt;quay-hostname&amp;gt;/oauth2/&amp;lt;service-id&amp;gt;/callback/attach" class="bare"&gt;https://&amp;lt;quay-hostname&amp;gt;/oauth2/&amp;lt;service-id&amp;gt;/callback/attach&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Optional) &lt;a href="https://&amp;lt;quay-hostname&amp;gt;/oauth2/&amp;lt;service-id&amp;gt;/callback/cli" class="bare"&gt;https://&amp;lt;quay-hostname&amp;gt;/oauth2/&amp;lt;service-id&amp;gt;/callback/cli&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The service-id is the parent key youâ€™ll use in Quay config â€” e.g. keycloak.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="olist loweralpha"&gt;
&lt;ol class="loweralpha" type="a"&gt;
&lt;li&gt;
&lt;p&gt;Save the client.&lt;/p&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Go to Credentials and create or copy the &lt;strong&gt;client secret&lt;/strong&gt; you will use in Quay.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
&lt;strong&gt;(Optional)&lt;/strong&gt; If you intend to use groups/roles from Keycloak for Quay team synchronization, ensure your Keycloak users or groups are populated and that Keycloak includes group membership in tokens or exposes group claims (you may need to add a protocol mapper in the client to include groups or roles in the ID token or userinfo response). Refer to Keycloak docs for adding protocol mappers.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_keycloak_create_users_and_groups"&gt;Keycloak: create users and groups&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Go to Users and create a new user or use an existing one.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you created a new user, set a password for them in the Credentials tab.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go to Groups and create a new group or use an existing one.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can assign users to that group.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_quay_add_oidc_configuration"&gt;Quay: add OIDC configuration&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Quayâ€™s OIDC block is a nested object under &lt;strong&gt;AUTHENTICATION_TYPE: OIDC&lt;/strong&gt;. The parent key can be any name (e.g., KEYCLOAK_LOGIN_CONFIG) â€” Quay uses the parent key name to form the callback paths.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock important"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-important" title="Important"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The parent key name (before _LOGIN_CONFIG) must match the service-id used in Keycloak redirect URIs.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Example config.yaml snippet for Keycloak:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Using Quay Operator this configuration is stored as a secret in the namespace where Quay is installed.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;AUTHENTICATION_TYPE: OIDC &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
KEYCLOAK_LOGIN_CONFIG: &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
CLIENT_ID: quay-enterprise &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
CLIENT_SECRET: &amp;lt;client-secret&amp;gt; &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
LOGIN_SCOPES:
- openid
- email
- profile
- groups
OIDC_SERVER: https://&amp;lt;keycloak hostname&amp;gt;/realms/quay/ &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
SERVICE_NAME: keycloak &lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;(6)&lt;/b&gt;
PREFERRED_USERNAME_CLAIM_NAME: preferred_username
VERIFIED_EMAIL_CLAIM_NAME: email
PREFERRED_GROUP_CLAIM_NAME: groups
OIDC_DISABLE_USER_ENDPOINT: false &lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;(7)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Set this parameter to &lt;strong&gt;OIDC&lt;/strong&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;parent key name can be any string (use KEYCLOAK_LOGIN_CONFIG here). This is important, and the part in front of _LOGIN_CONFIG must match the service-id you used in Keycloak.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The client id you used in Keycloak.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The client secret you used in Keycloak.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The realm base URL (no trailing /protocolâ€¦â€‹), e.g.:
&lt;a href="https://&amp;lt;keycloak_hostname&amp;gt;/realms/quay/" class="bare"&gt;https://&amp;lt;keycloak_hostname&amp;gt;/realms/quay/&lt;/a&gt; â€” Quay will discover .well-known/openid-configuration endpoints automatically.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The service name you used in Keycloak.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;7&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Set this parameter to &lt;strong&gt;false&lt;/strong&gt; to allow Quay to fetch user information from Keycloak. If using Azure Entra ID set this to &lt;strong&gt;true&lt;/strong&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Since we are using Quay Operator, the operator will pick up the changes automatically and restart Quay.
If you are using a standalone Quay installation, you need to restart Quay for the change to take effect. (Config is not hot-reloaded.)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_testing_login"&gt;Testing Login&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s do some tests to verify the configuration.
Above we have created some users and groups in Keycloak. Letâ€™s try to log in with one of them.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Open your Quay UI ( &lt;a href="https://&amp;lt;quay_hostname&amp;gt;" class="bare"&gt;https://&amp;lt;quay_hostname&amp;gt;&lt;/a&gt;; ) in a browser. On the login page you should see the external login option labelled Keycloak (or the SERVICE_NAME you set).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/quay/images/quay-login-oidc.png?width=320" alt="quay login oidc"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic" start="2"&gt;
&lt;li&gt;
&lt;p&gt;Click the &amp;#34;Sign in with Keycloak&amp;#34; option. and you will be redirected to Keycloakâ€™s login screen. Here you can authenticate with a Keycloak user.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/quay/images/quay-keycloak.png?width=320" alt="quay keycloak"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;After successful authentication, Keycloak redirects back to Quay and Quay creates a local user mapped to the OIDC claims (username, email). If you set USERNAME_CLAIM to preferred_username, look at the created Quay username in the UI under your userâ€™s profile.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_make_the_keycloak_user_a_quay_admin"&gt;Make the Keycloak user a Quay admin&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In Quay only one identity provider can be used at a time. If you previously set up a local admin account, you need to make a Keycloak user a Quay admin by adding their Quay username to SUPER_USERS where all administrators are listed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In the Quay configuration, you can define administrators using the SUPER_USERS array.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Make sure the Keycloak user has logged in once so Quay has created a local user record
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Edit the Quay configuration once more and add the Quay username (exact username created by Quay).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;For example, letâ€™s say we have created a user called &amp;#34;alice&amp;#34; in Keycloak. We need to add this user to the SUPER_USERS array.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;SETUP_COMPLETE: true
SUPER_USERS:
- admin # existing local admin (optional)
- alice # alice is the Quay username created from Keycloak login&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;After restart, the Quay account alice will have superuser privileges (same as if created in the UI).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_working_with_podman_on_cli"&gt;Working with Podman on CLI&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now, as we changed the authentication method, we need to verify if we can push/pull images from/to the registry.
The command line tool of the choice is &lt;strong&gt;podman&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;However, podman does not redirect to SSO login, so it needs a token to authenticate. We can generate a new token for a user in the Quay UI.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;In the Quay UI, go to the &lt;strong&gt;Account Settings&lt;/strong&gt; (upper right corner)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the settings section, under &lt;strong&gt;Docker CLI and other Application Tokens&lt;/strong&gt;, click on &lt;strong&gt;Create Application Token&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Enter a name for the token&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Select the newly created token and copy the podman login command (or any other command you need)&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/quay/images/quay-app-pass.png?width=320" alt="quay app pass"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The command for podman will look like this:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;podman login -u=&amp;#39;$app&amp;#39; -p=&amp;#39;&amp;lt;token&amp;gt;&amp;#39; &amp;lt;quay-url&amp;gt;
Login Succeeded!&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now you can push/pull images from/to the registry.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_troubleshooting"&gt;Troubleshooting&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Common issues and their solutions:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_redirect_uri_mismatch"&gt;Redirect URI Mismatch&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: Keycloak returns &amp;#34;Invalid redirect URI&amp;#34; error&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Ensure all three redirect URIs are configured in Keycloak client:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code&gt;https://&amp;lt;quay-hostname&amp;gt;/oauth2/keycloak/callback
https://&amp;lt;quay-hostname&amp;gt;/oauth2/keycloak/callback/attach
https://&amp;lt;quay-hostname&amp;gt;/oauth2/keycloak/callback/cli&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_username_mapping_issues"&gt;Username Mapping Issues&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: Quay creates unexpected usernames&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Check which claim Keycloak sends: inspect ID token in browser dev tools&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Common claims: preferred_username, email, sub, name&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update PREFERRED_USERNAME_CLAIM_NAME in Quay config accordingly&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_ssl_certificate_issues"&gt;SSL Certificate Issues&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: Quay cannot reach Keycloak due to TLS verification errors&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Solutions&lt;/strong&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Install your internal CA certificate in the Quay container&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use publicly trusted certificates for both services&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For testing only: disable SSL verification (not recommended for production)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_authentication_flow_failures"&gt;Authentication Flow Failures&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: Login redirects but fails to create user in Quay&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;Debugging steps&lt;/strong&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Check Quay logs: &lt;code&gt;oc logs deployment/quay-app&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Verify OIDC discovery endpoint: &lt;code&gt;&lt;a href="https://&amp;lt;keycloak&amp;gt;/realms/&amp;lt;realm&amp;gt;/.well-known/openid-configuration" class="bare"&gt;https://&amp;lt;keycloak&amp;gt;/realms/&amp;lt;realm&amp;gt;/.well-known/openid-configuration&lt;/a&gt;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Confirm client secret matches between Keycloak and Quay&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Test with curl: &lt;code&gt;curl -k &lt;a href="https://&amp;lt;keycloak&amp;gt;/realms/&amp;lt;realm&amp;gt;/.well-known/openid-configuration" class="bare"&gt;https://&amp;lt;keycloak&amp;gt;/realms/&amp;lt;realm&amp;gt;/.well-known/openid-configuration&lt;/a&gt;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Integrating Quay 3.15 with Keycloak via OIDC centralizes authentication, simplifies user management, and allows you to leverage enterprise identity features. The key steps are straightforward: create a Keycloak client with the correct redirect URIs, configure the corresponding OIDC block in Quayâ€™s config.yaml.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;By switching the initial database-backed admin to an identity managed in Keycloak, you reduce the risk of &amp;#34;orphaned&amp;#34; local accounts and align Quay administration with your organizationâ€™s identity and access management strategy.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>A second look into the Kubernetes Gateway API on OpenShift</title><link>https://blog.stderr.at/openshift-platform/day-2/networking/2025-08-31-openshift-gateway-api-ii/</link><pubDate>Sun, 31 Aug 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/day-2/networking/2025-08-31-openshift-gateway-api-ii/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;This is our second look into the Kubernetes Gateway API an itâ€™s
integration into OpenShift. This post covers TLS configuration.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Kubernetes Gateway API is new implementation of the ingress, load
balancing and service mesh APIâ€™s. See
&lt;a href="https://gateway-api.sigs.k8s.io/" target="_blank" rel="noopener"&gt;upstream&lt;/a&gt; for more information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Also the &lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/ingress_and_load_balancing/configuring-ingress-cluster-traffic#nw-ingress-gateway-api-overview_ingress-gateway-api" target="_blank" rel="noopener"&gt;OpenShift documentation&lt;/a&gt; provides an overview of the Gateway API and itâ€™s integration.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We demonstrate how to add TLS to our Nginx deployment, how to
implement a shared Gateway and finally how to implement HTTP to HTTPS
redirection with the Gateway API. Furthermore we cover how &lt;em&gt;HTTPRoute&lt;/em&gt;
objects attach to Gateways and dive into ordering of &lt;em&gt;HTTPRoute&lt;/em&gt;
objects.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_references"&gt;References&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://blog.stderr.at/openshift/2025/08/gateway-api/"&gt;A first look into the Kubernetes Gateway API on OpenShift&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_adding_tls_to_our_nginx_deployment"&gt;Adding TLS to our Nginx deployment&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In our fist post we simply exposed a Nginx web server via the
Gateway API. We only enabled HTTP, so letâ€™s try to do the same with
HTTPS now.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Remember we use a DNS wildcard domain &lt;code&gt;*.gtw.ocp.lan.stderr.at&lt;/code&gt; which
points to our Gateway. The gateway is exposed via a &lt;em&gt;Service&lt;/em&gt; of type
&lt;em&gt;LoadBalancer&lt;/em&gt;. We use
&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/networking_operators/metallb-operator"&gt;MetalLB&lt;/a&gt;
for this.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The first step is setting up a wildcard TLS certificate for our custom
domain &lt;em&gt;*.gtw.ocp.lan.stderr.at&lt;/em&gt;. We are using
&lt;a href="https://github.com/OpenVPN/easy-rsa"&gt;EasyRSA&lt;/a&gt; here, but use whatever tool you like.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Just for reference this is how we created a wildcard cert with EasyRSA:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ EASYRSA_CERT_EXPIRE=3650 EASYRSA_EXTRA_EXTS=&amp;#34;subjectAltName=DNS:*.gtw.ocp.lan.stderr.at&amp;#34; ./easyrsa gen-req gtw.ocp.lan.stderr.at
$ EASYRSA_CERT_EXPIRE=3650 ./easyrsa sign-req serverClient gtw.ocp.lan.stderr.at&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;EasyRSA stores the public key under &lt;em&gt;pki/issued&lt;/em&gt; and the private key
under &lt;em&gt;pki/private&lt;/em&gt;. We copied the certificate and the private key to
a temporary directory.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Next we need to remove the private key passphrase and create a
Kubernetes secret from the private and pubic key:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ openssl rsa -in gtw.ocp.lan.stderr.at.key -out gtw.ocp.lan.stderr.at-insecure.key
$ oc create secret -n openshift-ingress tls gateway-api --cert=gtw.ocp.lan.stderr.at.crt --key=gtw.ocp.lan.stderr.at-insecure.key&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now itâ€™s time to add a TLS listener to our &lt;em&gt;Gateway&lt;/em&gt; resource in the
&lt;em&gt;openshift-ingress&lt;/em&gt; namespace. Remember for the OpenShift Gateway API implementation, &lt;em&gt;Gateways&lt;/em&gt; have
to be deployed in the &lt;em&gt;openshift-ingress&lt;/em&gt; namespace.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
name: http-gateway
namespace: openshift-ingress
spec:
gatewayClassName: openshift-default
listeners:
- name: http
protocol: HTTP
port: 80
hostname: &amp;#34;*.gtw.ocp.lan.stderr.at&amp;#34;
- name: https
protocol: HTTPS &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
port: 443 &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
hostname: &amp;#34;*.gtw.ocp.lan.stderr.at&amp;#34; &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
tls:
mode: Terminate &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
certificateRefs:
- name: gateway-api &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
allowedRoutes: &lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;(6)&lt;/b&gt;
namespaces:
from: All&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;We want to support HTTPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;We use the default HTTPS port 443&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The URLs we support with this listener are the same as for HTTP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;We use edge termination for now, this means HTTP traffic will only be encrypted up to the gateway. From the gateway to our pod we speak plain HTTP.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;This is the name of the TLS secret we created above&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;We accept routes from all namespaces&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Also remember from our first post that we created a
&lt;em&gt;ReferenceGrant&lt;/em&gt; in the namespace where Nginx is running. Otherwise
HTTP routes will not be accepted.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Finally lets try to access our Nginx pod via HTTPS:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ curl -v https://nginx.gtw.ocp.lan.stderr.at
* Host nginx.gtw.ocp.lan.stderr.at:443 was resolved.
* IPv6: (none)
* IPv4: 10.0.0.150
* Trying 10.0.0.150:443...
* ALPN: curl offers h2,http/1.1
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* CAfile: /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem
* CApath: none
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (IN), TLS handshake, CERT verify (15):
* TLSv1.3 (IN), TLS handshake, Finished (20):
* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (OUT), TLS handshake, Finished (20):
* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384 / x25519 / RSASSA-PSS
* ALPN: server accepted h2
* Server certificate:
* subject: CN=gtw.ocp.lan.stderr.at
* start date: Aug 30 10:01:33 2025 GMT
* expire date: Aug 28 10:01:33 2035 GMT
* subjectAltName: host &amp;#34;nginx.gtw.ocp.lan.stderr.at&amp;#34; matched cert&amp;#39;s &amp;#34;*.gtw.ocp.lan.stderr.at&amp;#34;
* issuer: CN=tntinfra CA
* SSL certificate verify ok.
* Certificate level 0: Public key type RSA (2048/112 Bits/secBits), signed using sha256WithRSAEncryption
* Certificate level 1: Public key type RSA (2048/112 Bits/secBits), signed using sha256WithRSAEncryption
* Connected to nginx.gtw.ocp.lan.stderr.at (10.0.0.150) port 443
* using HTTP/2
* [HTTP/2] [1] OPENED stream for https://nginx.gtw.ocp.lan.stderr.at/
* [HTTP/2] [1] [:method: GET]
* [HTTP/2] [1] [:scheme: https]
* [HTTP/2] [1] [:authority: nginx.gtw.ocp.lan.stderr.at]
* [HTTP/2] [1] [:path: /]
* [HTTP/2] [1] [user-agent: curl/8.11.1]
* [HTTP/2] [1] [accept: */*]
&amp;gt; GET / HTTP/2
&amp;gt; Host: nginx.gtw.ocp.lan.stderr.at
&amp;gt; User-Agent: curl/8.11.1
&amp;gt; Accept: */*
&amp;gt;
* Request completely sent off
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):
&amp;lt; HTTP/2 200
&amp;lt; server: nginx/1.29.1
&amp;lt; date: Sat, 30 Aug 2025 14:30:20 GMT
&amp;lt; content-type: text/html
&amp;lt; content-length: 615
&amp;lt; last-modified: Wed, 13 Aug 2025 14:33:41 GMT
&amp;lt; etag: &amp;#34;689ca245-267&amp;#34;
&amp;lt; accept-ranges: bytes
(output omitted)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Yes, we can reach our Nginx via HTTPS, and the gateway presents the TLS certificate we created.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Be aware that we are still using the same &lt;em&gt;HTTPRoute&lt;/em&gt; for Nginx from our previous blog post.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Just for completeness here is the &lt;em&gt;HTTPRoute&lt;/em&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
name: nginx-route
spec:
parentRefs:
- name: http-gateway
namespace: openshift-ingress
hostnames: [&amp;#34;nginx.gtw.ocp.lan.stderr.at&amp;#34;]
rules:
- backendRefs:
- name: nginx
namespace: gateway-api-test
port: 8080&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Also Remember that we are using a dedicated &lt;em&gt;Gateway&lt;/em&gt; and all
&lt;em&gt;HTTPRoutes&lt;/em&gt; must be in the namespace &lt;em&gt;openshift-ingress&lt;/em&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_moving_to_a_shared_gateway"&gt;Moving to a shared gateway&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Up until now we had to create all &lt;em&gt;HTTPRoute&lt;/em&gt; objects in the
&lt;em&gt;openshift-ingress&lt;/em&gt; namespace. The Gateway API support two modes of
operations:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dedicated gateway: all &lt;em&gt;HTTPRoute&lt;/em&gt; object need to be in the same namespace as the gateway&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Shared gateway: The gateway runs in the &lt;em&gt;openshift-ingress&lt;/em&gt;
namespace and we allow &lt;em&gt;HTTPRoute&lt;/em&gt; objects from all or specific namespaces.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The first step in creating a shared gateway is to modify the gateway resource:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
name: http-gateway
namespace: openshift-ingress
spec:
gatewayClassName: openshift-default
listeners:
- name: http
protocol: HTTP
port: 80
hostname: &amp;#34;*.gtw.ocp.lan.stderr.at&amp;#34;
allowedRoutes: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
namespaces:
from: All&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;We now allow &lt;em&gt;HTTPRoute&lt;/em&gt; objects from all namespaces in the cluster&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Next we delete the existing &lt;em&gt;HTTPRoute&lt;/em&gt; for Nginx in the
&lt;em&gt;openshift-ingress&lt;/em&gt; namespaces, and verify that we canâ€™t reach Nginx:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc delete httproutes.gateway.networking.k8s.io -n openshift-ingress nginx-route
httproute.gateway.networking.k8s.io &amp;#34;nginx-route&amp;#34; deleted
$ curl -I http://nginx.gtw.ocp.lan.stderr.at
HTTP/1.1 404 Not Found &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
date: Sat, 30 Aug 2025 15:02:23 GMT
transfer-encoding: chunked&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Our Nginx route stopped working&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Next we apply our modified &lt;em&gt;Gateway&lt;/em&gt; resource in the
&lt;em&gt;openshift-ingress&lt;/em&gt; namespace and the &lt;em&gt;HTTPRoute&lt;/em&gt; object in the
&lt;em&gt;gateway-api-test&lt;/em&gt; namespace.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc apply -n openshift-ingress -f gateway--selector.yaml
gateway.gateway.networking.k8s.io/http-gateway configured
$ oc apply -n gateway-api-test -f httproute.yaml &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
httproute.gateway.networking.k8s.io/nginx-route created
$ curl -I http://nginx.gtw.ocp.lan.stderr.at
HTTP/1.1 200 OK &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
server: nginx/1.29.1
date: Sat, 30 Aug 2025 15:04:34 GMT
content-type: text/html
content-length: 615
last-modified: Wed, 13 Aug 2025 14:33:41 GMT
etag: &amp;#34;689ca245-267&amp;#34;
accept-ranges: bytes&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;We create the &lt;em&gt;HTTPRoute&lt;/em&gt; in the gateway-api-test namespace&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;We can reach our Nginx pod again&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;So our shared gateway seems to be working. But what if we want to
restrict which namespaces are allowed to create route objects?&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Gateway API allows the following settings under &lt;em&gt;spec.listeners[].allowedRoutes.namespaces.from&lt;/em&gt; field&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;All&lt;/strong&gt;: Allow from all namespaces&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Selector&lt;/strong&gt;: Specify a selector&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Same&lt;/strong&gt;: Only allow &lt;em&gt;HTTPRoutes&lt;/em&gt; in the same namespaces&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;None&lt;/strong&gt;: Do not allow any routes to attach&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;See the API specification &lt;a href="https://gateway-api.sigs.k8s.io/reference/spec/#fromnamespaces"&gt;FromNamespaces&lt;/a&gt; for details.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s try to use a more specific selector for our gateway:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
name: http-gateway
namespace: openshift-ingress
spec:
gatewayClassName: openshift-default
listeners:
- name: http
protocol: HTTP
port: 80
hostname: &amp;#34;*.gtw.ocp.lan.stderr.at&amp;#34;
allowedRoutes:
namespaces:
from: Selector &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
selector:
matchLabels:
kubernetes.io/metadata.name: gateway-api-test &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Now we are using the Selector option&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Because we do not have a specific label on the namespace we would like to use, letâ€™s use the &lt;em&gt;metadata.name&lt;/em&gt; label Kubernetes created for us&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We create a new yaml file &lt;em&gt;gateway-selector.yaml&lt;/em&gt; and appy the new configuration:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc apply -n openshift-ingress -f gateway-selector.yaml
gateway.gateway.networking.k8s.io/http-gateway configured
$ curl -I http://nginx.gtw.ocp.lan.stderr.at
HTTP/1.1 200 OK
server: nginx/1.29.1
date: Sat, 30 Aug 2025 15:17:17 GMT
content-type: text/html
content-length: 615
last-modified: Wed, 13 Aug 2025 14:33:41 GMT
etag: &amp;#34;689ca245-267&amp;#34;
accept-ranges: bytes&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;All good, still working.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Just for testing we modified the namespace name in the Gateway definition to &lt;strong&gt;NOT&lt;/strong&gt; match the namespace of our Nginx deployment and confirmed that we receive a &lt;em&gt;404&lt;/em&gt; not found response.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_implementing_http_to_https_redirect"&gt;Implementing HTTP to HTTPS redirect&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As a last test for this post letâ€™s try to implement HTTP to HTTPS redirects.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We deployed the following &lt;em&gt;Gateway&lt;/em&gt; configuration:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
name: http-gateway
namespace: openshift-ingress &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
spec:
gatewayClassName: openshift-default
listeners:
- name: http
protocol: HTTP
port: 80
hostname: &amp;#34;*.gtw.ocp.lan.stderr.at&amp;#34;
allowedRoutes:
namespaces:
from: Selector
selector:
matchLabels:
kubernetes.io/metadata.name: gateway-api-test2
- name: https &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
protocol: HTTPS
port: 443
hostname: &amp;#34;*.gtw.ocp.lan.stderr.at&amp;#34;
tls:
mode: Terminate
certificateRefs:
- name: gateway-api
allowedRoutes:
namespaces:
from: All&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Always deploy the gateway to the &lt;em&gt;openshift-ingress&lt;/em&gt; namespace for the OpenShift Gateway API implementation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;We added the HTTPS configuration back&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The &lt;a href="https://gateway-api.sigs.k8s.io/guides/http-redirect-rewrite/"&gt;upstream&lt;/a&gt; documentation contains an example on how to implements HTTP to HTTPS redirects. We created the following additional &lt;em&gt;HTTPRoute&lt;/em&gt; object in the &lt;em&gt;gateway-api-test&lt;/em&gt; namespace:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
name: http-https-redirect
spec:
parentRefs:
- name: http-gateway &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
namespace: openshift-ingress
sectionName: http &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
hostnames:
- nginx.gtw.ocp.lan.stderr.at
rules:
- filters:
- type: RequestRedirect
requestRedirect:
scheme: https
statusCode: 301&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Match our &lt;em&gt;Gateway&lt;/em&gt; &lt;em&gt;http-gateway&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Match the &lt;em&gt;http&lt;/em&gt; section in our gateway&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Just for reference this is the &lt;em&gt;HTTPRoute&lt;/em&gt; object to expose Nginx:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
name: nginx-route
spec:
parentRefs:
- name: http-gateway
namespace: openshift-ingress
hostnames: [&amp;#34;nginx.gtw.ocp.lan.stderr.at&amp;#34;]
rules:
- backendRefs:
- name: nginx
namespace: gateway-api-test
port: 8080&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;First we re-applied our &lt;em&gt;Gateway&lt;/em&gt; configuration&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc apply -f gateway-https-selector.yaml
gateway.gateway.networking.k8s.io/http-gateway configured&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s try and verify if our redirect is working, we need to apply both routes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc apply -f httproute.yaml
httproute.gateway.networking.k8s.io/nginx-route created
$ oc apply -f http-https-redirect-route.yaml
httproute.gateway.networking.k8s.io/http-https-redirect created&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;And test with curl:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ curl -I http://nginx.gtw.ocp.lan.stderr.at
HTTP/1.1 200 OK &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
server: nginx/1.29.1
date: Sat, 30 Aug 2025 15:37:20 GMT
content-type: text/html
content-length: 615
last-modified: Wed, 13 Aug 2025 14:33:41 GMT
etag: &amp;#34;689ca245-267&amp;#34;
accept-ranges: bytes&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Hm, strange we still get 200 OK and &lt;strong&gt;NOT&lt;/strong&gt; a redirect to HTTPS&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_understanding_httproute_ordering"&gt;Understanding HTTPRoute ordering&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;After a longer search through the documentation we found some hints on why this is happening.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s take a more detailed look at our http-to-https route again, as a
&lt;em&gt;HTTPRoute&lt;/em&gt; &lt;strong&gt;attaches&lt;/strong&gt; to a &lt;em&gt;Gateway&lt;/em&gt;, we focus on the &lt;em&gt;parentRefs&lt;/em&gt; in
the &lt;em&gt;HTTPRoute&lt;/em&gt; object. In our current understanding &lt;em&gt;parentRefs&lt;/em&gt; select a &lt;em&gt;Gateway&lt;/em&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
name: http-https-redirect
spec:
parentRefs: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
- name: http-gateway &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
namespace: openshift-ingress &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
sectionName: http &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
hostnames:
- nginx.gtw.ocp.lan.stderr.at
rules:
- filters:
- type: RequestRedirect
requestRedirect:
scheme: https
statusCode: 301&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Ok, this is the &lt;em&gt;parentRefs&lt;/em&gt; section we are looking for&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;name&lt;/em&gt; selects the name of the &lt;em&gt;Gateway&lt;/em&gt; we want to attach to&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;namespace&lt;/em&gt; specifies the namespace where we can find the &lt;em&gt;Gateway&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;sectionName&lt;/em&gt; selects the section in the &lt;em&gt;Gateway&lt;/em&gt; where we want to attach to.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;So this &lt;em&gt;HTTPRoute&lt;/em&gt; explicitly attaches to a &lt;em&gt;Gateway&lt;/em&gt; in a
&lt;em&gt;Namespace&lt;/em&gt; that has a &lt;em&gt;Section&lt;/em&gt; http defined.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;If you look at the Gateway configuration above you will see that we
have a section for HTTP traffic and one for HTTPS traffic.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s compare this with our Nginx &lt;em&gt;HTTPRoute&lt;/em&gt; definition:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
name: nginx-route
spec:
parentRefs: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
- name: http-gateway &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
namespace: openshift-ingress &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
hostnames: [&amp;#34;nginx.gtw.ocp.lan.stderr.at&amp;#34;]
rules:
- backendRefs:
- name: nginx
namespace: gateway-api-test
port: 8080&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The &lt;em&gt;parentRefs&lt;/em&gt; section&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The &lt;em&gt;Gateway&lt;/em&gt; we would like to attach to&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The &lt;em&gt;namespace&lt;/em&gt; where the &lt;em&gt;Gateway&lt;/em&gt; is deploy&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Note that &lt;em&gt;Section&lt;/em&gt; is missing in this configuration.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;So this &lt;em&gt;HTTPRoute&lt;/em&gt; actually attaches to &lt;strong&gt;both&lt;/strong&gt; sections in our
&lt;em&gt;Gateway&lt;/em&gt; definition, HTTP and HTTPS. Which is not what we want.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When a client hits the HTTP endpoint we want to redirect the traffic to HTTPS&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When a client hits the HTTPS endpoint we want the traffic to be forward to our Nginx deployment&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We found the following statement
&lt;a href="https://gateway-api.sigs.k8s.io/reference/spec/#httprouterule"&gt;statement&lt;/a&gt;
how ordering works in the Gateway API:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre&gt;If ties still exist across multiple Routes, matching precedence MUST be
determined in order of the following criteria, continuing on ties:
The oldest Route based on creation timestamp.&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;When we look at the timestamps of our &lt;em&gt;HTTPRoutes&lt;/em&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;oc get httproute -o jsonpath=&amp;#39;{range .items[*]}{.metadata.name}{&amp;#34;\t&amp;#34;}{.metadata.creationTimestamp}{&amp;#34;\n&amp;#34;}{end}&amp;#39;
http-https-redirect 2025-08-31T09:17:46Z &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
nginx-route 2025-08-31T09:17:40Z &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Creation timestamp of the redirect route&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Creation timestamp of the nginx route&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Nginx &lt;em&gt;HTTPRoute&lt;/em&gt; is &lt;strong&gt;older&lt;/strong&gt; than the HTTP-to-HTTP &lt;em&gt;HTTPRoute&lt;/em&gt;. So
this matches first and a 200 OK is returned.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;So letâ€™s try to revers how we applied our &lt;em&gt;HTTPRoutes&lt;/em&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc delete httproutes.gateway.networking.k8s.io --all
httproute.gateway.networking.k8s.io &amp;#34;http-https-redirect&amp;#34; deleted
httproute.gateway.networking.k8s.io &amp;#34;nginx-route&amp;#34; deleted
$ oc apply -f http-to-https-httproute.yaml
httproute.gateway.networking.k8s.io/http-https-redirect created
$ oc apply -f nginx-httproute.yaml
httproute.gateway.networking.k8s.io/nginx-route created
$ oc get httproute -o jsonpath=&amp;#39;{range .items[*]}{.metadata.name}{&amp;#34;\t&amp;#34;}{.metadata.creationTimestamp}{&amp;#34;\n&amp;#34;}{end}&amp;#39;
http-https-redirect 2025-08-31T10:34:55Z &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
nginx-route 2025-08-31T10:35:11Z &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Creation timestamp of the HTTP-to-HTTPS route&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Creation timestamp of the nginx route&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now the HTTP-to-HTTPS route is the oldest route. Letâ€™s try again calling Nginx with curl:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ curl -I http://nginx.gtw.ocp.lan.stderr.at
HTTP/1.1 301 Moved Permanently &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
location: https://nginx.gtw.ocp.lan.stderr.at/
date: Sun, 31 Aug 2025 10:37:13 GMT
transfer-encoding: chunked
$ curl -I https://nginx.gtw.ocp.lan.stderr.at
HTTP/2 200 &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
server: nginx/1.29.1
date: Sun, 31 Aug 2025 10:37:17 GMT
content-type: text/html
content-length: 615
last-modified: Wed, 13 Aug 2025 14:33:41 GMT
etag: &amp;#34;689ca245-267&amp;#34;
accept-ranges: bytes&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The HTTP endpoint returns a redirect&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;the HTTPS endpoint returns 200 OK from Nginx&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;So now we have the expected behavior: HTTP is redirect to HTTPS!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As depending on the time when an object is created is definitely &lt;strong&gt;NOT&lt;/strong&gt;
a good idea, letâ€™s be more specific in our Nginx &lt;em&gt;HTTPRoute&lt;/em&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
name: nginx-route
spec:
parentRefs:
- name: http-gateway
namespace: openshift-ingress
sectionName: https &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
hostnames: [&amp;#34;nginx.gtw.ocp.lan.stderr.at&amp;#34;]
rules:
- backendRefs:
- name: nginx
namespace: gateway-api-test
port: 8080&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;We explicitly select the &lt;strong&gt;HTTPS&lt;/strong&gt; section in our &lt;em&gt;Gateway&lt;/em&gt; configuration&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Next we delete our &lt;em&gt;HTTPRoutes&lt;/em&gt; again, and re-apply them in the order that didnâ€™t work the first time (Nginx is the oldest route):&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc delete httproutes.gateway.networking.k8s.io --all
httproute.gateway.networking.k8s.io &amp;#34;http-https-redirect&amp;#34; deleted
httproute.gateway.networking.k8s.io &amp;#34;nginx-route&amp;#34; deleted
$ oc apply -f http-to-https-httproute.yaml
httproute.gateway.networking.k8s.io/http-https-redirect created
$ oc get httproute -o jsonpath=&amp;#39;{range .items[*]}{.metadata.name}{&amp;#34;\t&amp;#34;}{.metadata.creationTimestamp}{&amp;#34;\n&amp;#34;}{end}&amp;#39;
http-https-redirect 2025-08-31T10:45:01Z
nginx-route 2025-08-31T10:44:57Z &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
$ curl -I http://nginx.gtw.ocp.lan.stderr.at
HTTP/1.1 301 Moved Permanently &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
location: https://nginx.gtw.ocp.lan.stderr.at/
date: Sun, 31 Aug 2025 10:46:22 GMT
transfer-encoding: chunked
$ curl -I https://nginx.gtw.ocp.lan.stderr.at
HTTP/2 200 &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
server: nginx/1.29.1
date: Sun, 31 Aug 2025 10:46:30 GMT
content-type: text/html
content-length: 615
last-modified: Wed, 13 Aug 2025 14:33:41 GMT
etag: &amp;#34;689ca245-267&amp;#34;
accept-ranges: bytes&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The Nginx route is the oldest route&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The HTTP endpoint returns a redirect to HTTPS&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The response from our Nginx deployment&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Finally everything works as expected!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
A &lt;em&gt;HTTPRoute&lt;/em&gt; attaches to a &lt;em&gt;Gateway&lt;/em&gt;. Always be as specific as
possible which &lt;em&gt;Gateway&lt;/em&gt; to match and which &lt;em&gt;section&lt;/em&gt; in the
&lt;em&gt;Gateway&lt;/em&gt;.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In this blog post we demonstrated to implement TLS with the
Gateway API. We also implemented a shared &lt;em&gt;Gateway&lt;/em&gt; with &lt;em&gt;HTTPRoute&lt;/em&gt;
objects in different namespaces.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Furthermore we configured HTTP to HTTPS redirects and dove into
&lt;em&gt;HTTPRoute&lt;/em&gt; ordering if a route matches multiple listeners in a
&lt;em&gt;Gateway&lt;/em&gt; definition.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>A first look into the Kubernetes Gateway API on OpenShift</title><link>https://blog.stderr.at/openshift-platform/day-2/networking/2025-08-29-openshift-gateway-api/</link><pubDate>Fri, 29 Aug 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/day-2/networking/2025-08-29-openshift-gateway-api/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;This blog post summarizes our first look into the Kubernetes Gateway
API and how it is integrated in OpenShift.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Kubernetes Gateway API is new implementation of the ingress, load
balancing and service mesh APIâ€™s. See
&lt;a href="https://gateway-api.sigs.k8s.io/" target="_blank" rel="noopener"&gt;upstream&lt;/a&gt; for more information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Also the &lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/ingress_and_load_balancing/configuring-ingress-cluster-traffic#nw-ingress-gateway-api-overview_ingress-gateway-api" target="_blank" rel="noopener"&gt;OpenShift documentation&lt;/a&gt; provides an overview of the Gateway API and itâ€™s integration.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_things_to_consider_when_using_gateway_api_with_openshift"&gt;Things to consider when using Gateway API with OpenShift&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Currently UDN (User Defined Networks) with Gateway API are not supported.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Only TLS termination on the edge is supported (no pass-through or re-encrypt), this needs to be confirmed. We canâ€™t find the original source of this statement&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The standard OpenShift ingress controller manages Gateway API Resources&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gateway API provides a standard on how to get client traffic into a
Kubernetes cluster. Vendors provide an implementation of the API. So
OpenShift provides &lt;strong&gt;ONE&lt;/strong&gt; possible implementation, but there could
be more than one in a cluster.&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We found the following sentence in the OpenShift documentation
interesting:&lt;/p&gt;
&lt;div class="literalblock"&gt;
&lt;div class="content"&gt;
&lt;pre&gt;Because OpenShift Container Platform uses a specific
version of Gateway API CRDs, any use of third-party implementations
of Gateway API must conform to the OpenShift Container Platform
implementation to ensure that all fields work as expected&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_setting_up_gateway_api_on_openshift"&gt;Setting up Gateway API on OpenShift&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Before you begin, ensure you have the following:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;OpenShift 4.19 or higher with cluster-admin access&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;First you need to create a &lt;code&gt;GatewayClass&lt;/code&gt; object.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Be aware that the &lt;code&gt;GatewayClass&lt;/code&gt; object is &lt;strong&gt;NOT&lt;/strong&gt; namespaced.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
name: openshift-default
spec:
controllerName: openshift.io/gateway-controller/v1 &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The controller name needs to be exactly as shown. Otherwise the
ingress controller will &lt;strong&gt;NOT&lt;/strong&gt; manage the gateway and associated
resources.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This creates a new pod in the openshift-ingress namespace:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc get po -n openshift-ingress
NAME READY STATUS RESTARTS AGE
istiod-openshift-gateway-7b567bc8b4-4lrt2 1/1 Running 0 12m &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
router-default-6db958cbd-dlbwz 1/1 Running 12 14d&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;this pod got create after applying the gateway class resource&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;code&gt;router-default&lt;/code&gt; is the default openshift ingress pod. The first
difference seems to be the SCC (security context constraint) the pods
are using.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc get po -n openshift-ingress -o jsonpath=&amp;#39;{range .items[*]}{.metadata.name}{&amp;#34;\t&amp;#34;}{.metadata.annotations.openshift\.io/scc}{&amp;#34;\n&amp;#34;}{end}&amp;#39;
istiod-openshift-gateway-7b567bc8b4-4lrt2 restricted-v2
router-default-6db958cbd-dlbwz hostnetwork&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The standard router used host networking for listing on port 80 and 443
on the node where it is running. Our &lt;code&gt;GatewayClass&lt;/code&gt; currently only
provides a pod running Istiod awaiting further configuration. To
actually listen for client request additional configuration is
required.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;A &lt;code&gt;Gateway&lt;/code&gt; is required to listen for client requests. We create the
following gateway:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
name: http-gateway
namespace: openshift-ingress &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
spec:
gatewayClassName: openshift-default
listeners:
- name: http
protocol: HTTP
port: 80
hostname: &amp;#34;*.apps.ocp.lan.stderr.at&amp;#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;We create this gateway in the same namespace as the istio
deployment. This is required for OpenShift.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This creates an additional pod in the &lt;code&gt;openshift-ingress&lt;/code&gt; namespace:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc get po
NAME READY STATUS RESTARTS AGE
http-gateway-openshift-default-d476664f5-h87mp 1/1 Running 0 36s&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We also got a new service for the our http-gateway&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;âžœ oc get svc
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
http-gateway-openshift-default LoadBalancer 172.30.183.48 10.0.0.150 15021:30251/TCP,80:30437/TCP 4m52s&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The interesting thing is the &lt;code&gt;TYPE&lt;/code&gt; of the service. Itâ€™s of type
&lt;code&gt;LoadBalancer&lt;/code&gt;. We have
&lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/networking_operators/metallb-operator"&gt;MetalLB&lt;/a&gt;
deployed in our cluster, this might be the reason for this. We will
try to configure a gateway without MetalLB in in upcoming post.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Lets take a look at the gateway resource&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc get gtw
NAME CLASS ADDRESS PROGRAMMED AGE
http-gateway openshift-default 10.0.0.150 True 3m&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;So this seems to be working. But know we have a problem: the &lt;code&gt;*.apps&lt;/code&gt;
domain that we used for our gateway points already to the default
OpenShift Ingress. We could either&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;redeploy the gateway with a different wildcard domain (e.g. *.gtwâ€¦â€‹)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;create a more specific DNS record that points to our new load balancer&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s try to confirm this with &lt;em&gt;curl&lt;/em&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ curl -I http://bla.apps.ocp.lan.stderr.at
HTTP/1.0 503 Service Unavailable
pragma: no-cache
cache-control: private, max-age=0, no-cache, no-store
content-type: text/html&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;code&gt;503&lt;/code&gt; is the response of default OpenShift Ingress.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ curl -I http://10.0.0.150
HTTP/1.1 404 Not Found
date: Fri, 29 Aug 2025 14:31:31 GMT
transfer-encoding: chunked&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Our new gateway returns a &lt;code&gt;404&lt;/code&gt; not found response. We choose the
first option and create another wildcard DNS entry for
&lt;code&gt;*.gtw.ocp.lan.stderr.at&lt;/code&gt;. We re-deployed our gateway with the new hostname:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
name: http-gateway
namespace: openshift-ingress
spec:
gatewayClassName: openshift-default
listeners:
- name: http
protocol: HTTP
port: 80
hostname: &amp;#34;*.gtw.ocp.lan.stderr.at&amp;#34; &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;New hostname for resources exposed via our gateway&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc apply -f gateway.yaml
gateway.gateway.networking.k8s.io/http-gateway created&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This also creates a DNSRecord resource:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc describe dnsrecords.ingress.operator.openshift.io -n openshift-ingress http-gateway-c8d7bfc67-wildcard
Name: http-gateway-c8d7bfc67-wildcard
Namespace: openshift-ingress
Labels: gateway.istio.io/managed=openshift.io-gateway-controller-v1
gateway.networking.k8s.io/gateway-name=http-gateway
istio.io/rev=openshift-gateway
Annotations: &amp;lt;none&amp;gt;
API Version: ingress.operator.openshift.io/v1
Kind: DNSRecord
Metadata:
Creation Timestamp: 2025-08-29T14:49:45Z
Finalizers:
operator.openshift.io/ingress-dns
Generation: 1
Owner References:
API Version: v1
Kind: Service
Name: http-gateway-openshift-default
UID: a023de5d-c428-4249-a190-de3cbfeb6964
Resource Version: 141150968
UID: 7a61a867-216e-40b7-88f3-e3934493c477
Spec:
Dns Management Policy: Managed
Dns Name: *.gtw.ocp.lan.stderr.at.
Record TTL: 30
Record Type: A
Targets:
10.0.0.150
Events: &amp;lt;none&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This resource is only internally used by the OpenShift ingress
operator (see &lt;code&gt;oc explain dnsrecord&lt;/code&gt; for details).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_creating_httproutes_for_exposing_our_service"&gt;Creating HTTPRoutes for exposing our service.&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To actually expose a HTTP pod via our new gateway we need:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A &lt;em&gt;Namespace&lt;/em&gt; to deploy an example pod. We will use a Nginx for this&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;em&gt;Service&lt;/em&gt; that exposes our Nginx pod&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;and finally a &lt;em&gt;HTTPRoute&lt;/em&gt; resource&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;For the nginx deployment we used the following manifest:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: v1
kind: Namespace
metadata:
name: gateway-api-test
spec:
finalizers:
- kubernetes
---
apiVersion: apps/v1
kind: Deployment
metadata:
name: nginx-deployment
namespace: gateway-api-test
labels:
app: nginx
spec:
replicas: 1
selector:
matchLabels:
app: nginx
template:
metadata:
labels:
app: nginx
spec:
containers:
- name: nginx
image: quay.io/nginx/nginx-unprivileged:1.29.1
ports:
- containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
name: nginx
namespace: gateway-api-test
spec:
selector:
app: nginx
ports:
- protocol: TCP
port: 8080
targetPort: 8080&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s see if our nginx pod got deployed successfully:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc get po,svc -n gateway-api-test
NAME READY STATUS RESTARTS AGE
pod/nginx-deployment-796cdf7474-b7bqz 1/1 Running 0 20s
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/nginx ClusterIP 172.30.42.36 &amp;lt;none&amp;gt; 8080/TCP 21s&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;And finally confirm our &lt;code&gt;Service&lt;/code&gt; is working:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc port-forward -n gateway-api-test svc/nginx 8080 &amp;amp;
Forwarding from 127.0.0.1:8080 -&amp;gt; 8080
Forwarding from [::1]:8080 -&amp;gt; 8080
$ curl -I localhost:8080
Handling connection for 8080
HTTP/1.1 200 OK
Server: nginx/1.29.1
Date: Fri, 29 Aug 2025 15:45:12 GMT
Content-Type: text/html
Content-Length: 615
Last-Modified: Wed, 13 Aug 2025 14:33:41 GMT
Connection: keep-alive
ETag: &amp;#34;689ca245-267&amp;#34;
Accept-Ranges: bytes&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We received a response from our nginx pod, hurray!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;So next letâ€™s try to create a &lt;code&gt;HTTPRoute&lt;/code&gt; to expose our nginx service to external clients:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
name: nginx-route
spec:
parentRefs:
- name: http-gateway
namespace: openshift-ingress
hostnames: [&amp;#34;nginx.gtw.ocp.lan.stderr.at&amp;#34;]
rules:
- backendRefs:
- name: nginx
namespace: gateway-api-test
port: 8080&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;One important point here, the &lt;code&gt;Gateway&lt;/code&gt; actually come in two flavors&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;dedicated gateways, only accepting HTTP routes in the same namespace (&lt;code&gt;openshift-ingress&lt;/code&gt;) in our case.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;shared gateways, which also accept HTTP route objects from other namespaces&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;see &lt;a href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/ingress_and_load_balancing/configuring-ingress-cluster-traffic#nw-ingress-gateway-api-deployment_ingress-gateway-api"&gt;Gateway API deployment topologies&lt;/a&gt; in the OpenShift documentation for more information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As this post is already rather long, we focus on the dedicated gateway topology for now.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The HTTP route must be deployed in the same namespace as the
gateway if the dedicated topology is used.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;So letâ€™s deploy our &lt;code&gt;HTTPRoute&lt;/code&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc apply -f httproute.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Verify we can reach our nginx pod:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;curl -I http://nginx.gtw.ocp.lan.stderr.at
HTTP/1.1 500 Internal Server Error
date: Fri, 29 Aug 2025 15:57:34 GMT
transfer-encoding: chunked&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This return a &lt;em&gt;500&lt;/em&gt; error, something seems to be wrong with our route,
letâ€™s take a look at the status of the &lt;code&gt;HTTPRoute&lt;/code&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ oc describe gtw http-gateway
.
. (output omitted)
.
Status:
Parents:
Conditions:
Last Transition Time: 2025-08-29T15:54:43Z
Message: Route was valid
Observed Generation: 1
Reason: Accepted
Status: True
Type: Accepted
Last Transition Time: 2025-08-29T15:54:43Z
Message: backendRef nginx/gateway-api-test not accessible to a HTTPRoute in namespace &amp;#34;openshift-ingress&amp;#34; (missing a ReferenceGrant?) &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
Observed Generation: 1
Reason: RefNotPermitted
Status: False &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
Type: ResolvedRefs
Controller Name: openshift.io/gateway-controller/v1
Parent Ref:
Group: gateway.networking.k8s.io
Kind: Gateway
Name: http-gateway
Namespace: openshift-ingress&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Something seems to be wrong as the status is &lt;em&gt;False&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Seems we are missing a ReferenceGrant&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Looking at the
&lt;a href="https://gateway-api.sigs.k8s.io/api-types/referencegrant/" target="_blank" rel="noopener"&gt;upstream&lt;/a&gt;
documentation reveals a security feature of the Gateway API. Before a
&lt;code&gt;HTTPRoute&lt;/code&gt; can reach a service in a &lt;em&gt;different&lt;/em&gt; namespace we must
create a &lt;code&gt;ReferenceGrant&lt;/code&gt; in the namespace providing the service.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;So letâ€™s try to deploy following &lt;code&gt;ReferenceGrant&lt;/code&gt; in the &lt;em&gt;gateway-api-test&lt;/em&gt; namespace:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: gateway.networking.k8s.io/v1beta1
kind: ReferenceGrant
metadata:
name: nginx
namespace: gateway-api-test
spec:
from:
- group: gateway.networking.k8s.io
kind: HTTPRoute
namespace: openshift-ingress
to:
- group: &amp;#34;&amp;#34;
kind: Service&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Checking the status field of our &lt;code&gt;HTTPRoute&lt;/code&gt; again:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;(output omitted)
Status:
Addresses:
Type: IPAddress
Value: 10.0.0.150
Conditions:
Last Transition Time: 2025-08-29T15:47:07Z
Message: Resource accepted
Observed Generation: 1
Reason: Accepted
Status: True
Type: Accepted
Last Transition Time: 2025-08-29T15:47:08Z
Message: Resource programmed, assigned to service(s) http-gateway-openshift-default.openshift-ingress.svc.cluster.local:80
Observed Generation: 1
Reason: Programmed
Status: True &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
Type: Programmed&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;Status&lt;/em&gt; is now &lt;em&gt;True&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;and finally calling the nginx pod again via our gateway:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;$ curl -I http://nginx.gtw.ocp.lan.stderr.at
HTTP/1.1 200 OK
server: nginx/1.29.1
date: Fri, 29 Aug 2025 16:01:33 GMT
content-type: text/html
content-length: 615
last-modified: Wed, 13 Aug 2025 14:33:41 GMT
etag: &amp;#34;689ca245-267&amp;#34;
accept-ranges: bytes&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Finally everything seems to be in place and working.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In this blog post we took a first look at the Kubernetes Gateway API
and itâ€™s integration into OpenShift. We enabled the Gateway API via a
&lt;code&gt;GatewayClass&lt;/code&gt; resource, created a simple HTTP Gateway via a
&lt;code&gt;Gateway&lt;/code&gt;, deploy a Nginx pod and a Service and exposed the service
via a &lt;code&gt;HTTPRoute&lt;/code&gt; and a &lt;code&gt;ReferenceGrant&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Hopefully an upcoming blog post will cover how to&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;How to deploy a Gateway without MetalLB&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deploy a TLS secured service&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;implement HTTP redirects&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;rewriting URLâ€™s (if possible)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;and other possibilities of the Gateway API&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>[Ep.14] Reusable Argo CD Application Template</title><link>https://blog.stderr.at/gitopscollection/2025-07-17-common-template-application/</link><pubDate>Thu, 17 Jul 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2025-07-17-common-template-application/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;When working with Argo CD at scale, you often find yourself creating similar Application manifests repeatedly. Each application needs the same basic structure but with different configurations for source repositories, destinations, and sync policies. Additionally, managing namespace metadata becomes tricky when you need to conditionally control whether Argo CD should manage namespace metadata based on sync options.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In this article, Iâ€™ll walk you through a reusable Helm template that solves these challenges by providing a flexible, DRY (Donâ€™t Repeat Yourself) approach to creating Argo CD Applications. This template is available in my public Helm Chart library and can easily be used by anyone.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_the_problem"&gt;The Problem&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Traditional Argo CD Application manifests suffer from several issues:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Repetitive Code&lt;/strong&gt;: Each application requires similar boilerplate YAML&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Configuration Validation&lt;/strong&gt;: Manual validation of required fields across multiple applications&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Maintenance Overhead&lt;/strong&gt;: Changes to common patterns require updates across multiple files&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_but_i_can_do_this_manually_right"&gt;But I can do this manually, right?&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Of course, nobody prevents you from creating an Argo CD Application manifest manually or using the UI to enter the values there, but sometimes you just want to get things done faster or help your team with a consistent way to create Argo CD Applications. Often teams do not want to learn about a new tool like Argo CD and maybe you want to automate the creation by using a CI/CD pipeline.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_the_solution_a_reusable_helm_template"&gt;The Solution: A Reusable Helm Template&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;I already work with a common template library for all of my Helm Charts. The idea is to have repeatable snippets in my tpl charts and I can reuse them in all other charts. Recently, I started testing the templating of entire Kubernetes manifests.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To address the above issues, Iâ€™ve created a comprehensive Helm template that will render an Argo CD Application. But letâ€™s start with the end result.
Your team wants to create a new Argo CD Application. They can do this either via the UI, via the CLI, by creating the YAML file manually, or through pull request or CI/CD integration. Ultimately, the minimum required information is:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The name of the application&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The namespace of the application&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The source repository&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The destination repository&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;So something like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;argocd_applications:
my-app: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
namespace: &amp;#34;openshift-gitops&amp;#34;
source: &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
repositoryURL: &amp;#34;https://github.com/argoproj/argocd-example-apps.git&amp;#34;
targetRevision: &amp;#34;HEAD&amp;#34;
path: &amp;#34;guestbook&amp;#34;
destination: &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
server: &amp;#34;https://kubernetes.default.svc&amp;#34; &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
namespace: &amp;#34;guestbook&amp;#34;
my-second-app:
....&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;This key will become the name of the Application&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The source repository, this is the repository that contains the Helm chart or the Kubernetes manifests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The destination, that defines the cluster and namespace where the application will be deployed&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Either &lt;strong&gt;server&lt;/strong&gt; or &lt;strong&gt;name&lt;/strong&gt; must be set, but not both.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;With the above values multiple applications can be created at once.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_integrating_the_template"&gt;Integrating the Template&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The values from the above example can be used to create the Application manifest.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;All the developers (or CI/CD pipelines) need to do is to define the values and create one template to include the source template.
This will look like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;{{- if .Values.argocd_applications }}
{{- range $name, $config := .Values.argocd_applications }} &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
{{- if $config.enabled | default false }} &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
---
{{- include &amp;#34;tpl.argocdApplication&amp;#34; (dict &amp;#34;name&amp;#34; $name &amp;#34;spec&amp;#34; $config) -}} &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
{{- end }}
{{- end }}
{{- end }}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Iterate over the applications, defining $name and $config&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Only include the application if it is enabled&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Include the template with the name of the application and the specification&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This is everything you need to do to create an Argo CD Application. Letâ€™s take a look at the template.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_key_features_of_the_template"&gt;Key Features of the template&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_1_flexible_destination_configuration"&gt;1. Flexible Destination Configuration&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The template supports both server URL and cluster name destinations with &lt;strong&gt;validation&lt;/strong&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;destination:
{{- if and ($spec.destination.server) ($spec.destination.name) }}
{{ fail &amp;#34;destination.server and destination.name cannot be set at the same time&amp;#34; }}
{{- else }}
{{- if $spec.destination.server }}
server: {{ $spec.destination.server }}
{{- else if $spec.destination.name }}
name: {{ $spec.destination.name }}
{{- else }}
server: https://kubernetes.default.svc
{{- end }}
{{- end }}
namespace: {{ $spec.destination.namespace | required &amp;#34;destination.namespace is required&amp;#34; }}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_2_comprehensive_sync_policy_support"&gt;2. Comprehensive Sync Policy Support&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The template handles all Argo CD sync policy features:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Automated sync with prune, selfHeal, and allowEmpty options&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Flexible sync options array&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Retry configuration with backoff strategies&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Conditional managed namespace metadata&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_3_conditional_namespace_management"&gt;3. Conditional Namespace Management&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The intelligent handling of &lt;code&gt;managedNamespaceMetadata&lt;/code&gt;. The template only includes this section when it &lt;strong&gt;CreateNamespace=&lt;/strong&gt; option is set to true:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;{{- if and $spec.syncPolicy.syncOptions (not (has &amp;#34;CreateNamespace=false&amp;#34; $spec.syncPolicy.syncOptions)) }}
{{- if $spec.syncPolicy.managedNamespaceMetadata }}
managedNamespaceMetadata:
{{- with $spec.syncPolicy.managedNamespaceMetadata.labels }}
labels:
{{- toYaml . | nindent 8 }}
{{- end }}
{{- with $spec.syncPolicy.managedNamespaceMetadata.annotations }}
annotations:
{{- toYaml . | nindent 8 }}
{{- end }}
{{- end }}
{{- end }}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_4_other_features"&gt;4. Other Features&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;I created the template to be as flexible as possible. However, I did not include everything in this template, only the most important features (from my point of view). Currently, the following is possible:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Create a template for an Argo CD Application using Git&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a template for an Argo CD Application using Helm defining all possible Helm parameters, like additional values files or other options.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using a &lt;strong&gt;single&lt;/strong&gt; source&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set required annotations and labels&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_5_not_possible_currently"&gt;5. Not possible (currently)&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Along with the supported features, there are some features that are currently not possible:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Defining multiple sources&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure Kustomize settings&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;However, if you feel this needs to be added, please let me know and create an issue. I can then try to add it.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_why_this_approach_works"&gt;Why This Approach Works&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_1_dry_principle"&gt;1. DRY Principle&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Instead of repeating the same YAML structure across multiple applications, you define it once and reuse it everywhere.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_2_intelligent_defaults"&gt;2. Intelligent Defaults&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The template provides sensible defaults (like &lt;code&gt;openshift-gitops&lt;/code&gt; namespace) while allowing customization when needed.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_3_validation"&gt;3. Validation&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Built-in validation ensures required fields are present and conflicting configurations are caught early.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_4_conditional_logic"&gt;4. Conditional Logic&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The template handles complex scenarios like namespace management automatically, reducing the chance of misconfigurations.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_real_world_benefits"&gt;Real-World Benefits&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In practice, this template has several advantages:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: All applications follow the same pattern&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Maintainability&lt;/strong&gt;: Changes to common patterns are made in one place&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Safety&lt;/strong&gt;: Validation prevents common misconfigurations&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;: Supports the full range of Argo CD features&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Development Guidelines&lt;/strong&gt;: Ensure all developers are using the same process&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_real_world_examples"&gt;Real-World Examples&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_1_define_a_ui_branding"&gt;1. Define a UI Branding&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;I would like to define a top banner in the OpenShift Console.
Everything is defined in the &lt;code&gt;clusters/management-cluster/branding&lt;/code&gt; folder in my git repository.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;All I need to do is to define the following values:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;argocd_applications:
my-branding:
enabled: true
namespace: &amp;#34;openshift-gitops&amp;#34;
source:
repositoryURL: &amp;#34;https://github.com/tjungbauer/openshift-clusterconfig-gitops&amp;#34;
targetRevision: &amp;#34;main&amp;#34;
path: &amp;#34;clusters/management-cluster/branding&amp;#34;
destination:
name: in-cluster
namespace: &amp;#34;default&amp;#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_2_define_a_ui_branding_with_custom_helm_value"&gt;2. Define a UI Branding with custom Helm value&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Like above I would like to define a top banner in the OpenShift Console. This time I want to use a custom Helm value to define the background color of the banner.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;argocd_applications:
my-branding-custom-helm:
enabled: true
namespace: &amp;#34;openshift-gitops&amp;#34;
source:
repositoryURL: &amp;#34;https://github.com/tjungbauer/openshift-clusterconfig-gitops&amp;#34;
targetRevision: &amp;#34;main&amp;#34;
path: &amp;#34;clusters/management-cluster/branding&amp;#34;
helm:
parameters:
- name: generic-cluster-config.console.console_banners.topbanner.backgroundcolor
value: &amp;#39;#FF9843&amp;#39;
destination:
name: in-cluster
namespace: &amp;#34;default&amp;#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_3_full_blown_example"&gt;3. Full-Blown Example&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;A full example with all features can be found in my Git repository at: &lt;a href="https://github.com/tjungbauer/helm-charts/blob/main/charts/tpl/values_example_ArgoCD-Application.yaml" target="_blank" rel="noopener"&gt;values_example_ArgoCD-Application.yaml&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_what_about_validation"&gt;What about Validation?&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Above I mentioned that the template is able to validate the values.
This is true for the most important parts.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;For example, try to define the following values:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;[...]
destination:
name: in-cluster
server: &amp;#34;https://kubernetes.default.svc&amp;#34;
namespace: &amp;#34;default&amp;#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;name&lt;/strong&gt; and &lt;strong&gt;server&lt;/strong&gt; are not allowed to be set at the same time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Helm (and Argo CD which is using Helm) will validate the values and fail with an error.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;Error: destination.server and destination.name cannot be set at the same time&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This Argo CD Application template demonstrates how Helmâ€™s templating capabilities can solve real-world GitOps challenges. By combining conditional logic, validation, and sensible defaults, we create a tool thatâ€™s both powerful and easy to use.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The conditional namespace management feature alone saves hours of debugging why Argo CD isnâ€™t behaving as expected with namespace metadata. When you combine this with the DRY benefits and built-in validation, you get a robust foundation for managing Argo CD applications at scale.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Whether youâ€™re managing a few applications or hundreds, this template pattern will help you maintain consistency, reduce errors, and improve your teamâ€™s GitOps experience.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>Cert-Manager Policy Approver in OpenShift</title><link>https://blog.stderr.at/openshift-platform/security/certificates/2025-06-03-cert-manager-approver-policy/</link><pubDate>Tue, 03 Jun 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/security/certificates/2025-06-03-cert-manager-approver-policy/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;One of the most commonly deployed operators in OpenShift environments is the &lt;strong&gt;Cert-Manager Operator&lt;/strong&gt;. It automates the management of TLS certificates for applications running within the cluster, including their issuance and renewal.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The tool supports a variety of certificate issuers by default, including ACME, Vault, and self-signed certificates. Whenever a certificate is needed, Cert-Manager will automatically create a CertificateRequest resource that contains the details of the certificate. This resource is then processed by the appropriate issuer to generate the actual TLS certificate. The approval process in this case is usually fully automated, meaning that the certificate is issued without any manual intervention.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;But what if you want to have more control? What if certificate issuance must follow strict organizational policies, such as requiring a specifc country code or organization name?
This is where the &lt;strong&gt;CertificateRequestPolicy&lt;/strong&gt; resource, a resource provided by the Approver Policy, comes into play.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This article walks through configuring the &lt;strong&gt;Cert-Manager Approver Policy&lt;/strong&gt; in OpenShift to enforce granular policies on certificate requests.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Before you begin, ensure you have the following:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;OpenShift 4.16 or higher with cluster-admin access&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cert-Manager Operator installed&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The installation of Cert-Manager itself is discussed in the article: &lt;a href="https://blog.stderr.at/gitopscollection/2024-07-04-managing-certificates-with-gitops/"&gt;Managing Certificates using GitOps approach&lt;/a&gt;.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The Cert-Manager Operator does not currently support the Approver Policy by default. You need to install the Approver Policy manually using a Helm Chart. There is a feature request to include the Approver Policy in the Cert-Manager Operator in the future.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_adding_approver_policy_chart_as_a_dependency"&gt;Adding Approver Policy Chart as a dependency&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Source: &lt;a href="https://github.com/tjungbauer/openshift-clusterconfig-gitops/tree/main/clusters/management-cluster/cert-manager" target="_blank" rel="noopener"&gt;Cert-Manager Deployment&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The above Chart contains the necessary resources to deploy the Cert-Manager itself and Cert-Manager Approver Policy in OpenShift.
To deploy the Approver Policy alongside Cert-Manager, add it as a dependency in your &lt;strong&gt;Chart.yaml&lt;/strong&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;[...]
- name: cert-manager-approver-policy
version: v0.19.0
repository: https://charts.jetstack.io
[...]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This is the official Helm Chart for the Cert-Manager Approver Policy tool provided by Jetstack. The version used in this example is v0.19.0, but you can use a newer version if available.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_configuration_of_the_helm_chart"&gt;Configuration of the Helm Chart&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The initial &lt;a href="https://github.com/tjungbauer/openshift-clusterconfig-gitops/blob/main/clusters/management-cluster/cert-manager/values.yaml" target="_blank" rel="noopener"&gt;values.yaml&lt;/a&gt; file was extended to:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;include the configuration for the Approver Policy Chart&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the configuration for a &lt;strong&gt;CertificateRequestPolicy&lt;/strong&gt; object&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;with some specific modifications to the CertManager resource itself&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_disabling_cert_managers_auto_approver"&gt;Disabling Cert-Managerâ€™s Auto-Approver&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The first step we need to do is to disable the auto-approver of the Cert-Manager. If this is not done, there will be a race condition between the auto-approver and the Approver Policy, which will lead to unexpected results.
These changes are done by:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;cert-manager:
certManager:
enable_patch: true &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
unsupportedConfigOverrides:
controller:
args:
- &amp;#39;--controllers=*,-certificaterequests-approver&amp;#39; &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;This enables the patching of the CertManager resource, which tells the chart to overwrite (patch) the automatically generated CertManager resource with the custom configuration.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;This disables the auto-approver Controller of the Cert-Manager.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
As the name suggests, this is currently an unsupported configuration, but it is necessary to disable the auto-approver for the Cert-Manager. In the future versions of the Cert-Manager, this might change, and the auto-approver might be supported out of the box.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In my case, the full CertManager resource looks like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: operator.openshift.io/v1alpha1
kind: CertManager
metadata:
annotations:
name: cluster
spec:
controllerConfig:
overrideArgs: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
- &amp;#39;--dns01-recursive-nameservers-only&amp;#39;
- &amp;#39;--dns01-recursive-nameservers=ns-362.awsdns-45.com:53,ns-930.awsdns-52.net:53&amp;#39;
logLevel: Normal
managementState: Managed
operatorLogLevel: Normal
unsupportedConfigOverrides: &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
controller:
args:
- &amp;#39;--controllers=*,-certificaterequests-approver&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Settings to support AWS Nameservers for DNS01 challenges.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;This disables the auto-approver Controller of the Cert-Manager.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_configuration_of_the_approver_policy"&gt;Configuration of the Approver Policy&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The second step is to configure the Approver Policy chart. This chart will deploy the necessary resources, most importantly a Deployment that will start the Pods which will process the CertificateRequestPolicy resources later on.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;My configuration for that chart looks like this (some default values are omitted for brevity):&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;cert-manager-approver-policy:
crds: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
# This option decides if the CRDs should be installed
# as part of the Helm installation.
enabled: true
# This option makes it so that the &amp;#34;helm.sh/resource-policy&amp;#34;: keep
# annotation is added to the CRD. This will prevent Helm from uninstalling
# the CRD when the Helm release is uninstalled.
# WARNING: when the CRDs are removed, all cert-manager-approver-policy custom resources
# (CertificateRequestPolicy) will be removed too by the garbage collector.
keep: true
# Number of replicas of approver-policy to run.
replicaCount: 1 &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
image: &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
# Target image repository.
repository: quay.io/jetstack/cert-manager-approver-policy
# Kubernetes imagePullPolicy on Deployment.
pullPolicy: IfNotPresent
tag: v0.19.0
app:
# List of signer names that approver-policy will be given permission to
# approve and deny. CertificateRequests referencing these signer names can be
# processed by approver-policy. Defaults to an empty array, allowing approval
# for all signers.
approveSignerNames: &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
- &amp;#39;issuers.cert-manager.io/*&amp;#39;
- &amp;#39;clusterissuers.cert-manager.io/*&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;This enables the installation of the CRDs that are required for the Approver Policy.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The number of replicas of the Approver Policy Deployment. In most cases, one replica is enough.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The image configuration for the Approver Policy. The image is pulled from the Jetstack Quay.io repository.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The list of signer names that the Approver Policy will be allowed to approve. In this case, it is configured to allow all issuers and clusterissuers.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The approveSignerNames are, if configured, an important setting, especially if you want to add custom (cluster)issuers. In such a case, you need to add the name of the custom issuer to this list. Otherwise the Approver Policy will not be able to approve the CertificateRequests for that issuer.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_creating_a_certificaterequestpolicy_and_rolebinding"&gt;Creating a CertificateRequestPolicy and Role(Binding)&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The final step in our configuration is to define a &lt;strong&gt;CertificateRequestPolicy&lt;/strong&gt; resource that will define the policy for the certificate requests. This resource will be processed by the Approver Policy and will determine if a certificate request is approved or denied based on the defined criteria.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following example shows a CertificateRequestPolicy that will:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Allow certificate requests with any common name, DNS names, IP addresses, URIs, and email addresses.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Require DNS names to be set.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Require the subject to contain a specific organization (MyOrganization) and country code (AT).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Allow usages for server auth and client auth.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set constraints for the certificate duration (1h-24h) and private key algorithm (RSA) and size (2048-4096).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Allow all issuers by using an empty selector.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;role: cert-manager-policy:global-approver &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
serviceAccount: cert-manager &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
cert_manager_Namespace: cert-manager &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
policies:
- name: my-approver-policy
enabled: true
allowed:
commonName:
required: false
value: &amp;#34;*&amp;#34;
validations: []
dnsNames: &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
required: true
values:
- &amp;#34;*&amp;#34;
validations: []
ipAddresses:
required: false
values: [&amp;#34;*&amp;#34;]
validations: []
uris:
required: false
values:
- &amp;#34;*&amp;#34;
validations: []
emailAddresses:
required: false
values:
- &amp;#34;*&amp;#34;
validations: []
# isCA: false
subject:
organizations: &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
required: true
values:
- &amp;#34;MyOrganization&amp;#34;
validations:
- rule: self.matches(&amp;#34;MyOrganization&amp;#34;)
message: Organization must be MyOrganization
countries:
required: true
values:
- AT
validations:
- rule: self.matches(&amp;#34;AT&amp;#34;)
message: Country code must be AT
usages:
- &amp;#34;server auth&amp;#34;
- &amp;#34;client auth&amp;#34;
constraints: &lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;(6)&lt;/b&gt;
minDuration: 1h
maxDuration: 24h
privateKey:
algorithm: RSA
minSize: 2048
maxSize: 4096
selector:
issuerRef: {} &lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;(7)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The role that is used to approve the certificate requests. This role must be created in the OpenShift cluster and must have the necessary permissions to approve certificate requests.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The service account that is used by the Approver Policy to process the certificate requests. This service account must have the necessary permissions to access the CertificateRequest resources.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The namespace where the Cert-Manager is deployed. This is usually the &lt;code&gt;cert-manager&lt;/code&gt; namespace, but you can change it if you have a different namespace.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The DNS names are required to be set for the certificate request.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The subject must contain the organization MyOrganization and the country code AT.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The constraints for the certificate request, such as the minimum and maximum duration, private key algorithm, and size.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="7"&gt;&lt;/i&gt;&lt;b&gt;7&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The selector is empty, which means that the policy applies to all issuers. If you want to limit the policy to specific issuers, you can specify the issuerRef here.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_rendered_certificaterequestpolicy_and_rolebinding"&gt;Rendered CertificateRequestPolicy and Role(Binding)&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The above configuration will create a CertificateRequestPolicy resource that looks like this:
&lt;div class="expand"&gt;
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});"&gt;
&lt;i style="font-size:x-small;" class="fas fa-chevron-right"&gt;&lt;/i&gt;
&lt;span&gt;
&lt;a&gt;Expand me...&lt;/a&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class="expand-content" style="display: none;"&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
# Source: cert-manager/templates/ClusterRole-Approver-Policy-approving.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
name: &amp;#34;cert-manager-policy:global-approver&amp;#34;
labels:
helm.sh/chart: cert-manager-2.0.0
app.kubernetes.io/name: cert-manager
app.kubernetes.io/instance: release-name
app.kubernetes.io/managed-by: Helm
rules:
- verbs:
- use
apiGroups:
- policy.cert-manager.io
resources:
- certificaterequestpolicies
resourceNames:
- my-approver-policy
---
# Source: cert-manager/charts/cert-manager-approver-policy/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
labels:
app.kubernetes.io/name: cert-manager-approver-policy
helm.sh/chart: cert-manager-approver-policy-v0.19.0
app.kubernetes.io/instance: release-name
app.kubernetes.io/version: &amp;#34;v0.19.0&amp;#34;
app.kubernetes.io/managed-by: Helm
name: cert-manager-approver-policy
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: cert-manager-approver-policy
subjects:
- kind: ServiceAccount
name: cert-manager-approver-policy
namespace: default
---
# Source: cert-manager/templates/CertificateRequestPolicy.yaml
apiVersion: policy.cert-manager.io/v1alpha1
kind: CertificateRequestPolicy
metadata:
name: my-approver-policy
annotations:
argocd.argoproj.io/sync-wave: &amp;#34;10&amp;#34;
labels:
helm.sh/chart: cert-manager-2.0.0
app.kubernetes.io/name: cert-manager
app.kubernetes.io/instance: release-name
app.kubernetes.io/managed-by: Helm
spec:
allowed:
commonName:
required: false
value: &amp;#34;*&amp;#34;
validations: []
dnsNames:
required: true
values:
- &amp;#34;*&amp;#34;
validations: []
emailAddresses:
required: false
values:
- &amp;#34;*&amp;#34;
validations: []
ipAddresses:
required: false
values:
- &amp;#34;*&amp;#34;
validations: []
uris:
required: false
values:
- &amp;#34;*&amp;#34;
validations: []
isCA: false
subject:
organizations:
required: true
values:
- &amp;#34;MyOrganization&amp;#34;
validations:
- rule: &amp;#34;self.matches(\&amp;#34;MyOrganization\&amp;#34;)&amp;#34;
message: &amp;#34;Organization must be MyOrganization&amp;#34;
countries:
required: true
values:
- &amp;#34;AT&amp;#34;
validations:
- rule: &amp;#34;self.matches(\&amp;#34;AT\&amp;#34;)&amp;#34;
message: &amp;#34;Country code must be AT&amp;#34;
organizationalUnits:
required: false
values: [&amp;#34;*&amp;#34;]
validations: []
localities:
required: false
values: [&amp;#34;*&amp;#34;]
validations: []
provinces:
required: false
values: [&amp;#34;*&amp;#34;]
validations: []
streetAddresses:
required: false
values: [&amp;#34;*&amp;#34;]
validations: []
postalCodes:
required: false
values: [&amp;#34;*&amp;#34;]
validations: []
serialNumber:
required: false
value: &amp;#34;*&amp;#34;
validations: []
usages:
- &amp;#34;server auth&amp;#34;
- &amp;#34;client auth&amp;#34;
constraints:
minDuration: 1h
maxDuration: 24h
privateKey:
algorithm: RSA
minSize: 2048
maxSize: 4096
selector:
issuerRef: {}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;One important note is about the ClusterRole and ClusterRoleBinding that are created by the Helm Chart. The role looks like the following and is required to allow the Approver Policy to approve certificate requests. This small bit, puzzled me for a while:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;rules:
- verbs:
- use
apiGroups:
- policy.cert-manager.io
resources:
- certificaterequestpolicies
resourceNames:
- my-approver-policy&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;With the above configuration we are good to go. The Helm Chart can be deployed to the OpenShift cluster (for example, using Argo CD), and the CertificateRequestPolicy will be created automatically.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;A new Pod is running in the &lt;strong&gt;cert-manager&lt;/strong&gt; namespace:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;â¯ oc get pods -n cert-manager | grep approver
NAME READY STATUS RESTARTS AGE
cert-manager-approver-policy-xxxxx 1/1 Running 0 XXm &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The Pod &lt;code&gt;cert-manager-approver-policy-xxxxx&lt;/code&gt; is the Pod that is responsible for processing the CertificateRequestPolicy resources.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_testing_the_policy"&gt;Testing the Policy&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_test_1_valid_certificate_request"&gt;Test 1 - Valid Certificate Request&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now it is time to test the policy. We need to create a Certificate and monitor the output of our approval pod.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As a reminder, the policy we created requires the following:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The subject must contain the organization MyOrganization&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The subject must contain the country code AT.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The keysize must be at least 2048 bits. (max 4096 bits)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The duration must be between 1 hour and 24 hours.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The usage must be server auth or client auth.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Letâ€™s create this example Certificate in the &lt;code&gt;myproject&lt;/code&gt; namespace:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
name: test-certificate1
namespace: myproject
spec:
dnsNames:
- test1.apps.ocp.aws.ispworld.at
duration: 24h
issuerRef:
kind: ClusterIssuer
name: letsencrypt-prod
privateKey:
algorithm: RSA
encoding: PKCS1
rotationPolicy: Always
secretName: test1
subject:
organizations:
- MyOrganization
countries:
- AT
usages:
- server auth&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In the log of the Approver Policy Pod, we should see the following output:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;time=2025-06-03T16:07:58.656Z level=DEBUG+3 msg=&amp;#34;Approved by CertificateRequestPolicy: \&amp;#34;my-approver-policy\&amp;#34;&amp;#34; logger=controller-manager/events type=Normal object=&amp;#34;{Kind:CertificateRequest Namespace:myproject Name:test-certificate1-1 [...]}&amp;#34; reason=Approved&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This indicates that the CertificateRequest was approved by the Approver Policy. The policy was able to validate the subject, keysize, duration, and usage of the certificate request and approved it accordingly.
The certificate has been created successfully in the &lt;strong&gt;myproject&lt;/strong&gt; namespace, and the secret &lt;strong&gt;test1&lt;/strong&gt; contains the TLS certificate and private key.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;â¯ oc get secret test1 -n myproject -o yaml
apiVersion: v1
data:
tls.crt: ...
tls.key: ...
kind: Secret
metadata:
labels:
controller.cert-manager.io/fao: &amp;#34;true&amp;#34;
name: test1
namespace: myproject
type: kubernetes.io/tls&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_test_2_invalid_certificate_request"&gt;Test 2 - Invalid Certificate Request&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;That was easy, but what happens if we create a CertificateRequest that does not meet the policy requirements? Letâ€™s try to create a Certificate without the required organization or a wrong country code:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
name: test-certificate2
namespace: myproject
spec:
dnsNames:
- test2.apps.ocp.aws.ispworld.at
duration: 24h
issuerRef:
kind: ClusterIssuer
name: letsencrypt-prod
privateKey:
algorithm: RSA
encoding: PKCS1
rotationPolicy: Always
secretName: test2
subject: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
countries:
- XX
usages:
- server auth&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The subject does not contain the required organization MyOrganization and the country code is set to XX, which is not allowed by the policy.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This will lead to the following error in the log of the Approver Policy Pod:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;time=2025-06-03T16:16:02.233Z level=DEBUG+3 msg=&amp;#34;No policy approved this request: [my-approver-policy: [spec.allowed.subject.organizations.required: Required value: true, spec.allowed.subject.countries.values: Invalid value: []string{\&amp;#34;XX\&amp;#34;}: AT, spec.allowed.subject.countries.validations[0]: Invalid value: \&amp;#34;XX\&amp;#34;: Country code must be AT]]&amp;#34; logger=controller-manager/events type=Warning object=&amp;#34;{Kind:CertificateRequest Namespace:myproject Name:test-certificate2-1 ...&amp;#34; reason=Denied&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;It complains that the subject does not meet the policy requirements and therefore the CertificateRequest was denied.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_ok_we_have_a_policy_but_whats_next"&gt;Ok we have a policy, but whats next?&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The above example shows how the Cert-Manager Approver Policy can be configured and deployed, even if it is not yet supported by the Cert-Manager Operator. However, we only scratched the surface of what is possible with the Approver Policy.
You can create more complex policies that include additional validations, such as checking the validity of the DNS names, IP addresses, or URIs. You can also create policies that require specific email addresses or organizational units in the subject.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;You can even create fine-grained policies that apply to specific issuers or namespaces by using the &lt;code&gt;selector&lt;/code&gt; field in the CertificateRequestPolicy resource. This allows you to create policies that are tailored to your specific requirements and use cases.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The best references can be found here:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Official documentation: &lt;a href="https://cert-manager.io/docs/policy/approval/approver-policy/" target="_blank" rel="noopener"&gt;Cert-Manager Approver Policy&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example Policies: &lt;a href="https://github.com/cert-manager/approver-policy/tree/main/docs/examples" target="_blank" rel="noopener"&gt;Cert-Manager Approver Policy Examples&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Cert-Manager Approver Policy is a powerful tool that allows you to implement custom policies for certificate requests in OpenShift. It provides a way to control the issuance of TLS certificates based on specific criteria, such as the subject, key size, duration, and usage of the certificate.
While not yet officially supported by the Cert-Manager Operator, it can be easily integrated into your OpenShift environment using a Helm Chart. Future support is currently being discussed, and it is expected that the Approver Policy will be included in the Cert-Manager Operator in the future.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>Single log out from Keycloak and OpenShift</title><link>https://blog.stderr.at/openshift-platform/security/authentication/2025-05-22-keycloak-openshift-singlelogout/</link><pubDate>Thu, 22 May 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/security/authentication/2025-05-22-keycloak-openshift-singlelogout/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;The following 1-minute article is a follow-up to my &lt;a href="https://blog.stderr.at/openshift/2025/05/step-by-step-using-keycloak-authentication-in-openshift/"&gt;previous article&lt;/a&gt; about how to use Keycloak as an authentication provider for OpenShift. In this article, I will show you how to configure Keycloak and OpenShift for Single Log Out (SLO). This means that when you log out from Keycloak, you will also be logged out from OpenShift automatically. This requires some additional configuration in Keycloak and OpenShift, but it is not too complicated.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following prerequisites are required to follow this article:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;OpenShift 4.16 or higher with cluster-admin privileges&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Keycloak installed and configured, as described in the &lt;a href="https://blog.stderr.at/openshift/2025/05/step-by-step-using-keycloak-authentication-in-openshift/"&gt;Step by Step - Using Keycloak Authentication in OpenShift&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_keycloak_configuration"&gt;Keycloak Configuration&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Configure the logout URL in Keycloak. This is done by adding a new &lt;strong&gt;Valid post logout redirect URIs&lt;/strong&gt; to the Keycloak client configuration. In this case, we want to call the OpenShift logout URL.
Which is: &lt;strong&gt;&lt;a href="https://oauth-openshift.apps.&amp;lt;your-cluster-name&amp;gt;/logout" class="bare"&gt;https://oauth-openshift.apps.&amp;lt;your-cluster-name&amp;gt;/logout&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This is done in the client settings:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/set-logout-url.png" alt="Set Logout URL"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_openshift_configuration"&gt;OpenShift Configuration&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now we need to configure OpenShift to use the logout URL. This is done by adding a new &lt;strong&gt;Post Logout Redirect URI&lt;/strong&gt; to the OpenShift Console configuration.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Modify the existing Console resource named cluster and add the logoutRedirect parameter to the authentication section.
The important pieces of the logout URL are the &lt;strong&gt;client_id&lt;/strong&gt; and the &lt;strong&gt;post_logout_redirect_uri&lt;/strong&gt;. The client ID must be the same as the Keycloak client ID, and the post logout redirect URI must be the OpenShift logout URL.
(Also change the &amp;lt;keycloakURL&amp;gt; and &amp;lt;realm&amp;gt; to your Keycloak URL and realm name.)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Actually, instead of &lt;strong&gt;client_id&lt;/strong&gt;, the &lt;strong&gt;id_token_hint&lt;/strong&gt; should be used. But OpenShift does not store the token, so we are using the client ID instead.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: config.openshift.io/v1
kind: Console
metadata:
name: cluster
spec:
authentication:
logoutRedirect: &amp;#39;https://&amp;lt;keycloakURL&amp;gt;/realms/&amp;lt;realm&amp;gt;/protocol/openid-connect/logout?client_id=&amp;lt;realm&amp;gt;&amp;amp;post_logout_redirect_uri=https://console-openshift-console.apps.ocp.aws.ispworld.at&amp;#39; &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The logout URL for OpenShift. This is the URL that will be called when you log out from Keycloak. The URL must contain the client ID and the post logout redirect URI (console of OpenShift).&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Wait a few moments until the OpenShift Console Operator applies the new configuration.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_testing_the_configuration"&gt;Testing the configuration&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now that the configuration is done, we can test the Single Log Out functionality.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;We are logged into OpenShift already as user &lt;strong&gt;testuser&lt;/strong&gt;&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/logged-in.png" alt="Logged in to OpenShift"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now we log out from OpenShift using the &lt;strong&gt;Log out&lt;/strong&gt; button in the upper right corner.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/logging-out.png" alt="Log out from OpenShift"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This will redirect us to the Keycloak logout page where we need to confirm the logout.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/keycloak-logout.png?width=420" alt="Keycloak Logout"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This will log us out from Keycloak and redirect us back to the OpenShift logout page.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/logged-out.png?width=420" alt="Logged out from OpenShift"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This is it. You are now logged out from both Keycloak and OpenShift. You can also check the Sessions in Keycloak to see that the session for the user is terminated.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As promised, this was a short article about how to configure Keycloak and OpenShift for Single Log Out. This is a beneficial feature if you want to ensure that users are logged out from all applications when they log out from Keycloak. It is also a good security practice to ensure that users are logged out from all applications when they are done using them.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>Step by Step - Using Keycloak Authentication in OpenShift</title><link>https://blog.stderr.at/openshift-platform/security/authentication/2025-05-17-step-by-step-keycloak-and-openshift/</link><pubDate>Sat, 17 May 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/security/authentication/2025-05-17-step-by-step-keycloak-and-openshift/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;I was recently asked about how to use Keycloak as an authentication provider for OpenShift. How to install Keycloak using the Operator and how to configure Keycloak and OpenShift so that users can log in to OpenShift using OpenID.
I have to admit that the exact steps are not easy to find, so I decided to write a blog post about it, describing each step in detail.
This time I will not use GitOps, but the OpenShift and Keycloak Web Console to show the steps, because before we put it into GitOps, we need to understand what is actually happening.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This article tries to explain every step required so that a user can authenticate to OpenShift using Keycloak as an Identity Provider (IDP) and that Groups from Keycloak are imported into OpenShift. This article does not cover a production grade installation of Keycloak, but only a test installation, so you can see how it works. For production, you might want to consider a proper database (maybe external, but at least with a backup), high availability, etc.).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following prerequisites are required to follow this article:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;OpenShift 4.16 or higher with cluster-admin privileges&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_installing_keycloak_operator"&gt;Installing Keycloak Operator&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The very first step is to install the Keycloak Operator. This can be done using the OpenShift Web Console or the CLI or GitOps. I will show the steps using the OpenShift Web Console.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Installing the Keycloak Operator via GitOps can be seen in this &lt;a href="https://github.com/tjungbauer/openshift-clusterconfig-gitops/tree/main/clusters/management-cluster/setup-rh-build-of-keycloak" target="_blank" rel="noopener"&gt;GitHub repository&lt;/a&gt;. By the time you read this article, this repository will install and configure a Keycloak instance automatically using a local &lt;strong&gt;EXAMPLE&lt;/strong&gt; Postgres database. It does not yet import any Realm or additional configuration, but I will try to add this in the future when time allows. Oh, and it is just a demo and not production ready, so please do not use it in production.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Log in to the OpenShift Web Console as a user with cluster-admin privileges.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the OpenShift Web Console, navigate to the Operators â†’ OperatorHub.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the OperatorHub, search for &lt;strong&gt;Red Hat build of Keycloak&lt;/strong&gt; and select the Operator from the list.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/search-operator.png?width=320" alt="Search Keycloak Operator"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the latest version of the Keycloak Operator.&lt;/p&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
You can keep the default settings for the installation, but I recommend using a specific namespace. In this example, I will use the namespace &lt;code&gt;keycloak&lt;/code&gt; for the installation.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/install-operator.png?width=840" alt="Install Keycloak Operator"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Press the &lt;strong&gt;Install&lt;/strong&gt; button to install the Keycloak Operator. After a few minutes, the Keycloak Operator should be installed and running in the OpenShift cluster. You can check the status of the Operator in the &lt;strong&gt;Installed Operators&lt;/strong&gt; view.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/installed-operator.png" alt="Installed Keycloak Operator"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_create_a_local_example_postgres_database"&gt;Create a local example Postgres Database&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Demo only! This is not production ready and should not be used in production.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As a next step, we need to create a local Postgres database for Keycloak. Secrets are used to store the database credentials, and a simple StatefulSet is used to create the database. The database will be created in the same namespace as the Keycloak Operator.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;First, letâ€™s create a Secret for the database credentials. In OpenShift select the &lt;strong&gt;keycloak&lt;/strong&gt; namespace and navigate to the &lt;strong&gt;Secrets&lt;/strong&gt; view. Create a new Secret with the following test configuration:&lt;/p&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;kind: Secret
apiVersion: v1
metadata:
name: keycloak-db-secret
namespace: keycloak
stringData:
password: thisisonly4testingNOT4prod &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
username: testuser
type: Opaque&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The password for the database. This is just a test password and should not be used in production.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, we need to create a StatefulSet for the Postgres database, again in the &lt;strong&gt;keycloak&lt;/strong&gt; namespace.
Again, as I cannot mention that enough, this is just a test configuration and should not be used in production.&lt;/p&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: apps/v1
kind: StatefulSet
metadata:
name: postgresql-db
namespace: keycloak
spec:
serviceName: postgresql-db-service
selector:
matchLabels:
app: postgresql-db
replicas: 1 &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
template:
metadata:
labels:
app: postgresql-db
spec:
containers:
- name: postgresql-db
image: postgres:15 &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
volumeMounts:
- mountPath: /data
name: psql
env: &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
- name: POSTGRES_USER
value: testuser
- name: POSTGRES_PASSWORD
value: thisisonly4testingNOT4prod
- name: PGDATA
value: /data/pgdata
- name: POSTGRES_DB
value: keycloak
volumeClaimTemplates: &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
- metadata:
name: psql
spec:
accessModes: [ &amp;#34;ReadWriteOnce&amp;#34; ]
storageClassName: &amp;#34;gp3-csi&amp;#34;
resources:
requests:
storage: 10Gi&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The number of replicas for the database. In this example, we are using a single replica.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The image for the database, postgres version 15 in this example.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The environment variables for the database. The username and password are the same as in the Secret we created before, and &lt;strong&gt;clear text&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The volume for the database. In this example, the StatefulSet uses a volume claim template to create a volume with the size of 10 GB for the database. The volume is created using the &lt;code&gt;gp3-csi&lt;/code&gt; storage class. You can use any other storage class that is available in your OpenShift cluster or even remove this line and use the default class instead.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, we need to create a Service for the database so that the Keycloak Operator can access the database. Again, in the &lt;strong&gt;keycloak&lt;/strong&gt; namespace.&lt;/p&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: v1
kind: Service
metadata:
name: postgres-db
namespace: keycloak
spec:
selector:
app: postgresql-db &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
type: LoadBalancer
ports:
- port: 5432
targetPort: 5432&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The selector for the Service. This must match the label of the StatefulSet we created before.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_creating_a_keycloak_instance"&gt;Creating a Keycloak Instance&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now that the Keycloak Operator is installed and our example database is running, we can create a Keycloak instance.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;In the OpenShift Web Console, navigate to the &lt;strong&gt;Installed Operators&lt;/strong&gt; view and select the Keycloak Operator. (Maybe you need to select the keycloak namespace first.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the Keycloak Operator view, create a new instance of &lt;strong&gt;Keycloak&lt;/strong&gt; and switch to the &lt;strong&gt;YAML&lt;/strong&gt; view.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/create-keycloak-instance.png?width=550" alt="Create Keycloak Instance"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The fun part here is that the YAML example the Operator provides is actually &lt;strong&gt;wrong and does not work&lt;/strong&gt;. Something that kept me busy for a while.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Replace the YAML with the following configuration:&lt;/p&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: k8s.keycloak.org/v2alpha1
kind: Keycloak
metadata:
name: keycloak &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
namespace: keycloak
labels:
app: sso
spec:
db: &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
host: postgres-db
passwordSecret:
key: password
name: keycloak-db-secret
usernameSecret:
key: username
name: keycloak-db-secret
vendor: postgres
hostname: &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
hostname: sso.apps.ocp.aws.ispworld.at
http: &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
tlsSecret: keycloak-certificate
instances: 1 &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The name and the namespace of the Keycloak instance.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The database configuration. In this example, we are using a local Postgres database. You can also use an external database, but you need to configure the connection string accordingly.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Hostname of our Keycloak instance.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The TLS secret for the Keycloak instance. You need to create a TLS secret with the certificate and key for the hostname. This is where the example YAML is wrong. It tries to put &lt;em&gt;tlsSecret&lt;/em&gt; under &lt;em&gt;spec&lt;/em&gt;, but it should be under &lt;em&gt;http&lt;/em&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The number of instances of Keycloak. In this example, we are using a single instance.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_what_about_the_ssl_certificate"&gt;What about the SSL Certificate?&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Keycloak Operator does not create a certificate for the Keycloak instance. You need to create a certificate manually and store it in a secret. The Operator will use this secret to create the TLS certificate for the Keycloak instance.
In the example above we are referencing a secret called &lt;code&gt;keycloak-certificate&lt;/code&gt; in the &lt;code&gt;keycloak&lt;/code&gt; namespace. This secret was created using the &lt;strong&gt;Cert Manager Operator&lt;/strong&gt;. For example, you can use the following configuration to create a certificate for the Keycloak instance.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
name: keycloak-certificate
namespace: keycloak
spec:
dnsNames:
- sso.apps.ocp.aws.ispworld.at &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
duration: 2160h0m0s
issuerRef:
kind: ClusterIssuer
name: letsencrypt-prod &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
privateKey:
algorithm: RSA
encoding: PKCS1
rotationPolicy: Always
secretName: keycloak-certificate &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The DNS name the Certificate is valid for. This should be the same as the hostname in the Keycloak instance.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The issuer for the certificate. In this example, we are using the &lt;strong&gt;LetsEncrypt&lt;/strong&gt; ClusterIssuer.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The name of the secret where the certificate is stored. This should be the same as the TLS secret in the Keycloak instance.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;I strongly recommend using the &lt;strong&gt;Cert Manager Operator&lt;/strong&gt; to automatically request and approve the certificate. However, if you do not have this automation in place, you can use a self-signed certificate. This certificate must be created manually and stored as a secret.
For example, you can use the following command to create a self-signed certificate and store it in a secret:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &amp;#34;/CN=test.keycloak.org/O=Test Keycloak./C=US&amp;#34;
oc create secret -n keycloak tls keycloak-certificate2 --cert=tls.crt --key=tls.key&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_login_in_to_keycloak"&gt;Login in to Keycloak&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Once the Keycloak instance is created and all Pods (1) are running, you can log in to the Keycloak Admin Console using the following URL: &lt;a href="https://sso.apps.ocp.aws.ispworld.at" class="bare"&gt;https://sso.apps.ocp.aws.ispworld.at&lt;/a&gt;
This is the hostname we configured in the keycloak instance.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To authenticate, you need to fetch the initial password for the admin user. This password is stored in a secret called &lt;strong&gt;keycloak-initial-admin&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;You can use the following command to fetch the password:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc extract secret/keycloak-initial-admin -n keycloak --to=-&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;or you can use the OpenShift Web Console to view the secret.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Once authenticated, you should see the Keycloak Admin Console:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/keycloak-initial-login.png" alt="Keycloak Admin Console"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The first thing you should do is to change the password for the admin user. I trust you know how to do this :)
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_configure_keycloak_to_be_used_by_openshift"&gt;Configure Keycloak to be used by OpenShift&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The next steps are to configure Keycloak to be used as an Identity Provider (IDP) for OpenShift. This is done by creating a new Realm and a new Client in Keycloak. The following steps will show you the minimum configuration required to use Keycloak as an IDP for OpenShift. It does not cover all the options and features of Keycloak (and there are a lot), but it should be enough to get you started.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The full documentation for Keycloak can be found at &lt;a href="https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/" target="_blank" rel="noopener"&gt;Keycloak Documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_create_a_new_realm_and_client"&gt;Create a new Realm and Client&lt;/h3&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;In the Realm Dropdown (upper left corner) select &lt;strong&gt;Create new Realm&lt;/strong&gt;&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/create-new-realm.png?width=420" alt="Create new Realm"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a new Realm called &lt;strong&gt;openshift&lt;/strong&gt; (Enabled, of course) and press &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/create-new-realm-openshift.png?width=1024" alt="Create new Realm OpenShift"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now, inside the Realm &lt;strong&gt;openshift&lt;/strong&gt;, select &lt;strong&gt;Clients&lt;/strong&gt; and press the &lt;strong&gt;Create client&lt;/strong&gt; button.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/create-new-client.png?width=1024" alt="Create new Client"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a new Client with the following configuration. Name it, for example, &lt;strong&gt;openshift&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Be sure the &lt;strong&gt;Client type&lt;/strong&gt; is set to &lt;strong&gt;OpenID Connect&lt;/strong&gt;&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/new-client-screen-1.png?width=1024" alt="Create new Client"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Enable &lt;strong&gt;Client authentication&lt;/strong&gt;. The rest can be left as default.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/new-client-screen-2.png?width=1024" alt="Create new Client"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add the following redirect URL and Web origin and press &lt;strong&gt;Save&lt;/strong&gt;.:&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Redirect URL: &lt;a href="https://oauth-openshift.apps.&amp;lt;your-cluster-name&amp;gt;/oauth2callback/*" class="bare"&gt;https://oauth-openshift.apps.&amp;lt;your-cluster-name&amp;gt;/oauth2callback/*&lt;/a&gt; â€¦â€‹ redirecting everything under oauth2callback&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Web origin: &lt;a href="https://oauth-openshift.apps.&amp;lt;your-cluster-name&amp;gt;" class="bare"&gt;https://oauth-openshift.apps.&amp;lt;your-cluster-name&amp;gt;&lt;/a&gt;;&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/new-client-screen-3.png?width=1024" alt="Create new Client"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_create_a_new_user_and_a_group"&gt;Create a new User and a Group&lt;/h3&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;In the &lt;strong&gt;openshift&lt;/strong&gt; Realm, select &lt;strong&gt;Groups&lt;/strong&gt;, press the &lt;strong&gt;Create group&lt;/strong&gt; button and create a group called, for example, &lt;strong&gt;openshift-users&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the &lt;strong&gt;openshift&lt;/strong&gt; Realm, select &lt;strong&gt;Users&lt;/strong&gt; and press the &lt;strong&gt;Add user&lt;/strong&gt; button. Be sure to join the group &lt;strong&gt;openshift-users&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/new-user.png?width=1024" alt="Create new User"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;No more configuration is needed for the (test) user at this point.&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set the password for the user. Select the user we have just created, select the &lt;strong&gt;Credentials&lt;/strong&gt; tab and press &lt;strong&gt;Set password&lt;/strong&gt;. Set the password to &lt;strong&gt;Temporary&lt;/strong&gt; to force the user to change the password on the first login.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/new-user-password.png?width=1024" alt="Set password for new User"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_configure_a_group_mapper"&gt;Configure a Group Mapper&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The above configuration is enough to log in to OpenShift using Keycloak as an IDP (except that we need to configure OpenShift itself). However, we also want to import the groups from Keycloak into OpenShift. This configuration was not easy to find, and is done by creating a Group Mapper in Keycloak.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;In the &lt;strong&gt;openshift&lt;/strong&gt; Realm, select &lt;strong&gt;Clients scopes&lt;/strong&gt; and select the &lt;strong&gt;profile&lt;/strong&gt; scope:&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/client-scopes.png?width=1024" alt="Client Scopes"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Select the &lt;strong&gt;Mappers&lt;/strong&gt; tab and Add a mapper &lt;strong&gt;By configuration&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/client-scopes-mappers.png?width=1024" alt="Client Scopes Mappers"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Select the &lt;strong&gt;Group Membership&lt;/strong&gt; mapper.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/client-scopes-new-mapper.png?width=1024" alt="Client Scopes Create Group Membership Mapper"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure the mapper with the following settings:&lt;/p&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Mapper Type: &lt;strong&gt;Group Membership&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Name: &lt;strong&gt;openshift-groups&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Token Claim Name: &lt;strong&gt;groups&lt;/strong&gt; â†’ This is the name of the claim that will be used to map the groups from Keycloak to OpenShift.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Full group path: &lt;strong&gt;OFF&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add to ID token: &lt;strong&gt;ON&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add to access token: &lt;strong&gt;ON&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add to userinfo: &lt;strong&gt;ON&lt;/strong&gt;&lt;/p&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Disable the &lt;strong&gt;Full groupo path&lt;/strong&gt; option, otherwise the group name will be prefixed with a &lt;strong&gt;/&lt;/strong&gt;. Moreover, be sure that you set the &lt;strong&gt;Token Claim Name&lt;/strong&gt; correctly to the claim we will configure in OpenShift (groups).
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/client-scopes-new-mapper-2.png?width=1024" alt="Client Scopes Create Group Membership Mapper"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_configure_openshift_to_use_keycloak_as_an_idp"&gt;Configure OpenShift to use Keycloak as an IDP&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now that Keycloak is configured, we need to configure OpenShift to use Keycloak as an IDP. This is done by creating a new Identity Provider in OpenShift.
Before we do this, we need to create a new OAuth client secret for OpenShift in the Namespace &lt;strong&gt;openshift-config&lt;/strong&gt; The secret will be used to authenticate OpenShift with Keycloak.
When we created the keycloak client, we enabled the &lt;strong&gt;Client authentication&lt;/strong&gt; option. This created a client secret we need to use in OpenShift.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;In Keycloak, select the &lt;strong&gt;openshift&lt;/strong&gt; client and select the &lt;strong&gt;Credentials&lt;/strong&gt; tab and copy the &lt;strong&gt;Client secret&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/keycloak-client-secret.png?width=1024" alt="Keycloak Client Secret"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Back in OpenShift, navigate to the &lt;strong&gt;openshift-config&lt;/strong&gt; namespace and select the &lt;strong&gt;Secrets&lt;/strong&gt; view. Create a new secret with the following configuration:&lt;/p&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;kind: Secret
apiVersion: v1
metadata:
name: openid-client-secret
namespace: openshift-config
stringData:
clientSecret: &amp;lt;you client secret from Keycloak&amp;gt; &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
type: Opaque&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The client secret we copied from Keycloak. This is the secret we will use to authenticate OpenShift with Keycloak.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now we need to create a new Identity Provider in OpenShift. In the OpenShift Web Console, navigate to the &lt;strong&gt;Administration&lt;/strong&gt; â†’ &lt;strong&gt;Cluster Settings&lt;/strong&gt; â†’ &lt;strong&gt;Configuration&lt;/strong&gt; search for &lt;strong&gt;OAuth&lt;/strong&gt; and select the YAML view.
Here the following must be created or added to an existing OAuth configuration:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;[...]
spec:
identityProviders:
- mappingMethod: claim
name: rhsso &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
openID:
claims: &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
email:
- email
groups:
- groups
name:
- name
preferredUsername:
- preferred_username
clientID: openshift &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
clientSecret:
name: openid-client-secret &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
extraScopes: []
issuer: &amp;#39;https://sso.apps.ocp.aws.ispworld.at/realms/openshift&amp;#39; &lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;(5)&lt;/b&gt;
type: OpenID &lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;(6)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The name of the Identity Provider. This is the name that will be displayed in the OpenShift login screen.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The claims that will be used to map the user to OpenShift. In this example, we are using the email, groups, name and preferred_username claims from Keycloak.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The client ID we created in Keycloak. This is the client ID that will be used to authenticate OpenShift with Keycloak.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The name of the secret we created in the &lt;strong&gt;openshift-config&lt;/strong&gt; namespace.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="5"&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The issuer URL for Keycloak. It is &amp;lt;hostname of keycloak&amp;gt;/realms/&amp;lt;realm name&amp;gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="6"&gt;&lt;/i&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The type of the Identity Provider. In this example, we are using OpenID Connect.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The above configuration will trigger a restart of the authentication Pods in OpenShift. Wait until all Pods have been restarted and the Operator is running again.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/restart-oauth.png" alt="OpenShift OAuth Restart"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_test_the_configuration"&gt;Test the configuration&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now it is time to test the configuration. Open a new browser window (or incognito window), navigate to the OpenShift login page and try to log in using Keycloak as an IDP &lt;strong&gt;rhsso&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
If you have multiple IDPs configured, it is important to select the correct IDP. &lt;strong&gt;rhsso&lt;/strong&gt; in this example.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;By selecting the &lt;strong&gt;rhsso&lt;/strong&gt; Identity Provider, you should be redirected to the Keycloak login page.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/openshift-keycloak-login.png?width=1024" alt="OpenShift Login Page"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
If you selected &lt;strong&gt;Temporary&lt;/strong&gt; for the password, you will now be asked to change the password on the first login.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After a successful login, you should see the OpenShift Web Console, and you should be logged in as the user you created in Keycloak.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/openshift-web-console.png?width=1024" alt="OpenShift Web Console"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In OpenShift you will see the user created. In the Identities column you will see that it starts with &lt;strong&gt;rhsso&lt;/strong&gt;, indicating that the user was authenticated using the &lt;strong&gt;rhsso&lt;/strong&gt; Identity Provider.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/openshift-user.png" alt="OpenShift User"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And finally, if you navigate to &lt;strong&gt;User Management&lt;/strong&gt; â†’ &lt;strong&gt;Groups&lt;/strong&gt;, you should see the group &lt;strong&gt;openshift-users&lt;/strong&gt; that was created in Keycloak.&lt;/p&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/security/authentication/images/keycloak/openshift-groups.png" alt="OpenShift Group"/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In this article, I have shown how to install and configure Keycloak in OpenShift for authentication. I have also shown how to configure Keycloak to be used as an Identity Provider for OpenShift and how to import groups from Keycloak into OpenShift.
The biggest two challenges were to find the correct callback URL and to configure the Group Mapper in Keycloak. The rest was pretty straightforward.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Whatâ€™s next? With the groups now mapped into OpenShift, you can now create RoleBindings and ClusterRoleBindings to assign the appropriate roles to the users in Keycloak. This is quite nice, as I do not need to create Users manually in OpenShift anymore (previously used HTPasswd) but instead use Keycloak as the single source of truth for users and groups. All I need to configure is the RoleBindings and ClusterRoleBindings in OpenShift.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;I hope this article was helpful and you learned something new. Remember, this is just a test configuration. In production you should use a proper database and keycloak setup (high Availability, backup, etc.).
If you have any questions or comments, please feel free to reach out to me on LinkedIn, Email or via GitHub issues.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_references"&gt;References&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.redhat.com/en/documentation/red_hat_build_of_keycloak/" target="_blank" rel="noopener"&gt;Keycloak Documentation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://blog.badgerops.net/keycloak-open-shift/" target="_blank" rel="noopener"&gt;Keycloak &amp;amp; Open Shift&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tjungbauer/openshift-clusterconfig-gitops/tree/main/clusters/management-cluster/setup-rh-build-of-keycloak" target="_blank" rel="noopener"&gt;Set up Keycloak using GitOps (No Realm/Client configuration yet)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>[Ep.13] ApplicationSet with Matrix Generator</title><link>https://blog.stderr.at/gitopscollection/2025-04-17-applicationset-defining-namespaces/</link><pubDate>Thu, 17 Apr 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2025-04-17-applicationset-defining-namespaces/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;During my day-to-day business, I am discussing the following setup with many customers: &lt;a href="https://blog.stderr.at/gitopscollection/2024-04-02-configure_app_of_apps/"&gt;Configure App-of-Apps&lt;/a&gt;. Here I try to explain how I use an ApplicationSet that watches over a folder in Git and automatically adds a new Argo CD Application whenever a new folder is found. This works great, but there is a catch: The ApplicationSet uses the same Namespace &lt;strong&gt;default&lt;/strong&gt; for all Applications. This is not always desired, especially when you have different teams working on different Applications.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Recently I was asked by the customer if this can be fixed and if it is possible to define different Namespaces for each Application. The answer is yes, and I would like to show you how to do this.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_the_current_situation"&gt;The Current Situation&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Currently, I am (or was) using the following ApplicationSet to watch over a folder in Git. The ApplicationSet uses the Matrix Generator to create a new Argo CD Application for each folder found in the Git repository. It also uses the list operator to define the targetCluster:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt; generatormatrix:
# Git: Walking through the specific folder and take whatever is there.
- git:
directories:
- path: clusters/management-cluster/*
repoURL: *repourl
revision: *branch
# List: simply define the targetCluster. The name of the cluster must be known by Argo CD
- list:
elements:
# targetCluster is important, this will define on which cluster it will be rolled out.
# The cluster name must be known in Argo CD
- targetCluster: *mgmtclustername&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This will create a new Application for any subfolder found in the &lt;strong&gt;clusters/management-cluster/&lt;/strong&gt; folder any every Application in Argo CD will be configured with the same target namespace: &lt;strong&gt;default&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Technically, this is not a problem, as I define the exact namespace in the different Helm Charts, but it is not always desired.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
I personally recommend defining the Namespace in the Helm Charts, since especially for the cluster configuration, sometimes there is no clear target Namespace or multiple Namespaces are modified.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_what_did_not_work"&gt;What did not work&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The first idea was to use the &lt;strong&gt;Matrix&lt;/strong&gt; generator and define the &lt;strong&gt;targetNamespace&lt;/strong&gt; in list.elements and if the namespace is not defined, use a default one. So similar like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt; generatormatrix:
# Git: Walking through the specific folder and take whatever is there.
- git:
directories:
- path: clusters/management-cluster/*
repoURL: *repourl
revision: *branch
# List: simply define the targetCluster. The name of the cluster must be known by Argo CD
- list:
elements:
# targetCluster is important, this will define on which cluster it will be rolled out.
# The cluster name must be known in Argo CD
- targetCluster: *mgmtclustername
path: clusters/management-cluster/cert-manager
targetNamespace: cert-manager
- targetCluster: *mgmtclustername
path: clusters/management-cluster/*
targetNamespace: default&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To make is short: &lt;strong&gt;This does not work&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The matrix operator walks over all folders and creates a cartesian product of the elements. This means, it will create a new Application for each folder and each element in the list. So if you have 10 folders and 2 elements in the list, you will end up with 20 Applications.
This is not what we want. We want to create a new Application for each folder and define the targetNamespace in the Git repository.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The second test was the use of the &lt;strong&gt;Merge&lt;/strong&gt; generator. This did not work as well, as it was not possible to define a default Namespace.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_the_solution"&gt;The Solution&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The solution is to use the &lt;strong&gt;Git FILES&lt;/strong&gt; generator and define the &lt;strong&gt;targetNamespace&lt;/strong&gt; in the Git repository. This is done by creating a file called &lt;strong&gt;config.json&lt;/strong&gt; in each subfolder. The content of the file is simple:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-json" data-lang="json"&gt;{
&amp;#34;namespace&amp;#34;: &amp;#34;default&amp;#34;,
&amp;#34;environment&amp;#34;: &amp;#34;in-cluster&amp;#34;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The advantage is that it is possible to define multiple parameters in that file. However, the disadvantage is that this file must be created, otherwise the ApplicationSet will ignore the folder and will not create a new Application.
I think this is a small disadvantage, and the file is easy to maintain.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Bringing everything together now opens two possibilities:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
This will require &lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/helper-argocd"&gt;helper-argocd&lt;/a&gt; version 2.0.41 or higher.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_option_1_keep_matrix_generator_and_use_git_file_sub_generator"&gt;Option 1: Keep Matrix Generator and use Git File sub-generator&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The first option simply replaces the git directory generator with the git file generator. The rest of the ApplicationSet remains unchanged.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
I like this option somehow better than the second one, because I can keep everything as I had it before, the only thing is to create the &lt;strong&gt;config.json&lt;/strong&gt; file in each subfolder and change two lines in the ApplicationSet.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt; # Switch to set the namespace to &amp;#39;.namespace&amp;#39; ... must be defined in config.json
use_configured_namespace: true &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
# Definition of Matrix Generator. Only 2 generators are supported at the moment
generatormatrix:
# Git: Walking through the specific folder and take whatever is there.
- git:
files: &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
- path: clusters/management-cluster/**/config.json &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
repoURL: *repourl
revision: *branch
# List: simply define the targetCluster. The name of the cluster must be known by Argo CD
- list:
elements:
# targetCluster is important, this will define on which cluster it will be rolled out.
# The cluster name must be known in Argo CD
- targetCluster: *mgmtclustername&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Switch to use the configured namespace. This is important, otherwise the namespace is set to &amp;#34;default&amp;#34;. This was added for backward compatibility.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The git file generator is used instead of the git directory generator.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The path is changed to the config.json file. The ** is important, as it defines to look into every subfolder.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The config.json can be shortened to:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-json" data-lang="json"&gt;{
&amp;#34;namespace&amp;#34;: &amp;#34;default&amp;#34;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_option_2_switch_to_plain_git_file_generator"&gt;Option 2: Switch to plain Git File Generator&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The second option is to switch to the plain Git generator. This removes the Matrix generator, but also requires defining the targetCluster in the config.json file. This is not a problem, as the config.json file can be used to define multiple parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt; generatorgit: &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
# Git: Walking through the specific folder and take whatever is there.
- files:
- clusters/management-cluster/**/config.json
repourl: *repourl
revision: *branch&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;No Matrix but Git generator instead.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Here the full config.json file is required, otherwise the targetCluster is not defined:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-json" data-lang="json"&gt;{
&amp;#34;namespace&amp;#34;: &amp;#34;default&amp;#34;,
&amp;#34;environment&amp;#34;: &amp;#34;in-cluster&amp;#34;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_full_working_example"&gt;Full working example&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Source: &lt;a href="https://github.com/tjungbauer/openshift-clusterconfig-gitops/blob/main/base/argocd-resources-manager/values.yaml" class="bare"&gt;https://github.com/tjungbauer/openshift-clusterconfig-gitops/blob/main/base/argocd-resources-manager/values.yaml&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;applicationsets:
######################################
# MATRIX GENERATOR EXAMPLE Git Files #
######################################
# The idea behind the GIT Generate (File) is to walk over a folder, for example /clusters/management-cluster and fetch a config.json from each folder.
# This is more or less similar as the Matrix generator (see below), but reqires a bit more configuration ... the config.json.
# The advantage is that you can configure individual namespaces for example in this config.json and provide an additional information
mgmt-cluster-matrix-gitfiles:
enabled: true
# Description - always usful
description: &amp;#34;ApplicationSet that Deploys on Management Cluster Configuration (using Git Generator)&amp;#34;
# Any labels you would like to add to the Application. Good to filter it in the Argo CD UI.
labels:
category: configuration
env: mgmt-cluster
# Using go text template. See: https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/GoTemplate/
goTemplate: true
argocd_project: *mgmtclustername
environment: *mgmtclustername
# preserve all resources when the application get deleted. This is useful to keep that workload even if Argo CD is removed or severely changed.
preserveResourcesOnDeletion: true
# Switch to set the namespace to &amp;#39;.namespace&amp;#39; ... must be defined in config.json
use_configured_namespace: true
# Definition of Matrix Generator. Only 2 generators are supported at the moment
generatormatrix:
# Git: Walking through the specific folder and take whatever is there.
- git:
files:
- path: clusters/management-cluster/**/config.json
repoURL: *repourl
revision: *branch
# List: simply define the targetCluster. The name of the cluster must be known by Argo CD
- list:
elements:
# targetCluster is important, this will define on which cluster it will be rolled out.
# The cluster name must be known in Argo CD
- targetCluster: *mgmtclustername
syncPolicy:
autosync_enabled: false
# Retrying in case the sync failed.
retries:
# number of failed sync attempt retries; unlimited number of attempts if less than 0
limit: 5
backoff:
# the amount to back off. Default unit is seconds, but could also be a duration (e.g. &amp;#34;2m&amp;#34;, &amp;#34;1h&amp;#34;)
# Default: 5s
duration: 5s
# a factor to multiply the base duration after each failed retry
# Default: 2
factor: 2
# the maximum amount of time allowed for the backoff strategy
# Default: 3m
maxDuration: 3m&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In this blog post I have shown you how to use the ApplicationSet with the Matrix generator and define individual Namespaces for each Application. This is done by using the Git File generator and defining a config.json file in each subfolder. The config.json file can be used to define multiple parameters, but it is required to create the file in each subfolder.
This is a small disadvantage, but I think it is worth the effort. The advantage is that you can define individual Namespaces for each Application, and you can use the same ApplicationSet for all your Applications.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;I hope this blog post was helpful and you learned something new. If you have any questions or comments, please feel free to reach out to me.
I am happy to help you.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item></channel></rss>