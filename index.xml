<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>YAUB Yet Another Useless Blog on TechBlog about OpenShift/Ansible/Satellite and much more</title><link>https://blog.stderr.at/</link><description>Recent content in YAUB Yet Another Useless Blog on TechBlog about OpenShift/Ansible/Satellite and much more</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Toni Schmidbauer &amp; Thomas Jungbauer</copyright><atom:link href="https://blog.stderr.at/index.xml" rel="self" type="application/rss+xml"/><item><title>Adventures in Java Land: JPA disconnected entities</title><link>https://blog.stderr.at/java/2022-02-25-jpa-disconnected-entity/</link><pubDate>Fri, 25 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/java/2022-02-25-jpa-disconnected-entity/</guid><description>Prefix An old man tries to refresh his Java skills and does DO378. He fails spectacularly at the first real example but learns a lot on the way.
The exception There is this basic example where you build a minimal REST API for storing speaker data in a database. Quarkus makes this quite easy. You just have to define your database connection properties in resources/application.</description></item><item><title>Advanced Cluster Security - Authentication</title><link>https://blog.stderr.at/acs/2021-12-11-acsauth/</link><pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/acs/2021-12-11-acsauth/</guid><description>&lt;div class="paragraph">
&lt;p>Red Hat Advanced Cluster Security (RHACS) Central is installed with one administrator user by default. Typically, customers request an integration with existing Identity Provider(s) (IDP). RHACS offers different options for such integration. In this article 2 IDPs will be configured as an example. First OpenShift Auth and second Red Hat Single Sign On (RHSSO) based on Keycloak&lt;/p>
&lt;/div></description></item><item><title>Ansible Style Guide</title><link>https://blog.stderr.at/ansible/2021/11/ansible-style-guide/</link><pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/ansible/2021/11/ansible-style-guide/</guid><description>&lt;div class="paragraph">
&lt;p>You should always follow the &lt;a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_best_practices.html" target="_blank" rel="noopener">Best Practices&lt;/a> and &lt;a href="https://ansible-lint.readthedocs.io/en/latest/" target="_blank" rel="noopener">Ansible Lint&lt;/a> rules defined by the Ansible documentation when developing playbooks.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Although very basic, the &lt;a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_best_practices.html" target="_blank" rel="noopener">Best Practices&lt;/a> document gives a few guidelines to be able to carry out well-structured playbooks and roles, it contains recommendations that evolve with the project, so it is recommended to review it regularly. It is advisable to review the organization of content in Ansible.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The &lt;a href="https://ansible-lint.readthedocs.io/en/latest/" target="_blank" rel="noopener">Ansible Lint&lt;/a> documentation shows us through this tool the syntax rules that will be checked in the testing of roles and playbooks, the rules that will be checked are indicated in this document in their respective section.&lt;/p>
&lt;/div></description></item><item><title>Automated ETCD Backup</title><link>https://blog.stderr.at/day-2/etcd/2021-11-29-automatedetcdbackup/</link><pubDate>Mon, 29 Nov 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/etcd/2021-11-29-automatedetcdbackup/</guid><description>&lt;div class="paragraph">
&lt;p>Securing ETCD is one of the major Day-2 tasks for a Kubernetes cluster. This article will explain how to create a backup using OpenShift Cronjob.&lt;/p>
&lt;/div></description></item><item><title>Working with Environments</title><link>https://blog.stderr.at/day-2/labels_environmets/2021-11-12-creatingenvironment/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/labels_environmets/2021-11-12-creatingenvironment/</guid><description>&lt;div class="paragraph">
&lt;p>Imagine you have one OpenShift cluster and you would like to create 2 or more environments inside this cluster, but also separate them and force the environments to specific nodes, or use specific inbound routers. All this can be achieved using labels, IngressControllers and so on. The following article will guide you to set up dedicated compute nodes for infrastructure, development and test environments as well as the creation of IngressController which are bound to the appropriate nodes.&lt;/p>
&lt;/div></description></item><item><title>Stumbling into Azure Part II: Setting up a private ARO cluster</title><link>https://blog.stderr.at/azure/2021-10-29-private-aro/</link><pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/azure/2021-10-29-private-aro/</guid><description>In Part I of our blog post we covered setting up required resources in Azure. Now we are finally going to set up a private cluster. Private
As review from Part I here is our planned setup, this time including the ARO cluster.
Azure Setup The diagram below depicts our planned setup:
On the right hand side can see the resources required for our lab:</description></item><item><title>Automation Controller ad LDAP Authentication</title><link>https://blog.stderr.at/ansible/2021/10/automation-controller-ad-ldap-authentication/</link><pubDate>Mon, 25 Oct 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/ansible/2021/10/automation-controller-ad-ldap-authentication/</guid><description>&lt;div class="paragraph">
&lt;p>The following article shall quickly, without huge background information, deploy an Identity Management Server (based on FreeIPA) and connect this IDM to an existing Automation Controller so authentication can be tested and verified based on LDAP.&lt;/p>
&lt;/div></description></item><item><title>Stumbling into Azure Part I: Building a site-to-site VPN tunnel for testing</title><link>https://blog.stderr.at/azure/2021-10-17-s2s-vpn/</link><pubDate>Sat, 16 Oct 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/azure/2021-10-17-s2s-vpn/</guid><description>So we want to play with ARO (Azure Red Hat OpenShift) private clusters. A private cluster is not reachable from the internet (surprise) and is only reachable via a VPN tunnel from other networks.
This blog post describes how we created a site-to-site VPN between a Hetzner dedicated server running multiple VM&amp;#39;s via libvirt and Azure.
An upcoming blog post is going to cover the setup of the private ARO cluster.</description></item><item><title>Stumbling into Quay: Upgrading from 3.3 to 3.4 with the quay-operator</title><link>https://blog.stderr.at/quay/2021-09-07-quay-upgrade-3.4/</link><pubDate>Sat, 16 Oct 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/quay/2021-09-07-quay-upgrade-3.4/</guid><description>We had the task of answering various questions related to upgrading Red Hat Quay 3.3 to 3.4 and to 3.5 with the help of the quay-operator.
Thankfully (sic!) everything changed in regards to the Quay operator between Quay 3.3 and Quay 3.4.
So this is a brain dump of the things to consider.
Operator changes With Quay 3.4 the operator was completely reworked and it basically changed from opinionated to very opinionated.</description></item><item><title>Secure your secrets with Sealed Secrets</title><link>https://blog.stderr.at/openshift/2021-09-25-sealed_secrets/</link><pubDate>Sat, 25 Sep 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2021-09-25-sealed_secrets/</guid><description>&lt;div class="paragraph">
&lt;p>Working with a GitOps approach is a good way to keep all configurations and settings versioned and in sync on Git. Sensitive data, such as passwords to a database connection, will quickly come around.
Obviously, it is not a idea to store clear text strings in a, maybe even public, Git repository. Therefore, all sensitive information should be stored in a secret object. The problem with secrets in Kubernetes is that they are actually not encrypted. Instead, strings are base64 encoded which can be decoded as well. Thats not good …​ it should not be possible to decrypt secured data. Sealed Secret will help here…​&lt;/p>
&lt;/div></description></item><item><title>Introduction</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/</guid><description>&lt;div class="paragraph">
&lt;p>Pod scheduling is an internal process that determines placement of new pods onto nodes within the cluster. It is probably one of the most important tasks for a Day-2 scenario and should be considered at a very early stage for a new cluster. OpenShift/Kubernetes is already shipped with a &lt;strong>default scheduler&lt;/strong> which schedules pods as they get created accross the cluster, without any manual steps.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>However, there are scenarios where a more advanced approach is required, like for example using a specifc group of nodes for dedicated workload or make sure that certain applications do not run on the same nodes. Kubernetes provides different options:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>Controlling placement with node selectors&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with pod/node affinity/anti-affinity rules&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with taints and tolerations&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with topology spread constraints&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This series will try to go into the detail of the different options and explains in simple examples how to work with pod placement rules.
It is not a replacement for any official documentation, so always check out Kubernetes and or OpenShift documentations.&lt;/p>
&lt;/div></description></item><item><title>Node Affinity</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-29-node-affinity/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-29-node-affinity/</guid><description>&lt;div class="paragraph">
&lt;p>Node Affinity allows to place a pod to a specific group of nodes. For example, it is possible to run a pod only on nodes with a specific CPU or disktype. The disktype was used as an example for the &lt;code>nodeSelector&lt;/code> and yes …​ Node Affinity is conceptually similar to nodeSelector but allows a more granular configuration.&lt;/p>
&lt;/div></description></item><item><title>NodeSelector</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/</guid><description>&lt;div class="paragraph">
&lt;p>One of the easiest ways to tell your Kubernetes cluster where to put certain pods is to use a &lt;code>nodeSelector&lt;/code> specification. A nodeSelector defines a key-value pair and are defined inside the specification of the pods and as a label on one or multiple nodes (or machine set or machine config). Only if selector matches the node label, the pod is allowed to be scheduled on that node.&lt;/p>
&lt;/div></description></item><item><title>Pod Affinity/Anti-Affinity</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-28-affinity/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-28-affinity/</guid><description>&lt;div class="paragraph">
&lt;p>While noteSelector provides a very easy way to control where a pod shall be scheduled, the affinity/anti-affinity feature, expands this configuration with more expressive rules like logical AND operators, constraints against labels on other pods or soft rules instead of hard requirements.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The feature comes with two types:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>pod affinity/anti-affinity - allows constrains against other pod labels rather than node labels.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>node affinity - allows pods to specify a group of nodes they can be placed on&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div></description></item><item><title>Taints and Tolerations</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/</guid><description>&lt;div class="paragraph">
&lt;p>While Node Affinity is a property of pods that attracts them to a set of nodes, taints are the exact opposite. Nodes can be configured with one or more taints, which mark the node in a way to only accept pods that do tolerate the taints. The tolerations themselves are applied to pods, telling the scheduler to accept a taints and start the workload on a tainted node.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>A common use case would be to mark certain nodes as infrastructure nodes, where only specific pods are allowed to be executed or to taint nodes with a special hardware (i.e. GPU).&lt;/p>
&lt;/div></description></item><item><title>Topology Spread Constraints</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/</guid><description>&lt;div class="paragraph">
&lt;p>&lt;strong>Topology spread constraints&lt;/strong> is a new feature since Kubernetes 1.19 (OpenShift 4.6) and another way to control where pods shall be started. It allows to use failure-domains, like zones or regions or to define custom topology domains. It heavily relies on configured node labels, which are used to define topology domains. This feature is a more granular approach than affinity, allowing to achieve higher availability.&lt;/p>
&lt;/div></description></item><item><title>Using Descheduler</title><link>https://blog.stderr.at/day-2/pod-placement/2021-09-01-descheduler/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-09-01-descheduler/</guid><description>&lt;div class="paragraph">
&lt;p>&lt;strong>Descheduler&lt;/strong> is a new feature which is GA since OpenShift 4.7. It can be used to evict pods from nodes based on specific strategies. The evicted pod is then scheduled on another node (by the Scheduler) which is more suitable.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This feature can be used when:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>nodes are under/over-utilized&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pod or node affinity, taints or labels have changed and are no longer valid for a running pod&lt;/p>
&lt;/li>
&lt;li>
&lt;p>node failures&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pods have been restarted too many times&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div></description></item><item><title>Ansible Tower and downloading collections</title><link>https://blog.stderr.at/ansible/2021/07/ansible-tower-and-downloading-collections/</link><pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/ansible/2021/07/ansible-tower-and-downloading-collections/</guid><description>&lt;div class="paragraph">
&lt;p>Every wondered why Ansible Tower does not start downloading required
collections when you synchronize a project? Here are the stumbling
blocks we discovered so far:&lt;/p>
&lt;/div></description></item><item><title>oc compliance command line plugin</title><link>https://blog.stderr.at/compliance/2021/07/oc-compliance-command-line-plugin/</link><pubDate>Tue, 20 Jul 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/compliance/2021/07/oc-compliance-command-line-plugin/</guid><description>&lt;div class="paragraph">
&lt;p>As described at &lt;a href="https://blog.stderr.at/compliance/2021/07/compliance-operator/">Compliance Operator&lt;/a> the Compliance Operator can be used to scan the OpenShift cluster environment against security benchmark, like CIS.
Fetching the actual results might be a bit tricky tough.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>With OpenShift 4.8 plugins to the &lt;code>oc&lt;/code> command are allowed. One of these plugin os &lt;code>oc compliance&lt;/code>, which allows you to easily fetch scan results, re-run scans and so on.
Let’s install and try it out.&lt;/p>
&lt;/div></description></item><item><title>Compliance Operator</title><link>https://blog.stderr.at/compliance/2021/07/compliance-operator/</link><pubDate>Mon, 19 Jul 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/compliance/2021/07/compliance-operator/</guid><description>&lt;div class="paragraph">
&lt;p>OpenShift comes out of the box with a highly secure operating system, called Red Hat CoreOS. This OS is immutable, which means that no direct changes are done inside the OS, instead any configuration is managed by OpenShift itself using MachineConfig objects. Nevertheless, hardening certain settings must still be considered. Red Hat released a hardening guide (CIS Benchmark) which can be downloaded at &lt;a href="https://www.cisecurity.org/" class="bare">https://www.cisecurity.org/&lt;/a>.&lt;/p>
&lt;/div></description></item></channel></rss>