<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Other Topics on TechBlog about OpenShift/Ansible/Satellite and much more</title><link>https://blog.stderr.at/openshift-platform/other-topics/</link><description>TechBlog about OpenShift/Ansible/Satellite and much more</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Toni Schmidbauer &amp; Thomas Jungbauer</copyright><atom:link href="https://blog.stderr.at/openshift-platform/other-topics/index.xml" rel="self" type="application/rss+xml"/><item><title>Hosted Control Planes behind a Proxy</title><link>https://blog.stderr.at/openshift-platform/other-topics/2025-12-15-hosted-control-planes-and-proxy/</link><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/other-topics/2025-12-15-hosted-control-planes-and-proxy/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;Recently, I encountered a problem deploying a Hosted Control Plane (HCP) at a customer site. The installation started successfully—etcd came up fine—but then it just stopped. The virtual machines were created, but they never joined the cluster. No OVN or Multus pods ever started.
The only meaningful message in the cluster-version-operator pod logs was:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-console" data-lang="console"&gt;I1204 08:22:35.473783 1 status.go:185] Synchronizing status errs=field.ErrorList(nil) status=&amp;amp;cvo.SyncWorkerStatus{Generation:1, Failure:(*payload.UpdateError)(0xc0006313b0), Done:575, Total:623,&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This message did appear over and over again.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_problem_summary"&gt;Problem Summary&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;etcd started successfully, but the installation stalled&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;API server was running&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VMs started but never joined the cluster&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;No OVN or Multus pods ever started&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The installation was stuck in a loop&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_troubleshooting"&gt;Troubleshooting&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Troubleshooting was not straightforward. The cluster-version-operator logs provided the only clue.
However, the VMs were already running, so we could log in to them—and there it was, the reason for the stalled installation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_connect_to_a_vm"&gt;Connect to a VM&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To connect to a VM, use the &lt;code&gt;virtctl&lt;/code&gt; command. This connects you to the machine as the &lt;code&gt;core&lt;/code&gt; user:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
You can download &lt;code&gt;virtctl&lt;/code&gt; from the OpenShift Downloads page in the OpenShift web console.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock warning"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-warning" title="Warning"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
You need the SSH key for the &lt;code&gt;core&lt;/code&gt; user.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;virtctl ssh -n clusters-my-hosted-cluster core@vmi/my-node-pool-dsj7z-sss8w &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Replace &lt;strong&gt;my-hosted-cluster&lt;/strong&gt; with the name of your hosted cluster and &lt;strong&gt;my-node-pool-dsj7z-sss8w&lt;/strong&gt; with the name of your VM.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_verify_the_problem"&gt;Verify the Problem&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Once logged in, check the &lt;code&gt;journalctl -xf&lt;/code&gt; output. In case of a proxy issue, you’ll see an error like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&amp;gt; Dec 10 06:23:09 my-node-pool2-gwvjh-rwtx8 sh[2143]: time=&amp;#34;2025-12-10T06:23:09Z&amp;#34; level=warning msg=&amp;#34;Failed, retrying in 1s ... (3/3). Error: initializing source docker://quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:500704de3ef374e61417cc14eda99585450c317d72f454dda0dadd5dda1ba57a: pinging container registry quay.io: Get \&amp;#34;https://quay.io/v2/\&amp;#34;: dial tcp 3.209.93.201:443: i/o timeout&amp;#34;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;So we have a timeout when trying to pull the image from the container registry. This is a classic proxy issue. When you try &lt;code&gt;curl&lt;/code&gt; or manual pulls you will get the same message.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_configuring_the_proxy_for_the_hosted_control_plane"&gt;Configuring the Proxy for the Hosted Control Plane&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The big question is: How do we configure the proxy for the Hosted Control Plane?&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This is not well documented yet—in fact, it’s barely documented at all.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Hosted Control Plane is managed through a custom resource called &lt;code&gt;HostedCluster&lt;/code&gt;, and that’s exactly where we configure the proxy.
The upstream documentation at &lt;a href="https://hypershift.pages.dev/how-to/configure-ocp-components/#overview" target="_blank" rel="noopener"&gt;HyperShift Documentation&lt;/a&gt; explains that you can add a &lt;code&gt;configuration&lt;/code&gt; section to the resource. Let’s do that:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Update the &lt;code&gt;HostedCluster&lt;/code&gt; resource with the following configuration:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;spec:
configuration:
proxy:
httpProxy: http://proxy.example.com:8080 &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
httpsProxy: https://proxy.example.com:8080 &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
noProxy: .cluster.local,.svc,10.128.0.0/14,127.0.0.1,localhost &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
trustedCA:
name: user-ca-bundle &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;HTTP proxy URL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;HTTPS proxy URL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Comma-separated list of domains, IPs, or CIDRs to exclude from proxy routing&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;OPTIONAL: Name of a ConfigMap containing a custom CA certificate bundle&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
If you’re using a custom CA, create the ConfigMap beforehand or alongside the &lt;code&gt;HostedCluster&lt;/code&gt; resource. The ConfigMap must contain a key named &lt;code&gt;ca-bundle.crt&lt;/code&gt; with your CA certificate(s).
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_but_wait_there_is_more"&gt;But wait there is more…​&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;If you are using such a proxy, which is injecting it’s own certificate, then you probably saw the following: The &lt;strong&gt;Release Image&lt;/strong&gt; is not available when you try to deploy a new Hosted Control Plane.
The drop down in the UI is empty and you can’t select a release image.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This happens because the Pod that is trying to fetch the image is not able to connect to Github.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;You can test this with the following commands:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc rsh cluster-image-set-controller-XXXXX -n multicluster-engine &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
curl -v https://github.com/stolostron/acm-hive-openshift-releases.git/info/refs?service=git-upload-pack &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Replace &lt;strong&gt;XXXXX&lt;/strong&gt; with the name of the Pod cluster-image-set-controller in the &lt;strong&gt;multicluster-engine&lt;/strong&gt; namespace&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Try to execute the curl command to see if you can connect to Github&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;If this command fails with a certificate error, then you need to add the certificate to the Pod/Deployment.
A Configmap called &lt;strong&gt;trusted-ca-bundle&lt;/strong&gt; should already exist in the multicluster-engine namespace. If not, it must be created with the certificate chain of your Proxy (key: ca-bundle.crt)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The following command will add the ConfigMap to the deployment.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc -n multicluster-engine set volume deployment/cluster-image-set-controller --add --type configmap --configmap-name trusted-ca-bundle --name trusted-ca-bundle --mount-path /etc/pki/tls/certs/ --overwrite&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Wait a couple of minutes after the Pods has been restarted. It will try to download the release images from Github again.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;You can check the logs of the Pod to see if the download was successful.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc logs cluster-image-set-controller-XXXXX -n multicluster-engine &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Replace &lt;strong&gt;XXXXX&lt;/strong&gt; with the name of the Pod cluster-image-set-controller in the &lt;strong&gt;multicluster-engine&lt;/strong&gt; namespace&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;That should do it, you should now be able to select a release image and deploy a new Hosted Control Plane.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/other-topics/images/HCP-Release-Images.png" alt="Release Images"/&gt;
&lt;/div&gt;
&lt;div class="title"&gt;Figure 1. Drow Down with Release Images&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_summary"&gt;Summary&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;That’s actually everything you need for proxy configuration with Hosted Control Planes. Hopefully, the official OpenShift documentation will be updated soon to include this information.
red Hat created two tickets to track the progress of this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://issues.redhat.com/browse/ACM-10151" target="_blank" rel="noopener"&gt;ACM-10151&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://issues.redhat.com/browse/ACM-23664" target="_blank" rel="noopener"&gt;ACM-23664&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>Writing Operator using Ansible</title><link>https://blog.stderr.at/openshift-platform/other-topics/2021-01-27-writingoperatoransible/</link><pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/other-topics/2021-01-27-writingoperatoransible/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;This quick post shall explain, without any fancy details, how to write an Operator based on Ansible. It is assumed that you know what purpose an Operator has.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As a short summary: Operators are a way to create custom controllers in OpenShift or Kubernetes. It watches for custom resource objects and creates the application based on the parameters in such custom resource object.
Often written in &lt;strong&gt;Go&lt;/strong&gt;, the SDK supports &lt;strong&gt;Ansible&lt;/strong&gt;, &lt;strong&gt;Helm&lt;/strong&gt; and (new) &lt;strong&gt;Java&lt;/strong&gt; as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In this example we will install Gogs, a painless self-hosted Git services.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As general prerequisites we have:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Installed OpenShift 4.6+ cluster (could be Minicube)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Possibility to execute Ansible scripts and oc/kubectl commands&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Commands: make, docker (or podman)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;hr/&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_install_operator_sdk"&gt;Install Operator SDK&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As explained at &lt;a href="https://sdk.operatorframework.io/docs/installation/" class="bare"&gt;https://sdk.operatorframework.io/docs/installation/&lt;/a&gt; the following prerequisites must be met prior installing the SDK at least:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Docker v17.03+ or podman v1.9.3+ or buildah v1.7+&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;OpenShift CLU v4.6+&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kubernetes/OpenShift cluster&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Access to container registry, for example quay.io&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optional: Go v1.13+ (for Operators based on Golang)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ansible v2.9.0+&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Following the instructions of the SDK documentation, the &lt;strong&gt;operator-sdk&lt;/strong&gt; command will be installed.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_creating_the_operator"&gt;Creating the Operator&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To begin we create a new folder for the Operator and initialize the Operator project.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;mkdir gogs-operator
cd gogs-operator
operator-sdk init --plugins=ansible --domain=example.com.at
operator-sdk create api --group gogs --version=v1alpha1 --kind Gogs --generate-playbook&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This will create a new project structure with the following parameters:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;--plugin&lt;/strong&gt;: Type of Operator (Ansible or Helm)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;--domain&lt;/strong&gt;: Defines the api endpoint together with group and version.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;--group&lt;/strong&gt;: Usually short product name&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;strong&gt;--version&lt;/strong&gt;: Defines version of API endpoint&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The folder structure which will be created automatically looks as follows:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;.
|-- Dockerfile
|-- Makefile
|-- PROJECT
|-- config
| |-- crd
| | |-- bases
| | | |-- gogs.example.com.at_gogs.yaml
| | |-- kustomization.yaml
| |-- default
| | |-- kustomization.yaml
| | |-- manager_auth_proxy_patch.yaml
| |-- manager
| | |-- kustomization.yaml
| | |-- manager.yaml
| |-- prometheus
| | |-- kustomization.yaml
| | |-- monitor.yaml
| |-- rbac
| | |-- auth_proxy_client_clusterrole.yaml
| | |-- auth_proxy_role.yaml
| | |-- auth_proxy_role_binding.yaml
| | |-- auth_proxy_service.yaml
| | |-- gogs_editor_role.yaml
| | |-- gogs_viewer_role.yaml
| | |-- kustomization.yaml
| | |-- leader_election_role.yaml
| | |-- leader_election_role_binding.yaml
| | |-- role.yaml
| | |-- role_binding.yaml
| |-- samples
| | |-- gogs_v1alpha1_gogs.yaml
| | |-- kustomization.yaml
| |-- scorecard
| | |-- bases
| | | |-- config.yaml
| | |-- kustomization.yaml
| | |-- patches
| | |-- basic.config.yaml
| | |-- olm.config.yaml
| |-- testing
| |-- debug_logs_patch.yaml
| |-- kustomization.yaml
| |-- manager_image.yaml
| |-- pull_policy
| |-- Always.yaml
| |-- IfNotPresent.yaml
| |-- Never.yaml
|-- molecule
| |-- default
| | |-- converge.yml
| | |-- create.yml
| | |-- destroy.yml
| | |-- kustomize.yml
| | |-- molecule.yml
| | |-- prepare.yml
| | |-- tasks
| | | |-- gogs_test.yml
| | |-- verify.yml
| |-- kind
| |-- converge.yml
| |-- create.yml
| |-- destroy.yml
| |-- molecule.yml
|-- playbooks
| |-- gogs.yml
|-- requirements.yml
|-- roles
|-- watches.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The &lt;strong&gt;watches.yaml&lt;/strong&gt; file maps Custom Resources (identified by Group, Version, and Kind [GVK]) to Ansible Roles and Playbooks. It tells the Operator where to find the actual Ansible playbook.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
# Use the &amp;#39;create api&amp;#39; subcommand to add watches to this file.
- version: v1alpha1
group: gogs.example.com.at
kind: Gogs
playbook: playbooks/gogs.yml
# +kubebuilder:scaffold:watch&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Other files, especially inside &lt;strong&gt;playbooks&lt;/strong&gt; and &lt;strong&gt;roles&lt;/strong&gt; are created as placeholders. These files (or folders) are waiting for you to add the Ansible logic.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_defining_roles_and_playbook"&gt;Defining Roles and Playbook&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;With the folder structure above, a playbook and different roles can be created in order to tell the Operator what it needs to do.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
Since the Operator will constantly watch for changes, all tasks must be &lt;strong&gt;idempotent&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;In our example we will try to install Gogs, a Git service. It contains a Postgres database system and a webservice.
To use some example roles and not fully start from scratch let’s clone the following repository and copy the folders to our Operator.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;cd ..
https://github.com/tjungbauer/ansible-operator-roles
cd gogs-operator
# Remove placeholder
rm -Rf roles/
# Copy Postgres deployment role
cp -R ../ansible-operator-roles/roles/postgresql-ocp ./roles
# Copy Gogs Deplyoment role
cp -R ../ansible-operator-roles/roles/gogs-ocp ./roles&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;When we examine the folder, we see 2 typical Ansible roles. The simple purpose is, to create all required OpenShift objects, like Deployment, Route, Service and so on, fully automated by the Operator.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;|-- playbooks
| |-- gogs.yaml
|-- roles
|-- gogs-ocp
| |-- README.adoc
| |-- defaults
| | |-- main.yml
| |-- meta
| | |-- main.yml
| |-- tasks
| | |-- main.yml
| |-- templates
| |-- config_map.j2
| |-- deployment.j2
| |-- persistent_volume_claim.j2
| |-- route.j2
| |-- service.j2
| |-- service_account.j2
|-- postgresql-ocp
|-- README.adoc
|-- defaults
| |-- main.yml
|-- meta
| |-- main.yml
|-- tasks
| |-- main.yml
|-- templates
|-- deployment.j2
|-- persistent_volume_claim.j2
|-- secret.j2
|-- service.j2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Copy (or create) the following playbook under &lt;strong&gt;playbooks/gogs.yaml&lt;/strong&gt;. As you can see there are 2 tasks: the first one will create the postgres application, the seconds one the Gogs service.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
# Persistent Gogs deployment playbook.
#
# The Playbook expects the following variables to be set in the CR:
# (Note that Camel case gets converted by the ansible-operator to Snake case)
# - PostgresqlVolumeSize
# - GogsVolumeSize
# - GogsSSL
# The following variables come from the ansible-operator
# - ansible_operator_meta.namespace
# - ansible_operator_meta.name (from the name of the CR)
- hosts: localhost
gather_facts: no
tasks:
- name: Set up PostgreSQL
include_role:
name: ../roles/postgresql-ocp &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt;
vars: &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt;
_postgresql_namespace: &amp;#34;{{ ansible_operator_meta.namespace }}&amp;#34;
_postgresql_name: &amp;#34;postgresql-gogs-{{ ansible_operator_meta.name }}&amp;#34;
_postgresql_database_name: &amp;#34;gogsdb&amp;#34;
_postgresql_user: &amp;#34;gogsuser&amp;#34;
_postgresql_password: &amp;#34;gogspassword&amp;#34;
_postgresql_volume_size: &amp;#34;{{ postgresql_volume_size|d(&amp;#39;4Gi&amp;#39;) }}&amp;#34;
_postgresql_image: &amp;#34;{{ postgresql_image|d(&amp;#39;registry.redhat.io/rhscl/postgresql-10-rhel7&amp;#39;) }}&amp;#34;
_postgresql_image_tag: &amp;#34;{{ postgresql_image_tag|d(&amp;#39;latest&amp;#39;) }}&amp;#34;
_postgresql_size: 1
- name: Set Gogs Service name to default value
set_fact:
gogs_service_name: &amp;#34;gogs-{{ ansible_operator_meta.name }}&amp;#34;
when:
gogs_service_name is not defined
- name: Set up Gogs
include_role:
name: ../roles/gogs-ocp &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt;
vars: &lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;(4)&lt;/b&gt;
_gogs_namespace: &amp;#34;{{ ansible_operator_meta.namespace }}&amp;#34;
_gogs_name: &amp;#34;{{ gogs_service_name }}&amp;#34;
_gogs_ssl: &amp;#34;{{ gogs_ssl|d(False)|bool }}&amp;#34;
_gogs_route: &amp;#34;{{ gogs_route | d(&amp;#39;&amp;#39;) }}&amp;#34;
_gogs_image_tag: &amp;#34;{{ gogs_image_tag | d(&amp;#39;latest&amp;#39;) }}&amp;#34;
_gogs_volume_size: &amp;#34;{{ gogs_volume_size|d(&amp;#39;4Gi&amp;#39;) }}&amp;#34;
_gogs_postgresql_service_name: &amp;#34;postgresql-gogs-{{ ansible_operator_meta.name }}&amp;#34;
_gogs_postgresql_database_name: gogsdb
_gogs_postgresql_user: gogsuser
_gogs_postgresql_password: gogspassword
_gogs_size: 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="colist arabic"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Path to Postgres Role&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Parameters for Postgres service&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Path to Gogs Role&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class="conum" data-value="4"&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Parameters for Gogs service&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_operator_permissions"&gt;Operator Permissions&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Operator will require correct permissions in order to create objects like Routes or Services in OpenShift. The SDK automatically created a default role.yaml which can be modified.
Open the file &lt;strong&gt;config/rbac/role.yaml&lt;/strong&gt; and add permissions for:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;for apiGroups &amp;#34;&amp;#34;&lt;/p&gt;
&lt;div class="olist loweralpha"&gt;
&lt;ol class="loweralpha" type="a"&gt;
&lt;li&gt;
&lt;p&gt;services&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;routes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;peristentvlumeclaims&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;serviceaccounts&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;configmaps&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;for apiGroups: route.operanshift.io the resource &lt;strong&gt;routes&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;At the end, the role.yaml should look like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
name: manager-role
rules:
##
## Base operator rules
##
- apiGroups:
- &amp;#34;&amp;#34;
resources:
- secrets
- pods
- pods/exec
- pods/log
- services
- routes
- configmaps
- persistentvolumeclaims
- serviceaccounts
verbs:
- create
- delete
- get
- list
- patch
- update
- watch
- apiGroups:
- apps
resources:
- deployments
- daemonsets
- replicasets
- statefulsets
verbs:
- create
- delete
- get
- list
- patch
- update
- watch
##
## Rules for gogs.example.com.at/v1alpha1, Kind: Gogs
##
- apiGroups:
- gogs.example.com.at
resources:
- gogs
- gogs/status
- gogs/finalizers
verbs:
- create
- delete
- get
- list
- patch
- update
- watch
- apiGroups:
- route.openshift.io
resources:
- routes
verbs:
- create
- update
- delete
- get
- list
- watch
- patch
- apiGroups:
- route.openshift.io
resources:
- routes
verbs:
- create
- update
- delete
- get
- list
- watch
- patch&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_building_and_deploy_the_operator"&gt;Building and Deploy the Operator&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now it is time to build the Operator and push it to a repository. In this example a repository was created at quay.io and is called &lt;strong&gt;gogs-operator&lt;/strong&gt;.
The SDK will automatically create a Makefile during the initialization, which we will use now.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock caution"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-caution" title="Caution"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The Makefile is prepared for &lt;em&gt;docker&lt;/em&gt;. If you use &lt;strong&gt;podman&lt;/strong&gt; some modifications must be done first. Run the command &lt;strong&gt;sed -i &amp;#39;s/docker/podman/g&amp;#39; Makefile&lt;/strong&gt; to replace all docker commands inside the Makefile.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The next commands will &lt;em&gt;build, push, install and deploy&lt;/em&gt; the Operator. Before we start we must be logged in to you Registry of choice (i.e. docker login …​) as well as into our OpenShift cluster.
Moreover, it is required that the &lt;strong&gt;IMG&lt;/strong&gt; environment variable is exported with the correct value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Build the Operator and push into the registry&lt;/p&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;# export IMG, be sure that the correct tag is used
export IMG=quay.io/tjungbau/gogs-operator:v1.0.0
# Build and push into registry
make podman-build podman-push
podman build . -t quay.io/tjungbau/gogs-operator:v1.0.0
STEP 1: FROM quay.io/operator-framework/ansible-operator:v1.3.0
STEP 2: COPY requirements.yml ${HOME}/requirements.yml
--&amp;gt; Using cache 4f84e7064b066c2cac5179b56490a0ef85591170c501ec8a480b617d6e91cff3
STEP 3: RUN ansible-galaxy collection install -r ${HOME}/requirements.yml &amp;amp;&amp;amp; chmod -R ug+rwx ${HOME}/.ansible
--&amp;gt; Using cache 2a3a5d44451a45a4c38e1c314e8887c6c45f2551cbef87ef0d1ce518c1969c0d
STEP 4: COPY watches.yaml ${HOME}/watches.yaml
--&amp;gt; Using cache 642f8361a7b358b89d2e4e5211c1c7a1e22488c53bba0bf1ba2ba275fd56ee69
STEP 5: COPY roles/ ${HOME}/roles/
--&amp;gt; Using cache 93c1af8782bad84d8b81d2d2294c405caab70e2d01c232440f7eb8e5001746c1
STEP 6: COPY playbooks/ ${HOME}/playbooks/
--&amp;gt; Using cache 1cdeee1456ac67d70d4233b0f9ed8052465aaa2cded6bd8ae962dfcc848e5b92
STEP 7: COMMIT quay.io/tjungbau/gogs-operator:v1.0.0
--&amp;gt; 1cdeee1456a
1cdeee1456ac67d70d4233b0f9ed8052465aaa2cded6bd8ae962dfcc848e5b92
podman push quay.io/tjungbau/gogs-operator:v1.0.0
Getting image source signatures
Copying blob d5ca8c3b3d34 skipped: already exists
Copying blob 4b036ae478b7 skipped: already exists
Copying blob 5cfcd0621ffc skipped: already exists
Copying blob c6f3d1432bd0 skipped: already exists
Copying blob 92538e92de29 skipped: already exists
Copying blob eb7bf34352ca skipped: already exists
Copying blob 80c43a11288f done
Copying blob 803eb2035c9a done
Copying blob 40d943ae1834 done
Copying blob f4d9024614ee done
Copying blob 5143a36c6002 done
Copying blob 5050e1080446 skipped: already exists
Copying config 1cdeee1456 done
Writing manifest to image destination
Copying config 1cdeee1456 [--------------------------------------] 0.0b / 6.2KiB
Writing manifest to image destination
Writing manifest to image destination
Storing signatures&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the CRD into OpenShift&lt;/p&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;# Install the custom resource definition
make install
/root/projects/gogs-operator/bin/kustomize build config/crd | kubectl apply -f -
customresourcedefinition.apiextensions.k8s.io/gogs.gogs.example.com.at created&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deploy the Operator and all required objects into OpenShift&lt;/p&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;# Deploy the Operator into OpenShift
make deploy
cd config/manager &amp;amp;&amp;amp; /root/projects/gogs-operator/bin/kustomize edit set image controller=quay.io/tjungbau/gogs-operator:v1.0.0
/root/projects/gogs-operator/bin/kustomize build config/default | kubectl apply -f -
namespace/gogs-operator-system created
customresourcedefinition.apiextensions.k8s.io/gogs.gogs.example.com.at unchanged
role.rbac.authorization.k8s.io/gogs-operator-leader-election-role created
clusterrole.rbac.authorization.k8s.io/gogs-operator-manager-role created
clusterrole.rbac.authorization.k8s.io/gogs-operator-metrics-reader created
clusterrole.rbac.authorization.k8s.io/gogs-operator-proxy-role created
rolebinding.rbac.authorization.k8s.io/gogs-operator-leader-election-rolebinding created
clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-manager-rolebinding created
clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-proxy-rolebinding created
service/gogs-operator-controller-manager-metrics-service created
deployment.apps/gogs-operator-controller-manager created&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This will create a new project in OpenShift called &lt;strong&gt;gogs-operator-system&lt;/strong&gt;. Here, the Operator is running and waiting that somebody creates a CRD of the kind &lt;strong&gt;Gogs&lt;/strong&gt;. Once this happens the Operator will execute the playbooks and therefore create a Postgres and a Gogs pod.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;# Operator Namespace
oc get pods -n gogs-operator-system
NAME READY STATUS RESTARTS AGE
gogs-operator-controller-manager-6747bb6c6-s8794 2/2 Running 0 6m8s&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_using_the_operator"&gt;Using the Operator&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now we need to create a CRD of the kind &lt;em&gt;Gogs&lt;/em&gt;. This will happen in a new project, where the Gogs service shall be hosted.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Create a new OpenShift project&lt;/p&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc new-project gogs&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Verify the sample resource&lt;/p&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;cat config/samples/gogs_v1alpha1_gogs.yaml
apiVersion: gogs.example.com.at/v1alpha1
kind: Gogs
metadata:
name: gogs-sample
spec:
foo: bar&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Apply the sample resource&lt;/p&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc apply -f config/samples/gogs_v1alpha1_gogs.yaml -n gogs&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;This will create two services:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;postgresql&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Gogs&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Operator will be responsible to roll out all required objects. This includes the Deployments for the container, the Openshift service and the route.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc get all -n gogs
NAME READY STATUS RESTARTS AGE
pod/gogs-gogs-sample-57778fd76-ghg8j 1/1 Running 0 74s
pod/postgresql-gogs-gogs-sample-bbc49b794-mnltb 1/1 Running 0 115s
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/gogs-gogs-sample ClusterIP 172.30.47.31 &amp;lt;none&amp;gt; 3000/TCP 80s
service/postgresql-gogs-gogs-sample ClusterIP 172.30.47.158 &amp;lt;none&amp;gt; 5432/TCP 117s
NAME READY UP-TO-DATE AVAILABLE AGE
deployment.apps/gogs-gogs-sample 1/1 1 1 74s
deployment.apps/postgresql-gogs-gogs-sample 1/1 1 1 115s
NAME DESIRED CURRENT READY AGE
replicaset.apps/gogs-gogs-sample-57778fd76 1 1 1 74s
replicaset.apps/postgresql-gogs-gogs-sample-bbc49b794 1 1 1 115s
NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD
route.route.openshift.io/gogs-gogs-sample gogs-gogs-sample-gogs.apps.ocp.ispworld.at gogs-gogs-sample &amp;lt;all&amp;gt; None&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;At the end, all Pods are alive, ready and are fully controlled by the Operator. We can access the Gogs web interface via the route and start using our own Git service.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_updating_operator"&gt;Updating Operator&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;While the Operator is running fine now, at some point you might want to do some changes. For example, let’s run the Gog service with a replica of 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Perform the following actions:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Set the variable &lt;strong&gt;_gogs_size&lt;/strong&gt; to 3 in playbooks/gogs.yml&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Build and push the new version&lt;/p&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;export IMG=quay.io/tjungbau/gogs-operator:v1.0.8
make podman-build podman-push
podman build . -t quay.io/tjungbau/gogs-operator:v1.0.8
STEP 1: FROM quay.io/operator-framework/ansible-operator:v1.3.0
STEP 2: COPY requirements.yml ${HOME}/requirements.yml
--&amp;gt; Using cache 4f84e7064b066c2cac5179b56490a0ef85591170c501ec8a480b617d6e91cff3
STEP 3: RUN ansible-galaxy collection install -r ${HOME}/requirements.yml &amp;amp;&amp;amp; chmod -R ug+rwx ${HOME}/.ansible
--&amp;gt; Using cache 2a3a5d44451a45a4c38e1c314e8887c6c45f2551cbef87ef0d1ce518c1969c0d
STEP 4: COPY watches.yaml ${HOME}/watches.yaml
--&amp;gt; Using cache 642f8361a7b358b89d2e4e5211c1c7a1e22488c53bba0bf1ba2ba275fd56ee69
STEP 5: COPY roles/ ${HOME}/roles/
--&amp;gt; Using cache 55785493e215d933ef7a93fe000afa6fbb088d87eeffcdddeea4e7fd1896f5b5
STEP 6: COPY playbooks/ ${HOME}/playbooks/
STEP 7: COMMIT quay.io/tjungbau/gogs-operator:v1.0.8
--&amp;gt; bb9d6a995d0
bb9d6a995d059eab7758f9ac17d3ce12f8759518e231f77d32a4b820e4b14396
podman push quay.io/tjungbau/gogs-operator:v1.0.8
Getting image source signatures
Copying blob 5cfcd0621ffc skipped: already exists
Copying blob d5ca8c3b3d34 skipped: already exists
Copying blob eb7bf34352ca skipped: already exists
Copying blob 4b036ae478b7 skipped: already exists
Copying blob c6f3d1432bd0 skipped: already exists
Copying blob 92538e92de29 skipped: already exists
Copying blob 41e53e538a36 done
Copying blob 5050e1080446 skipped: already exists
Copying blob 40d943ae1834 skipped: already exists
Copying blob 803eb2035c9a skipped: already exists
Copying blob 80c43a11288f skipped: already exists
Copying blob ee0361a14e3b skipped: already exists
Copying config bb9d6a995d done
Writing manifest to image destination
Copying config bb9d6a995d [--------------------------------------] 0.0b / 6.2KiB
Writing manifest to image destination
Writing manifest to image destination
Storing signatures&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deploy the new version&lt;/p&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;make deploy
cd config/manager &amp;amp;&amp;amp; /root/projects/gogs-operator/bin/kustomize edit set image controller=quay.io/tjungbau/gogs-operator:v1.0.8
/root/projects/gogs-operator/bin/kustomize build config/default | kubectl apply -f -
namespace/gogs-operator-system unchanged
customresourcedefinition.apiextensions.k8s.io/gogs.gogs.example.com.at unchanged
role.rbac.authorization.k8s.io/gogs-operator-leader-election-role unchanged
clusterrole.rbac.authorization.k8s.io/gogs-operator-manager-role unchanged
clusterrole.rbac.authorization.k8s.io/gogs-operator-metrics-reader unchanged
clusterrole.rbac.authorization.k8s.io/gogs-operator-proxy-role unchanged
rolebinding.rbac.authorization.k8s.io/gogs-operator-leader-election-rolebinding unchanged
clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-manager-rolebinding unchanged
clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-proxy-rolebinding unchanged
service/gogs-operator-controller-manager-metrics-service unchanged
deployment.apps/gogs-operator-controller-manager configured&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The Operator will restart with a new version. After a while the changes will take affect and 3 Gogs pods will run.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc get pods -n gogs
NAME READY STATUS RESTARTS AGE
gogs-gogs-sample-57778fd76-4m98m 1/1 Running 0 12m
gogs-gogs-sample-57778fd76-5hrdn 1/1 Running 0 6m23s
gogs-gogs-sample-57778fd76-xgh2f 1/1 Running 0 6m24s
postgresql-gogs-gogs-sample-bbc49b794-z84wt 1/1 Running 0 13m&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_what_else_references"&gt;What Else? - References&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Above example is a very quick overview about what can be done. There are many other options. You can create Operators using Go or Helm.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The best starting points are the following websites:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://redhat-connect.gitbook.io/certified-operator-guide/"&gt;Certified Operator Build Guide&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://sdk.operatorframework.io/docs/"&gt;Operator SDK Documentation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item><item><title>OpenShift Pipelines - Tekton Introduction</title><link>https://blog.stderr.at/openshift-platform/other-topics/2020-04-16-tekton/</link><pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift-platform/other-topics/2020-04-16-tekton/</guid><description>&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;em&gt;OpenShift Pipelines is a cloud-native, continuous integration and delivery (CI/CD) solution for building pipelines using Tekton. Tekton is a flexible, Kubernetes-native, open-source CI/CD framework that enables automating deployments across multiple platforms (Kubernetes, serverless, VMs, etc) by abstracting away the underlying details.&lt;/em&gt; [&lt;a href="#source_1"&gt;1&lt;/a&gt;]&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_openshift_pipelines_features"&gt;OpenShift Pipelines features&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;&lt;em&gt;Source: [&lt;a href="#source_1"&gt;1&lt;/a&gt;]&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Standard CI/CD pipeline definition based on Tekton&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Build images with Kubernetes tools such as S2I, Buildah, Buildpacks, Kaniko, etc&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deploy applications to multiple platforms such as Kubernetes, serverless and VMs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Easy to extend and integrate with existing tools&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Scale pipelines on-demand&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Portable across any Kubernetes platform&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Designed for microservices and decentralized teams&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Integrated with the OpenShift Developer Console&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;OpenShift 4.x cluster. Try yourself at &lt;a href="https://try.openshift.com" class="bare"&gt;https://try.openshift.com&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optional&lt;/strong&gt;: &lt;a href="https://github.com/tektoncd/cli#installing-tkn" target="_blank" rel="noopener"&gt;Tekton CLI&lt;/a&gt; - Optional for now, since you could do everything via UI as well.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;oc/kubectl CLI or WebUI&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The Tekton CLI is optional in case you prefer to do everything via the OpenShift WebUI. However, below examples make use of the Tekton CLI and I personally would recommend to at least install it. (Like oc client it is good to have a CLI option as well). In any case, all described action can be done directly via the WebUI as well.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_basic_concepts"&gt;Basic Concepts&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Tekton makes use of several custom resources (CRD).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;These CRDs are:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="black silver-background"&gt;Task&lt;/span&gt;: each step in a pipeline is a task, while a task can contain several steps itself, which are required to perform a specific task. For each Task a pod will be allocated and for each step inside this Task a container will be used. This helps in better scalability and better performance throughout the pipeline process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="black silver-background"&gt;Pipeline&lt;/span&gt;: is a series of tasks, combined to work together in a defined (structured) way&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="black silver-background"&gt;TaskRun&lt;/span&gt;: is the result of a Task, all combined TaskRuns are used in the PipelineRun&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class="black silver-background"&gt;PipelineRun&lt;/span&gt;: is the actual execution of a whole Pipeline, containing the results of the pipeline (success, failed…​)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Pipelines and Tasks should be generic and never define possible variables, like input git repository, directly in their definition. For this, the concept of PipelineResources has been created, which defines these parameters and which are used during a PipelineRun.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_installation"&gt;Installation&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The &lt;strong&gt;OpenShift Pipeline&lt;/strong&gt; is an operator which can e installed using the following yaml:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;cat &amp;lt;&amp;lt;&amp;#39;EOF&amp;#39; &amp;gt; deploy-OpenShift-Pipelines.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
name: openshift-pipelines-operator
namespace: openshift-operators
spec:
channel: dev-preview
name: openshift-pipelines-operator
source: community-operators
sourceNamespace: openshift-marketplace
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc create -f deploy-OpenShift-Pipelines.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;As an alternative, you can also use the WebUI to rollout the operator:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="olist arabic"&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;
&lt;p&gt;Search for &amp;#34;OpenShift Pipeline&amp;#34; under OperatorHub and install it&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Select:&lt;/p&gt;
&lt;div class="olist loweralpha"&gt;
&lt;ol class="loweralpha" type="a"&gt;
&lt;li&gt;
&lt;p&gt;All Namespaces: since the operator needs to watch for Tekton Custom Resources across all namespaces.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Channel: Dev Preview&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Approve Strategy: Automatic&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_prepare_a_tutorial_project"&gt;Prepare a tutorial project&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;To test our OpenShift Pipelines, we need to deploy an example application. This application let’s you vote what pet you like more: Cats or Dogs? It contais of a backend and a frontend part, which both will be deployed in a namespace.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Let’s first create a new project:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc new-project pipelines-tutorial&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The OpenShift Pipeline operator will automatically create a &lt;em&gt;pipeline&lt;/em&gt; serviceaccount with all required permissions to build and push an image and which is used by PipelineRuns:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc get sa pipeline
NAME SECRETS AGE
pipeline 2 15s&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_create_a_task"&gt;Create a Task&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;A Task is the smallest block of a Pipeline which by itself can contain one or more steps which are executed in order to process a specific element. For each Task a pod is allocated and each step is running in a container inside this pod. Tasks are reusable by other Pipelines. &lt;em&gt;Input&lt;/em&gt; and &lt;em&gt;Output&lt;/em&gt; specifications can be used to interact with other Tasks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Let’s create two tasks &lt;a href="https://github.com/openshift/pipelines-tutorial/blob/master/01_pipeline" target="_blank" rel="noopener"&gt;Source: Pipeline-Tutorial&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;cat &amp;lt;&amp;lt;&amp;#39;EOF&amp;#39; &amp;gt; deploy-Example-Tasks.yaml
apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
name: apply-manifests
spec:
inputs:
resources:
- {type: git, name: source}
params:
- name: manifest_dir
description: The directory in source that contains yaml manifests
type: string
default: &amp;#34;k8s&amp;#34;
steps:
- name: apply
image: quay.io/openshift/origin-cli:latest
workingDir: /workspace/source
command: [&amp;#34;/bin/bash&amp;#34;, &amp;#34;-c&amp;#34;]
args:
- |-
echo Applying manifests in $(inputs.params.manifest_dir) directory
oc apply -f $(inputs.params.manifest_dir)
echo -----------------------------------
---
apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
name: update-deployment
spec:
inputs:
resources:
- {type: image, name: image}
params:
- name: deployment
description: The name of the deployment patch the image
type: string
steps:
- name: patch
image: quay.io/openshift/origin-cli:latest
command: [&amp;#34;/bin/bash&amp;#34;, &amp;#34;-c&amp;#34;]
args:
- |-
oc patch deployment $(inputs.params.deployment) --patch=&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;template&amp;#34;:{&amp;#34;spec&amp;#34;:{
&amp;#34;containers&amp;#34;:[{
&amp;#34;name&amp;#34;: &amp;#34;$(inputs.params.deployment)&amp;#34;,
&amp;#34;image&amp;#34;:&amp;#34;$(inputs.resources.image.url)&amp;#34;
}]
}}}}&amp;#39;
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc create -f deploy-Example-Tasks.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Verify that the two tasks have been created using the Tekton CLI:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;tkn task ls
NAME AGE
apply-manifests 52 seconds ago
update-deployment 52 seconds ago&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_create_a_pipeline"&gt;Create a Pipeline&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;A pipeline is a set of Tasks, which should be executed in a defined way to achieve a specific goal.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The example Pipeline below uses two resources:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;git-repo: defines the Git-Source&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;image: Defines the target at a repository&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;It first uses the Task &lt;strong&gt;buildah&lt;/strong&gt;, which is a standard Task the OpenShift operator created automatically. This task will build the image. The resulted image is pushed to an image registry, defined in the &lt;strong&gt;output&lt;/strong&gt; parameter. After that our created tasks &lt;strong&gt;apply-manifest&lt;/strong&gt; and &lt;strong&gt;update-deployment&lt;/strong&gt; are executed. The execution order of these tasks is defined with the &lt;strong&gt;runAfter&lt;/strong&gt; Parameter in the yaml definition.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="admonitionblock note"&gt;
&lt;table&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td class="icon"&gt;
&lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt;
&lt;/td&gt;
&lt;td class="content"&gt;
The Pipeline should be re-usable accross multiple projects or environments, thats why the resources (git-repo and image) are not defined here. When a Pipeline is executed, these resources will get defined.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;cat &amp;lt;&amp;lt;&amp;#39;EOF&amp;#39; &amp;gt; deploy-Example-Pipeline.yaml
apiVersion: tekton.dev/v1alpha1
kind: Pipeline
metadata:
name: build-and-deploy
spec:
resources:
- name: git-repo
type: git
- name: image
type: image
params:
- name: deployment-name
type: string
description: name of the deployment to be patched
tasks:
- name: build-image
taskRef:
name: buildah
kind: ClusterTask
resources:
inputs:
- name: source
resource: git-repo
outputs:
- name: image
resource: image
params:
- name: TLSVERIFY
value: &amp;#34;false&amp;#34;
- name: apply-manifests
taskRef:
name: apply-manifests
resources:
inputs:
- name: source
resource: git-repo
runAfter:
- build-image
- name: update-deployment
taskRef:
name: update-deployment
resources:
inputs:
- name: image
resource: image
params:
- name: deployment
value: $(params.deployment-name)
runAfter:
- apply-manifests
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc create -f deploy-Example-Pipeline.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Verify that the Pipeline has been created using the Tekton CLI:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;tkn pipeline ls
NAME AGE LAST RUN STARTED DURATION STATUS
build-and-deploy 3 seconds ago --- --- --- ---&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;hr/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_trigger_pipeline"&gt;Trigger Pipeline&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;After the Pipeline has been created, it can be triggered to execute the Tasks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_create_pipelineresources"&gt;Create PipelineResources&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Since the Pipeline is generic, we need to define 2 &lt;strong&gt;PipelineResources&lt;/strong&gt; first, to execute a Pipepline.
Our example application contains a frontend (vote-ui) AND a backend (vote-api), therefore 4 PipelineResources will be created. (2 times git repository to clone the source and 2 time output image)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Quick overview:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ui-repo: will be used as &lt;em&gt;git_repo&lt;/em&gt; in the Pipepline for the Frontend&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ui-image: will be used as &lt;em&gt;image&lt;/em&gt; in the Pipeline for the Frontend&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;api-repo: will be used as &lt;em&gt;git_repo&lt;/em&gt; in the Pipepline for the Backend&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;api-image: will be used as &lt;em&gt;image&lt;/em&gt; in the Pipeline for the Backend&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;cat &amp;lt;&amp;lt;&amp;#39;EOF&amp;#39; &amp;gt; deploy-Example-PipelineResources.yaml
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
name: ui-repo
spec:
type: git
params:
- name: url
value: http://github.com/openshift-pipelines/vote-ui.git
---
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
name: ui-image
spec:
type: image
params:
- name: url
value: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-ui:latest
---
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
name: api-repo
spec:
type: git
params:
- name: url
value: http://github.com/openshift-pipelines/vote-api.git
---
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
name: api-image
spec:
type: image
params:
- name: url
value: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-api:latest
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc create -f deploy-Example-PipelineResources.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The resources can be listed with:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;tkn resource ls
NAME TYPE DETAILS
api-repo git url: http://github.com/openshift-pipelines/vote-api.git
ui-repo git url: http://github.com/openshift-pipelines/vote-ui.git
api-image image url: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-api:latest
ui-image image url: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-ui:latest&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_execute_pipelines"&gt;Execute Pipelines&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;We start a PipelineRune for the backend and frontend of our application.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-yaml" data-lang="yaml"&gt;cat &amp;lt;&amp;lt;&amp;#39;EOF&amp;#39; &amp;gt; deploy-Example-PipelineRun.yaml
apiVersion: tekton.dev/v1alpha1
kind: PipelineRun
metadata:
name: build-deploy-api-pipelinerun
spec:
pipelineRef:
name: build-and-deploy
resources:
- name: git-repo
resourceRef:
name: api-repo
- name: image
resourceRef:
name: api-image
params:
- name: deployment-name
value: vote-api
---
apiVersion: tekton.dev/v1alpha1
kind: PipelineRun
metadata:
name: build-deploy-ui-pipelinerun
spec:
pipelineRef:
name: build-and-deploy
resources:
- name: git-repo
resourceRef:
name: ui-repo
- name: image
resourceRef:
name: ui-image
params:
- name: deployment-name
value: vote-ui
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;oc create -f deploy-Example-PipelineRun.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;The PipelineRuns can be listed with&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;tkn pipelinerun ls
NAME STARTED DURATION STATUS
build-deploy-api-pipelinerun 3 minutes ago --- Running
build-deploy-ui-pipelinerun 3 minutes ago --- Running&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Moreover, the logs can be viewed with the following command and select the appropriate PipelineRun:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="listingblock"&gt;
&lt;div class="content"&gt;
&lt;pre class="highlight"&gt;&lt;code class="language-bash" data-lang="bash"&gt;tkn pipeline logs -f
? Select pipelinerun: [Use arrows to move, type to filter]
&amp;gt; build-deploy-api-pipelinerun started 2 minutes ago
build-deploy-ui-pipelinerun started 2 minutes ago&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect2"&gt;
&lt;h3 id="_checking_your_application"&gt;Checking your application&lt;/h3&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Now our Pipeline built and deployed the voting application, where you can vote if you prefere cats or dogs (Cats or course :) )&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Get the route of your project and open the URL in the browser. (Should be something like vote-ui-pipelines-tutorial.apps.yourclustername)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/other-topics/images/pipelines/Tekton-Vote-App.png" alt="Tekton Vote App"/&gt;
&lt;/div&gt;
&lt;div class="title"&gt;Figure 1. Tekton: Example Application&lt;/div&gt;
&lt;/div&gt;
&lt;hr/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_openshift_webui"&gt;OpenShift WebUI&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;With the OpenShift Pipeline operator a new menu item is introduced on the WebUI of OpenShift. All Tekton CLI command which are used above, can actually be replaced with the web interface, in case you prefere this. The big advantage is th graphical presentation of Pipelines and their lifetime.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;I will not create screenshots for every screen, but for example pipelines:&lt;/p&gt;
&lt;/div&gt;
&lt;div class="paragraph"&gt;
&lt;p&gt;Under &lt;em&gt;Pipelines&lt;/em&gt; a list of pipelines will be shown.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="imageblock"&gt;
&lt;div class="content"&gt;
&lt;img src="https://blog.stderr.at/openshift-platform/other-topics/images/pipelines/Pipelines.png" alt="Pipelines"/&gt;
&lt;/div&gt;
&lt;div class="title"&gt;Figure 2. OpenShift UI: List of Pipelines&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_additional_resources"&gt;Additional Resources&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="sect1"&gt;
&lt;h2 id="_sources"&gt;Sources&lt;/h2&gt;
&lt;div class="sectionbody"&gt;
&lt;div class="ulist"&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a id="source_1"&gt;&lt;/a&gt;[1]: &lt;a href="https://github.com/openshift/pipelines-tutorial" target="_blank" rel="noopener"&gt;OpenShift Pipelines Tutorial&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://tekton.dev/"&gt;Tekon&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/tektoncd/catalog" target="_blank" rel="noopener"&gt;Tekton Task Catalog&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description></item></channel></rss>