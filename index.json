[{"uri":"https://blog.stderr.at/day-2/etcd/","title":"etcd","tags":["OCP","Day-2","OpenShift","etcd"],"description":"","content":""},{"uri":"https://blog.stderr.at/gitopscollection/","title":"GitOps Episodes","tags":["OCP","GitOps","OpenShift","GitOps Approach","Argo CD","ArgoCD"],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/labels_environmets/","title":"Labels &amp; Environments","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector","Environments","Ingress","IngressController","Labels","Annotations"],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/","title":"OpenShift","tags":["Pipelines","OpenShift","OCP","Tekton","GitOps","Operator","Grafana","Thanos","Sealed Secrets","Storage","Day-2","Vault","oc","kubectl","SSL","Cert Manager"],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/","title":"OpenShift Day-2","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/pod-placement/","title":"Pod Placement","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector","Taints","Tollerations","Topology Spread Contstraints","Descheduler","Affinity","Anit-Affinity"],"description":"","content":""},{"uri":"https://blog.stderr.at/securesupplychain/","title":"Secure Supply Chain","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","Rekor","cosign","SBOM","ACS"],"description":"","content":""},{"uri":"https://blog.stderr.at/compliance/","title":"Compliance","tags":["Quay","Security","OpenShift","Compliance","Compliance Operator"],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/","title":"Service Mesh","tags":["OCP","Istio","OpenShift","Service Mesh"],"description":"","content":""},{"uri":"https://blog.stderr.at/general/","title":"General","tags":["Satellite","github","git"],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/","title":"Ansible","tags":["Ansible","Automation","Tower","Controller","AAP","YAML","CICD","Azure","Style Guide"],"description":"","content":""},{"uri":"https://blog.stderr.at/acs/","title":"Advanced Cluster Security","tags":["ACS","Advanced Cluster Security","OpenShift","Security","Keycloak","Authentication","SSO","Stackrox"],"description":"","content":""},{"uri":"https://blog.stderr.at/authors/","title":"Authors","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/azure/","title":"Azure","tags":["Azure","ARO","OpenShift","VPN"],"description":"","content":""},{"uri":"https://blog.stderr.at/java/","title":"Java","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/quay/","title":"Quay","tags":["Quay","Registry","OpenShift","Container Security","Operator"],"description":"","content":""},{"uri":"https://blog.stderr.at/legaldisclosure/","title":"Legal Disclosure","tags":[],"description":"","content":" Legal Disclosure Information in accordance with Section 5 TMG\nToni Schmidbauer \u0026amp; Thomas Jungbauer 1200 Vienna, Austria\nContact Information Toni Schmidbauer:\nThomas Jungbauer: https://jungbauer.stderr.at\nInternet address: https://blog.stderr.at/\nDisclaimer Accountability for content The contents of our pages have been created with the utmost care. However, we cannot guarantee the contents\u0026#39; accuracy, completeness or topicality. According to statutory provisions, we are furthermore responsible for our own content on these web pages. In this matter, please note that we are not obliged to monitor the transmitted or saved information of third parties, or investigate circumstances pointing to illegal activity. Our obligations to remove or block the use of information under generally applicable laws remain unaffected by this as per §§ 8 to 10 of the Telemedia Act (TMG).\nAccountability for links Responsibility for the content of external links (to web pages of third parties) lies solely with the operators of the linked pages. No violations were evident to us at the time of linking. Should any legal infringement become known to us, we will remove the respective link immediately.\nCopyright Our web pages and their contents are subject to German copyright law. Unless expressly permitted by law, every form of utilizing, reproducing or processing works subject to copyright protection on our web pages requires the prior consent of the respective owner of the rights. Individual reproductions of a work are only allowed for private use. The materials from these pages are copyrighted and any unauthorized use may violate copyright laws.\nQuelle: http://translate-24h.de\n"},{"uri":"https://blog.stderr.at/tags/application/","title":"Application","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/applicationset/","title":"ApplicationSet","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/argo-cd/","title":"Argo CD","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/compliance/","title":"Compliance","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/compliance/","title":"Compliance","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/compliance-operator/","title":"Compliance Operator","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/gitops/","title":"GitOps","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/gitops/","title":"GitOps","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ocp/","title":"OCP","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/openshift/","title":"OpenShift","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/openshift/","title":"OpenShift","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/security/","title":"Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/gitopscollection/2024-04-25-installing-compliance-operator/","title":"Setup &amp; Configure Compliance Operator using GitOps","tags":["ApplicationSet","Application","Compliance","OpenShift","OCP","GitOps","Argo CD","Compliance Operator"],"description":"Installing and configuring Compliance Operator using GitOps approach.","content":" In the previous articles, we have discussed the Git repository folder structure and the configuration of the App-Of-Apps. Now it is time to deploy our first configuration. One of the first things I usually deploy is the Compliance Operator. This Operator is recommended for any cluster and can be deployed without any addition to the Subscription.\nIn this article, I will describe how it is installed and how the Helm Chart is configured.\nPrerequisites Argo CD (OpenShift GitOps) deployed\nApp-Of-Apps deployed\nIntroduction As a reminder, at Git repository folder structure I described my preferred folder structure. I would like to deploy the Compliance Operator in the Management Cluster now. All my examples can be found at GitHub repository OpenShift Clusterconfig GitOps. The folder clusters/management-cluster/setup-compliance-operator is the one I am interested in.\nInside this folder, you will find another Helm Chart. The Helm Chart has no local templates, instead, it uses dependencies to call other (sub-) charts. However, the values.yaml is the main part to configure everything.\nIn case you want to have any local template, that you do NOT want to integrate into one of the sub-charts, you can easily do so, by storing them in the templates folder. Why \u0026#34;empty\u0026#34; Helm Charts? Actually, it would be possible to use the Helm Chart of the Chart repository directly, without creating a separate chart, that does nothing else than using dependency charts.\nThe reasons why I am using such an \u0026#34;empty\u0026#34; Chart are the following (in no particular order):\nWith that way it is possible to add templates (i.e. SealedSecrets) and modify the values-file without packaging and releasing a new Chart version every time you change a small thing.\nThe Multi-Source Option, which allows you to use a Helm Chart from repository A and a values file from repository B is still a TechPreview feature (Argo CD 2.10). I am using this for the App-of-Apps already, but I did not do this for all charts. This feature is on the list for Argo CD version 2.11 to become globally available.\nAs an alternative, it is also possible to mix Kustomize and Helm. That way you only need a kustomization.yaml file and reference to a Helm Chart. In the folder clusters/management-cluster/ingresscontroller I have such an example.\nInstalling Compliance Operator Analysing Chart.yaml As any Helm Chart a Chart.yaml file exists, that stores the basic information. The most important ones for now are the dependencies.\nThe file looks like the following. Three sub-charts are defined as required to deploy and configure the Compliance Operator.\napiVersion: v2 name: setup-compliance-operator description: Deploy and configure the Compliance Operator version: 1.0.1 dependencies: - name: compliance-operator-full-stack (1) version: ~1.0.0 (2) repository: https://charts.stderr.at/ - name: helper-operator (3) version: ~1.0.21 repository: https://charts.stderr.at/ - name: helper-status-checker (4) version: ~4.0.0 repository: https://charts.stderr.at/ condition: helper-status-checker.enabled (5) 1 Dependency: Compliance Operator Full Stack 2 Version that will be used. The \u0026#34;~\u0026#34; means that the latest version of 1.0.X will be used. 3 Dependency: Helper Operator 4 Dependency: Helper Status Checker 5 Only use this dependency when \u0026#34;enabled\u0026#34; is set Verify the READMEs of the different Charts for detailed information on how to configure them. As you can see three other Helm Charts are used to actually deploy and configure the Operator.\nConfiguration of the Chart To configure the Compliance Operator, the values files must be prepared accordingly.\nThe important thing here is, that any value that should be bypassed to a sub-chart is defined under the name of the sub-chart. For example, everything under helper-operator: will be sent to the helper-operator Chart and is used there for its configuration. The following is a full example of the values I typically use.\n# Install Operator Compliance Operator # Deploys Operator --\u0026gt; Subscription and Operatorgroup helper-operator: operators: compliance-operator: enabled: true syncwave: \u0026#39;0\u0026#39; namespace: name: openshift-compliance create: true subscription: channel: stable approval: Automatic operatorName: compliance-operator source: redhat-operators sourceNamespace: openshift-marketplace operatorgroup: create: true notownnamespace: true # Verify if the Operator has been successfully deployed helper-status-checker: enabled: true checks: - operatorName: compliance-operator namespace: name: openshift-compliance serviceAccount: name: \u0026#34;status-checker-compliance\u0026#34; # Setting for the Compliance Operator compliance-operator-full-stack: compliance: namespace: name: openshift-compliance syncwave: \u0026#39;0\u0026#39; descr: \u0026#39;Red Hat Compliance\u0026#39; scansettingbinding: enabled: true syncwave: \u0026#39;3\u0026#39; profiles: - name: ocp4-cis-node kind: Profile # Could be Profile or TailedProfile - name: ocp4-cis kind: Profile scansetting: default Let us walk through the settings in more detail.\nInstalling the Operator The first thing to do is to deploy the Operator. Two resources are relevant to install an Operator:\nSubscription\nOperatorGroup\nBoth objects should be deployed at the very beginning of Argo CD synchronisation. This is done by setting the Syncwave to 0.\nThe main settings are the operatorName, the channel (which is the version of the operator) and the approval (which defines if the Operator is updated automatically or manually).\nIn addition, a Namespace object is deployed, because this Operator should run in its very own namespace.\nThis will start the Operator installation process.\nhelper-operator: operators: compliance-operator: (1) enabled: true (2) syncwave: \u0026#39;0\u0026#39; (3) namespace: name: openshift-compliance (4) create: true subscription: (5) channel: stable # Version of the Operator approval: Automatic # Automatic or Manual operatorName: compliance-operator # Name of the Operator source: redhat-operators sourceNamespace: openshift-marketplace operatorgroup: (6) create: true notownnamespace: true 1 Key that can be freely defined. Theoretically, you can deploy multiple operators at once. 2 Is this Operator enabled yes/no. 3 Syncwave for the Operator deployment. (Subscription and OperatorGroup etc.) 4 The Namespace where the Operator shall be deployed and if this namespace shall be created. 5 Configuration of the Subscription resource. 6 Configuration of the OperatorGroup Verify the README at Helper Operator to find additional possible configurations. Verify the Status of the Operator After Argo CD creates the subscription and operatorgroup resources (and namespace), OpenShift will start the installation of the Operator. This installation will take a while but Argo CD does not see this. All it sees is that the Subscription resource is available and it tries to continue with the configuration of the Operator. Here it will fail because the CRDs are not available yet.\nTherefore, I created a mechanism to verify if an Operator is ready or not.\nAlso verify the separate article Operator Installation with Argo CD that addresses the problem in more detail. All it does is to start a small Job inside OpenShift and to verify the status of the Operator installation. If everything is fine, the Job will end successfully and Argo CD will continue with the next syncwave. Argo CD Hook and syncwaves are required here. The Job should be started after the Subscription/OperatorGroup resources have been created, which means any syncwave after \u0026#34;0\u0026#34;.\nThe following annotations will be used by the Job:\nargocd.argoproj.io/hook: Sync (1) argocd.argoproj.io/hook-delete-policy: HookSucceeded (2) argocd.argoproj.io/sync-wave: {{ .syncwave | default 1 | quote }} (3) 1 Hooks are ways to run scripts before, during, and after a Sync operation. 2 Deletes the OpenShift Job again. The hook resource is deleted after the hook succeeded (e.g. Job/Workflow completed successfully). 3 Syncwave: can be configured. Must be after helper-operator (default 0) and before the Operator is configured further. Default value is 1. The configuration for hepler_status_checker will look like the following:\n# Verify if the Operator has been successfully deployed helper-status-checker: enabled: true (1) checks: (2) - operatorName: compliance-operator (3) namespace: name: openshift-compliance (4) serviceAccount: name: \u0026#34;status-checker-compliance\u0026#34; (5) 1 Enable status checker or not. Default: false 2 List of operators to check. Typically, only one is checked, but there could be more. 3 Name of the Operator to check (same as for helper-operator) 4 Namespace where the Operator has been installed (same as for helper-operator) 5 Name of the ServiceAccount that will be created to check the status. Verify the README at Helper Operator Status Checker to find additional possible configurations. Configuring Compliance Operator Finally, the Operator has been deployed and has been verified. Now the time is right to configure the Operator with any configuration we would like. This means, using CRDs to do whatever the Operator offers.\nThis is reflected in the following part of the values file. All these settings are handed over to the sub-chart compliance-operator-full-stack.\nVerify the README at Compliance Operator Chart to find additional possible configurations. Especially, if you like to do Tailored Profiles. The compliance operator requires a so-called ScanSettingBinding that uses Profiles which are used to check the cluster compliance once a day. In this case, I am using CIS Benchmarks. There are two profiles:\nocp4-cis-node: will check the node operating system for missing but suggested configuration.\nocp4-cis: will check the OpenShift cluster for missing but suggested configuration.\n# Setting for the Compliance Operator compliance-operator-full-stack: (1) compliance: namespace: name: openshift-compliance (2) syncwave: \u0026#39;0\u0026#39; descr: \u0026#39;Red Hat Compliance\u0026#39; scansettingbinding: (3) enabled: true syncwave: \u0026#39;3\u0026#39; profiles: (4) - name: ocp4-cis-node kind: Profile # Could be Profile or TailedProfile - name: ocp4-cis kind: Profile scansetting: default 1 Handing everything that comes below to the sub-chart compliance-operator-full-stack 2 Namespace where the configuration should be deployed. The Syncwave at this point could be omitted. 3 The configuration for the ScanSettingBinding. It is enabled (default = false) and has a Syncwave AFTER the helper-status-checker. 4 The list of profiles that shall be used. These must exist. The Compliance Operator offers several profiles. I usually use these two for full CIS compliance check. Conclusion With this configuration, the Compliance Operator will not only be installed but also configured with the same Argo CD Application. All you need to do is to synchronize Argo CD and let the magic happen. After a few minutes, everything should be in sync.\nFigure 1. Sync Compliance Operator Inside OpenShift the Operator is configured and starts doing its job:\nFigure 2. Configured Compliance Operator This concludes the deployment of the Compliance Operator. For further information about the Operator itself, please read the documentation or articles:\nOfficial Documentation: Compliance Operator\nBlog: Compliance Operator\nAlso, be sure to check out the READMEs of the different Charts:\nHelper Operator\nHelper Operator Status Checker\nCompliance Operator Chart\nCompliance Operator Setup\nIf you have any questions or problems, feel free to create a GitHub issue at any time.\n"},{"uri":"https://blog.stderr.at/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/app-of-apps/","title":"App-of-Apps","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/gitopscollection/2024-04-02-configure_app_of_apps/","title":"Configure App-of-Apps","tags":["ApplicationSet","Application","App-of-Apps","OpenShift","OCP","GitOps","Argo CD","Matrix Generator","Git Generator","List Generator","Helm"],"description":"Adding ApplicationSets and Applications to our GitOps","content":" In the article Install GitOps to the cluster OpenShift GitOps is deployed using a shell script. This should be the very first installation and the only deployment that is done manually on a cluster. This procedure automatically installs the so-called App-of-Apps named Argo CD Resources Manager which is responsible for all further Argo CD Applications and ApplicationSets. No other configuration should be done manually if possible.\nThis article will demonstrate how to configure the App-of-Apps in an easy and declarative way, using ApplicationSet mainly.\nPrerequisites At this stage, the OpenShift cluster with the openshift-gitops operator and the App-of-Apps must be deployed. Your Argo CD should look somehow like this:\nFigure 1. Argo CD: Initial Applications But how do all these Applications end up in Argo CD and how can you add additional ones?\nUnderstanding the Argo CD Resources Manager For any further references, I am using the GitHub repository OpenShift Clusterconfig GitOps The Argo CD Resources Manager is, in fact, the App-of-Apps. Its configuration file can be found in the directory base/argocd-resources-manager. It is simply a values file and uses the Helm Chart helper-argocd to create additional Applications or ApplicationSets for Argo CD.\nAnalyzing the example values file The example values file seems to be huge and confusing at first look. But it is quite easy to understand …​ trust me :)\nLet’s walk through the file bit by bit:\nDefining Header Variables At the top of the file, some variables are defined as so-called anchors. These definitions might be used multiple times and are defined at the top to allow us to find and change them easily.\nI define here the clusters and the information about the GitHub repository for example:\nmgmt-cluster: \u0026amp;mgmtcluster https://kubernetes.default.svc (1) mgmt-cluster-name: \u0026amp;mgmtclustername in-cluster (2) production-cluster: \u0026amp;prodcluster https://api.ocp.aws.ispworld.at:6443 production-cluster-name: \u0026amp;prodclustername prod repourl: \u0026amp;repourl \u0026#39;https://github.com/tjungbauer/openshift-clusterconfig-gitops\u0026#39; (3) repobranch: \u0026amp;branch main (4) 1 Define the API URL for the cluster as configured in Argo CD. (The local cluster where Argo CD is running might be called kubernetes.default.svc) 2 Define the short name of the cluster as configured in Argo CD. (The local cluster where Argo CD is running might be called in-cluster) 3 Define the URL to the GitHub repository that is used in this file. 4 Define the git branch that will be used. Any additional anchor can be used to define values, that should show up at the very top and/or are used multiple times and you do not want to write them each time.\nWhenever you see a value defined as *repourl for example, such an anchor is used. Understanding Naming Conventions Each Application or ApplicationSet must have a unique name inside Argo CD. Whenever Applications are generated by ApplicationSet a prefix with the name of the cluster is usually added.\nIn the values file multiple Applications or ApplicationSets can be defined. They are all bypassed to the Helm Chart which takes care of everything. The name that will be used for an ApplicationSet for example will be the (yaml) key of the definition.\nFor example, ApplicationSets are defined as:\napplicationsets: mgmt-cluster: ... enable-etcd-encryption: ... The keys, for example, mgmt-cluster or enable-etcd-encryption are used as names of the ApplicationSets. This way you do not need to take care of unique names, as YAML will already complain if some kwy is used twice.\nEnable/Disable Like with all of my Helm Charts, I added a switch to enable (or disable) a certain configuration. This way, you can easily remove Applications without actually deleting the specification.\nThe default value is false, so you actively need to set it to true mgmt-cluster: # Is the ApplicationSet enabled or not enabled: true Supported Generators The helm chart helper-argocd supports the following generators currently:\nMatrix Generator\nList Generator\nGit Generator (for files)\nCluster Generator\nAdditional generators might be added in the future (ping me or create a pull request), but I found these the most useful ones.\nBut what is the difference between these generators from the configuration point of view? The different generators require different configurations and therefore provide different placeholders for variables. While the Git generator might use variables that are defined in a file that it finds {{environment}} the List (or Cluster) generator is using {{url}} to define the target cluster.\nThis might make the specification of an ApplicationSet quite complex …​ and that’s the whole reason for creating the helper-argocd\nExample ApplicationSet - Matrix Generator The first ApplicationSet I would like to show is probably the most important one. As described in GitOps Repository Structure I am using a folder structure like clusters/management-cluster/ and in this folder I am defining any configuration that is applicable for that specific cluster. If I want to add a new cluster, I simply create a new folder (and a new App-of-Apps configuration). With this, you will always see which settings a specific cluster has without much hassle.\nThe idea is to walk over this folder and automatically create a new Argo CD Application for any sub-folder that is found. This has the advantage, that whenever I want to create an additional configuration for a cluster, I simply add another sub-folder and the ApplicationSet will automatically create a new Argo CD application.\nTo achieve this the so-called Matrix Generator is used. This generator combines two (currently two are possible only) generators. In our case, it combines:\ngit generator: to walk over the folder and get and sub-folder\nlist generator: to define the target cluster\nThe snippet of the configuration will look like the following:\n# Definition of Matrix Generator. Only 2 generators are supported at the moment generatormatrix: (1) # Git: Walking through the specific folder and take whatever is there. - git: (2) directories: - path: clusters/management-cluster/* - path: clusters/management-cluster/waves exclude: true repoURL: *repourl revision: *branch # List: simply define the targetCluster. The name of the cluster must be known by Argo CD - list: (3) elements: # targetCluster is important, this will define on which cluster it will be rolled out. # The cluster name must be known in Argo CD - targetCluster: *mgmtclustername 1 Using matrix generator 2 The first generator is Git: It will observe any changes in the folder clusters/management-cluster and will create a new Argo CD Application if a new sub-folder is found. However, it excludes the folder clusters/management-cluster/waves/ 3 The second generator is List: It simply defines the target cluster where the Application that is created by the ApplicationSet shall be deployed. Now let us bring the whole example together:\nmgmt-cluster: (1) # Is the ApplicationSet enabled or not enabled: true (2) # Description - always useful description: \u0026#34;ApplicationSet that Deploys on Management Cluster Configuration (using Matrix Generator)\u0026#34; (3) # Any labels you would like to add to the Application. Good to filter it in the Argo CD UI. labels: (4) category: configuration env: mgmt-cluster # Using go text template. See: https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/GoTemplate/ goTemplate: true (5) argocd_project: *mgmtclustername (6) # preserve all resources when the application get deleted. This is useful to keep that workload even if Argo CD is removed or severely changed. preserveResourcesOnDeletion: true (7) # Definition of Matrix Generator. Only 2 generators are supported at the moment generatormatrix: (8) # Git: Walking through the specific folder and take whatever is there. - git: directories: - path: clusters/management-cluster/* - path: clusters/management-cluster/waves exclude: true repoURL: *repourl revision: *branch # List: simply define the targetCluster. The name of the cluster must be known by Argo CD - list: elements: # targetCluster is important, this will define on which cluster it will be rolled out. # The cluster name must be known in Argo CD - targetCluster: *mgmtclustername syncPolicy: (9) autosync_enabled: false 1 Key of the ApplicationSet inside the yaml specification, that will be used as object name 2 Is the ApplicationSet enabled or not (Default: false) 3 A useful description 4 Labels that can be used to filter 5 Enable the usage of Go Template for this ApplicationSet 6 The Argo CD project (not OpenShift project) the ApplicationSet belongs to 7 Be sure that resources are not deleted when deleting the ApplicationSet. I found this quite useful …​ otherwise, all Applications the ApplicationSet created will be removed INCLUDING the resources they have created. 8 The specification of the matrix generator 9 Any kind of syncPolicy …​ in this case automatic synchronization of the Applications that are created is disabled. Based on these settings the helper-argocd helm chart will render an ApplicationSet object automatically. As mentioned above it will be called mgmt-cluster and creates an Application for any sub-folder it finds in clusters/management-cluster.\nAny new folder that is added will automatically create a new Application. You do not need to configure anything else.\nThe full objects will look like this:\napiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: mgmt-cluster (1) namespace: openshift-gitops labels: app.kubernetes.io/instance: argocd-resources-manager app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: helper-argocd category: configuration env: mgmt-cluster helm.sh/chart: helper-argocd-2.0.28 spec: generators: (2) - matrix: generators: - git: directories: - path: clusters/management-cluster/* - exclude: true path: clusters/management-cluster/waves repoURL: \u0026#39;https://github.com/tjungbauer/openshift-clusterconfig-gitops\u0026#39; revision: main - list: elements: - targetCluster: in-cluster goTemplate: true goTemplateOptions: - missingkey=error syncPolicy: preserveResourcesOnDeletion: true template: metadata: name: \u0026#39;{{ .targetCluster }}-{{ .path.basenameNormalized }}\u0026#39; (3) spec: destination: (4) name: \u0026#39;{{ .targetCluster }}\u0026#39; namespace: default info: - name: Description value: ApplicationSet that Deploys on Management Cluster Configuration (using Matrix Generator) project: in-cluster source: (5) path: \u0026#39;{{ .path.path }}\u0026#39; repoURL: \u0026#39;https://github.com/tjungbauer/openshift-clusterconfig-gitops\u0026#39; targetRevision: main 1 Name of the object == name of the key in the values file definition 2 Configuration of the matrix generator 3 Name of the Applications that this ApplicationSet will generate. In this case, it will concat the name of the target cluster and the name of the path. 4 Target cluster 5 Definition of the source for the Application Example ApplicationSet - Git Generator Now let us take a look at a second example using the git generator. The basic idea is quite similar and just a few minor changes must be made to our configuration.\nIn the following object, a git file generator is used to observe a specific folder and look for the file named values.yaml. For each file that it found an Application is created.\nThis example is also explained in my article at Project onboarding using GitOps and Helm # Tenant Onboarding (using Git Generator) onboarding-tenant-workload: (1) # Is the ApplicationSet enabled or not enabled: true # Description - always useful description: \u0026#34;Onboarding Workload to the cluster\u0026#34; # Any labels you would like to add to the Application. Good to filter it in the Argo CD UI. labels: catagory: tenant-onboarding # Path to the Git repository. The default URL and revision are defined as anchors at the beginning of the file, but could be overwritten here. path: clusters/all/project-onboarding (2) repourl: *repourl targetrevision: *branch # Using go text template. See: https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/GoTemplate/ goTemplate: true # Helm configuration. A list of helm values files helm: (3) per_cluster_helm_values: false value_files: - \u0026#39;/{{ .path.path }}/values.yaml\u0026#39; - /tenants/values-global.yaml # Generator: currently list, git and cluster are possible. # either \u0026#34;generatorlist\u0026#34;, \u0026#34;generatorgit\u0026#34; or \u0026#34;generatorclusters\u0026#34; # Define the repository that shall be checked for configuration file generatorgit: (4) - repourl: *repourl targetrevision: *branch files: - tenants/**/values.yaml (5) # preserve all resources when the application gets deleted. This is useful to keep that workload even if Argo CD is removed or severely changed. preserveResourcesOnDeletion: true 1 Name of the ApplicationSet 2 The repo URL and path which shall be read for the ApplicationSet 3 A list of values files, that shall be used. 4 The specification of the Git generator 5 The path that shall be observed by this ApplicationSet. ** will return all files and directories recursively. Again, the Helm chart helper-argocd will render an ApplicationSet for us.\napiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: onboarding-tenant-workload namespace: openshift-gitops labels: app.kubernetes.io/instance: argocd-resources-manager app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: helper-argocd catagory: tenant-onboarding helm.sh/chart: helper-argocd-2.0.28 spec: generators: - git: files: - path: tenants/**/values.yaml repoURL: \u0026#39;https://github.com/tjungbauer/openshift-clusterconfig-gitops\u0026#39; revision: main goTemplate: true goTemplateOptions: - missingkey=error syncPolicy: preserveResourcesOnDeletion: true template: metadata: name: \u0026#39;{{ index .path.segments 1 | normalize }}-{{ .path.basename }}\u0026#39; spec: destination: name: \u0026#39;{{ .environment }}\u0026#39; namespace: default info: - name: Description value: Onboarding Workload to the cluster project: default source: helm: valueFiles: - \u0026#39;/{{ .path.path }}/values.yaml\u0026#39; - /tenants/values-global.yaml path: clusters/all/project-onboarding repoURL: \u0026#39;https://github.com/tjungbauer/openshift-clusterconfig-gitops\u0026#39; targetRevision: main Example ApplicationSet - List Generator At this point, we have seen two examples of ApplicationSets defined for helper-argocd. The List generator will be very easy to understand as it simply uses a list of target clusters to render the ApplicationSet.\nThe following snippet demonstrates that all you need to set are the clustername and clusterurl.\nPlease also verify the article Argo CD and Release Management with Helm Charts and ApplicationSets to understand the usage of the setting chart_version. # List of clusters # \u0026#34;clustername\u0026#34; (string): Is the name of the cluster a defined in Argo CD # \u0026#34;clusterurl\u0026#34; (string): Is the URL of the cluster API # \u0026#34;chart_version\u0026#34; (string, optional): Defines which chart version shall be deployed on each cluster. generatorlist: - clustername: *mgmtclustername clusterurl: *mgmtcluster This is all the magic. The rendered ApplicationSet will look like:\napiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: install-sonarqube namespace: openshift-gitops labels: app.kubernetes.io/instance: argocd-resources-manager app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: helper-argocd category: project helm.sh/chart: helper-argocd-2.0.28 spec: generators: - list: elements: - cluster: in-cluster url: \u0026#39;https://kubernetes.default.svc\u0026#39; template: metadata: name: \u0026#39;{{ cluster }}-install-sonarqube\u0026#39; spec: destination: namespace: sonarqube server: \u0026#39;{{ url }}\u0026#39; info: - name: Description value: Install Sonarqube project: \u0026#39;{{ cluster }}\u0026#39; source: chart: sonarqube helm: releaseName: sonarqube repoURL: \u0026#39;https://charts.stderr.at/\u0026#39; targetRevision: 1.0.1 The list generator can be used to deploy on ALL clusters too. Simply define generatorlist: [] The example above also demonstrates how to use a Helm chart instead of a git repository. What about Applications? The examples above show the usage of ApplicationSet and recently I migrated any specification of an Application to ApplicationSets as I believe this is easier to use, especially when the Chart is rendering it for you. However, it is still possible to define Applications as well.\nThe following example defines such an Application. The configuration differs compared to the ApplicationSet, however, the main idea stays the same:\napplications: (1) node-labelling: (2) enabled: true description: \u0026#34;Deploy Node Labels\u0026#34; labels: category: configuration namespace: name: default create: false server: *mgmtcluster (3) project: default syncOptions: (4) - name: ServerSideApply value: true - name: Validate value: false source: (5) path: clusters/management-cluster/node-labels helm: valuesfiles: - name: values.yaml repourl: *repourl targetrevision: *branch 1 Defining Applications 2 Name of the Application. Be sure that it is unique since this time no prefix will be added 3 The target cluster for this Application 4 Different options for the synchronization 5 The specification of the source. This contains the path, URL and branch of the repository and (in this case) the definition of a Helm values file. Summary I hope I was able to explain the usage of my chart helper-argocd and how I configure it. You can also verify the README to find additional possible settings and the example values.file that I use for all my clusters when I to a demo.\n"},{"uri":"https://blog.stderr.at/tags/git-generator/","title":"Git Generator","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/helm/","title":"Helm","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/list-generator/","title":"List Generator","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/matrix-generator/","title":"Matrix Generator","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/bucket/","title":"Bucket","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/lifecycle/","title":"Lifecycle","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/noobaa/","title":"Noobaa","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/odf/","title":"ODF","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/openshift-data-foundation/","title":"OpenShift Data Foundation","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2024/02/openshift-data-foundation-noobaa-bucket-data-retention-lifecycle/","title":"OpenShift Data Foundation - Noobaa Bucket Data Retention (Lifecycle)","tags":["OpenShift","OCP","ODF","OpenShift Data Foundation","Noobaa","Bucket","Lifecycle"],"description":"How to configure a bucket lifecycle for Noobaa and OpenShift Data Foundation","content":" Data retention or lifecycle configuration for S3 buckets is done by the S3 provider directly. The provider keeps track and files are automatically rotated after the requested time.\nThis article is a simple step-by-step guide to configure such lifecycle for OpenShift Data Foundation (ODF), where buckets are provided by Noobaa. Knowledge about ODF is assumed, however similar steps can be reproduced for any S3-compliant storage operator.\nPrerequisites Installed OpensShift 4.x cluster (latest version during the creation of this article 4.14)\nInstalled Open Data Foundation Operator (4.14+)\nConfigured Multi-Cloud Gateway (for example) to provide OpenShift with object storage.\nInstalled aws command line tool. The deployment for different operating system is explained at: Installing AWS Client\nA configured bucket and running openshift logging.\nIn the further steps we use the bucket that is created for OpenShift Logging (using Lokistack) as a reference. Working with aws client The first thing to do to work with the aws client is to retrieve the access key and secret key to be able to authenticate against the S3 API. During the deployment of OpenShift logging and the Lokistack a bucket has been created. The secret to authenticate against this bucket can be found in the openshift-logging namespace. In my example the name of this secret is logging-loki-s3 and it contains the following values:\nThe easiest way to authenticate against the S3 API is to create the following configuration file:\n\u0026gt; vim ~/.aws/credentials [default] aws_access_key_id = \u0026lt;value of access_key_id\u0026gt; aws_secret_access_key = \u0026lt;value of access_key_secret\u0026gt; Be sure that this file is not globally accessible. Listing Objects As a first test, we can try to list objects. The following command uses the S3 endpoint and the name of the bucket. Again, these values can be found in the Loki secret, mentioned above:\n$ aws --endpoint https://s3-openshift-storage.apps.ocp.local --no-verify-ssl s3 ls s3://logging-bucket-d39a258c-e971-4a0f-a1fa-302cb7e76a56 PRE application/ PRE audit/ PRE index/ PRE infrastructure/ This command simply lists the current content (some folders in our case) of this bucket.\nCopy a file into the bucket For further testing we copy a test file into the bucket. This can be any file, I have created an empty one called testfile.txt\n$ aws --endpoint https://s3-openshift-storage.apps.ocp.local --no-verify-ssl s3 cp testfile.txt s3://logging-bucket-d39a258c-e971-4a0f-a1fa-302cb7e76a56 upload: ./testfile.txt to s3://logging-bucket-d39a258c-e971-4a0f-a1fa-302cb7e76a56/testfile.txt Listing the content again, will now show the uploaded testfile.txt\n$ aws --endpoint https://s3-openshift-storage.apps.ocp.local --no-verify-ssl s3 ls s3://logging-bucket-d39a258c-e971-4a0f-a1fa-302cb7e76a56 PRE application/ PRE audit/ PRE index/ PRE infrastructure/ 2024-02-10 04:23:14 32 testfile.txt Verifying current Lifecycle configuration To verify the current lifecycle configuration, execute the following command. It will show any configuration that is currently available, or an empty value if there is no such setting yet.\n$ aws --endpoint https://s3-openshift-storage.apps.ocp.local --no-verify-ssl s3api get-bucket-lifecycle-configuration --bucket logging-bucket-d39a258c-e971-4a0f-a1fa-302cb7e76a56 Put a lifecycle configuration in place To configure a data retention of 4 days for our bucket we first need to create the following JSON file:\ncat logging-bucket-lifecycle.json { \u0026#34;Rules\u0026#34;: [ { \u0026#34;Expiration\u0026#34;: { \u0026#34;Days\u0026#34;: 4 (1) }, \u0026#34;ID\u0026#34;: \u0026#34;123\u0026#34;, (2) \u0026#34;Filter\u0026#34;: { \u0026#34;Prefix\u0026#34;: \u0026#34;\u0026#34; (3) }, \u0026#34;Status\u0026#34;: \u0026#34;Enabled\u0026#34; } ] } 1 Defines the retention period is days. 2 A simple ID for this Rule. (string). 3 A filter used to identify objects that a Lifecycle Rule applies to, here the filter is empty, so all objects are affected. There are much more setting possible as describe at AWS S3API. The following command will put the defined rule in place:\nAlready defined rules will be overwritten by the JSON file. If there have been previous configurations, put them into the JSON file as well. $ aws --endpoint https://s3-openshift-storage.apps.ocp.local --no-verify-ssl s3api put-bucket-lifecycle-configuration --bucket logging-bucket-d39a258c-e971-4a0f-a1fa-302cb7e76a56 --lifecycle-configuration file://logging-bucket-lifecycle.json Checking again with the previous command, the rule should now be configured:\n$ aws --endpoint https://s3-openshift-storage.apps.ocp.local --no-verify-ssl s3api get-bucket-lifecycle-configuration --bucket logging-bucket-d39a258c-e971-4a0f-a1fa-302cb7e76a56 What now? Now you have to wait. With the configuration used above, it takes 4 days until the file is rotated. I have tested this using a 1 day retention period and saw that the file will be rotated after about 30 hours. So the rotation will not happen exactly at 24 hours but a bit afterwards.\nConsclusion This article describes very, and I mean very, briefly how to configure such data retention for OpenShift Data Foundation. Unfortunately, public documentation can be confusing, so I summarized here the commands I have used.\nThere are some limitations with the Noobaa integration though. For example file transition (to a different storage class) is (currently) not supported.\nAlso, there are much more possible API calls that might be interesting. Please follow the AWS documentation:\nS3 API\nS3\n"},{"uri":"https://blog.stderr.at/categories/other/","title":"Other","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/kubectl/","title":"Kubectl","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/oc/","title":"Oc","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/gitopscollection/2024-02-02-setup-argocd/","title":"Setup OpenShift GitOps/Argo CD","tags":["oc","kubectl","OpenShift","OCP","GitOps","Argo CD"],"description":"Openshift 4.x - Install GitOps to the cluster","content":" „If it is not in GitOps, it does not exist“ - is a mantra I hear quite often and also try to practice at customer engagements. The idea is to have Git as the only source of truth on what happens inside the environment. That said, Everything as Code is a practice that treats every aspect of the system as a code. Storing this code in Git provides a shared understanding, traceability and repeatability of changes.\nWhile there are many articles about how to get GitOps into the deployment process of applications, this one rather sets the focus on the cluster configuration and tasks system administrators usually have to do.\nAlso check out the article GitOps Argo CD Prerequisites It all begins with an OpenShift cluster. Such a cluster must be installed and while we will not discuss a bootstrap of the whole cluster … yes, it is possible to even automate the cluster deployment using Advanced Cluster Management as an example, we will simply assume that one cluster is up and running.\nFor our setup, an OpenShift cluster 4.14 is deployed and we will use the repository OpenShift Cluster Configuration using GitOps to deploy our configuration onto this cluster. This repository shall act as the source of truth for any configuration. In the article Choosing the right Git repository structure I have explained the folder structure I am usually using. As tool I am usually using Helm Charts.\nThe openshift-clusterconfig-gitops repository heavily uses the Helm Repository found at https://charts.stderr.at/ Deploy OpenShift-GitOps The first thing we need to do is to deploy OpenShift-GitOps, which is based on the Argo CD project. OpenShift-GitOps comes as an Operator and is available to all OpenShift customers. The Operator will deploy and configure Argo CD and provide several custom resources to configure Argo CD Applications or ApplicationSets for example.\nTo automate the operator deployment the following shell script can be used: init_GitOps.sh.\nThis Shell script is the only script that is executed manually. It installs and configures Argo CD. Any other operation on the cluster must then be done using GitOps processes. I am using this to quickly install a new Demo-cluster. There are alternatives and maybe better way, but for my purpose it works pretty well. Clone the repository to your local machine\ngit clone https://github.com/tjungbauer/openshift-clusterconfig-gitops.git Be sure that you are logged in the the required cluster\noc whoami --show-server Execute the init_GitOps.sh\n./init_GitOps.sh The script will deploy the operator and configure/patch the Argo CD instance. In addition, it will create the so-called Application of Applications, which acts as an umbrella Application, that automatically creates all other Argo CD Application(Sets). For now, the App of Apps is the only Argo CD Application that automatically synchronizes all changes found in Git. This is for security, purposes so you can test the cluster configuration one after another.\nOf course, it is up to you if you want to use the shell script. The Operator can also be installed manually, using Advanced Cluster Manager, or using Platform Operators and installing the Operating during the cluster installation (However, this feature is currently (v4.15) TechPreview) What will this script do? I will not de-assemble the script line by line, but in general, the following will happen:\nAdding repository https://charts.stderr.at/ and install the Chart openshift-gitops\nThis FIRST OpenShift-GitOps will be deployed with cluster-admin privileges since we want to manage the whole cluster configuration. This Argo CD instance should not be used for application deployment. For that, deploy additional instances of GitOps. Waiting for Deployments to become ready\nDeploy the Application of Applications that is responsible for automatically deploying a set of Applications or ApplicationSets (see [The Argo CD Object Manager Application])\nThe following shows the output of the command:\nExpand me... ❯ ./init_GitOps.sh Starting Deployment Deploying OpenShift GitOps Operator Adding Helm Repo https://charts.stderr.at/ WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /Users/tjungbau/openshift-aws/aws/auth/kubeconfig \u0026#34;tjungbauer\u0026#34; has been added to your repositories WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /Users/tjungbau/openshift-aws/aws/auth/kubeconfig Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;sealed-secrets\u0026#34; chart repository ...Successfully got an update from the \u0026#34;tjungbauer\u0026#34; chart repository ...Successfully got an update from the \u0026#34;apache-airflow\u0026#34; chart repository ...Successfully got an update from the \u0026#34;hashicorp\u0026#34; chart repository ...Successfully got an update from the \u0026#34;bitnami\u0026#34; chart repository Update Complete. ⎈Happy Helming!⎈ WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /Users/tjungbau/openshift-aws/aws/auth/kubeconfig Release \u0026#34;openshift-gitops-operator\u0026#34; has been upgraded. Happy Helming! NAME: openshift-gitops-operator LAST DEPLOYED: Mon Sep 26 13:22:33 2022 NAMESPACE: openshift-operators STATUS: deployed REVISION: 2 TEST SUITE: None Give the gitops-operator some time to be installed. Waiting for 45 seconds... Waiting for operator to start. Chcking every 10 seconds. NAME READY UP-TO-DATE AVAILABLE AGE gitops-operator-controller-manager 1/1 1 1 4d4h Waiting for openshift-gitops namespace to be created. Checking every 10 seconds. NAME STATUS AGE openshift-gitops Active 4d4h Waiting for deployments to start. Checking every 10 seconds. NAME READY UP-TO-DATE AVAILABLE AGE cluster 1/1 1 1 4d4h Waiting for all pods to be created Waiting for deployment cluster deployment \u0026#34;cluster\u0026#34; successfully rolled out Waiting for deployment kam deployment \u0026#34;kam\u0026#34; successfully rolled out Waiting for deployment openshift-gitops-applicationset-controller deployment \u0026#34;openshift-gitops-applicationset-controller\u0026#34; successfully rolled out Waiting for deployment openshift-gitops-redis deployment \u0026#34;openshift-gitops-redis\u0026#34; successfully rolled out Waiting for deployment openshift-gitops-repo-server deployment \u0026#34;openshift-gitops-repo-server\u0026#34; successfully rolled out Waiting for deployment openshift-gitops-server deployment \u0026#34;openshift-gitops-server\u0026#34; successfully rolled out GitOps Operator ready Lets use our patched Argo CD CRD argocd.argoproj.io/openshift-gitops unchanged clusterrolebinding.rbac.authorization.k8s.io/cluster-admin-0 unchanged Waiting for deployment cluster deployment \u0026#34;cluster\u0026#34; successfully rolled out Waiting for deployment kam deployment \u0026#34;kam\u0026#34; successfully rolled out Waiting for deployment openshift-gitops-applicationset-controller deployment \u0026#34;openshift-gitops-applicationset-controller\u0026#34; successfully rolled out Waiting for deployment openshift-gitops-redis deployment \u0026#34;openshift-gitops-redis\u0026#34; successfully rolled out Waiting for deployment openshift-gitops-repo-server deployment \u0026#34;openshift-gitops-repo-server\u0026#34; successfully rolled out Waiting for deployment openshift-gitops-server deployment \u0026#34;openshift-gitops-server\u0026#34; successfully rolled out GitOps Operator ready... again WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /Users/tjungbau/openshift-aws/aws/auth/kubeconfig Release \u0026#34;app-of-apps\u0026#34; has been upgraded. Happy Helming! NAME: app-of-apps LAST DEPLOYED: Mon Sep 26 13:23:59 2022 NAMESPACE: openshift-gitops STATUS: deployed REVISION: 2 TEST SUITE: None Logging into Argo CD At this point, we have GitOps and the \u0026#34;App of Apps\u0026#34; deployed. Argo CD comes with a WebUI and a command line tool. The latter must installed to your local environment. In this article, we will use the WebUI.\nTo access the WebUI use the applications menu of the top right corner in Openshift.\nFigure 1. Argo CD: WebUI Link Use the button \u0026#34;Login via OpenShift\u0026#34;.\nFigure 2. Argo CD: Authentication The Argo CD Resources Manager Application The Application of Applications (short App of Apps) is called Argo CD Resources Manager and it is the only Argo CD application that is deployed using the init script. This single Argo CD Application has the sole purpose of deploying other Argo CD objects, such as Applications, ApplicationSets and AppProjects.\nFigure 3. Argo CD: App of Apps It synchronizes everything that is found in the repository in the path: base/argocd-resources-manager (main branch)\nWhenever you would like to create a new Argo CD application(set) it is supposed to be done using this App-of-Apps or to be more exact: in the path mentioned above.\nThe App-of-Apps is the only Argo CD Application (at this moment) that has automatic synchronization enabled. Thus any changes in the App-of-Apps will be propagated automatically as soon as GitOps syncs with Git. The current Applications or ApplicationSets that come with the bootstrap repository are for example:\nDeployment of Advanced Cluster Security (RHACS)\nDeployment of Advanced Cluster Management (RHACM)\nDeployment of basic cluster configuration (i.e. etcd encryption, some UI tweaks …​)\nDeployment of Compliance Operator\nand many more.\nCheck out the deployed Argo CD objects or the openshift-clusterconfig-gitops repository.\nA deep dive into the argocd-resources-manager will be topic of a different episode of this serie.\n"},{"uri":"https://blog.stderr.at/tags/argocd/","title":"ArgoCD","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/gitopscollection/2023-12-28-gitops-repostructure/","title":"GitOps - Choosing the right Git repository structure","tags":["OCP","GitOps","OpenShift","GitOps Approach","Argo CD","ArgoCD"],"description":"Choosing the right Git repository/folder structure","content":" One of the most popular questions asked before adopting the GitOps approach is how to deploy an application to different environments (Test, Dev, Production, etc.) in a safe and repeatable way.\nEach organisation has different requirements, and the choice will depend on a multitude of factors that also include non-technical aspects.\nTherefore, it is important to state: \u0026#34;There is no unique “right” way, there are common practices\u0026#34;.\nIn this series, I will focus on cluster configuration. Git Repository Strategy - Options As written in the introduction: There is no unique “right” way, there are common practices. But how shall the Git repository structure look like? How shall the folder structure look like? Multiple options might be considered. Each has advantages and disadvantages, some I would recommend, some I would not recommend.\nIt is important to understand that:\nThe Git repository structure will depend heavily on how the organisation is laid out.\nThe final repo and directory structure is unique for every organisation, as such the right one will be a discovery process within the organisation and the teams involved in the GitOps engineering process.\nBefore I describe what I usually try to leverage, let’s see the different options.\nEnvironment-per-branch In this case, there is a Git branch for each environment. A “Dev” branch holds the configuration for the DEV environments, a “production” branch for production and so on. This approach is very popular and will be familiar to people who have adopted git flow in the past. However, it is focused on application source code and not environment configuration and is best used when you need to support multiple versions of your application in production. I do not recommend this approach for GitOps, the main reasons are that pull requests and merges will be very complex and promotions between environments are a hurdle. The whole life cycle of a cluster configuration will be very complex.\nEnvironment-per-folder - Monorepo In this case, all environments are in a single Git repository, and all are in the same branch. The filesystem has different folders that hold configuration files for each environment. The configuration of the “DEV” environment is described by a “DEV” folder, the “production” environment is found in a “production” folder and so on.\nFigure 1. GitOps Monorepo Approach This is the approach I usually recommend, especially when someone is new to the whole GitOps workflow and because of the simplicity of setting up such a repository.\nThe following advantages and disadvantages should be considered:\nPros\nProvides a central location for configuration changes.\nThis simplicity enabled straightforward Git workflows that will be centrally visible to the entire organisation, allowing a smoother and clearer approval process and merging.\nBetter suitable for small teams that are managing the cluster and easy to read and understand\nEasy to debug problems.\nCons\nScalability \u0026gt;\u0026gt; Increase complexity \u0026gt;\u0026gt; Management\nPerformance for huge repositories\nChallenging to control access permissions on a single repository\nEnvironment-per-repository - Multirepo In this case, each environment is on its separate git repository. So, the DEV environment is in a git repository called “DEV”, the “production” environment is in a “production” git repository and so on. The GitOps agent (OpenShift GitOps) connects to multiple repositories and takes care to apply the correct configuration to the correct target cluster.\nFigure 2. GitOps Multirepo Approach Like the Monorepo approach, Multirepo comes with some advantages and disadvantages:\nPros\nAllows separating concerns between different departments of organisations (a repository for the security team, a repository for the operations team, etc.)\nCons\nMore complex to manage\nHarder to understand and read the configuration (what is coming from where)\nArgo CD Application dependencies might not be solved (i.e., Security tries to manage the same object as the operating team. Who is the leader?)\nExample Setup The Approach I choose I usually use and recommend Monorepo approach. The environment-per-folder approach is a very good way to organise your GitOps applications. Not only is it very simple to implement and maintain, but it is also the optimal method for promoting releases between different GitOps environments. This approach can also work for any number of environments without any additional effort. Cluster configurations (for multiple clusters) are typically done by one team, therefore controlling access permissions in Git is not a big issue.\nExample Folder Structure Over time the following folder structure evolved or my repository:\n├── base (1) │ ├── argocd-resources-manager (2) │ └── init_app_of_apps (3) ├── charts (4) ├── clusters (5) │ ├── all (6) │ │ ├── base-operators │ │ ├── etcd-encryption │ ├── management-cluster (7) │ │ ├── branding │ │ ├── generic-cluster-config │ │ ├── management-gitops │ │ ├── node-labels │ │ ├── openshift-data-foundation │ │ ├── setup-acm │ │ ├── setup-acs │ │ ├── setup-compliance-oeprator │ │ ├── setup-openshift-logging │ │ └── setup-quay │ └── production-cluster (8) │ │ ├── branding │ │ ├── generic-cluster-config │ │ ├── node-labels │ │ ├── openshift-data-foundation │ │ ├── setup-acs │ │ ├── setup-compliance-oeprator │ │ └── setup-openshift-logging ├── init_GitOps.sh (9) ├── scripts (10) │ ├── example_htpasswd │ ├── sealed_secrets ├── tenant-projects (11) ├── my-main-app └── my-second-app 1 The base folder contains basic configurations or Argo CD itself. 2 The argocd-resources-manager is a Helm Chart that configures Applications and ApplicationSets or Argo CD using a single configuration file. 3 The init_app_of_apps is used during the initial installation of OpenShift GitOps and installs the App-of-Apps that manages other Applications or Argo CD. This Application automatically synchronises and watches for changes in the folder argocd-resources-manager. 4 The charts folder is optional and can store local Helm Charts. Usually, it is better to release the Charts in a Helm repository, where they can be managed independently to the cluster configuration repository. 5 The folder for the different clusters. 6 Configurations that are equal for all clusters and simple to achieve without any deeper configuration. Currently, for example, the activation of the etcd encryption and the deployment of base Operators that every cluster will require. In this case, the Operators are installed only, without further configuration. 7 Configuration for the management-cluster. For example, deploying ACM, ACS, Quay or any generic cluster configuration. Here we see immediately what is deployed and where I can modify the configuration for that cluster. 8 Configuration for the production-clusters 9 The deployment script to install and configure the OpenShift GitOps Operator. This might be replaced or at least modified in the future once PlatformOperators are generally available and not in a technology preview state anymore. 10 The scripts folder simply contains some shell scripts that might be useful. For example, to backup a Sealed Secrets key or generate a htpasswd file. 11 The tenant-projects folder is a special folder to store the configuration or projects. Any project onboarding is configured here, such as Quota, LimitRanges, NetworkPolicies etc. Why the repeating folders? Some may argue why certain folders are equal for management and production clusters, for example, \u0026#34;setup-compliance-operator\u0026#34;, when this could be done more easily by defining such folder only once and using different overlays (using Kustomize) or different values-files (using Helm Charts). However, while this is a very valid question, I personally, like to see immediately what is configured on each cluster. I see, based on the folders, what is configured on the management cluster and where I could modify the configuration.\nUsing Kustomize overlays, for example, would mean recreating the overlays for each configuration (if you want to have a clean separation and not combine all manifests into one overlay). Using different values-files is again a valid option, but (also again), you do not see what is configured on which cluster with one look.\nTherefore, I like this folder structure, even if it may look weird (especially if you are used to Kustomize overlays). However, everyone is invited to define their very own structure :)\nManaging Kubernetes Manifests The Kubernetes manifests (the yaml files) must be managed in a way Argo CD can read and synchronise them.\nThree main options are commonly used:\nHelm: Helm uses a packaging format called charts. A chart is a collection of files that describe a related set of Kubernetes resources.\nKustomize: A Template-free way to customise application configuration that simplifies the use of off-the-shelf applications.\nPlain Text: Plain text Kubernetes objects provided in YAML of JSON format.\nArgo CD also understands jsonnet or even custom plugins. However, I had no customer up until now, who wanted to use something else than Kustomize or Helm. The different tools are not explained in detail in this article, but the choice of the tool highly depends on the existing knowledge and individual preferences inside the company. Every option has advantages and disadvantages that will become visible when they are used.\nI have seen companies tend to use Helm Charts or Plain Text, especially when they are new to the tools. However, no tool is better than the other. Instead, the tools can be combined which might be useful for some use cases.\nKustomize and Helm do not exclude each other and can be combined. However, for the start, a single tool should be selected. "},{"uri":"https://blog.stderr.at/tags/gitops-approach/","title":"GitOps Approach","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/gitopscollection/2023-12-11-gitops-intro/","title":"Introducing the GitOps Approach","tags":["OCP","GitOps","OpenShift","GitOps Approach","Argo CD","ArgoCD"],"description":"Introduction to the GitOps Approach","content":" When managing one or more clusters, the question arises as to how cluster configurations and applications can be installed securely, regularly, and in the same way. This is where the so-called GitOps approach helps, according to the mantra: \u0026#34;If it is not in Git, it does not exist\u0026#34;.\nThe idea is to have Git as the only source of truth on what happens inside the environment. While there are many articles about how to get GitOps into the deployment process of applications, this series of articles tries to set the focus on the cluster configuration and tasks system administrators usually have to do, for example: Setup an Operator.\nThe GitOps Approach This series includes the following articles:\nChoosing the right Git repository structure\nInstall GitOps to the cluster\nConfigure App-of-Apps\nSetup Compliance Operator\nIn this series, I will focus on cluster configuration. The GitOps approach is a very common practice and the-facto \u0026#34;standard\u0026#34; as of today.\nWhen I write standard, then be assured, that the approach itself should be followed, but HOW this is done can be a topic of many tough discussions. But what is it and why should a company invest time to follow this approach?\n_GitOps is a declarative way to implement continuous deployment for cloud-native applications. It should be a repeatable process to manage multiple clusters.\nGitOps adds the following features to company processes:\nEverything as code: The entire state of the application, infrastructure and configuration is declaratively defined as code.\nGit and the single source of truth: Every setting and every manifest is stored and versioned in Git. Any change must first be saved to Git.\nOperations via Git workflows: Standard Git procedures, such as pull or merge requests, should be used to track any changes to the applications or cluster configurations.\nIt is important that not only the manifests of the applications but also the cluster configuration is stored in Git. The goal should be to ensure that no manual changes are made directly to the cluster.\nBenefits/Challenges Deploying new versions of applications or cluster configurations with a high degree of confidence is a desirable goal as getting features reliably to production is one of the most important characteristics of fast-moving organisations. GitOps is a set of common practices where the entire code delivery process is controlled via Git, including infrastructure and application definition as code and automation to complete updates and rollbacks.\nGitOps constantly watches for changes in Git repositories and compares them with the current state of the cluster. If there is a drift it will either automatically synchronise to the wanted state or warn accordingly (manual sync must then be performed).\nThe key GitOps advantages are:\nCluster and application configuration versioned in Git\nVisualisation of desired system state\nAutomatically syncs configuration from Git to clusters (if enabled)\nDrift detection, visualisation, and correction\nRollback and roll-forward to any Git commit.\nManifest templating support (Helm, Kustomize, etc.)\nVisual insight into sync status and history.\nRole-Based access support\nPipeline integration\nAdopting GitOps has enormous benefits but does pose some challenges. Many teams will have to adjust their culture and way of working to support using Git as the single source of truth. Strictly adhering to GitOps processes will mean all changes will be committed. This may present a challenge when it comes to debugging a live environment. There may be times when that is necessary and will require suspending GitOps in some way.\nSome other prerequisites for adopting GitOps include\nGood testing and CI processes are in place.\nA strategy for dealing with promotions between environments.\nStrategy for Secrets management.\nUsed Tools The following list of tools (or specifications) are used for our GitOps Approach.\nOpenShift GitOps\nHelm\nUsed Repositories The following two Git repositories are used throughout the series:\nOpenShift Configuration\nHelm Repository\n"},{"uri":"https://blog.stderr.at/openshift/2023/11/running-falco-on-openshift-4.12/","title":"Running Falco on OpenShift 4.12","tags":[],"description":"","content":" As mentioned in our previous post about Falco, Falco is a security tool to monitor kernel events like system calls or Kubernetes audit logs to provide real-time alerts.\nIn this post I\u0026#39;ll show to customize Falco for a specific use case. We would like to monitor the following events:\nAn interactive shell is opened in a container Log all commands executed in an interactive shell in a container Log read and writes to files within an interactive shell inside a container Log commands execute via `kubectl/oc exec` which leverage the pod/exec K8s endpoint The rules we created for those kind of events are available here.\nDeploying custom rules and disabling the default ruleset Falco comes with a quite elaborate ruleset for creating security relevant events. But for our use case we just want to deploy a specific set of rules (see the list above).\nAs we are deploying Falco via Helm, we use the following values for `rules_files`:\nfalco: rules_file: - /etc/falco/extra-rules.d - /etc/falco/rules.d This instructs Falco to only load rules from the directories mentioned above.\nWe use a Kustomize configMapGenerator to create a Kubernetes ConfigMap from our custom rules file:\nconfigMapGenerator: - name: falco-extra-rules options: disableNameSuffixHash: true files: - falco-extra-rules.yaml The complete Kustomize configuration is here.\nFurthermore we instruct Falco to mount our custom rule ConfigMap created above in the Helm values file:\nmounts: volumes: - name: falco-extra-rules-volume optional: true configMap: name: falco-extra-rules volumeMounts: - mountPath: /etc/falco/extra-rules.d name: falco-extra-rules-volume The complete Helm values files is available here.\nDisable automatic rule updates Falco updates all rules when it starts (via an initContainer) and also updates those rules on a regular basis. We would also like to disable this behavior:\nfalcoctl: artifact: install: enabled: false follow: enabled: false Create events for kubectl/oc exec One problem problem is monitoring pod exec events. Using Falcos eBPF monitoring capabilities we found no way to limit those events to pod exec\u0026#39;s. This might be because the Falco rule language is new to us and maybe there is a way to use eBPF filtering. Just let us know if you find a solution!\nBut we came up with a different way of capturing pod/exec events:\nFalco also allows monitoring Kubernetes audit events, logged by the kube-apiserver. Every time you hit the pod/exec endpoint, K8s logs the following event in the audit log:\n{\u0026#34;kind\u0026#34;:\u0026#34;Event\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;audit.k8s.io/v1\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;Metadata\u0026#34;,\u0026#34;auditID\u0026#34;:\u0026#34;5c19c1d0-00a7-4af5-a236-5345b5963581\u0026#34;,\u0026#34;stage\u0026#34;:\u0026#34;ResponseComplete\u0026#34;,\u0026#34;requestURI\u0026#34;:\u0026#34;/api/v1/namespaces/falco/pods/falco-8mqj7/exec?command=cat\\u0026command=%2Fetc%2Ffalco%2Fextra-rules.d%2Ffalco-extra-rules.yaml\\u0026container=falco\\u0026stderr=true\\u0026stdout=true\u0026#34;,\u0026#34;verb\u0026#34;:\u0026#34;create\u0026#34;,\u0026#34;user\u0026#34;:{\u0026#34;username\u0026#34;:\u0026#34;root\u0026#34;,\u0026#34;uid\u0026#34;:\u0026#34;d82ec74a-75e3-4798-a084-4b766dcea5ef\u0026#34;,\u0026#34;groups\u0026#34;:[\u0026#34;cluster-admins\u0026#34;,\u0026#34;system:authenticated:oauth\u0026#34;,\u0026#34;system:authenticated\u0026#34;],\u0026#34;extra\u0026#34;:{\u0026#34;scopes.authorization.openshift.io\u0026#34;:[\u0026#34;user:full\u0026#34;]}},\u0026#34;sourceIPs\u0026#34;:[\u0026#34;10.0.32.220\u0026#34;],\u0026#34;userAgent\u0026#34;:\u0026#34;oc/4.13.0 (linux/amd64) kubernetes/92b1a3d\u0026#34;,\u0026#34;objectRef\u0026#34;:{\u0026#34;resource\u0026#34;:\u0026#34;pods\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;falco\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;falco-8mqj7\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;subresource\u0026#34;:\u0026#34;exec\u0026#34;},\u0026#34;responseStatus\u0026#34;:{\u0026#34;metadata\u0026#34;:{},\u0026#34;code\u0026#34;:101},\u0026#34;requestReceivedTimestamp\u0026#34;:\u0026#34;2023-11-13T17:23:16.999602Z\u0026#34;,\u0026#34;stageTimestamp\u0026#34;:\u0026#34;2023-11-13T17:23:17.231121Z\u0026#34;,\u0026#34;annotations\u0026#34;:{\u0026#34;authorization.k8s.io/decision\u0026#34;:\u0026#34;allow\u0026#34;,\u0026#34;authorization.k8s.io/reason\u0026#34;:\u0026#34;RBAC: allowed by ClusterRoleBinding \\\u0026#34;root-cluster-admin\\\u0026#34; of ClusterRole \\\u0026#34;cluster-admin\\\u0026#34; to User \\\u0026#34;root\\\u0026#34;\u0026#34;}} As you can hopefully see, the command executed is available in the requestURI field.\nSo we enabled the k8saudit Falco plugin and created an additional rule for those kind of events.\nfalco: plugins: - name: k8saudit library_path: libk8saudit.so init_config: # maxEventSize: 262144 # webhookMaxBatchSize: 12582912 # sslCertificate: /etc/falco/falco.pem # open_params: \u0026#34;http://:9765/k8s-audit\u0026#34; open_params: \u0026#34;/host/var/log/kube-apiserver/audit.log\u0026#34; - name: json library_path: libjson.so init_config: \u0026#34;\u0026#34; Implementing event routing We had an additional requirement to route events based on the following rules:\nEvents that do not contain sensitive data (like usernames) should go to a specific Kafka topic Events that do contain sensitive data (like usernames) should be routed to another Kafka topic Our first thought was to leverage Falcosidekick\u0026#39;s minimumpriority field for routing. Events with sensitive data would get a higher priority. But the sink with a lower minimumpriority would get events with higher priority as well, which means events with sensitive data.\nFurthermore as far as we know Falco currently only supports one Kafka configuration (we need two for two topics).\nAt this point in time we are not aware of a possibility to implement this with Falco or Falcosidekick directly.\nThere are some discussions upstream on implementing such a feature:\nhttps://github.com/falcosecurity/falcosidekick/issues/161 https://github.com/falcosecurity/falcosidekick/issues/161#issuecomment-747714289 https://github.com/falcosecurity/falcosidekick/issues/224 Our current idea is to use Vector for event routing. We will try to implement the following pipeline:\nTips and Tricks Monitor Redis disk usage One small hint when using falcosidekick-ui to debug/monitor events. It happened to us that the Redis volume was full and suddenly we couldn\u0026#39;t see new events in the UI.\nWe stopped the UI and Redis pods, removed the PVC and just ran our kustomization again, to recreate the PVC and the pods.\nMonitor falco pod logs when changing rules It\u0026#39;s always wise to monitor one Falco pod for errors when deploying new rules, for example at one point we hit the following error:\n{\u0026#34;hostname\u0026#34;:\u0026#34;falco-2hlkm\u0026#34;,\u0026#34;output\u0026#34;:\u0026#34;Falco internal: hot restart failure: /etc/falco/extra-rules.d/falco-extra-rules.yaml: Invalid\\n1 Errors:\\nIn rules content: (/etc/falco/extra-rules.d/falco-extra-rules.yaml:0:0)\\n rule \u0026#39;Terminal shell in container\u0026#39;: (/etc/falco/extra-rules.d/falco-extra-rules.yaml:25:2)\\n condition expression: (\\\u0026#34;spawned_process a...\\\u0026#34;:26:71)\\n------\\n...ocess and container and shell_procs and proc.tty != 0 and container_entrypoint\\n ^\\n------\\nLOAD_ERR_VALIDATE (Error validating rule/macro/list/exception objects): Undefined macro \u0026#39;container_entrypoint\u0026#39; used in filter.\\n\u0026#34;,\u0026#34;output_fields\u0026#34;:{},\u0026#34;priority\u0026#34;:\u0026#34;Critical\u0026#34;,\u0026#34;rule\u0026#34;:\u0026#34;Falco internal: hot restart failure\u0026#34;,\u0026#34;source\u0026#34;:\u0026#34;internal\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2023-11-13T11:47:14.639547735Z\u0026#34;} Falco is quite resilient when it comes to errors in rules files and provides useful hints on what might be wrong:\nUndefined macro \u0026#39;container_entrypoint\u0026#39; used in filter So we just added the missing macro and all was swell again.\n"},{"uri":"https://blog.stderr.at/tags/quay/","title":"Quay","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2023/11/quay-deployment-and-configuration-using-gitops/","title":"Quay Deployment and Configuration using GitOps","tags":["OpenShift","OCP","GitOps","Argo CD","Quay"],"description":"Using GitOps approach to install and configure Quay Enterprise","content":" Installing and configuring Quay Enterprise using a GitOps approach is not as easy as it sounds. On the one hand, the operator is deployed easily, on the other hand, the configuration of Quay is quite tough to do in a declarative way and syntax rules must be strictly followed.\nIn this article, I am trying to explain how I solved this issue by using a Kubernetes Job and a Helm Chart.\nWhat about Quay configuration? Quay Enterprise is using a (quite big) Secrets object that defines tons of settings for the registry. The syntax must strictly be followed. For example, a Boolean must be true or false. Quay ignores if a string like \u0026#34;true\u0026#34; or \u0026#34;false\u0026#34; is provided.\nThis alone is already a hassle since working with Booleans in Helm is not as easy as you might think.\nThe Secret combines non-sensitive data (like DEFAULT_TAG_EXPIRATION) with sensitive data (like settings for the Object Store)\nIf there is any error in the configuration file, or if the Operator is configured to manage a specific component, but finds settings for this component in the Secret, the deployment will fail.\nThe solution? I thought for a long time about how to solve this issue. One solution might be to create the whole Secret upfront and simply provide it during the deployment. This works and I have done this previously, but I wanted to generate the Secret during the deployment.\nTherefore, I am now trying to create a ConfigMap that holds a complete skeleton of the required Secret. This ConfigMap is used by a Kubernetes Job, which reads the required sensitive information out of other existing Secrets (such as S3 information) and generates a quay-secret by replacing the required fields.\nIs this the perfect and optimal way to do that? Probably not, however, it works :)\nLet us see that in action Prerequisites First, we have some prerequisites.\nQuay is very …​ very hungry for resources. Quay application pods require 8 CPU and 16GB Memory per pod and per default…​ and it tries to spin up 2 pods. The same goes for Clair and so on. Therefore, I will configure Quay to only use 1 replica for these services.\nQuay requires node roles infra. I am not sure if this is new or if I never saw that, but the nodeSelector, which it seems you cannot configure, is looking for the label infra. Therefore, the nodes that should host Quay must have the label: node-role.kubernetes.io/infra: \u0026#39;\u0026#39;\nCurrently, I am using a BucketClaim object to create an S3-bucket and once created I read the required information of the bucket to replace the settings in the generated Quay configuration accordingly. The BucketClaim object comes from the OpenShift Data Foundation. I installed the Multicloud Object Gateway only, which allows me to provide Object Storage (S3). (Very useful to test other solutions too, like OpenShift Logging or Network Observability)\nMeeting these requirements, allows us to deploy Quay. But first some theory.\nDeployment Workflow The workflow of the deployment is as the following image demonstrates\nFigure 1. Argo CD: Syncwaves Everything starts with the Helm Chart setup-quay. This Chart itself provides the logic to create an S3-bucket, a Job to generate the configuration and the creation of a Secret, that provides the initial administrator credential.\nsetup-quay has multiple dependencies to other Helm charts, that can be found at my Helm repository:\nquay-registry-setup\nhelper-operator\nhelper-status-checker\nConfiguration All values for the Helm Charts can be found in the values file\nSince sub-charts are used the file is divided into 4 blocks:\nhelper-operator: Here the sub-chart helper-operator is configured. All settings here are bypassed to the subchart. It defines the required settings to deploy the operator.\nhelper-status-checker: Here settings for the status-checker Job are defined.\nquay-registry-setup: This defines the actual configuration of the QuayEnterprise object. It will spin up the Quay instance. Some components are set to \u0026#34;false\u0026#34; or \u0026#34;replica == 1\u0026#34; to minimize the required resources in my lab. For a production environment, additional replicas or components might be required.\nquay: These are the actual values for the configuration. It defines the bucketClaim as well as settings for the Quay configuration that might be overwritten.\nDeploy it Using for example the following Argo CD Application we can deploy everything.\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: in-cluster-setup-quay namespace: openshift-gitops spec: destination: name: in-cluster (1) namespace: default ignoreDifferences: (2) - jsonPointers: - /data/password kind: Secret name: init-user namespace: quay-enterprise info: - name: Description value: ApplicationSet that Deploys on Management Cluster (Matrix Generator) project: in-cluster source: path: clusters/management-cluster/setup-quay (3) repoURL: \u0026#39;https://github.com/tjungbauer/openshift-clusterconfig-gitops\u0026#39; targetRevision: main 1 The destination cluster. Here the \u0026#34;in-cluster\u0026#34; means the local cluster of Argo CD. 2 The initial credential for Quay is being generated and would change if the Argo CD application gets refreshed and therefore it would be out of sync. So, we are ignoring differences in the password field. 3 The source of the Helm Chart. In Argo CD this Application will look like\nFigure 2. Quay in Argo CD Deployment means in GitOps approach: synchronizing the Argo CD Application.\nThis will install the Operator and spin up all required Pods and Jobs. It will take several minutes until everything is up and running. During the deployment, some Pods may fail and will get restarted automatically. This happens because they are dependent on the Postgres DB which must be started first.\nQuay is Alive Congratulations, you have now a Quay instance. Use the auto-generated credentials, that are stored in the Secret init-user to authenticate.\nFigure 3. Quay Login Is that All - Kind of Summary? Several configurations are done here now. However, there are tons to follow. For example, log forwarding or additional certificates. Some settings will contain sensitive information some will not. All these settings can be added to the ConfigMap skeleton and be replaced accordingly with \u0026#34;little\u0026#34; effort. For me, it is simply not possible to test every setting and possibility. Maybe I will extend the Helm Chart during the journey. If you find this useful, feel free to re-use it and of course, if you find any issues feel free to create a GitHub issue.\n"},{"uri":"https://blog.stderr.at/openshift/2023/10/setting-up-falco-on-openshift-4.12/","title":"Setting up Falco on OpenShift 4.12","tags":[],"description":"","content":" Falco is a security tool to monitor kernel events like system calls to provide real-time alerts. In this post I\u0026#39;ll document the steps taken to get Open Source Falco running on an OpenShift 4.12 cluster.\nUPDATE: Use the falco-driver-loader-legacy image for OpenShift 4.12 deployments.\nFirst Try We will use the Falco Helm chart version 3.8.0 for our first try of setting up Falco on our OpenShift cluster.\nThis is our values file:\ndriver: kind: ebpf falco: json_output: true json_include_output_property: true log_syslog: false log_level: info falcosidekick: enabled: true webui: enabled: true We would like to use the eBPF driver to monitor kernel events, enable falco sidekick, which is used to route events and the falco sidekick UI for easier testing.\nBecause of reasons we leverage kustomize to render the helm chart. The final kustomize config is here (after fixing all problems mentioned below).\nSo after deploying the chart via ArgoCD (another story), we have the following pods:\n$ oc get pods -n falco NAME READY STATUS RESTARTS AGE falco-4cc8j 0/2 Init:CrashLoopBackOff 5 (95s ago) 4m31s falco-bx87j 0/2 Init:CrashLoopBackOff 5 (75s ago) 4m29s falco-ds9w6 0/2 Init:CrashLoopBackOff 5 (99s ago) 4m30s falco-falcosidekick-ui-redis-0 1/1 Running 0 4m28s falco-gxznz 0/2 Init:CrashLoopBackOff 3 (14s ago) 4m30s falco-vtnk5 0/2 Init:CrashLoopBackOff 5 (98s ago) 4m29s falco-wbn2k 0/2 Init:CrashLoopBackOff 5 (80s ago) 4m29s hm, not so good. Seems like some initContainers are failing.\nLet\u0026#39;s check the falco-driver-loader initContainer:\n* Setting up /usr/src links from host * Running falco-driver-loader for: falco version=0.36.1, driver version=6.0.1+driver, arch=x86_64, kernel release=4.18.0-372.73.1.el8_6.x86_64, kernel version=1 * Running falco-driver-loader with: driver=bpf, compile=yes, download=yes * Mounting debugfs mount: /sys/kernel/debug: permission denied. dmesg(1) may have more information after failed mount system call. * Filename \u0026#39;falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o\u0026#39; is composed of: - driver name: falco - target identifier: rhcos - kernel release: 4.18.0-372.73.1.el8_6.x86_64 - kernel version: 1 * Trying to download a prebuilt eBPF probe from https://download.falco.org/driver/6.0.1%2Bdriver/x86_64/falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o curl: (22) The requested URL returned error: 404 Unable to find a prebuilt falco eBPF probe * Trying to compile the eBPF probe (falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o) expr: syntax error: unexpected argument \u0026#39;1\u0026#39; make[1]: *** /lib/modules/4.18.0-372.73.1.el8_6.x86_64/build: No such file or directory. Stop. make: *** [Makefile:38: all] Error 2 mv: cannot stat \u0026#39;/usr/src/falco-6.0.1+driver/bpf/probe.o\u0026#39;: No such file or directory Unable to load the falco eBPF probe Seems to be an issue with a missing directory /lib/modules/4.18.0-372.73.1.el8_6.x86_64/build.\nAnd we told the helm chart to enable falco-sidekick and falco-sidekick-ui, but where are they?\nLet\u0026#39;s check the events with oc get events as well, and what do we see?\n8s Warning FailedCreate replicaset/falco-falcosidekick-7cfbbbf89f Error creating: pods \u0026#34;falco-falcosidekick-7cfbbbf89f-\u0026#34; is forbidden: unable to validate against any security context constraint: [provider \u0026#34;anyuid\u0026#34;: Forbidden: not usable by user or serviceaccount, provider restricted-v2: .spec.securityContext.fsGroup: Invalid value: []int64{1234}: 1234 is not an allowed group, provider restricted-v2: .containers[0].runAsUser: Invalid value: 1234: must be in the ranges: [1000730000, 1000739999], provider \u0026#34;restricted\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;nonroot-v2\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;nonroot\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;hostmount-anyuid\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;machine-api-termination-handler\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;hostnetwork-v2\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;hostnetwork\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;hostaccess\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;falco\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;node-exporter\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;privileged\u0026#34;: Forbidden: not usable by user or serviceaccount] 6s Warning FailedCreate replicaset/falco-falcosidekick-ui-76885bd484 Error creating: pods \u0026#34;falco-falcosidekick-ui-76885bd484-\u0026#34; is forbidden: unable to validate against any security context constraint: [provider \u0026#34;anyuid\u0026#34;: Forbidden: not usable by user or serviceaccount, provider restricted-v2: .spec.securityContext.fsGroup: Invalid value: []int64{1234}: 1234 is not an allowed group, provider restricted-v2: .containers[0].runAsUser: Invalid value: 1234: must be in the ranges: [1000730000, 1000739999], provider \u0026#34;restricted\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;nonroot-v2\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;nonroot\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;hostmount-anyuid\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;machine-api-termination-handler\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;hostnetwork-v2\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;hostnetwork\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;hostaccess\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;falco\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;node-exporter\u0026#34;: Forbidden: not usable by user or serviceaccount, provider \u0026#34;privileged\u0026#34;: Forbidden: not usable by user or serviceaccount] Looks like a problem with OpenShifts Security Context constraints (SCC\u0026#39;s).\nSummary of problems The falco DaemonSet fails to start pods because there is an issue with a missing directory Falco Sidekick and Falco Sidekick UI fails to start because of Security Context Constraint (SCC) issues Fixing the Falco daemonset Falco tries to download a pre-compiled eBPF probe, fails and then tries to compile that probe for our host OS kernel. This fails with the message:\nmake[1]: *** /lib/modules/4.18.0-372.73.1.el8_6.x86_64/build: No such file or directory. Stop. As far as we know there are no kernel sources installed on RHCOS nodes in OpenShift. After a little bit of searching the interweb we found the following issue comment on Github:\nOpenShift under vsphere: Download failed, consider compiling your own falco module and loading it or getting in touch with the Falco community\nSo we need to enable the kernel-devel extension, the official docs are here. It does not mention kernel-devel, but there\u0026#39;s a knowledge base article mentioning kernel-devel, so let\u0026#39;s give it a try.\nWe deploy two MachineConfigs, one for worker and one for master nodes to rollout the extension, the worker configuration looks like this:\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: worker name: 99-worker-kernel-devel-extensions spec: extensions: - kernel-devel See also our Kustomize configuration here.\nAs soon as we apply our MachineConfigs, OpenShift starts the rollout via MaschineConfigPool\u0026#39;s:\n$ oc get mcp NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE master rendered-master-ce464ff45cc049fce3e8a63e36a4ee9e False True False 3 0 0 0 13d worker rendered-worker-a0f8f0d915ef01ba4a1ab3047b6c863d False True False 3 0 0 0 13d When the rollout is done, let\u0026#39;s restart all Falco DaemonSet pods:\n$ oc delete pods -l app.kubernetes.io/name=falco And check the status:\n$ oc get pods NAME READY STATUS RESTARTS AGE falco-5wfnk 0/2 Init:Error 1 (3s ago) 7s falco-66fxw 0/2 Init:0/2 1 (2s ago) 6s falco-6fbc7 0/2 Init:CrashLoopBackOff 1 (2s ago) 8s falco-8h8n4 0/2 Init:0/2 1 (2s ago) 6s falco-falcosidekick-ui-redis-0 1/1 Running 0 18m falco-nhld2 0/2 Init:CrashLoopBackOff 1 (2s ago) 6s falco-xqv4b 0/2 Init:CrashLoopBackOff 1 (3s ago) 8s still, the initContainers fail. Lets check the log again\n$ oc logs -c falco-driver-loader falco-5wfnk * Setting up /usr/src links from host * Running falco-driver-loader for: falco version=0.36.1, driver version=6.0.1+driver, arch=x86_64, kernel release=4.18.0-372.73.1.el8_6.x86_64, kernel version=1 * Running falco-driver-loader with: driver=bpf, compile=yes, download=yes * Mounting debugfs mount: /sys/kernel/debug: permission denied. dmesg(1) may have more information after failed mount system call. * Filename \u0026#39;falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o\u0026#39; is composed of: - driver name: falco - target identifier: rhcos - kernel release: 4.18.0-372.73.1.el8_6.x86_64 - kernel version: 1 * Trying to download a prebuilt eBPF probe from https://download.falco.org/driver/6.0.1%2Bdriver/x86_64/falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o curl: (22) The requested URL returned error: 404 Unable to find a prebuilt falco eBPF probe * Trying to compile the eBPF probe (falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o) Makefile:1005: *** \u0026#34;Cannot generate ORC metadata for CONFIG_UNWINDER_ORC=y, please install libelf-dev, libelf-devel or elfutils-libelf-devel\u0026#34;. Stop. make: *** [Makefile:38: all] Error 2 mv: cannot stat \u0026#39;/usr/src/falco-6.0.1+driver/bpf/probe.o\u0026#39;: No such file or directory Unable to load the falco eBPF probe So this time we get another error, the culprit is the following line\nMakefile:1005: *** \u0026#34;Cannot generate ORC metadata for CONFIG_UNWINDER_ORC=y, please install libelf-dev, libelf-devel or elfutils-libelf-devel\u0026#34;. Stop. Back to searching the interweb only reveals an old issue, that should be fixed already.\nSo as a quick hack we modified the falco-driver-loader image to contain libelf-dev and pushed to image to quay.\nWe then modified our falco helm configuration to use the updated image:\ndriver: kind: ebpf loader: initContainer: image: registry: quay.io repository: tosmi/falco-driver-loader tag: 0.36.1-libelf-dev falco: json_output: true json_include_output_property: true log_syslog: false log_level: info falcosidekick: enabled: true webui: enabled: true Note the updated diver.loader.initContainer section.\nLet\u0026#39;s check the our pods again:\n$ oc get pods NAME READY STATUS RESTARTS AGE falco-2ssgx 2/2 Running 0 66s falco-5hqgg 1/2 Running 0 66s falco-82kq9 2/2 Running 0 65s falco-99zxw 2/2 Running 0 65s falco-falcosidekick-test-connection 0/1 Error 0 67s falco-falcosidekick-ui-redis-0 1/1 Running 0 31m falco-slx5k 2/2 Running 0 65s falco-tzm8d 2/2 Running 0 65s Success! This time the DaemonSet pods started successfully. Just note that you have to be patient. The first start took about 1-2 minutes to complete.\nLet\u0026#39;s check the logs of one DaemonSet pod just to sure:\noc logs -c falco-driver-loader falco-2ssgx * Setting up /usr/src links from host * Running falco-driver-loader for: falco version=0.36.1, driver version=6.0.1+driver, arch=x86_64, kernel release=4.18.0-372.73.1.el8_6.x86_64, kernel version=1 * Running falco-driver-loader with: driver=bpf, compile=yes, download=yes * Mounting debugfs mount: /sys/kernel/debug: permission denied. dmesg(1) may have more information after failed mount system call. * Filename \u0026#39;falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o\u0026#39; is composed of: - driver name: falco - target identifier: rhcos - kernel release: 4.18.0-372.73.1.el8_6.x86_64 - kernel version: 1 * Trying to download a prebuilt eBPF probe from https://download.falco.org/driver/6.0.1%2Bdriver/x86_64/falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o curl: (22) The requested URL returned error: 404 Unable to find a prebuilt falco eBPF probe * Trying to compile the eBPF probe (falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o) * eBPF probe located in /root/.falco/6.0.1+driver/x86_64/falco_rhcos_4.18.0-372.73.1.el8_6.x86_64_1.o * Success: eBPF probe symlinked to /root/.falco/falco-bpf.o Especially the line\n* Success: eBPF probe symlinked to /root/.falco/falco-bpf.o looks promising. So up to the next problem, getting falco-sidekick and falco-sidekick-ui running.\nWe also opened a bug report upstream to get feedback from the developers on this issue.\nUPDATE Andreagit97 was so nice mentioning in the issue above that actually there is an image with libelf-dev available, falco-driver-loader-legacy. We can confirm that this image fixes the problem mentioned above.\nSo this is our final falco helm chart values.yaml:\ndriver: kind: ebpf loader: initContainer: image: repository: falcosecurity/falco-driver-loader-legacy falco: json_output: true json_include_output_property: true log_syslog: false log_level: info falcosidekick: enabled: true webui: enabled: true Fixing falco-sidekick and falco-sidekick-ui Remember pod startup actually failed because of the following event (check with oc get events):\n.spec.securityContext.fsGroup: Invalid value: []int64{1234}: 1234 is not an allowed group It seems the sidekick pods want to run with a specific UID. The default OpenShift Security Context Constraint (SCC) restricted prohibits this.\nLets confirm our suspicion:\n$ oc get deploy -o jsonpath=\u0026#39;{.spec.template.spec.securityContext}{\u0026#34;\\n\u0026#34;}\u0026#39; falco-falcosidekick {\u0026#34;fsGroup\u0026#34;:1234,\u0026#34;runAsUser\u0026#34;:1234} $ oc get deploy -o jsonpath=\u0026#39;{.spec.template.spec.securityContext}{\u0026#34;\\n\u0026#34;}\u0026#39; falco-falcosidekick-ui {\u0026#34;fsGroup\u0026#34;:1234,\u0026#34;runAsUser\u0026#34;:1234} Bingo! securityContext is set to 1234 for both deployments. There is another SCC that we could leverage, nonroot, which basically allows any UID expect 0. We just need to get the ServiceAccount that falco-sidekick and falco-sidekick-ui are actually using:\n$ oc get deploy -o jsonpath=\u0026#39;{.spec.template.spec.serviceAccount}{\u0026#34;\\n\u0026#34;}\u0026#39; falco-falcosidekick falco-falcosidekick $ oc get deploy -o jsonpath=\u0026#39;{.spec.template.spec.serviceAccount}{\u0026#34;\\n\u0026#34;}\u0026#39; falco-falcosidekick-ui falco-falcosidekick-ui So falco-sidekick uses falco-sidekick as ServiceAccount and falco-sidekick-ui falco-sidekick-ui. Lets grant both ServiceAccounts access to the nonroot SCC.\nkind: ClusterRoleBinding metadata: name: falco-falcosidekick-scc:nonroot roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:openshift:scc:nonroot subjects: - kind: ServiceAccount name: falco-falcosidekick namespace: falco - kind: ServiceAccount name: falco-falcosidekick-ui namespace: falco We\u0026#39;ve already added this file to our Kustomize configuration.\nLet\u0026#39;s trigger a redeployment by deleting the ReplicaSets of both deployments, they will be re-created automatically:\n$ oc delete rs -l app.kubernetes.io/name=falcosidekick $ oc delete rs -l app.kubernetes.io/name=falcosidekick-ui Finally let\u0026#39;s confirm everything is up and running:\n$ oc get deploy,ds,pods NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/falco-falcosidekick 2/2 2 2 5d deployment.apps/falco-falcosidekick-ui 2/2 2 2 5d NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/falco 6 6 6 6 6 \u0026lt;none\u0026gt; 6d2h NAME READY STATUS RESTARTS AGE pod/falco-2ssgx 2/2 Running 0 21m pod/falco-5hqgg 2/2 Running 0 21m pod/falco-82kq9 2/2 Running 0 21m pod/falco-99zxw 2/2 Running 0 21m pod/falco-falcosidekick-7cfbbbf89f-qxwxs 1/1 Running 0 118s pod/falco-falcosidekick-7cfbbbf89f-rz5lj 1/1 Running 0 118s pod/falco-falcosidekick-ui-76885bd484-p7lqm 1/1 Running 0 2m18s pod/falco-falcosidekick-ui-76885bd484-sfgh4 1/1 Running 0 2m18s pod/falco-falcosidekick-ui-redis-0 1/1 Running 0 51m pod/falco-slx5k 2/2 Running 0 21m pod/falco-tzm8d 2/2 Running 0 21m Testing Falco Now that everything seems to be running, lets do a quick test. First we will try to access the Falco Sidekick user interface.\nFalco will not deploy a route for the UI automatically, instead we\u0026#39;ve created a Kustomize overlay with a custom route:\napiVersion: route.openshift.io/v1 kind: Route metadata: name: falco-falcosidekick-ui namespace: falco spec: host: falcosidekick-ui.apps.hub.aws.tntinfra.net port: targetPort: http tls: termination: edge to: kind: Service name: falco-falcosidekick-ui wildcardPolicy: None After deploying the Route we can access the Falco UI with the hostname specified in the route object. The default username seems to be admin/admin which is kind of strange for a security tool, maybe that\u0026#39;s the reason Falco does not expose the UI per default.\nCreating an event As a last test let\u0026#39;s try to trigger an event. We open a shell to one of the falco DaemonSet pods and execute a suspicious command:\n$ oc rsh falco-2ssgx Defaulted container \u0026#34;falco\u0026#34; out of: falco, falcoctl-artifact-follow, falco-driver-loader (init), falcoctl-artif# cat /etc/shadow root:*:19639:0:99999:7::: daemon:*:19639:0:99999:7::: bin:*:19639:0:99999:7::: sys:*:19639:0:99999:7::: sync:*:19639:0:99999:7::: games:*:19639:0:99999:7::: man:*:19639:0:99999:7::: lp:*:19639:0:99999:7::: mail:*:19639:0:99999:7::: news:*:19639:0:99999:7::: uucp:*:19639:0:99999:7::: proxy:*:19639:0:99999:7::: www-data:*:19639:0:99999:7::: backup:*:19639:0:99999:7::: list:*:19639:0:99999:7::: irc:*:19639:0:99999:7::: _apt:*:19639:0:99999:7::: nobody:*:19639:0:99999:7::: # and we can see an event with priority Warning in the Falco ui.\nThat\u0026#39;s it, seems like Falco is successfully running on OpenShift 4.12.\n"},{"uri":"https://blog.stderr.at/openshift/2023/10/how-to-force-a-machineconfig-rollout/","title":"How to force a MachineConfig rollout","tags":[],"description":"","content":" While playing around with Falco (worth another post) I had to force a MachineConfig update even so the actual configuration of the machine did not change.\nThis posts documents the steps taken.\nAs this seems to be not clearly documented here it comes\nGet the list of current MachineConfigs\n$ oc get mc NAME GENERATEDBYCONTROLLER IGNITIONVERSION AGE 00-master 7101fb0720d05771bdc174af918b64deb4efa604 3.2.0 8d 00-worker 7101fb0720d05771bdc174af918b64deb4efa604 3.2.0 8d 01-master-container-runtime 7101fb0720d05771bdc174af918b64deb4efa604 3.2.0 8d 01-master-kubelet 7101fb0720d05771bdc174af918b64deb4efa604 3.2.0 8d 01-worker-container-runtime 7101fb0720d05771bdc174af918b64deb4efa604 3.2.0 8d 01-worker-kubelet 7101fb0720d05771bdc174af918b64deb4efa604 3.2.0 8d 99-kernel-devel-extensions 25h 99-master-generated-registries 7101fb0720d05771bdc174af918b64deb4efa604 3.2.0 8d 99-master-ssh 3.2.0 8d 99-worker-generated-registries 7101fb0720d05771bdc174af918b64deb4efa604 3.2.0 8d 99-worker-ssh 3.2.0 8d rendered-master-b8a2011b0b09e36088acf47e225b0ed2 7101fb0720d05771bdc174af918b64deb4efa604 3.2.0 5h49m rendered-master-ce464ff45cc049fce3e8a63e36a4ee9e 7101fb0720d05771bdc174af918b64deb4efa604 3.2.0 8d rendered-worker-5baefb5bb7ad1d69cd7a0c3dc52ef2f3 7101fb0720d05771bdc174af918b64deb4efa604 3.2.0 25h rendered-worker-a0f8f0d915ef01ba4a1ab3047b6c863d 7101fb0720d05771bdc174af918b64deb4efa604 3.2.0 8d We want to force the rollout of a worker node, so remember the name of an old worker config, in our case rendered-worker-5baefb5bb7ad1d69cd7a0c3dc52ef2f3\nCurrently the desiredConfig and the currentConfig should have the same value\n$oc get node node1 -o jsonpath=\u0026#39;{.metadata.annotations.machineconfiguration\\.openshift\\.io/desiredConfig}{\u0026#34;\\n\u0026#34;}\u0026#39; rendered-worker-a0f8f0d915ef01ba4a1ab3047b6c863d $ oc get node node1 -o jsonpath=\u0026#39;{.metadata.annotations.machineconfiguration\\.openshift\\.io/currentConfig}{\u0026#34;\\n\u0026#34;}\u0026#39; rendered-worker-a0f8f0d915ef01ba4a1ab3047b6c863d Touch a file called 4 touch /run/machine-config-daemon-force\noc debug node/node1 -- touch /host/run/machine-config-daemon-force patch the node and set the annotation machineconfiguration.openshift.io/currentConfig to the old rendered config rendered-worker-5baefb5bb7ad1d69cd7a0c3dc52ef2f3\noc patch node ip-10-0-182-18.eu-central-1.compute.internal --patch \u0026#39;{ \u0026#34;metadata\u0026#34;: { \u0026#34;annotations\u0026#34;: { \u0026#34;machineconfiguration.openshift.io/currentConfig\u0026#34;: \u0026#34;rendered-worker-5baefb5bb7ad1d69cd7a0c3dc52ef2f3\u0026#34; } } }\u0026#39; Watch the MachineConfigPool\n$ oc get mcp Wait for the config rollout to complete.\n"},{"uri":"https://blog.stderr.at/tags/acs/","title":"ACS","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/advanced-cluster-security/","title":"Advanced Cluster Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ci/cd/","title":"CI/CD","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/cosign/","title":"Cosign","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-15-securesupplychain-intro/","title":"Introduction to a Secure Supply Chain","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","Rekor","cosign","SBOM","ACS","Sigstore"],"description":"Introduction to Secure Supply Chain","content":" The goal of the following (\u0026#34;short\u0026#34;) series is to build a secure CI/CD pipeline step by step using OpenShift Pipelines (based on Tekton). The whole build process shall pull and build an image, upload it to a development environment and subsequently update the production environment.\nThe main focus here is security. Several steps and tools shall help to build and deploy a Secure Supply Chain.\nThe whole process is part of a Red Hat workshop which can present to your organization. I did some tweaks and created a step-by-step plan in order to remember it …​ since I am getting old :)\nThe Journey to Secure Supply Chain This series includes the following articles:\nListen to Events\nPipelines\nSonarQube\nVerify Git Commit\nBuild and Sign Image\nScanning with ACS\nGenerating a SBOM\nUpdating Kubernetes Manifests\nLinting Kubernetes Manifests\nThe Example Application\nACS Deployment Check\nVerify TLOG Signature\nBring it to Production\nPrerequisites In order to develop our Secure Supply Chain, we need an OpenShift 4 Cluster. I am currently using OpenShift 4.13. Moreover, the OpenShift Pipelines operator must be deployed. It is based on Tekton and provides a Kubernetes-native way to create CI/CD pipelines.\nThe operator is deployed using the Operator Hub inside your cluster. Simply search for OpenShift Pipelines and install the operator using the default settings.\nFigure 1. Install OpenShift Pipelines Finally, you will need a GitHub account to be able to fork some repositories.\nSome steps in the pipeline are working tightly with GitHub, especially the very last one that is talking GitHub’s API. However, any Git-system should work, and probably just minor changes will be required. Everything else will be installed during the different steps described in the upcoming articles, while we build and tweak our pipeline.\nRemember, the big goal of our pipeline is NOT to simply pull, build and push our code, but to integrate certain security tools like code scanning, image scanning and linting. Otherwise, it would be boring.\nUsed Tools The following list of tools (or specifications) are used for our pipeline. They will be deployed when the appropriate step requires it.\nAdvanced Cluster Security\nroxctl\nSonarQube\nQuay - quay.io as public registry\nCoSign (sigstore)\nRekor (sigstore)\nSoftware Bill of Material (SBOM)\nKubeLinter\nKubeScore\nYamlLint\nkubesplit\n"},{"uri":"https://blog.stderr.at/tags/linting/","title":"Linting","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/pipleines/","title":"Pipleines","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/rekor/","title":"Rekor","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/sbom/","title":"SBOM","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/sigstore/","title":"Sigstore","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/sonarqube/","title":"SonarQube","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-16-securesupplychain-step1/","title":"Step 1 - Listen to Events","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain"],"description":"Step 1 Secure Supply Chain","content":" In this first step, we will simply prepare our environment to be able to retrieve calls from Git. In Git we will fork a prepared source code into a repository and any time a developer pushes a new code into our repository a webhook will notify OpenShift Pipelines to start the pipeline. Like most pipelines, the first task to be executed is to fetch the source code so it can be used for the next steps. The application I am going to use is called globex-ui and is an example webUI build with Angular.\nGoals The goals of this step are:\nCreate the EventListener and Trigger-Settings that will take care of notifications by GitHub.\nCreate a secret for GitHub authentication.\nFork a prepared source code and create a webhook inside Git.\nCreate the EventListener The first thing we need to create is a Namespace that will be responsible for all our Pipeline-objects. In this example, it is called ci:\noc new-project ci Now we need to create the so-called EventListener. This requires the creation of several objects:\nCreate a TriggerBinding: A TriggerBinding captures fields from an event and provides them as named parameters to the TriggerTemplate and subsequently to the PipelineRun.\nWe will create two TriggerBindings:\nglobex-ui - For the required settings of our example application\ngithub-push - For the relevant parameters to push into git. These parameters will be provided by Git whenever Git is using the Webhook to inform OpenShift that a new push event happened.\nCopy the following examples into your cluster.\nThe list of parameters in these manifests will be extended throughout this series. apiVersion: triggers.tekton.dev/v1beta1 kind: TriggerBinding metadata: name: globex-ui namespace: ci spec: params: - name: tlsVerify (1) value: \u0026#34;false\u0026#34; - name: gitRepoHost (2) value: github.com 1 Default values for verifying SSL is \u0026#34;false\u0026#34; (Since I do not have certificates in place) 2 The default value for the Git URL is github.com apiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerBinding metadata: name: github-push namespace: ci spec: params: - name: gitrepositoryurl (1) value: $(body.repository.clone_url) - name: fullname value: $(body.repository.full_name) - name: io.openshift.build.commit.ref value: $(extensions.ref) - name: io.openshift.build.commit.id value: $(body.head_commit.id) - name: io.openshift.build.commit.date value: $(body.head_commit.timestamp) - name: io.openshift.build.commit.message value: $(body.head_commit.message) - name: io.openshift.build.commit.author value: $(body.head_commit.author.name) 1 Several parameters, coming from Git via the Webhook, for example the exact URL to the repository, the ID of the commit, the date of the commit etc. Create a TriggerTemplate: A TriggerTemplate acts as a blueprint for PipelineRuns (or TaskRuns). The resources and parameters here will be used when our Pipeline is executed. It also defines the workspaces, that will be used by the pipeline. For now, we are using the space shared-data where we will pull the source code for further checks.\napiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerTemplate metadata: name: app-globex-ui-template namespace: ci spec: params: (1) - description: The git repository URL. name: gitrepositoryurl - description: The repository name for this PullRequest. name: fullname - description: The git branch for this PR. name: io.openshift.build.commit.ref - description: the specific commit SHA. name: io.openshift.build.commit.id - description: The date at which the commit was made name: io.openshift.build.commit.date - description: The commit message name: io.openshift.build.commit.message - description: The name of the github user handle that made the commit name: io.openshift.build.commit.author - description: The host name of the git repo name: gitRepoHost - description: Enable image repository TLS certification verification. name: tlsVerify - description: Extra parameters passed for the push command when pushing images. name: build_extra_args - description: Target image repository name name: imageRepo resourcetemplates: (2) - apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: secure-supply-chain- (3) spec: params: (4) - name: REPO_HOST value: $(tt.params.gitRepoHost) - name: GIT_REPO value: $(tt.params.gitrepositoryurl) - name: TLSVERIFY value: $(tt.params.tlsVerify) - name: BUILD_EXTRA_ARGS value: $(tt.params.build_extra_args) - name: IMAGE_REPO value: $(tt.params.imageRepo) - name: IMAGE_TAG value: \u0026gt;- $(tt.params.io.openshift.build.commit.ref)-$(tt.params.io.openshift.build.commit.id) - name: COMMIT_SHA value: $(tt.params.io.openshift.build.commit.id) - name: GIT_REF value: $(tt.params.io.openshift.build.commit.ref) - name: COMMIT_DATE value: $(tt.params.io.openshift.build.commit.date) - name: COMMIT_AUTHOR value: $(tt.params.io.openshift.build.commit.author) - name: COMMIT_MESSAGE value: $(tt.params.io.openshift.build.commit.message) pipelineRef: (5) name: secure-supply-chain serviceAccountName: pipeline (6) workspaces: (7) - name: shared-data volumeClaimTemplate: metadata: creationTimestamp: null spec: accessModes: - ReadWriteOnce resources: requests: storage: 3Gi status: {} 1 List of parameters for this TriggerTemplate, that should be used further for the pipeline. 2 The resources we are going to use. 3 The name prefix of the generated PipelineRun 4 List of parameters that shall be provided to the pipeline 5 The reference to the pipeline that shall be executed. 6 Name of the ServiceAccount that will execute the Pipeline. Per default, this is pipeline which is managed by the Operator. 7 The workspaces that will be used by the PipelineRun. Currently shared-data only. Create an EventListener that sets up a Service and listens for specific events and exposes a sink that receives incoming events, for example from a GitHub Webhook. It connects TriggerTemplate to a TriggerBinding. In this example, we create a Listener with 1 replica (that’s enough for testing) and connect our two TriggerBindings.\nWe also refer to the secret webhook-secret-globex-ui which will hold the password for GitHub to authenticate. We filter any push event coming from my Git repository tjungbauer/globex-ui\napiVersion: triggers.tekton.dev/v1alpha1 kind: EventListener metadata: name: globex-ui-event-listener namespace: ci spec: namespaceSelector: {} resources: kubernetesResource: replicas: 1 spec: template: metadata: creationTimestamp: null spec: containers: null serviceAccountName: pipeline triggers: (1) - bindings: - kind: TriggerBinding ref: globex-ui - kind: TriggerBinding ref: github-push interceptors: - params: - name: secretRef value: secretKey: webhook-secret-key secretName: webhook-secret-globex-ui (2) ref: kind: ClusterInterceptor name: github - params: - name: filter (3) value: \u0026gt;- (header.match(\u0026#39;X-GitHub-Event\u0026#39;, \u0026#39;push\u0026#39;) \u0026amp;\u0026amp; body.repository.full_name == \u0026#39;tjungbauer/globex-ui\u0026#39;) - name: overlays value: - expression: \u0026#39;body.ref.split(\u0026#39;\u0026#39;/\u0026#39;\u0026#39;)[2]\u0026#39; key: ref ref: kind: ClusterInterceptor name: cel name: build-from-push-globex-ui template: (4) ref: app-globex-ui-template 1 TriggerBindings that are used. 2 Reference to the secret. 3 A filter for push events and our repository name. 4 The TriggerTemplate that will be used. Now let us create a Route object to allow external traffic (from Git) to the EventListener.\napiVersion: route.openshift.io/v1 kind: Route metadata: name: el-event-listener namespace: ci spec: port: targetPort: http-listener to: kind: Service name: el-globex-ui-event-listener (1) weight: 100 tls: termination: edge insecureEdgeTerminationPolicy: Redirect wildcardPolicy: None 1 Service that will be automatically created when the EventListener has been created. And finally, we create a Secret to allow GitHub to authenticate. The name of the Secret is referenced inside the EventListener object.\nkind: Secret apiVersion: v1 metadata: name: webhook-secret-globex-ui (1) namespace: ci stringData: webhook-secret-key: yoursecret (2) type: Opaque 1 Name as referenced in the EventListener 2 Your super secure password Prepare GitHub Now we have everything in place to prepare our source code in Git. All we need to do is to create a repository that holds our source code and a Webhook.\nFork the Source Code: https://github.com/redhat-gpte-devopsautomation/globex-ui\nWhy fork? I want to be able to update the files and trigger the Pipeline whenever I want to. My forked repository can be found at: https://github.com/tjungbauer/globex-ui\nCreate a Webhook in GitHub. Go to Settings \u0026gt; Webhooks and add a new Webhook using:\nFigure 1. Create a new Webhook. The Route URL that was created.\nContent type: application/json.\nYour Password as used in the secret above.\nEnable or disable SSL verification, since I was too lazy to create a certificate at my demo cluster, I disabled it.\nAnd select which events, shall be sent to the Listener. In our case, push events are just fine.\nAfter a few seconds GitHub should have validated the Webhook (reload the page eventually)\nFigure 2. Verify Webhook Summary That’s it, we now have a Git repository, that will send any push-event to the EventListener, which uses the Triggers to fill out any required parameters and starts the pipeline named: secure-supply-chain.\nThis pipeline does not exist yet and will be created in the next step together with its first task to pull from the Git repository.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-17-securesupplychain-step2/","title":"Step 2 - Pipelines","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain"],"description":"Step 2 Secure Supply Chain","content":" We will now create the Pipeline and try to trigger it for the first time to verify if our Webhook works as intended.\nGoals The goals of this step are:\nCreate the Pipeline with a first task\nUpdate the Github repository, to verify if the Webhook works\nVerify if the PipelineRun is successful\nCreate the Pipeline The Pipeline object is responsible to define the Tasks (steps) that should be executed. Whenever a Pipeline is started a PipelineRun is created that performs each defined Task in the defined order and logs the output. Tasks can run subsequently or in parallel.\nCurrently, the Pipeline has one task pull-source-code which is defined as a ClusterTask \u0026#34;git-clone\u0026#34;. The purpose is to simply pull the source code to the workspace \u0026#34;shared-data\u0026#34;.\napiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: secure-supply-chain (1) namespace: ci spec: params: (2) - name: REPO_HOST type: string - name: COMMIT_SHA type: string - name: TLSVERIFY type: string - name: BUILD_EXTRA_ARGS type: string - name: IMAGE_REPO type: string - name: IMAGE_TAG type: string - name: GIT_REF type: string - name: COMMIT_DATE type: string - name: COMMIT_AUTHOR type: string - name: COMMIT_MESSAGE type: string - name: GIT_REPO type: string tasks: (3) - name: pull-source-code (4) params: - name: url (5) value: $(params.GIT_REPO) - name: revision value: $(params.GIT_REF) - name: deleteExisting value: \u0026#39;true\u0026#39; taskRef: (6) kind: ClusterTask name: git-clone workspaces: (7) - name: output workspace: shared-data workspaces: (8) - name: shared-data 1 Name of the Pipeline as referenced in the TriggerTemplate. 2 List of Parameters, hopefully, injected by the EventListener. 3 List of Tasks that will be executed. 4 Name of the Task. 5 Parameters used in this Task. 6 The Reference to the task. Here a ClusterTask named \u0026#34;git-clone\u0026#34; is used. 7 Workspace that shall be used in this Task. 8 Workspaces available in this Pipeline. The initial Pipeline will now look like the following (Go to: Pipelines \u0026gt; Pipelines \u0026gt; secure-supply-chain)\nFigure 1. Initial Pipeline Our first Run Now it is time to update something in our Git Repository and verify if everything can be executed successfully.\nTo update, it is enough to simply add a space in the README.md file and push it to Git.\nIf the Webhook works as expected, Git will notify our EventListener, which will then trigger the Pipeline. A PipelineRun is created, that executes all Tasks that are defined in the Pipeline (currently just 1)\nYou can monitor the progress of the PipelineRun:\nFigure 2. PipelineRun Overview On the Details-page you can see which step is currently executed:\nFigure 3. PipelineRun Details Eventually, the PipelineRun finishes successfully.\nFigure 4. PipelineRun Finished You can analyze the Logs in case of an Error or to get more details of a certain Task:\nFigure 5. Task Logs Summary We have now created our first Pipeline and tested the GitHub Webhook. Whenever we push changes to the code, Git will notify the EventListener which will trigger the Pipeline with all required Parameters.\nA PipelineRun is generated and is executing the defined Tasks. Currently, not much is done, expect cloning the Git repository.\nIn the next steps, we will evolve our Pipeline to perform security checks and sign our image.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-18-securesupplychain-step3/","title":"Step 3 - SonarQube","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","SonarQube"],"description":"Step 3 Secure Supply Chain","content":" After the Pipeline has been created and tested we will add another Task to verify the source code and check for possible security issues, leveraging the tool SonarQube by Sonar.\nGoals The goals of this step are:\nInstall and configure SonarQube\nAdd a Task to scan our source code for vulnerabilities\nVerify the results.\nSonarQube SonarQube by Sonar helps developers to deliver clean code. With the integration into our CICD pipeline it will detect issues and reports them back to the developers. The results will be shown in a dashboard. We will install the Community version of SonarQube, which is enough for our showcase.\nSonarQube Installation To install SonarQube I have prepared a Helm Chart that I use with GitOps when I deploy a new lab environment. Feel free to use it. It simply calls the Chart that is provided by Sonar. In addition, it creates a Job that changes the default administrator password to a different one.\nThe values file is good to go, the only item you must change is the route in the very first line. Also, if you prefer not to deploy any plugins (for example, the German language pack), you can remove the appropriate line.\nBefore you run the Helm, you need to provide a Sealed Secret (or manually create a Secret) like the following:\nkind: Secret apiVersion: v1 metadata: name: credentials namespace: sonarqube data: adminpass: \u0026lt;your base64 password string\u0026gt; type: Opaque This password will be used by the Job \u0026#34;change-admin-password\u0026#34; and will configure a new password for the user \u0026#34;admin\u0026#34;\nOnce everything is installed (this will take several minutes), you can access SonarQube using the URL you defined in the values file.\nFigure 1. SonarQube SonarQube Create Token Our Pipeline will talk to SonarQube and request a scan of the source code. To be able to do this, we need to create a Token in SonarQube and store it as a Secret in our ci namespace.\nClick on \u0026#34;My Account\u0026#34; (Upper right corner) \u0026gt; Security and create a new token:\nFigure 2. SonarQube Token Copy the token and create the following Secret:\nkind: Secret apiVersion: v1 metadata: name: globex-ui-sonarqube-secret namespace: ci stringData: token: \u0026lt;your SonarQube Token\u0026gt; (1) type: Opaque 1 The generated Token NOT base64 encrypted (I am using stringData in this case) Modify the Pipeline Now it is time to bring the SonarQube scan task into our Pipeline. This requires some modifications to existing objects and the creation of some new ones.\nModify the TriggerBinding.\nAdd the following lines to globex-ui TriggerBinding\n- name: sonarqubeHostUrl value: http://sonarqube.apps.ocp.aws.ispworld.at/ (1) 1 The URL of SonarQube So, it will look like this:\napiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerBinding metadata: name: globex-ui namespace: ci spec: params: - name: tlsVerify value: \u0026#39;false\u0026#39; - name: gitRepoHost value: github.com - name: sonarqubeHostUrl value: http://sonarqube.apps.ocp.aws.ispworld.at/ (1) 1 The URL of SonarQube Modify the TriggerTemplate\nAdd the following lines:\nspec: params: (1) - description: Sonarqube host url name: sonarqubeHostUrl ... resourcetemplates: - apiVersion: tekton.dev/v1beta1 spec: params: (2) - name: SONARQUBE_HOST_URL value: $(tt.params.sonarqubeHostUrl) - name: SONARQUBE_PROJECT_KEY value: globex-ui (3) - name: SONARQUBE_PROJECT_SECRET value: globex-ui-sonarqube-secret (4) ... 1 Parameters provided by the TriggerBinding. 2 Parameters provided to the Pipeline. 3 Project that will be created in SonarQube. 4 Secret of the SonarQube token. The result should look like the following:\napiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerTemplate metadata: name: app-globex-ui-template namespace: ci spec: params: - description: The git repository URL. name: gitrepositoryurl - description: The repository name for this PullRequest. name: fullname - description: The git branch for this PR. name: io.openshift.build.commit.ref - description: the specific commit SHA. name: io.openshift.build.commit.id - description: The date at which the commit was made name: io.openshift.build.commit.date - description: The commit message name: io.openshift.build.commit.message - description: The name of the github user handle that made the commit name: io.openshift.build.commit.author - description: The host name of the git repo name: gitRepoHost - description: Enable image repository TLS certification verification. name: tlsVerify - description: Extra parameters passed for the push command when pushing images. name: build_extra_args - description: Target image repository name name: imageRepo - description: Sonarqube host url name: sonarqubeHostUrl resourcetemplates: - apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: secure-supply-chain- spec: params: - name: REPO_HOST value: $(tt.params.gitRepoHost) - name: GIT_REPO value: $(tt.params.gitrepositoryurl) - name: TLSVERIFY value: $(tt.params.tlsVerify) - name: BUILD_EXTRA_ARGS value: $(tt.params.build_extra_args) - name: IMAGE_REPO value: $(tt.params.imageRepo) - name: IMAGE_TAG value: \u0026gt;- $(tt.params.io.openshift.build.commit.ref)-$(tt.params.io.openshift.build.commit.id) - name: COMMIT_SHA value: $(tt.params.io.openshift.build.commit.id) - name: GIT_REF value: $(tt.params.io.openshift.build.commit.ref) - name: COMMIT_DATE value: $(tt.params.io.openshift.build.commit.date) - name: COMMIT_AUTHOR value: $(tt.params.io.openshift.build.commit.author) - name: COMMIT_MESSAGE value: $(tt.params.io.openshift.build.commit.message) - name: SONARQUBE_HOST_URL value: $(tt.params.sonarqubeHostUrl) - name: SONARQUBE_PROJECT_KEY value: globex-ui - name: SONARQUBE_PROJECT_SECRET value: globex-ui-sonarqube-secret pipelineRef: name: secure-supply-chain serviceAccountName: pipeline workspaces: - name: shared-data volumeClaimTemplate: metadata: creationTimestamp: null spec: accessModes: - ReadWriteOnce resources: requests: storage: 3Gi status: {} Create the Task scan-source. This task will use the pulled source code and uses SonarQube to let it scan our code.\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: scan-code namespace: ci spec: description: \u0026gt;- Source code scan using sonar-scanner and SonarQube. params: - default: \u0026#39;docker.io/sonarsource/sonar-scanner-cli:latest\u0026#39; (1) name: scanImage type: string - default: \u0026#39;https://sonarqube-sonarqube.myplaceholder.com/\u0026#39; (2) name: sonarqubeHostUrl type: string - default: object-detection-rest name: sonarqubeProjectKey type: string - default: object-detection-rest-sonarqube-secret name: sonarqubeProjectSecret type: string - default: \u0026#39;true\u0026#39; name: verbose type: string steps: - env: - name: SONAR_TOKEN_WEB_UI (3) valueFrom: secretKeyRef: key: token name: $(params.sonarqubeProjectSecret) (4) image: $(params.scanImage) name: scan-code resources: {} script: \u0026gt; (5) set -x echo $(ls -a) sonar-scanner -X -Dsonar.projectKey=$(params.sonarqubeProjectKey) -Dsonar.sources=./ -Dsonar.host.url=$(params.sonarqubeHostUrl) -Dsonar.login=$SONAR_TOKEN_WEB_UI workingDir: /workspace/repository workspaces: (6) - name: repository 1 Image containing SonarQube command line tool. The cluster must be able to connect to docker.io. 2 Default parameters for this Task that might be overwritten. 3 The Secret with the token. 4 Parameter as set by the PipelineRun which gets the value from the TriggerTemplate. 5 Script that is executed to scan the source code. 6 The workspace where we can find the source code. Update your Pipeline and add the following task:\nspec: params: ... - name: SONARQUBE_HOST_URL type: string - name: SONARQUBE_PROJECT_KEY type: string - name: SONARQUBE_PROJECT_SECRET type: string tasks: ... - name: scan-source params: (1) - name: sonarqubeHostUrl value: $(params.SONARQUBE_HOST_URL) - name: sonarqubeProjectKey value: $(params.SONARQUBE_PROJECT_KEY) - name: sonarqubeProjectSecret value: $(params.SONARQUBE_PROJECT_SECRET) runAfter: (2) - pull-source-code taskRef: (3) kind: Task name: scan-code workspaces: (4) - name: repository workspace: shared-data 1 Parameters that shall be provided for the Task. 2 The task should run AFTER the source has been pulled …​ which makes sense. 3 Reference to the Task we created above. 4 Workspace shared-data where the source code was pulled from the previous Task. The full pipeline objects now look like the following:\nExpand me... apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: secure-supply-chain namespace: ci spec: params: - name: REPO_HOST type: string - name: COMMIT_SHA type: string - name: TLSVERIFY type: string - name: BUILD_EXTRA_ARGS type: string - name: IMAGE_REPO type: string - name: IMAGE_TAG type: string - name: GIT_REF type: string - name: COMMIT_DATE type: string - name: COMMIT_AUTHOR type: string - name: COMMIT_MESSAGE type: string - name: GIT_REPO type: string - name: SONARQUBE_HOST_URL type: string - name: SONARQUBE_PROJECT_KEY type: string - name: SONARQUBE_PROJECT_SECRET type: string tasks: - name: pull-source-code params: - name: url value: $(params.GIT_REPO) - name: revision value: $(params.GIT_REF) - name: deleteExisting value: \u0026#39;true\u0026#39; taskRef: kind: ClusterTask name: git-clone workspaces: - name: output workspace: shared-data - name: scan-source params: - name: sonarqubeHostUrl value: $(params.SONARQUBE_HOST_URL) - name: sonarqubeProjectKey value: $(params.SONARQUBE_PROJECT_KEY) - name: sonarqubeProjectSecret value: $(params.SONARQUBE_PROJECT_SECRET) runAfter: - pull-source-code taskRef: kind: Task name: scan-code workspaces: - name: repository workspace: shared-data workspaces: - name: shared-data The Pipeline now has a second task:\nFigure 3. Pipeline Execute the Pipeline Let’s update the README.md of our source code again to trigger another PipelineRun. After the code has been pulled it should now perform the second task and scan the quality of the source code.\nYou can monitor the progress of the PipelineRun again:\nFigure 4. PipelineRun Details Once the PipelineRun executed both tasks successfully, we can check SonarQube.\nThe project globex-ui has been created which shows the results of our scan:\nFigure 5. SonarQube Results Summary We have now added a Task to our Pipeline that performs a code analysis of our source code. The results are shown in SonarQube and the developers can react accordingly.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-19-securesupplychain-step4/","title":"Step 4 - Verify Git Commit","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain"],"description":"Step 4 Secure Supply Chain","content":" Besides checking the source code quality, we should also verify if the commit into Git was done by someone/something we trust. It is a good practice to sign all commits to Git. You need to prepare your Git account and create trusted certificates.\nI will not describe how exactly you need to configure Git to sign your commit. Verify the following link to learn more about Signing Commits Goals The goals of this step are:\nVerify if the last commit has been signed\nPrerequisites Signing public key\nConfigured Git to verify your gpg signature\nWhen your commit is signed, Git will show that:\nFigure 1. Pipeline Steps Create the following Secret that contains your PUBLIC key.\nkind: Secret apiVersion: v1 metadata: name: gpg-public-key namespace: ci data: public.key: \u0026gt;- \u0026lt;Base64 PUBLIC GPG KEY\u0026gt; (1) type: Opaque 1 Public key, containing BEGIN/END lines base64 encoded. Create the following Task:\napiVersion: tekton.dev/v1 kind: Task metadata: name: verify-source-code-commit-signature namespace: ci spec: description: This task verifies the latest commit and signature against the gpg public key params: - default: \u0026#39;registry.redhat.io/openshift-pipelines/pipelines-git-init-rhel8:v1.10.4-4\u0026#39; name: gitInit type: string steps: - computeResources: {} image: $(params.gitInit) name: git-verify script: | set -x (1) gpg --import /workspace/secrets/public.key git config --global --add safe.directory /workspace/repository git verify-commit HEAD || (echo \u0026#34;Unable to verify commit at HEAD!\u0026#34; \u0026amp;\u0026amp; exit 1) workingDir: /workspace/repository workspaces: - name: repository - name: secrets (2) 1 The script to verify the signature of the commit, 2 The workspace that mounts the Secret containing the gpg key, Modify the TriggerTemplate and add the following 3 lines\nworkspaces: ... - name: secrets secret: secretName: gpg-public-key (1) 1 The name of the Secret where the public key can be found. Update the pipeline to execute the task verify-commit-signature, which is running in parallel to the SonarQube scan.\n- name: verify-commit-signature runAfter: - pull-source-code (1) taskRef: kind: Task name: verify-source-code-commit-signature (2) workspaces: (3) - name: repository workspace: shared-data - name: secrets workspace: secrets workspaces: ... - name: secrets (4) 1 This task runs after pull-source-code but in parallels with the SonarQube task. 2 Task reference 3 Workspaces that are used in this Task 4 Additional workspace for the Pipeline The full pipeline objects now look like the following:\nExpand me... apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: secure-supply-chain namespace: ci spec: params: - name: REPO_HOST type: string - name: COMMIT_SHA type: string - name: TLSVERIFY type: string - name: BUILD_EXTRA_ARGS type: string - name: IMAGE_REPO type: string - name: IMAGE_TAG type: string - name: GIT_REF type: string - name: COMMIT_DATE type: string - name: COMMIT_AUTHOR type: string - name: COMMIT_MESSAGE type: string - name: GIT_REPO type: string - name: SONARQUBE_HOST_URL type: string - name: SONARQUBE_PROJECT_KEY type: string - name: SONARQUBE_PROJECT_SECRET type: string tasks: - name: pull-source-code params: - name: url value: $(params.GIT_REPO) - name: revision value: $(params.GIT_REF) - name: deleteExisting value: \u0026#39;true\u0026#39; taskRef: kind: ClusterTask name: git-clone workspaces: - name: output workspace: shared-data - name: scan-source params: - name: sonarqubeHostUrl value: $(params.SONARQUBE_HOST_URL) - name: sonarqubeProjectKey value: $(params.SONARQUBE_PROJECT_KEY) - name: sonarqubeProjectSecret value: $(params.SONARQUBE_PROJECT_SECRET) runAfter: - pull-source-code taskRef: kind: Task name: scan-code workspaces: - name: repository workspace: shared-data - name: verify-commit-signature runAfter: - pull-source-code taskRef: kind: Task name: verify-source-code-commit-signature workspaces: - name: repository workspace: shared-data - name: secrets workspace: secrets workspaces: - name: shared-data - name: secrets The status of the Pipeline now is:\nFigure 2. Pipeline Execute the Pipeline Let’s update the README.md of our source code again to trigger another PipelineRun.\nNow the 3rd task will verify if the commit was signed.\nFigure 3. PipelineRun Details In the logs of the Task, we can see that the commit was signed and could be verified. See:\n... gpg: Good signature from \u0026#34;Thomas Jungbauer \u0026lt;tjungbau@redhat.com\u0026gt;\u0026#34; ... Figure 4. Signature Verification Summary At this stage we have a Pipeline, that pulls our code, does a code analysis, and verifies if the commit has been signed. The very next step is to build the image and push it into an Image Registry.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-20-securesupplychain-step5/","title":"Step 5 - Build and Sign Image","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","Sigstore"],"description":"Step 5 Secure Supply Chain","content":" Finally, after pulling and checking the code, we are going to create the image. During this process the image will be signed and uploaded to the public registry Quay.io.\nGoals The goals of this step are:\nBuild the image\nSign the image\nUpload it Quay.io\nYou can use any registry you like. Prerequisites You need to have an account in Quay.io and create a repository there.\nI have created the repository https://quay.io/repository/tjungbau/secure-supply-chain-demo which will contain the images and the signatures.\nSteps Create a Robot account in Quay.io and get the authentication json. Go to \u0026#34;Account Settings \u0026gt; Robot Accounts\u0026#34; and create a new account.\nFigure 1. Quay Robot Account Allow this account WRITE permissions to the repository\nFigure 2. Quay Robot Account Permissions Select the just-created account and go to Kubernetes Secret.\nHere you can view or download the Secret. It should look like the following:\napiVersion: v1 kind: Secret metadata: name: tjungbau-secure-supply-chain-demo-pull-secret data: .dockerconfigjson: \u0026lt;SECURE\u0026gt; type: kubernetes.io/dockerconfigjson Copy the content and store it in your ci Namespace.\nThe name will be probably different in other examples. To be able for the ServiceAccount, which is executing the Pipelines, to use this Secret, we need to modify the ServiceAccount object.\nIn our case the ServiceAccount is called pipeline. Modify it and add the following lines:\nsecrets: ... - name: tjungbau-secure-supply-chain-demo-pull-secret (1) imagePullSecrets: - name: tjungbau-secure-supply-chain-demo-pull-secret (2) 1 Use your appropriate name for the secret 2 Use your appropriate name for the secret, required to be able to sign the image Update the Pipeline object and add the following block\nThis time we do not need to add a Task object, because we are using a ClusterTask \u0026#34;buildah\u0026#34; that takes care of the building process. As workspace \u0026#34;shared-data\u0026#34; is used again, which has all the source code stored:\n- name: build-sign-image params: - name: TLSVERIFY value: $(params.TLSVERIFY) - name: BUILD_EXTRA_ARGS value: \u0026gt;- --label=io.openshift.build.commit.author=\u0026#39;$(params.COMMIT_AUTHOR)\u0026#39; --label=io.openshift.build.commit.date=\u0026#39;$(params.COMMIT_DATE)\u0026#39; --label=io.openshift.build.commit.id=\u0026#39;$(params.COMMIT_SHA)\u0026#39; --label=io.openshift.build.commit.message=\u0026#39;$(params.COMMIT_MESSAGE)\u0026#39; --label=io.openshift.build.commit.ref=\u0026#39;$(params.GIT_REF)\u0026#39; --ulimit=nofile=4096:4096 - name: IMAGE value: \u0026#39;$(params.IMAGE_REPO):$(params.IMAGE_TAG)\u0026#39; retries: 1 runAfter: - scan-source - verify-commit-signature taskRef: kind: ClusterTask name: buildah workspaces: - name: source workspace: shared-data Update the TriggerBinding globex-ui and define the URL for your registry respectively for your image repository\nspec: parameters: - name: imageRepo value: quay.io/tjungbau/secure-supply-chain-demo With this configuration, we could already build the image and push it into the registry. However, we also would like to sign it. Prepare Tekton for Image Signing To automatically sign images, we use TektonChains which is a controller that allows you to manage your supply chain security by signing TaskRuns and/or OCI images. Once the build-Task has pushed the images to Quay, Tekton will detect this event (by monitoring specific values of Task results) and then creates and pushes the signature for that image.\nBy default, Tekton Chains observes all task run executions. When the task runs complete, Tekton Chains signs and stores all artefacts.\nTekton Chain is part of OpenShift Pipelines and has the following main features:\nYou can sign task runs, task run results, and OCI registry images with cryptographic keys that are generated by tools such as cosign and skopeo.\nYou can use attestation formats such as in-toto.\nYou can securely store signatures and signed artefacts using OCI repository as a storage backend.\nIn our case, we will use CoSign for signing the images and Rekor for the attestation.\nTo activate image signing add the following to the TektonConfig object by removing the \u0026#34;chain{}\u0026#34; entry.\nchain: artifacts.oci.storage: oci (1) artifacts.taskrun.format: in-toto (2) artifacts.taskrun.storage: oci (3) transparency.enabled: true (4) 1 The storage backend for storing OCI signatures. 2 The format for storing TaskRun payloads. Can be in-toto or slsa/v1. 3 The storage backend for TaskRun signatures. Multiple can be defined. 4 Enable or disable automatic binary transparency uploads. The URL for uploading binary transparency attestations is https://rekor.sigstore.dev by default. More details can be found at https://docs.openshift.com/container-platform/4.13/cicd/pipelines/using-tekton-chains-for-openshift-pipelines-supply-chain-security.html CoSign - Signing the Image We will use CoSign to sign our image. To do so we need to download the cosign binary and generate a key pair:\nBe sure that you are logged into the OpenShift cluster. cosign generate-key-pair k8s://openshift-pipelines/signing-secrets Enter password for private key: Enter password for private key again: Successfully created secret signing-secrets in namespace openshift-pipelines oc get secrets signing-secrets -n openshift-pipelines NAME TYPE DATA AGE signing-secrets Opaque 3 46h Cosign will ask you to enter a password and will then create a Kubernetes secret in the Namespace openshift-pipelines.\nWhen OpenShift Pipelines now execute a Task that is pushing an image, this Secret will be used to sign the image.\nLet’s update our source code. The image will be built and pushed into Quay. The small black shield indicates that this image has been signed.\nFigure 3. Quay Signed Image Besides the actual image the files: *.sig and *.att can be seen. The first one is the signature, the second one shows metadata about the attestation retrieved from Rekor which can be compared with the Rekor URL.\nRekor Rekor’s goals are to provide an immutable tamper resistant ledger of metadata generated within a software projects supply chain. Rekor will enable software maintainers and build systems to record signed metadata to an immutable record. Other parties can then query said metadata to enable them to make informed decisions on trust and non-repudiation of an object’s lifecycle. For more details visit the sigstore website.\nThe Rekor project provides a restful API based server for validation and a transparency log for storage. A CLI application is available to make and verify entries, query the transparency log for inclusion proof, integrity verification of the transparency log or retrieval of entries by either public key or artifact. Rekor Git\nAn important feature of Tekton Chains is that it integrates seamlessly with an application called Rekor. The configuration transparency.enabled: true enables the call to the Rekor API that can be found by default at: https://rekor.sigstore.dev. This will create a transparency log which can be used for verification purposes later.\nOf course, it would be possible to install your own server. Learn in the official Docs how this can be achieved. This means every time our build-Tasks successfully completes a URL to the transparency logs will be attached to the Task.\nAfter our PipelineRuns were successful, we can verify if the image has an entry in the Rekor transparency log. This is important to ensure that the image went through a valid signing process. At a later step (before we create the pull request for the production update) we will verify if the log exists.\nThe following annotations should have been added to the TaskRun or PipelineRun secure-supply-chain-XXX-build-sign-image automatically:\nmetadata: annotations: chains.tekton.dev/signed: \u0026#39;true\u0026#39; chains.tekton.dev/transparency: \u0026#39;https://rekor.sigstore.dev/api/v1/log/entries?logIndex=25593495\u0026#39; This created the following transparency logs:\n{ \u0026#34;24296fb24b8ad77a7ae84f4b950336d1872a066aeb9463dfbc231a7d3b73dd040dd5faefb3c013a7\u0026#34;: { \u0026#34;body\u0026#34;: \u0026#34;eyJhcGlWZXJzaW9uIjoiMC4wLjEiLCJraW5kIjoiaGFzaGVkcmVrb3JkIiwic3BlYyI6eyJkYXRhIjp7Imhhc2giOnsiYWxnb3JpdGhtIjoic2hhMjU2IiwidmFsdWUiOiJmYzE2ODM3Mzc1ODUyNjc0MTQ4MjIyMDJhMWU2Y2RkZDkxNWI4NWUzYzhhNDVhZmI0YzEyYmE2YmNhMGNmNDQyIn19LCJzaWduYXR1cmUiOnsiY29udGVudCI6Ik1FWUNJUUNRMnlnU2ZGTllHS2pzbXRIYjVielAwdlRNMFgyNHVlNXlKbjJxdWFWSyt3SWhBSmhoSWpweEFybDJERjFsVmpMT0ZzVnRhMzJmbXJXRmxXWkdRQ015b2xrayIsInB1YmxpY0tleSI6eyJjb250ZW50IjoiTFMwdExTMUNSVWRKVGlCUVZVSk1TVU1nUzBWWkxTMHRMUzBLVFVacmQwVjNXVWhMYjFwSmVtb3dRMEZSV1VsTGIxcEplbW93UkVGUlkwUlJaMEZGVEc0clduaENNMHNyS3pWSk1pOWlNWFJqVFVKcVZXODRjWGx6YVFwNlQzUlpVbGRQUTBwTVRWQlNWVGR4WTFJeVMyWlZZazAzYlVwUlNtbHZjbXR6VW1KUmNHMTBTekV4WjJNM1pIVkZObGg0WmxOdlpWUkJQVDBLTFMwdExTMUZUa1FnVUZWQ1RFbERJRXRGV1MwdExTMHRDZz09In19fX0=\u0026#34;, \u0026#34;integratedTime\u0026#34;: 1688060588, \u0026#34;logID\u0026#34;: \u0026#34;c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d\u0026#34;, \u0026#34;logIndex\u0026#34;: 25593495, \u0026#34;verification\u0026#34;: { \u0026#34;inclusionProof\u0026#34;: { \u0026#34;checkpoint\u0026#34;: \u0026#34;rekor.sigstore.dev - 2605736670972794746\\n21430853\\n10sx5mwekx2c8jltWbt0WeJu+tNM9FOBNQoyPiC3CPQ=\\nTimestamp: 1688061275289390813\\n\\n— rekor.sigstore.dev wNI9ajBFAiEAyzFYKKghB+RQsbMIIzjywdA9vFjsusQoMWMUJPRjw3ECIDVeYSGKn7zwXWxGLhQlIGAI1Ca0Gu7ZuLJ64xR3SrY0\\n\u0026#34;, \u0026#34;hashes\u0026#34;: [ \u0026#34;5cc5dba1f5f420acc9ed049c77b04c4fdd66cf6ef1ebd46b66e26039bb9ba06c\u0026#34;, \u0026#34;3041520c9fec6177225063053503f6d20b09e86b72968b737e3211a233dab6ce\u0026#34;, \u0026#34;ddb57132bba122df920d4816af7ac02bef03aec533cc7b45f8552ce19c023d10\u0026#34;, \u0026#34;8a510da356706a0a822daf3c3a935176d3634c6e0cce6830fe90a8771a3bc83d\u0026#34;, \u0026#34;7d254234192620827306cc5024ffb8ae699bfe2725c8e6aa13757f5471d416d4\u0026#34;, \u0026#34;648a4e45da960f4ad286dfa22ae93721c9ba82ee05edd5a7134d9b19e35735e0\u0026#34;, \u0026#34;9dcf0f21690ec420bffbc1d10c383b7d3dc568d26bd8fd9e8771abb01f5976dc\u0026#34;, \u0026#34;86575ffec011d78ad393e7af267693c0c59360e2299b308369951d8362032733\u0026#34;, \u0026#34;0088ef1f6ec1e992fa6c91837287f2bd7eea238e5af7467e21e6df0dc8efe516\u0026#34;, \u0026#34;b149d95903a4e554ac1c381f1afff31bb62b6e12b6b2fc95f36b8e198e1a42cd\u0026#34;, \u0026#34;de73a694862cebd660ef1cecdd1bb66e273ab0651ca939bf7fa6f5f567bafe93\u0026#34;, \u0026#34;a3c84734ddae3102952584443dea70e3dec2cc085af7cf0ba3530751966861ec\u0026#34;, \u0026#34;0be5c7bbcf481d1efcfc63a27fce447cf6345f7bb3155cf5de39de592c16d52d\u0026#34;, \u0026#34;f597f4bae8df3d6fc6eebfe3eabd7d393e08781f6f16b42398eca8512398fff1\u0026#34;, \u0026#34;4e35fcb3c0a59e7f329994002b38db35f5d511f499ba009e10b31f5d27563607\u0026#34;, \u0026#34;47044b7ac3aab820e44f0010538d7de71e17a11f4140cbbe9eeb37f78b77cc7d\u0026#34;, \u0026#34;eee63677e2591eefe06ab537d6dd1b4060770682744c8287879b5dcd3365a5b2\u0026#34;, \u0026#34;ff41aa21106dbe03996b4335dd158c7ffafd144e45022193de19b2b9136c3e42\u0026#34;, \u0026#34;e6ebdeef2e23335d8d7049ba5a0049a90593efdfe9c1b4548946b44a19d7214f\u0026#34;, \u0026#34;dd51e840e892d70093ad7e1db1e2dea3d50334c7345d360e444d22fc49ed9f5e\u0026#34;, \u0026#34;ad712c98424de0f1284d4f144b8a95b5d22c181d4c0a246518e7a9a220bdf643\u0026#34; ], \u0026#34;logIndex\u0026#34;: 21430064, \u0026#34;rootHash\u0026#34;: \u0026#34;d74b31e66c1e931d9cf2396d59bb7459e26efad34cf45381350a323e20b708f4\u0026#34;, \u0026#34;treeSize\u0026#34;: 21430853 }, \u0026#34;signedEntryTimestamp\u0026#34;: \u0026#34;MEYCIQDqmqb4k95FjiBNogOAmjTskkIPaGslgvrSND4pQdUALwIhAMt85yLsa5Ei+3NsmJ906/T9Hx1YZDDHQfHlTEYqqcaE\u0026#34; } } } Summary Finally, we have a signed image uploaded to Quay. In addition, a transparency log has been created. Now let us add some security checks in the next steps.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-21-securesupplychain-step6/","title":"Step 6 - Scanning with ACS","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","ACS","Advanced Cluster Security"],"description":"Step 6 Secure Supply Chain","content":" In this step we will install Advanced Cluster Security (ACS) and create 2 new steps in our Pipeline to scan the image for vulnerabilities and security policy. A custom security policy, configured in ACS, will verify if the image is signed.\nGoals The goals of this step are:\nInstall ACS.\nCreate a custom security policy into ACS.\nConfigure CoSign integration in ACS.\nCreate a step in the Pipeline to perform an image scan (vulnerabilities).\nCreate a step in the Pipeline to perform an image check (security policies).\nInstall Advanced Cluster Security (ACS) We will install ACS on our cluster which will be called local-cluster inside ACS.\nThe deployment will happen via Argo CD (OpenShift GitOps) that will use the following Helm Chart: RHACS\nThe Argo CD Application will trigger the deployment which will take several minutes.\nThe deployment will install a minimal version of ACS and tries to limit the required resources and replicas. For production environments, you probably want to increase the settings. apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: in-cluster-init-rhacs namespace: openshift-gitops spec: destination: namespace: default server: \u0026#39;https://kubernetes.default.svc\u0026#39; (1) info: - name: Description value: \u0026gt;- Initialize Red Hat Advanced Cluster Security and deploy Central and SecuredCluster project: in-cluster source: path: charts/rhacs-full-stack (2) repoURL: \u0026#39;https://github.com/tjungbauer/helm-charts\u0026#39; (3) targetRevision: main 1 Installing on the local cluster. 2 Path to the Helm chart. 3 URL to the Helm chart repository. Figure 1. Syncing ACS Deployment Application This GitOps Application will start a full-stack ACS deployment. It does (in this exact order):\nInstalls the ACS Operator into rhacs-operator\nVerifies if the Operator is ready\nCreates the Namespace stackrox\nAdds a ConsoleLink in the upper right action menu of OpenShift\nFigure 2. ACS ConsoleLink Creates the Central custom resource with minimum resources\nCreates an init-bundle to add the first (local) cluster\nAdds the custom resource SecureCluster to install the rest of the ACS components\nAnd finally, runs a basic configuration that enables authentication via OpenShift and provides the kubeadmin user admin privileges. This can be disabled in the values file if you prefer not to configure this, or if kubeadmin does not exist anymore in your cluster.\nThe whole process will take a while and uses syncwaves and hooks.\nConfigure ACS Integration Generate Authentication Token Create an authentication Token in ACS\nGo to \u0026#34;Platform Configuration \u0026gt; Integrations \u0026gt; API Token\u0026#34;\nFigure 3. ACS Generate Token Generate a new Token with at least Continuous Integration privileges and save the created Token.\nBack in OpenShift, a Secret must be created with the key rox_api_token\nkind: Secret apiVersion: v1 metadata: name: stackrox-secret namespace: ci data: rox_api_token: \u0026lt;base 64 Token\u0026gt; (1) type: Opaque 1 Base64 decoded ACS Token. Create a 2nd Secret that contains the ACS endpoint and its port:\nkind: Secret apiVersion: v1 metadata: name: stackrox-endpoint namespace: ci stringData: rox_central_endpoint: central-stackrox.apps.ocp.aws.ispworld.at:443 (1) type: Opaque 1 The endpoint URL of my example cluster. Note: the port MUST be added here. Integrate CoSign with ACS Configure CoSign Integration\nDuring Step 5 we have created a CoSign key pair to sign our images. To integrate with ACS, we need to retrieve the public key. In OpenShift, open the Secret signing-secrets in the Namespace openshift-pipelines and extract the key cosign.pub.\nIt will look something like tjis:\n-----BEGIN PUBLIC KEY----- key... -----END PUBLIC KEY----- In ACS, go to \u0026#34;Platform Configuration \u0026gt; Integrations \u0026gt; Signature\u0026#34; and create a new CoSign integration.\nFigure 4. ACS CoSign Integration Enter a name and the public key and activate this integration\nFigure 5. ACS Create CoSign Integration This will enable ACS to verify the CoSign signature of the image.\nCreate a Custom Policy that verifies the Signature Create a custom security policy, that verifies if our image has been signed or not.\nTo be sure that every image is correctly signed, we create a custom security policy that verifies this signature using our CoSign integration. This policy can be configured as inform or enforce. Enforce means that the pipeline will fail (during the Task acs-image-check) if the image signature cannot be checked.\nLet’s create our policy by hand for this time. However, it is also possible to export/import rules. The important part here is that link to the CoSign integration is valid, otherwise ACS cannot verify the signature.\nOpen \u0026#34;Platform Configuration \u0026gt; Policy \u0026gt; and click the button Create Policy\u0026#34;\nName: Trusted Signature Policy\nSeverity: High\nCategories: Security Best Practices\nClick Next\nLifecycle stage: check Build and Deploy\nResponse method: Inform and enforce \u0026gt; This will make the Task in the Pipeline fail when the signature cannot be verified.\nConfigure enforcement behavior:\nActivate: Enforce on Build \u0026gt; Only fail for build lifecycles.\nLeave Deployment disabled for now. A good practice would be to enable it, but for other steps in this series, it will be required to keep it disabled for now.\nLeave Runtime disabled.\nClick Next\nAs policy criteria find \u0026#34;Image Registry \u0026gt; Image signature\u0026#34; and drag and drop it to the policy section\nFigure 6. ACS Policy Criteria Click on Select and select the CoSign integration\nFigure 7. ACS Assign CoSign integration to policy Click Next\nLeave the Policy scope for now and click Next\nReview the Policy and Save it.\nPrepare the Pipeline Tasks Now, finally, after all these preparations we are going to integrate ACS into our Pipeline. For this, we will create 2 tasks:\nacs-image-scan: Will scan the image for vulnerabilities\nacs-image-check: Will verify if the image would violate any security policy, especially the custom policy we have created.\nBoth checks will use the roxctl command line tool.\nIt is recommended to add these two Tasks to any Pipeline system you are using when you have ACS installed to leverage the full potential of ACS. It does not matter if you are using Tekton, Jenkins or something else. You can find examples of integrations at the Git repository: https://github.com/stackrox/contributions/. Task acs-image-scan To scan for vulnerabilities, create the following Task. It leverages the command line tool roxctl which can also be used on your local machine to perform such scans manually.\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: acs-image-scan namespace: ci spec: description: \u0026gt;- Scan an image with StackRox/RHACS. This tasks allows you to check an image against vulnerabilities. params: - description: | Secret containing the address:port tuple for StackRox Central) (example - rox.stackrox.io:443) name: rox_central_endpoint type: string - description: Secret containing the StackRox API token with CI permissions name: rox_api_token type: string - description: | Full name of image to scan (example -- gcr.io/rox/sample:5.0-rc1) name: image type: string - default: \u0026#39;false\u0026#39; description: | When set to `\u0026#34;true\u0026#34;`, skip verifying the TLS certs of the Central endpoint. Defaults to `\u0026#34;false\u0026#34;`. name: insecure-skip-tls-verify type: string - default: \u0026#39;registry.access.redhat.com/ubi9@sha256:089bd3b82a78ac45c0eed231bb58bfb43bfcd0560d9bba240fc6355502c92976\u0026#39; name: ubi9 type: string results: - description: Output of `roxctl image check` name: check_output type: string steps: - env: (1) - name: ROX_API_TOKEN valueFrom: secretKeyRef: key: rox_api_token name: $(params.rox_api_token) - name: ROX_CENTRAL_ENDPOINT valueFrom: secretKeyRef: key: rox_central_endpoint name: $(params.rox_central_endpoint) image: $(params.ubi9) name: rox-image-scan resources: {} script: | (2) #!/usr/bin/env bash set +x curl -s -k -L -H \u0026#34;Authorization: Bearer $ROX_API_TOKEN\u0026#34; \\ \u0026#34;https://$ROX_CENTRAL_ENDPOINT/api/cli/download/roxctl-linux\u0026#34; \\ --output ./roxctl \\ \u0026gt; /dev/null chmod +x ./roxctl \u0026gt; /dev/null if [ \u0026#34;$(params.insecure-skip-tls-verify)\u0026#34; = \u0026#34;true\u0026#34; ]; then export ROX_INSECURE_CLIENT_SKIP_TLS_VERIFY=true fi ./roxctl image scan -e \u0026#34;$ROX_CENTRAL_ENDPOINT\u0026#34; --image \u0026#34;$(params.image)\u0026#34; (3) 1 Token end endpoint which we defined as Secrets 2 Script that downloads and executes the CLI roxctl 3 Calling \u0026#34;roxctl image scan\u0026#34; Task Image Check To verify the policies that are configured in ACS we create the Task acs-image-check. We will use the command line tool roxctl again.\nACS comes with several (80+) build-in policies. Some of them are activated, but none of them configured to enforce a policy. Except, the policy Trusted Signature Policy we have created above. apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: acs-image-check namespace: ci spec: description: \u0026gt;- Policy check an image with StackRox/RHACS This tasks allows you to check an image against build-time policies and apply enforcement to fail builds. It\u0026#39;s a companion to the stackrox-image-scan task, which returns full vulnerability scan results for an image. params: - description: | Secret containing the address:port tuple for StackRox Central) (example - rox.stackrox.io:443) name: rox_central_endpoint type: string - description: Secret containing the StackRox API token with CI permissions name: rox_api_token type: string - description: | Full name of image to scan (example -- gcr.io/rox/sample:5.0-rc1) name: image type: string - default: \u0026#39;false\u0026#39; description: | When set to `\u0026#34;true\u0026#34;`, skip verifying the TLS certs of the Central endpoint. Defaults to `\u0026#34;false\u0026#34;`. name: insecure-skip-tls-verify type: string - default: \u0026#39;registry.access.redhat.com/ubi9@sha256:089bd3b82a78ac45c0eed231bb58bfb43bfcd0560d9bba240fc6355502c92976\u0026#39; name: ubi9 type: string results: - description: Output of `roxctl image check` name: check_output type: string steps: - env: (1) - name: ROX_API_TOKEN valueFrom: secretKeyRef: key: rox_api_token name: $(params.rox_api_token) - name: ROX_CENTRAL_ENDPOINT valueFrom: secretKeyRef: key: rox_central_endpoint name: $(params.rox_central_endpoint) image: $(params.ubi9) name: rox-image-check resources: {} script: | #!/usr/bin/env bash (2) set +x curl -s -k -L -H \u0026#34;Authorization: Bearer $ROX_API_TOKEN\u0026#34; \\ \u0026#34;https://$ROX_CENTRAL_ENDPOINT/api/cli/download/roxctl-linux\u0026#34; \\ --output ./roxctl \\ \u0026gt; /dev/null chmod +x ./roxctl \u0026gt; /dev/null if [ \u0026#34;$(params.insecure-skip-tls-verify)\u0026#34; = \u0026#34;true\u0026#34; ]; then export ROX_INSECURE_CLIENT_SKIP_TLS_VERIFY=true fi ./roxctl image check -e \u0026#34;$ROX_CENTRAL_ENDPOINT\u0026#34; --image \u0026#34;$(params.image)\u0026#34; (3) 1 Token end endpoint which we defined as Secrets 2 Script that downloads and executes the CLI roxctl 3 Calling \u0026#34;roxctl image check\u0026#34; Extend the Pipeline Now it is time to extend our Pipeline with the two tasks. Add the following blocks to the Pipeline secure-supply-chain:\n- name: acs-image-scan (1) params: (2) - name: rox_central_endpoint value: stackrox-endpoint - name: rox_api_token value: stackrox-secret - name: image value: \u0026#39;$(params.IMAGE_REPO):$(params.IMAGE_TAG)\u0026#39; - name: insecure-skip-tls-verify (3) value: \u0026#39;true\u0026#39; runAfter: (4) - build-sign-image taskRef: kind: Task name: acs-image-scan - name: acs-image-check (5) params: (6) - name: rox_central_endpoint value: stackrox-endpoint - name: rox_api_token value: stackrox-secret - name: image value: \u0026#39;$(params.IMAGE_REPO):$(params.IMAGE_TAG)\u0026#39; - name: insecure-skip-tls-verify (7) value: \u0026#39;true\u0026#39; runAfter: - build-sign-image (8) taskRef: kind: Task name: acs-image-check 1 Scanning for vulnerabilities 2 Provide parameters to the Task 3 Set to \u0026#39;false\u0026#39; if you have valid certificates. 4 Run after the Task \u0026#34;build-sign-image\u0026#34; 5 Scanning security policies 6 Provide parameters to the Task 7 Set to \u0026#39;false\u0026#39; if you have valid certificates. 8 Run after the Task \u0026#34;build-sign-image\u0026#34; The Pipeline will now look like this:\nFigure 8. Pipeline Details Execute the Pipeline Let’s trigger another PipelineRun by updating the README.md of our source code. While the previous Tasks should run successfully, let’s monitor our two ACS tasks. However, now the Pipeline is running quite long already. This means it is time for a coffee.\nStill here? Good, eventually our two ACS tasks have finished successfully.\nThe Task image-scan is looking for vulnerabilities and did not find any high-severity issues.\nFigure 9. ACS Image Scan Result The Task image-check verified the security policies. You can see in the Logs that the *Trusted Signature Policy was not violated. Another might be shown, but since all other policies are configured to notify only (instead of \u0026#34;enforce\u0026#34;), the whole Task will end successfully. This is good enough for our first tests. The important bit is that the image has been signed.\nFigure 10. ACS Image Check Result Summary Now we have integrated ACS and are using its powers to scan images for vulnerabilities and check them against security policies. Two Tasks have been created that are executed after the image has been built and are leveraging ACS’s command line tool roxctl\nWhenever vulnerabilities are found, or security policies are violated (if the policy is set to enforce) the Pipeline will fail.\nSuch tasks should be built in any pipeline you are creating, independently of using ACS or any other security verification tool.\nIn the next step, we will create a Task that creates a Software Bill of Material (SBOM) for us.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-22-securesupplychain-step7/","title":"Step 7 - Generating a SBOM","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","SBOM"],"description":"Step 7 Secure Supply Chain","content":" A software bill of materials (SBOM) offers an insight into the makeup of an application developed using third-party commercial tools and open-source software. It is a machine-readable, formally structured list of all components. This list is important to gain visibility into what risks might exist in a package and their potential impact. It allows to prioritize the risks and define a remediation path. The teams can use the SBOM to monitor all components that are used across an application.\nWe will create a Task that will generate a SBOM and uploads it into an SBOM repository.\nGoals The goals of this step are:\nInstall a SBOM repository.\nGenerate a SBOM and upload it to the repository.\nTool/Standard CycloneDX OWASP CycloneDX is a full-stack Bill of Materials (BOM) standard that provides advanced supply chain capabilities for cyber risk reduction. The specification supports:\nSoftware Bill of Materials (SBOM)\nSoftware-as-a-Service Bill of Materials (SaaSBOM)\nHardware Bill of Materials (HBOM)\nOperations Bill of Materials (OBOM)\nVulnerability Disclosure Reports (VDR)\nVulnerability Exploitability eXchange (VEX)\nStrategic direction of the specification is managed by the CycloneDX Core Working Group, is backed by the OWASP Foundation, and is supported by the global information security community.\nSource: https://cyclonedx.org/ The CycloneDX module for Node.js creates a valid CycloneDX Software Bill-of-Materials (SBOM) containing an aggregate of all project dependencies. CycloneDX is a lightweight SBOM specification that is easily created, human and machine-readable, and simple to parse.\nhttps://cyclonedx.org/capabilities/sbom/ We will use this standard and will deploy a small Pod in our cluster to be able to upload our SBOM locally.\nInstall CycloneDX Repository Server We can install a CycloneDX Repository Server, which is a simple Pod running in our cluster. You can use the following Argo CD application, which will create the Namespace (cyclonedx),Deployment, Route, and Service.\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: in-cluster-install-cyclonedx namespace: openshift-gitops spec: destination: namespace: cyclonedx server: \u0026#39;https://kubernetes.default.svc\u0026#39; info: - name: Description value: Install CycloneDX SBOM Repository project: in-cluster source: path: clusters/management-cluster/cyclonedx (1) repoURL: \u0026#39;https://github.com/tjungbauer/openshift-clusterconfig-gitops\u0026#39; targetRevision: main 1 Path and URL to the Git repository. In Argo CD the following Application will be created:\nFigure 1. CycloneDX Installation Update the Tekton Pipeline Update the TriggerBinding globex-ui and add the following:\n- name: cyclonedxHostUrl value: \u0026gt;- https://cyclonedx-bom-repo-server-cyclonedx.apps.ocp.aws.ispworld.at (1) 1 DO NOT add a trailing slash here. Update the TriggerTemplate and add the following to the appropriate blocks:\nspec: parames: ... - description: Cyclonedx host url name: cyclonedxHostUrl (1) resourcetemplates: ... spec: params: ... - name: CYCLONEDX_HOST_URL value: $(tt.params.cyclonedxHostUrl) (2) 1 The parameter defined in the TriggerBinding 2 The parameter as it will be provided to the Pipeline. Install the following Task\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: generate-sbom namespace: ci spec: params: (1) - default: \u0026gt;- https://cyclonedx-bom-repo-server-cyclonedx.apps.placeholder.com/ name: cyclonedxHostUrl type: string - default: \u0026#39;cyclonedx/cyclonedx-node\u0026#39; name: cycloneDXnodeImage type: string results: - description: The url location of the generate SBOM name: sbomUrl type: string steps: - image: $(params.cycloneDXnodeImage) name: generate-sbom resources: requests: memory: 1Gi script: \u0026gt; (2) apk add git curl npm install . /usr/src/cyclonedx-bom/bin/make-bom.js -o bom.xml curl -v -k $(params.cyclonedxHostUrl)/v1/bom -H \u0026#34;Content-Type: application/vnd.cyclonedx+xml; version=1.4\u0026#34; -H \u0026#34;Accept: */*\u0026#34; -d @bom.xml -D /tmp/header.txt LOCATION=$(cat /tmp/header.txt | grep location: | awk \u0026#39;{print $2}\u0026#39; | sed \u0026#39;s|http:|https:|g\u0026#39;) printf \u0026#34;%s\u0026#34; \u0026#34;$LOCATION\u0026#34; \u0026gt; \u0026#34;$(results.sbomUrl.path)\u0026#34; echo \u0026#34;SBOM URL accessible on Results of TaskRun $(context.taskRun.name)\u0026#34; workingDir: /workspace/repository workspaces: - name: repository 1 Default parameters for this task. Might be overwritten, by the EventListener 2 Script to be executed. It will download a script and upload the SBOM to our CycloneDX server. Update the Pipeline object\nspec: params: ... - name: CYCLONEDX_HOST_URL (1) type: string tasks: ... - name: generate-sbom (2) params: - name: cyclonedxHostUrl value: $(params.CYCLONEDX_HOST_URL) runAfter: - build-sign-image (3) taskRef: kind: Task name: generate-sbom workspaces: - name: repository workspace: shared-data 1 Add this to the parameter section 2 The new Task to generate a SBOM 3 Run after build-sign-image (and in parallel to the ACS tasks) Figure 2. Pipeline Execute the Pipeline Let’s trigger the pipeline again. If everything works well, the URL to the uploaded SBOM will be visible in the TaskRun log.\nFigure 3. Pipeline When you open this URL, you will get a huge list of components end dependencies.\nFor example, the following snippet shows that the angular component animations (version 13.2.6) is used.\n\u0026lt;components\u0026gt; \u0026lt;component type=\u0026#34;library\u0026#34; bom-ref=\u0026#34;pkg:npm/%40angular/animations@13.2.6\u0026#34;\u0026gt; \u0026lt;author\u0026gt;angular\u0026lt;/author\u0026gt; \u0026lt;group\u0026gt;@angular\u0026lt;/group\u0026gt; \u0026lt;name\u0026gt;animations\u0026lt;/name\u0026gt; \u0026lt;version\u0026gt;13.2.6\u0026lt;/version\u0026gt; \u0026lt;description\u0026gt;Angular - animations integration with web-animations\u0026lt;/description\u0026gt; \u0026lt;licenses\u0026gt; \u0026lt;license\u0026gt; \u0026lt;id\u0026gt;MIT\u0026lt;/id\u0026gt; \u0026lt;/license\u0026gt; \u0026lt;/licenses\u0026gt; \u0026lt;purl\u0026gt;pkg:npm/%40angular/animations@13.2.6\u0026lt;/purl\u0026gt; \u0026lt;externalReferences\u0026gt; \u0026lt;reference type=\u0026#34;website\u0026#34;\u0026gt; \u0026lt;url\u0026gt;https://github.com/angular/angular#readme\u0026lt;/url\u0026gt; \u0026lt;/reference\u0026gt; \u0026lt;reference type=\u0026#34;issue-tracker\u0026#34;\u0026gt; \u0026lt;url\u0026gt;https://github.com/angular/angular/issues\u0026lt;/url\u0026gt; \u0026lt;/reference\u0026gt; \u0026lt;reference type=\u0026#34;vcs\u0026#34;\u0026gt; \u0026lt;url\u0026gt;git+https://github.com/angular/angular.git\u0026lt;/url\u0026gt; \u0026lt;/reference\u0026gt; \u0026lt;/externalReferences\u0026gt; \u0026lt;/component\u0026gt; Summary This concludes our security scans for the whole build process. With the SBOM you have now a complete list of components and dependencies of your images.\nDuring the next steps, we will work with the Kubernetes manifests and update them, perform a linting on these manifests, and try to deploy the updated image to our DEV environment.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-23-securesupplychain-step8/","title":"Step 8 - Updating Kubernetes Manifests","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain"],"description":"Step 8 Secure Supply Chain","content":" With the finalization of the build process and the security checks during this phase, it is now time to update the Kubernetes manifests and provide the new tag for the created image. I have forked (and cleaned up) another repository that will store all yaml specifications that are required for OpenShift. You can find this repository at: Kubernetes Manifests\nGoals The goals of this step are:\nUpdate the manifest in Git with the new image tag\nPreparing the Pipeline Let’s create the Task object that will take care of the update.\nThe Task will use git commands to update the manifests accordingly.\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: update-manifest namespace: ci spec: description: \u0026gt;- This task updates the manifest for the current application to point to the image tag created with the short commit. params: (1) - description: Used to tag the built image. name: image type: string - default: main description: Target branch to push to name: target-branch type: string - default: Tekton Pipeline description: Git user name for performing the push operation. name: git_user_name type: string - default: tekton@tekton.com description: Git user email for performing the push operation. name: git_user_email type: string - description: File in which the image configuration is stored. name: configuration_file type: string - description: Repo in which the image configuration is stored. name: repository type: string - default: \u0026#39;registry.redhat.io/openshift-pipelines/pipelines-git-init-rhel8:v1.10.4-4\u0026#39; name: gitInit type: string - name: verbose description: Verbose output type: string default: \u0026#34;true\u0026#34; steps: (2) - image: $(params.gitInit) name: git env: - name: PARAM_VERBOSE value: $(params.verbose) resources: {} script: \u0026gt; #!/usr/bin/env sh set -eu if [ \u0026#34;${PARAM_VERBOSE}\u0026#34; = \u0026#34;true\u0026#34; ] ; then set -x fi # Setting up the git config. git config --global user.email \u0026#34;$(params.git_user_email)\u0026#34; git config --global user.name \u0026#34;$(params.git_user_name)\u0026#34; # Checkout target branch to avoid the detached HEAD state TMPDIR=$(mktemp -d) cd $TMPDIR git clone $(params.repository) cd securing-software-supply-chain git checkout $(params.target-branch) # Set to the short commit value passed as parameter. # Notice the enclosing \u0026#34; to keep it as a string in the resulting YAML. IMAGE=\\\u0026#34;$(params.image)\\\u0026#34; sed -i \u0026#34;s#\\(.*value:\\s*\\).*#\\1 ${IMAGE}#\u0026#34; $(params.configuration_file) git add $(params.configuration_file) git commit -m \u0026#34;Automatically updated manifest to point to image tag $IMAGE\u0026#34; git push origin $(params.target-branch) 1 Required default parameters. 2 Script to clone and push the update to Git. To the Pipeline we have to add the following section\nspec: params: (1) - name: MANIFEST_FILE type: string - name: MANIFEST_FILE_PROD type: string - name: MANIFEST_REPO type: string - name: MANIFEST_REPO_NAME type: string - name: MANIFEST_GIT_REF type: string ... tasks: ... - name: update-dev-manifest params: - name: image value: \u0026#39;$(params.IMAGE_REPO):$(params.IMAGE_TAG)\u0026#39; - name: configuration_file value: $(params.MANIFEST_FILE) - name: repository value: $(params.MANIFEST_REPO) - name: git_user_name value: $(params.COMMIT_AUTHOR) runAfter: (2) - acs-image-check - acs-image-scan - generate-sbom taskRef: kind: Task name: update-manifest 1 Parameters that define the settings for the manifest repository 2 This time we are running after these three tasks Update the TriggerBinding globex-ui\n- name: manifestRepo value: git@github.com:tjungbauer/securing-software-supply-chain.git (1) - name: manifestRepoRef value: main - name: manifestFile (2) value: application/globex/overlays/dev/kustomization.yaml - name: manifestFileProd (3) value: application/globex/overlays/prod/kustomization.yaml - name: manifestRepoName value: tjungbauer/securing-software-supply-chain 1 The SSH url to GitHub 2 The overlay that will be used for the DEV environment 3 The overlay that will be used for the PROD environment Update the TriggerTemplate and add:\nspec: params: - description: The file to update to point to newly built image name: manifestFile - description: The file to update to point to newly built image in prod name: manifestFileProd - description: The repo to update to point to newly built image name: manifestRepo - description: The reference to the repo name: manifestRepoRef - description: The full name of the repo name: manifestRepoName ... resourcetemplates: ... spec: params: - name: MANIFEST_FILE value: $(tt.params.manifestFile) - name: MANIFEST_FILE_PROD value: $(tt.params.manifestFileProd) - name: MANIFEST_REPO value: $(tt.params.manifestRepo) - name: MANIFEST_GIT_REF value: $(tt.params.manifestRepoRef) - name: MANIFEST_REPO_NAME value: $(tt.params.manifestRepoName) ... Git SSH Key To be able to do changes in Git, which is using SSH keys to authenticate, we need to specify a Secret that knows the SSH private key and Github’s host keys.\nBe sure that to create an SSH key (for example using ssh-keygen) and that GitHub knows about this key. You can add your key at \u0026#34;Account \u0026gt; Settings \u0026gt; SSH and GPG keys\u0026#34;.\nRun the following command to get the host keys of GitHub:\nssh-keyscan -H github.com # github.com:22 SSH-2.0-babeld-c89ab1f3 # github.com:22 SSH-2.0-babeld-c89ab1f3 |1|cr4dkqIl2SnZqktvRsFnx1lA5Ag=|eie8EHXqQHgCZJq7F/TtRRZcBbc= ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk= # github.com:22 SSH-2.0-babeld-c89ab1f3 |1|r6z4yeEV9Cog/2I4stZp1A34BAE=|EU8VcdD+KHMJJt9uPL4jh5zK0fI= ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg= # github.com:22 SSH-2.0-babeld-c89ab1f3 |1|hVnTyBzb/gSR8jpz+NUziUkHy1A=|ENMDVCVsLfz2CWLa1C+BnzZI8Yg= ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl # github.com:22 SSH-2.0-babeld-c89ab1f3 Finally, we need to create a Secret with the following content:\napiVersion: v1 kind: Secret metadata: annotations: tekton.dev/git-0: github.com (1) name: git-ssh-key namespace: ci type: kubernetes.io/ssh-auth data: ssh-privatekey: \u0026lt;BASE64 PRIVAT SSH KEY \u0026gt; (2) known_hosts: \u0026lt; BASE64 of Githubs hostkey\u0026gt; (3) 1 This annotation defines for which host Tekton will inject the Secret. 2 A base64 decoded string of your PRIVATE ssh key. 3 A base64 decoded string of the host keys Update the pipeline ServiceAccount and add the new Secret:\nsecrets: ... - name: git-ssh-key Execute the Pipeline Time to trigger the Pipeline, which now looks like:\nFigure 1. Pipeline Details The new Task will automatically push the new image tag to the Kubernetes manifests Git repository. From there it can later be used to roll out the new version.\nIn Git you can track the changes:\nFigure 2. Git Commit Summary We have now updated the Kubernetes manifests with the new image tag. In the next steps, we will verify these manifests (linting) and then try to deploy the new image on the DEV environment.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-24-securesupplychain-step9/","title":"Step 9 - Linting Kubernetes Manifests","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","Linting"],"description":"Step 9 Secure Supply Chain","content":" At this point we have checked our source code, verified that it has been signed and has no vulnerabilities, generated a SBOM and updated the Kubernetes manifests, that are responsible to deploy our application on OpenShift. As everything with OpenShift these Kubernetes objects are simple yaml files. In this step we will perform a linting on these files, to verify if they follow certain rules and best practices.\nGoals The goals of this step are:\nClone the Kubernetes manifest into a workspace\nCreate Tasks that perform the linting\nWARN when the manifests do not follow best practices (we will not let the Task fail here, because I would need to modify all manifests)\nWhat is linting A linter is an analysis tool that verifies if your code has any errors, bugs or stylistic errors. SonarQube could be seen as such tool. We used it to scan our source code. For Kubernetes manifests we can use tools that check the yaml files against best practices or security. For example, it could notify you if you forgot to define resources or probes in your manifests.\nTools In this step, I will use three linting tools. In no way, this means you need to use all three. I only use them for demonstration purposes. However, to lint your yaml files or Helm charts is a common practice you should consider.\nFor this demonstration I am leveraging:\nKubeLinter\nYamllint\nKube-score\nIn the end, you should choose the tool that fits best for you. Manifests The manifest we are going to use can be found at my GitHub repository: Securing Software Supply Chain. It is a fork and the original can be found here.\nPrepare Pipeline Modify the TriggerTemplate and add a parameter LINTING_INFORM_ONLY that either sets the Tasks to inform or enforce (failing when linting does find issues). In addition, we define a new workspace, where we will download and store the Kubernetes manifests.\nspec: params: ... - description: Only inform on linting errors (Log) but do not actually fail name: lintingInformOnly (1) resourcetemplates: ... spec: params: ... - name: LINTING_INFORM_ONLY value: $(tt.params.lintingInformOnly) ... workspaces: ... - name: shared-data-manifests (2) volumeClaimTemplate: metadata: creationTimestamp: null spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi status: {} 1 New parameter to either inform only when linting is unsuccessful or to enforce that the Task will end with an error. 2 Workspace to download and store the yaml files. Update the TriggerBinding to set the values for the PipelineRun.\nspec: params: ... - name: lintingInformOnly (1) value: \u0026#39;true\u0026#39; 1 Set the parameter to \u0026#39;true\u0026#39; Update the Pipeline object. Here we will need to add the parameter and four Tasks (clone the Git Repo, KubeLinter, Yamllint and kube-score) as well as the workspace.\nspec: params: ... - name: LINTING_INFORM_ONLY (1) type: string ... - name: pull-manifests (2) params: - name: url value: $(params.MANIFEST_REPO) - name: revision value: $(params.MANIFEST_GIT_REF) - name: deleteExisting value: \u0026#39;true\u0026#39; runAfter: - update-dev-manifest (3) taskRef: kind: ClusterTask (4) name: git-clone workspaces: (5) - name: output workspace: shared-data-manifests - name: kube-linter (6) params: - name: informLintingOnly (7) value: $(params.LINTING_INFORM_ONLY) runAfter: - pull-manifests (8) taskRef: kind: Task name: kube-linter workspaces: - name: repository workspace: shared-data-manifests - name: kube-score (9) params: - name: informLintingOnly value: $(params.LINTING_INFORM_ONLY) runAfter: - pull-manifests (10) taskRef: kind: Task name: kube-score workspaces: - name: repository (11) workspace: shared-data-manifests - name: yaml-lint (12) params: - name: informLintingOnly value: $(params.LINTING_INFORM_ONLY) runAfter: - pull-manifests (13) taskRef: kind: Task name: yaml-lint workspaces: - name: repository (14) workspace: shared-data-manifests workspaces: ... - name: shared-data-manifests 1 New parameter assigned to the Pipeline. 2 Task to clone the repository to the workspace. 3 Will run after the Pipeline has updated the manifests with the new image. 4 Is a child of the ClusterTask git-clone. 5 The workspace to clone the repository. 6 Task to execute KubeLinter. 7 Parameter to either enforce or inform only. 8 Will run after the repository has been cloned. 9 Task to execute kube-score. 10 Will run after the repository has been cloned. 11 Workspace where the cloned repository can be found. 12 Task to execute Yamllint. 13 Will run after the repository has been cloned. 14 Workspace where the cloned repository can be found. Remember: It is not required to execute three different linter tools. It is only done as a showcase. I personally like KubeLinter. Choose whatever tool is suitable for you. Create the different Task objects for the linter tools. Each Task will execute a linter program and provides its very own Log.\nI have created the image linter-image that contains the three required binaries. It is available at Quay.io and its original Dockerfile can be found here. Use it at your own risk :). KubeLinter\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: kube-linter namespace: ci spec: description: \u0026gt;- Task to run KubeLinter and perform a linting of Kubernetes manifets. params: - default: \u0026#39;false\u0026#39; name: informLintingOnly type: string - default: \u0026#39;quay.io/tjungbau/linter-image:v1.0.2\u0026#39; name: linterImage type: string steps: - image: $(params.linterImage) name: kube-linter resources: {} script: \u0026gt; #!/usr/bin/env bash RC=0 kube-linter lint /workspace/repository/. --config \u0026#34;/workspace/repository/.kube-linter.yaml\u0026#34; (1) if [ $? -gt 0 ]; then RC=1 fi # We actually do not fail but inform only if [ \u0026#34;$(params.informLintingOnly)\u0026#34; = \u0026#34;true\u0026#34; ]; then echo \u0026#34;Informing only, task will not fail. Actual return code was $RC\u0026#34; exit 0; fi (exit $RC) workingDir: /workspace/repository workspaces: - name: repository 1 Execute kube-linter using the configuration stored in the repository. kube-score\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: kube-score namespace: ci spec: description: \u0026gt;- Task to run kube-score and perform a linting of Kubernetes manifets. params: - default: \u0026#39;false\u0026#39; name: informLintingOnly type: string - default: \u0026#39;quay.io/tjungbau/linter-image:v1.0.2\u0026#39; name: linterImage type: string steps: - image: $(params.linterImage) name: kube-linter resources: {} script: \u0026gt; #!/usr/bin/env bash RC=0 KUBESCORE_IGNORE_TESTS=\u0026#34;${KUBESCORE_IGNORE_TESTS:-container-image-pull-policy,pod-networkpolicy}\u0026#34; (1) for i in `find . -name \u0026#39;*.yaml\u0026#39; -type f`; do kube-score score --ignore-test ${KUBESCORE_IGNORE_TESTS} $i; let RC=RC+$?; done if [ $? -gt 0 ]; then RC=1 fi # We actually do not fail but inform only if [ \u0026#34;$(params.informLintingOnly)\u0026#34; = \u0026#34;true\u0026#34; ]; then echo \u0026#34;Informing only, task will not fail. Actual return code was $RC\u0026#34; exit 0; fi (exit $RC) workingDir: /workspace/repository workspaces: - name: repository 1 Disable checks for Network Policies or image Pull policy for kube-score. Yammllint\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: yaml-lint namespace: ci spec: description: \u0026gt;- Task to run yamllint and perform a linting of Kubernetes manifets. params: - default: \u0026#39;false\u0026#39; name: informLintingOnly type: string - default: \u0026#39;quay.io/tjungbau/linter-image:v1.0.2\u0026#39; name: linterImage type: string steps: - image: $(params.linterImage) name: yaml-lint resources: {} script: | #!/usr/bin/env bash for files in `find . -type f -name \u0026#39;*.yaml\u0026#39;`; do (1) yamllint -c /workspace/repository/.yamllint.yaml ${files}; let var=var+$? done # We actually do not fail but inform only if [ \u0026#34;$(params.informLintingOnly)\u0026#34; = \u0026#34;true\u0026#34; ]; then echo \u0026#34;Informing only, task will not fail. Actual return code was $var\u0026#34; exit 0; fi (exit $var) workingDir: /workspace/repository workspaces: - name: repository 1 Execute kube-linter using the configuration stored in the repository. Execute the Pipeline The Pipeline now looks like this:\nFigure 1. Pipeline Details Remember, you typically need only one linter tool, not three different ones. Since we inform only you will see some errors in the logs. For example, for kube-linter:\nFigure 2. Kube-Linter Results Summary Now, all our yaml manifests have been linted, with three different tools. And because we do not fail at this stage, we can continue. The next steps will be some deployment checks.\nSince everything is done using Argo CD and the manifests have been updated during the step \u0026#34;update-manifest\u0026#34;, the changes will be most likely already deployed. Even if the linting-step comes later and might even fail. This is fine because we first deploy on a DEV environment. So, if linting fails, it will prohibit the rollout to production, while some application testing can still be done on DEV. "},{"uri":"https://blog.stderr.at/tags/supply-chain/","title":"Supply Chain","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/supply-chain/","title":"Supply Chain","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/tekton/","title":"Tekton","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-25-securesupplychain-step10/","title":"Step 10 - The Example Application","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","Linting"],"description":"Step 10 Secure Supply Chain","content":" If you read all articles up to here (congratulations) you know that we always update the README file of \u0026#34;The Application\u0026#34;. But what is this application exactly and how can we update it like in a real-life example? The Globex UI application is built with Angular and was prepared for this journey. It is a quite complex application and requires Kafka to be installed as well. However, since I am not a developer and this tool was already available, I forked and re-used it. The original can be found at Globex UI\nGoals The goals of this step are:\nDeploy AMQ Streams Operator\nDeploy Globex DEV\nDeploy Globex Prod\nVerify Deployments\nIntroduction The Globex UI, although quite complex, will provide a simple web interface. Since I do not have multiple clusters, I will use the Namespaces globex-dev and globex-prod to distinguish between the two environments. This means, our DEV environment will run inside globex-dev and PROD in globex-prod. Kafka related stuff will be deployed in the Namespace kafka.\nFigure 1. Globex UI The deployment will be done automatically by GitOps. As soon as our Pipeline will be executed and subsequently updates our image in the Kubernetes manifests (see step: Linting Kubernetes Manifests), a new version of Globex UI will be deployed. The production update will follow in later steps in the pipeline.\nGitOps will monitor changes in our Kubernetes manifest repository. Here, we use Kustomize overlays to separate between DEV and PROD.\nAs a general recommendation: Do everything with GitOps. If it is not in Git, it does not exist. Installation To install all required components, we create several GitOps Applications.\nAlthough we create these Argo CD Applications by hand, it is recommended to put these definitions into Git also. Since this is just a demo, I am using the cluster instance of OpenShift GitOps. DO NOT deploy customer workload using the default instance, since it has privileged permissions on the cluster. Instead, create a second instance (or multiple) for the developers. The following Applications will be created:\nFigure 2. GitOps Applications Install AMQ Streams Operator As the very first step, we need to deploy the AMQ Streams Operator. Simply search for the operator in the Operator Hub and deploy it using the default values.\nAlso, this deployment could and should be done using Argo CD. For the sake of speed, I have done this manually here. Figure 3. AMQ Streams Operator Install Kafka for Globex Now we can deploy all Kafka-related stuff using GitOps. Let’s create the following Application object:\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: globex-kafka namespace: openshift-gitops spec: destination: namespace: kafka (1) server: \u0026#39;https://kubernetes.default.svc\u0026#39; (2) project: default source: path: application/kafka/overlays/dev (3) repoURL: \u0026#39;https://github.com/tjungbauer/securing-software-supply-chain\u0026#39; targetRevision: HEAD syncPolicy: automated: (4) selfHeal: true syncOptions: - RespectIgnoreDifferences=true - CreateNamespace=true 1 The deployment must be done in the Namespace kafka 2 We install using the local GitOps server instance. 3 Path/URL to the GitHub Kustomize repository. 4 Auto-Update in case of changes are detected. Install Globex DEV The DEV installation will deploy the test version of our application into the Namespace globex-dev\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: globex-dev (1) namespace: openshift-gitops spec: destination: namespace: globex-dev (2) server: \u0026#39;https://kubernetes.default.svc\u0026#39; (3) ignoreDifferences: - group: \u0026#39;*\u0026#39; jqPathExpressions: - \u0026#39;.imagePullSecrets[] | select(.name|test(\u0026#34;.-dockercfg-.\u0026#34;))\u0026#39; kind: ServiceAccount project: default source: path: application/globex/overlays/dev (4) repoURL: \u0026#39;https://github.com/tjungbauer/securing-software-supply-chain\u0026#39; targetRevision: HEAD syncPolicy: automated: (5) selfHeal: true syncOptions: - RespectIgnoreDifferences=true - CreateNamespace=true (6) 1 Globex DEV instance 2 Install into the Namespace globex-dev 3 We install using the local GitOps server instance. 4 Path/URL to the GitHub Kustomize repository. 5 Auto-Update in case of changes are detected. 6 Create the Namespace if it does not exist. Install Globex PROD And finally, the same for the PROD instance:\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: globex-prod (1) namespace: openshift-gitops spec: destination: namespace: globex-prod (2) server: \u0026#39;https://kubernetes.default.svc\u0026#39; (3) ignoreDifferences: - group: \u0026#39;*\u0026#39; jqPathExpressions: - \u0026#39;.imagePullSecrets[] | select(.name|test(\u0026#34;.-dockercfg-.\u0026#34;))\u0026#39; kind: ServiceAccount project: default source: path: application/globex/overlays/prod (4) repoURL: \u0026#39;https://github.com/tjungbauer/securing-software-supply-chain\u0026#39; targetRevision: HEAD syncPolicy: automated: (5) selfHeal: true syncOptions: - RespectIgnoreDifferences=true - CreateNamespace=true (6) 1 Globex DEV instance 2 Install into the Namespace globex-prod 3 We install using the local GitOps server instance. 4 Path/URL to the GitHub Kustomize repository. 5 Auto-Update in case of changes are detected. 6 Create the Namespace if it does not exist. Summary During this step we have added nothing new to our pipeline, but deployed our example application Globex UI instead, including Kafka. The next steps will now do another verification against ACS and if the transparency logs are available, and then finally prepares everything for production deployment.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-26-securesupplychain-step11/","title":"Step 11 - ACS Deployment Check","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","ACS"],"description":"Step 11 Secure Supply Chain","content":" After the Pipeline prepared the new image for DEV it should be checked against ACS for policy compliance. This ensures that the deployment manifest adheres to policy requirements. The command line tool roxctl will be leveraged to perform this task.\nGoals The goals of this step are:\nCreate a Task that performs a deployment check using roxctl\nCreate the Task Create the following Task object. This Task is a bit more complex, since we want to verify the Deployment, we first need to build the Kustomize files which would create a big yaml file and will store it into the output folder (all.yaml). Then we use kubesplit to split this huge file into different smaller ones. Finally, the roxctl step will search for a file called deployment—​globex-ui.yml and performs the security checks (verify if any security policy is violated in ACS) against this file.\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: acs-deploy-check namespace: ci spec: description: \u0026gt;- Policy check a deployment with StackRox/RHACS This tasks allows you to check a deployment against build-time policies and apply enforcement to fail builds. It\u0026#39;s a companion to the stackrox-image-scan task, which returns full vulnerability scan results for an image. params: - description: | Secret containing the address:port tuple for StackRox Central) (example - rox.stackrox.io:443) name: rox_central_endpoint type: string - description: Secret containing the StackRox API token with CI permissions name: rox_api_token type: string - default: \u0026#39;https://default-git-url.git\u0026#39; name: gitRepositoryUrl type: string - default: main name: gitRepositoryRevision type: string - default: \u0026#39;true\u0026#39; name: verbose type: string - default: \u0026#39;false\u0026#39; description: | When set to `\u0026#34;true\u0026#34;`, skip verifying the TLS certs of the Central endpoint. Defaults to `\u0026#34;false\u0026#34;`. name: insecure-skip-tls-verify type: string - default: \u0026#39;quay.io/wpernath/kustomize-ubi\u0026#39; name: kustomieBuildImage type: string - default: \u0026#39;looztra/kubesplit\u0026#39; name: kubesplitImage type: string - default: \u0026#39;registry.access.redhat.com/ubi8:8.7-1026\u0026#39; name: ubi8Image type: string results: - description: Output of `roxctl deployment check` name: check_output type: string steps: - image: $(params.kustomieBuildImage) name: kustomize-build resources: {} script: | (1) #!/usr/bin/env sh set -eu -o pipefail cd /workspace/repository/application/globex/overlays/dev mkdir -p /workspace/repository/input mkdir -p /workspace/repository/output kustomize build . --output /workspace/repository/input/all.yaml workingDir: /workspace/repository - image: $(params.kubesplitImage) (2) name: kustomize-split resources: {} script: \u0026gt; #!/usr/bin/env sh set -eu -o pipefail kubesplit -i /workspace/repository/input/all.yaml -o /workspace/repository/output workingDir: /workspace/repository - env: - name: ROX_API_TOKEN valueFrom: secretKeyRef: key: rox_api_token name: $(params.rox_api_token) - name: ROX_CENTRAL_ENDPOINT valueFrom: secretKeyRef: key: rox_central_endpoint name: $(params.rox_central_endpoint) image: $(params.ubi8Image) name: rox-deploy-scan (3) resources: {} script: | #!/usr/bin/env bash set +x cd /workspace/repository/output curl -s -k -L -H \u0026#34;Authorization: Bearer $ROX_API_TOKEN\u0026#34; \\ \u0026#34;https://$ROX_CENTRAL_ENDPOINT/api/cli/download/roxctl-linux\u0026#34; \\ --output ./roxctl \\ \u0026gt; /dev/null chmod +x ./roxctl \u0026gt; /dev/null DEPLOYMENT_FILE=$(ls -1a | grep *deployment--globex-ui.yml) if [ \u0026#34;$(params.insecure-skip-tls-verify)\u0026#34; = \u0026#34;true\u0026#34; ]; then export ROX_INSECURE_CLIENT_SKIP_TLS_VERIFY=true fi ./roxctl deployment check -e \u0026#34;$ROX_CENTRAL_ENDPOINT\u0026#34; --file \u0026#34;$DEPLOYMENT_FILE\u0026#34; workingDir: /workspace/repository workspaces: - name: repository 1 Build Kustomize and store the output into /workspace/repository/input/all.yaml. 2 Split the huge yaml file into separate ones and store them into /workspace/repository/output. 3 Search for the file deployment—​globex-ui.yml and perform a roxctl deployment check. Update the Pipeline The Pipeline object must be extended with another Task:\n- name: acs-deploy-check params: - name: rox_central_endpoint (1) value: stackrox-endpoint - name: rox_api_token value: stackrox-secret - name: insecure-skip-tls-verify value: \u0026#39;true\u0026#39; runAfter: (2) - yaml-lint - kube-score - kube-linter taskRef: kind: Task name: acs-deploy-check workspaces: - name: repository workspace: shared-data-manifests (3) 1 The parameters required for ACS. 2 This task runs after the linting tasks. 3 The workspace, where the manifests have been pulled. Execute the Pipeline Again, we trigger our pipeline by simply updating the README.md of our source code.\nThe ACS check will verify if any security policies, that are valid for Deployment-states, are violated.\nMy test returned the following result.\nFigure 1. Pipeline Details However, since the policies are configured to \u0026#34;inform only\u0026#34;, the PipelineRun will finish successfully.\nSummary All ACS checks have been done now. The deployment does not violate any security policy that is configured in ACS …​ or to be more exact: It does not violate enforced policy, thus the Task will end successfully.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-27-securesupplychain-step12/","title":"Step 12 - Verify TLog Signature","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","Rekor"],"description":"Step 12 Secure Supply Chain","content":" Since the image has been deployed on DEV now, we need to prepare everything for production. Before we start, we need to confirm that the whole signing process has been passed and that our signature has been applied correctly. With CoSign we signed our image and used Rekor to create a transparency log. We will use this log to confirm that the image was signed.\nGoals The goals of this step are:\nVerify if the image has been signed using Rekor transparency log.\nCoSign public key Before we start, we need a secret in the Namespace ci that provides the PUBLIC key of CoSign. This key was generated in a previous Step 5 - CoSign - Signing the Image\nkind: Secret apiVersion: v1 metadata: name: cosign-secret (1) namespace: ci data: cosign.pub: \u0026gt;- \u0026lt;base64 CoSign PUBLIC key\u0026gt; type: Opaque 1 base64 decoded PUBLIC key of CoSign Create the Task The following task will use several steps to fetch and parse the transparency log, that has been created. The different tasks are using images from Redhat-gpte. The command line tool rekor-cli will be used to verify the entries in the log.\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: rekor-verify namespace: ci spec: params: - default: image-registry-secret (1) name: registrySecret type: string - default: cosign-secret (2) name: cosignSecret type: string - default: \u0026gt;- quay.io/placeholder/test:latest name: image type: string - default: \u0026#39;quay.io/tjungbau/pipeline-tools:v1.0.0\u0026#39; name: pipelinetoolsImage type: string steps: - env: - name: REGISTRY_SECRET valueFrom: secretKeyRef: key: .dockerconfigjson name: $(params.registrySecret) - name: COSIGN_PUBLIC_KEY valueFrom: secretKeyRef: key: cosign.pub name: $(params.cosignSecret) image: $(params.pipelinetoolsImage) name: cosign-verify-image resources: {} script: \u0026gt; mkdir -p /home/cosign/.docker/ echo \u0026#34;${REGISTRY_SECRET}\u0026#34; \u0026gt; /home/cosign/.docker/config.json echo \u0026#34;${COSIGN_PUBLIC_KEY}\u0026#34; \u0026gt; /workspace/cosign.pub cosign verify --key /workspace/cosign.pub $(params.image) --output-file /workspace/cosign.verify cat /workspace/cosign.verify | jq --raw-output \u0026#39;.[0] | .critical | .image | .[\u0026#34;docker-manifest-digest\u0026#34;]\u0026#39; \u0026gt; /workspace/cosign.sha rekor-cli search --sha $(cat /workspace/cosign.sha) --format json \u0026gt; /workspace/rekor.search cat /workspace/rekor.search | jq \u0026#39;.UUIDs[0]\u0026#39; | sed \u0026#39;s/\\\u0026#34;//g\u0026#39; \u0026gt; /workspace/rekor.uuid rekor-cli get --uuid $(cat /workspace/rekor.uuid) --format json \u0026gt; /workspace/rekor.get cat /workspace/rekor.get | jq -r .Attestation 1 Registry pull secret. 2 CoSign public key. Update the Pipeline The Pipeline object must be extended with another Task:\n- name: verify-tlog-signature params: (1) - name: registrySecret (2) value: tjungbau-secure-supply-chain-demo-pull-secret - name: cosignSecret (3) value: cosign-secret - name: image value: \u0026#39;$(params.IMAGE_REPO):$(params.IMAGE_TAG)\u0026#39; runAfter: (4) - yaml-lint - kube-score - kube-linter taskRef: kind: Task name: rekor-verify 1 The parameters required for this task. 2 Pull secret for quay.io, which was created during step 5. 3 CoSign public key. 4 This task runs after the linting tasks. Execute the Pipeline Triggering the pipeline will now check the signature of an image.\nThe logs should show something like this:\nFigure 1. Rekor Image Signature Verification As you can see: The image has been signed correctly and the transparency logs can be parsed.\nSummary Finally, everything has been verified and we can bring everything into production. The final two steps will create a new branch in the Git manifest repository and a pull request that can be manually merged.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-28-securesupplychain-step13/","title":"Step 13 - Bring it to Production","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","Rekor"],"description":"Step 13 Secure Supply Chain","content":" If you reached this article, congratulations! You read through tons of pages to build up a Pipeline. The last two steps in our Pipeline are: Creating a new branch and creating a pull request, with the changes of the image tag that must be approved and will be merged then (We will not update the main branch directly!). Finally, we will do a \u0026#34;real\u0026#34; update to the application to see the actual changes.\nGoals The goals of this step are:\nCreate a task that creates a new branch in Git repository.\nCreate a task that creates a pull request in Git repository.\nPerform a full End2End run of the Pipeline and approve the pull request.\nCreate a token at GitHub To create a pull request we need to authenticate against the Git api. For this, we will need a token. In GitHub open your personal settings (by clicking on your avatar) and then go to \u0026#34;Developer settings\u0026#34;.\nSelect \u0026#34;Personal access tokens\u0026#34; and \u0026#34;fine-graining tokens\u0026#34; to create a new token.\nFigure 1. GitHub Personal Access Token Enter the required fields:\nName: Secure Supply Chain\nExpiration: Whatever your like (max is 1 year)\nRepository access: Only select repositories\nSelect your repository, for example: tjungbauer/securing-software-supply-chain\nPermissions: \u0026#34;Pull requests\u0026#34; \u0026gt; Read and write\nFigure 2. GitHub Personal Access Token Created Save the created token and create the following secret:\nkind: Secret apiVersion: v1 metadata: name: github-token namespace: ci stringData: token: \u0026lt;Token Value\u0026gt; (1) type: Opaque 1 Clear-text token. If already base64 encoded, change stringData to data. Task: Create a new branch Since we do not update the main branch directly, we will create a new (feature) branch, that will be used for a pull request and can be deleted after the pull request has been merged.\nCreate the following Task object\nThis Task will use git commands to create a new feature-branch and pushes the changes into that branch.\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: new-branch-manifest-repo namespace: ci spec: description: \u0026gt;- This task creates a branch for a PR to point to the image tag created with the short commit. params: - description: Used to tag the built image. name: image type: string - default: main description: Target branch to push to name: target-branch type: string - default: Tekton Pipeline description: Git user name for performing the push operation. name: git_user_name type: string - default: tekton@tekton.com description: Git user email for performing the push operation. name: git_user_email type: string - description: File in which the image configuration is stored. name: configuration_file type: string - description: Repo in which the image configuration is stored. name: repository type: string - default: \u0026#39;registry.redhat.io/openshift-pipelines/pipelines-git-init-rhel8:v1.10.4-4\u0026#39; name: gitInit type: string steps: - image: $(params.gitInit) name: git resources: {} script: \u0026gt;- # Setting up the git config. git config --global user.email \u0026#34;$(params.git_user_email)\u0026#34; git config --global user.name \u0026#34;$(params.git_user_name)\u0026#34; # Checkout target branch to avoid the detached HEAD state TMPDIR=$(mktemp -d) cd $TMPDIR git clone $(params.repository) (1) cd securing-software-supply-chain git checkout -b $(params.target-branch) (2) # Set to the short commit value passed as parameter. # Notice the enclosing \u0026#34; to keep it as a string in the resulting YAML. IMAGE=\\\u0026#34;$(params.image)\\\u0026#34; sed -i \u0026#34;s#\\(.*value:\\s*\\).*#\\1 ${IMAGE}#\u0026#34; $(params.configuration_file) git add $(params.configuration_file) (3) git commit -m \u0026#34;Automatically updated manifest to point to image tag $IMAGE\u0026#34; git push origin $(params.target-branch) 1 Clone the main repository. 2 Create a new feature branch. 3 Add, commit, and push everything to the new branch. Modify the Pipeline object\nThe Task must be added to the Pipeline, it provides several required parameters.\n- name: create-prod-manifest-branch params: - name: image value: \u0026#39;$(params.IMAGE_REPO):$(params.IMAGE_TAG)\u0026#39; - name: configuration_file value: $(params.MANIFEST_FILE_PROD) - name: repository value: $(params.MANIFEST_REPO) - name: git_user_name value: $(params.COMMIT_AUTHOR) - name: target-branch value: feature-for-$(params.COMMIT_SHA) runAfter: - acs-deploy-check - verify-tlog-signature taskRef: kind: Task name: new-branch-manifest-repo Task: Create a Pull request Create the following Task object\nThe following task will take the token and create a new pull request at GitHub:\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: git-open-pull-request namespace: ci spec: description: \u0026gt;- This task will open a PR on Github based on several parameters. This could be useful in GitOps repositories for example. params: - default: api.github.com description: | The GitHub host, adjust this if you run a GitHub enteprise or Gitea name: GITHUB_HOST_URL type: string - default: \u0026#39;\u0026#39; description: | The API path prefix, GitHub Enterprise has a prefix e.g. /api/v3 name: API_PATH_PREFIX type: string - description: | The GitHub repository full name, e.g.: tektoncd/catalog name: REPO_FULL_NAME type: string - default: github description: \u0026gt; The name of the kubernetes secret that contains the GitHub token, default: github name: GITHUB_TOKEN_SECRET_NAME type: string - default: token description: \u0026gt; The key within the kubernetes secret that contains the GitHub token, default: token name: GITHUB_TOKEN_SECRET_KEY type: string - default: Bearer description: \u0026gt; The type of authentication to use. You could use the less secure \u0026#34;Basic\u0026#34; for example name: AUTH_TYPE type: string - description: | The name of the branch where your changes are implemented. name: HEAD type: string - description: | The name of the branch you want the changes pulled into. name: BASE type: string - description: | The body description of the pull request. name: BODY type: string - description: | The title of the pull request. name: TITLE type: string - default: \u0026#39;registry.access.redhat.com/ubi8/python-38:1\u0026#39; name: ubi8PythonImage type: string results: - description: Number of the created pull request. name: NUMBER type: string - description: URL of the created pull request. name: URL type: string steps: - env: - name: PULLREQUEST_NUMBER_PATH value: $(results.NUMBER.path) - name: PULLREQUEST_URL_PATH value: $(results.URL.path) image: $(params.ubi8PythonImage) name: open-pr resources: {} script: \u0026gt;- #!/usr/libexec/platform-python (1) \u0026#34;\u0026#34;\u0026#34;This script will open a PR on Github\u0026#34;\u0026#34;\u0026#34; import json import os import sys import http.client github_token = (2) open(\u0026#34;/etc/github-open-pr/$(params.GITHUB_TOKEN_SECRET_KEY)\u0026#34;, \u0026#34;r\u0026#34;).read() open_pr_url = \u0026#34;/repos/$(params.REPO_FULL_NAME)/pulls\u0026#34; data = { (3) \u0026#34;head\u0026#34;: \u0026#34;$(params.HEAD)\u0026#34;, \u0026#34;base\u0026#34;: \u0026#34;$(params.BASE)\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;\u0026#34;\u0026#34;$(params.TITLE)\u0026#34;\u0026#34;\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;\u0026#34;\u0026#34;$(params.BODY)\u0026#34;\u0026#34;\u0026#34; } print(\u0026#34;Sending this data to GitHub: \u0026#34;) print(data) authHeader = \u0026#34;Bearer \u0026#34; + github_token giturl = \u0026#34;api.\u0026#34;+\u0026#34;$(params.GITHUB_HOST_URL)\u0026#34; conn = http.client.HTTPSConnection(giturl) conn.request( \u0026#34;POST\u0026#34;, open_pr_url, body=json.dumps(data), headers={ \u0026#34;User-Agent\u0026#34;: \u0026#34;OpenShift Pipelines\u0026#34;, \u0026#34;Authorization\u0026#34;: authHeader.strip(), \u0026#34;Accept\u0026#34;: \u0026#34;application/vnd.github+json\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-GitHub-Api-Version\u0026#34;: \u0026#34;2022-11-28\u0026#34; }) resp = conn.getresponse() if not str(resp.status).startswith(\u0026#34;2\u0026#34;): print(\u0026#34;Error: %d\u0026#34; % (resp.status)) print(resp.read()) sys.exit(1) else: # https://docs.github.com/en/rest/reference/pulls#create-a-pull-request body = json.loads(resp.read().decode()) open(os.environ.get(\u0026#39;PULLREQUEST_NUMBER_PATH\u0026#39;), \u0026#39;w\u0026#39;).write(f\u0026#39;{body[\u0026#34;number\u0026#34;]}\u0026#39;) open(os.environ.get(\u0026#39;PULLREQUEST_URL_PATH\u0026#39;), \u0026#39;w\u0026#39;).write(body[\u0026#34;html_url\u0026#34;]) print(\u0026#34;GitHub pull request created for $(params.REPO_FULL_NAME): \u0026#34; f\u0026#39;number={body[\u0026#34;number\u0026#34;]} url={body[\u0026#34;html_url\u0026#34;]}\u0026#39;) volumeMounts: - mountPath: /etc/github-open-pr name: githubtoken readOnly: true volumes: - name: githubtoken secret: secretName: $(params.GITHUB_TOKEN_SECRET_NAME) 1 Python script to create the pull request. 2 The token from the secret object. 3 The data we will send to GitHub. Modify the Pipeline object\n- name: issue-prod-pull-request params: - name: GITHUB_HOST_URL value: $(params.REPO_HOST) - name: GITHUB_TOKEN_SECRET_NAME value: github-token - name: REPO_FULL_NAME value: $(params.MANIFEST_REPO_NAME) - name: HEAD value: feature-for-$(params.COMMIT_SHA) - name: BASE value: main - name: BODY value: Update prod image for $(params.COMMIT_MESSAGE) - name: TITLE value: \u0026#39;Production update: $(params.COMMIT_MESSAGE)\u0026#39; runAfter: - create-prod-manifest-branch taskRef: kind: Task name: git-open-pull-request Review the whole Pipeline We did it, we created a Secure Supply Chain using Tekton Tasks. The full Pipeline now looks like this:\nFigure 3. Pipeline Details The last step will create a pull request on Git. When this request is approved and merged, the update will finally happen in the production environment. This is a manual process to have control what comes in production and what does not.\nExecute full Pipeline E2E It is time to execute the whole pipeline now end to end. We will do a real update to the application now, so we can see the differences.\nAs described in step 10, the DEV and PROD environments are running on the same cluster. In the field, this will probably not happen, but for now, it is good enough. GitOps/Argo CD monitors any changes and automatically updates whenever the Git repository (Kubernetes Manifests) is changed. During the PipelineRun we will update the image tag for DEV, which automatically rolls out and create a Pull request which is waiting for approval and will roll out the changes onto production.\nBoth environments have a route to access the application. At the moment both will look the same:\nFigure 4. Globex DEV origin Update application The repository of Globex UI is forked at: https://github.com/tjungbauer/globex-ui. We used it throughout this journey to update the README.md file. The readme file does not really change anything. So, let’s update the UI itself.\nlook for the file src/index.html and add the following line before \u0026lt;/body\u0026gt;\n\u0026lt;center\u0026gt;\u0026lt;strong\u0026gt;My very important update\u0026lt;/strong\u0026gt;\u0026lt;/center\u0026gt; Save this change and push it to GitHub. This will trigger the Pipeline which is running quite long. However, once it is finished, the DEV environment should now show the new line in the UI.\nAfter the pipeline updated the image tag in Git, the GitOps process must fetch this change. This may take a while. You can speed this up by refreshing the \u0026#34;Application\u0026#34; inside the Argo CD interface. It should then automatically synchronize. The update can now be seen in the browser. The \u0026#34;important update\u0026#34; is visible at the bottom of the page.\nFigure 5. Globex DEV updated The production environment was not yet updated. Instead, a pull request has been created:\nFigure 6. Open pull request This request can be reviewed and merged. As you can see there was only one change in the files:\nFigure 7. Open pull request - changed files Merge the pull request and wait until Argo CD fetched the changes and updates the production environment. This is it, the changes are done and promoted to production:\nFigure 8. Globex PROD updated Conclusion This concludes this journey to a Secure Supply Chain using Tekton (OpenShift Pipelines). Is this the best must-have you need to do? No, it is an example, a demonstration. Feel free to use and modify it. You can also use other tools for the tasks or the pipeline as such. It does not matter if you use Tekton, Jenkins, Gitlab Runner etc. What is important is that you secure your whole supply chain as much as possible. Any image you create should be signed to ensure that the source can be trusted. Every source code should be verified against best practices and all images should be scanned for vulnerabilities and policy violations during the build AND the deployment process.\n"},{"uri":"https://blog.stderr.at/tags/operator/","title":"Operator","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2023/03/operator-installation-with-argo-cd/","title":"Operator installation with Argo CD","tags":["oc","operator","OpenShift","OCP","GitOps","Argo CD"],"description":"Openshift 4.x - Using Argo CD to deploy Operators","content":" GitOps for application deployment and cluster configuration is a must-have I am trying to convince every customer to follow from the very beginning when starting the Kubernetes journey. For me, as more on the infrastructure side of things, I am more focused on the configuration of an environment. Meaning, configuring a cluster, installing an operator etc.\nIn this article, I would like to share how I deal with cluster configuration when certain Kubernetes objects are dependent on each other and how to use Kubernetes but also Argo CD features to resolve these dependencies.\nThis article assumes that you have the openshift-gitops Operator, which provides Argo CD, already installed, and configured. If you are new to GitOps check out this article: Argo CD TL;DR If you want to jump directly to the technical fun part, go here: Let’s start.\nThe Idea Everything should be seen as a code. Everything should be possible to be deployed in a repeatable way. With a GitOps approach, everything is stored naturally in Git and from there, a GitOps agent validates and synchronizes changes to one or more clusters.\nWhen it comes to OpenShift, Red Hat supports Argo CD using the Operator openshift-gitops. This gives you everything you need to deploy an Argo CD instance. The only thing you need to take care of is a Git repository, no matter if it is GitHub, Gitlab, Bitbucket etc.\nThe Problem Sometimes Kubernetes objects depend on each other. This is especially true when you would like to install and configure Operators, where the configuration, based on a Customer Resource Definition (CRD), can only happen after the Operator has been installed and is ready.\nWhy is that? Well, when you want to deploy an Operator, you will store a “Subscription object” in Git. Argo CD will take this object and applies it to the cluster. However, for an Operator, the creation of the Subscription object is just the first step. A lot of other steps are required until the Operator gets ready. Unfortunately, Argo CD cannot verify if the installation is successful. All it sees is that the Subscription object has been created and then it immediately tries to deploy the CRD. The CRD which is not yet available on the system because the Operator is still installing it.\nEven if you use Argo CD features like Sync waves it would not wait until the Operator is successfully installed because for Argo CD the “success” is the creation of the Subscription object.\nSubsequently, the Argo CD synchronisation process will fail. You could now try to automatically “Retry” the sync or use multiple Argo CD applications that you execute one after each other, but I was not fully happy with that and tried a different approach.\nMy Solution Let’s say I would like to deploy and configure the Compliance Operator. The steps would be:\nInstall the Operator.\nWait until the Operator is ready.\nConfigure Operator specific CRDs.\nThis “Wait until the Operator is ready” is the tricky party for Argo CD. What I have done is the following:\nInstall the Operator, this is the first step and is done during Sync Wave 0.\nCreate a Kubernetes Job that verifies the status of the Operator. This Job additionally requires a ServiceAccount and a role with a binding. They are configured during Sync Wave is 1. Moreover, I use a Hook (another Argo CD feature) with the deletion policy “HookSucceeded”. This makes sure that the Job, ServiceAccount, Role and RoleBinding are removed after the status has been verified. The verification is successful as soon as the Operator status says “Succeeded”. In fact, all the Job does is to execute some oc commands. For example,\noc get clusterserviceversion openshift-gitops-operator.v1.8.0 -n openshift-gitops -o jsonpath={.status.phase} Succeeded Finally, during the next Sync Wave (2+) the CRD can be deployed. In this case, I deploy the object ScanSettingBinding.\nIn Argo CD everything is correctly synchronized, and the Operator and its configuration is in place.\nIf you are new to the compliance operator, I recommend the following article: Compliance Operator I use this approach for every Operator that I would like to install and configure at the same time. For example, I do the same for Advanced Cluster Security or Advanced Cluster Management where I use the Job to verify if everything is ready before I let Argo CD continue.\nMore information about Sync Waves and Hooks can be found in the official Argo CD documentation: Sync Phases and Waves Let’s see this in Action Prerequisites\nOpenShift cluster 4.x\nopenshift-gitops is installed and ready to be used.\nAccess to GitHub (or to your own Repository)\nI will be using my Helm Chart repository at https://charts.stderr.at/ and from there the charts:\ncompliance-operator-full-stack\nhelper-operator (sub chart): Responsible to install the Operators.\nhelper-status-checker (sub chart): Responsible to check the status of the Operator.\nWhy do I use Helm charts? There is no specific reason for that. I started with Helm for the cluster configuration and now it has evolved with a separate Chart repository and sub-charts and so on.\nArgo CD Application In Argo CD I have the following Application:\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: in-cluster-install-compliance-scans namespace: openshift-gitops spec: destination: namespace: default server: \u0026#39;https://kubernetes.default.svc\u0026#39; (1) info: - name: Description value: Deploy and configure the Compliance Scan Operator project: in-cluster source: path: charts/compliance-operator-full-stack (2) repoURL: \u0026#39;https://github.com/tjungbauer/helm-charts\u0026#39; targetRevision: main 1 Installing on the local cluster where Argo CD is installed. 2 Git configuration, including path and revision. Actually, this Application is created out of an ApplicationSet, but I did not want to make it too complex :) The Application would like to synchronize the objects:\nSubscription\nOperatorGroup\nNamespace (openshift-compliance)\nScanSettingBinding\nFigure 1. Argo CD: Installing Compliance Operator Where are the objects we need for the Job? Since they are only available during the Sync-Hook they will not show up here. In fact, they will only show up during the time they are alive and will disappear again after the status of the operator has been verified. Helm Chart Configuration The Helm Chart gets its configuration from a values file. You can verify the whole file on GitHub.\nThe important pieces here are that some variables are handed over to the appropriate Sub Charts.\nOperator Configuration This part is handed over to the Chart “helper-operator”.\nhelper-operator: operators: compliance-operator: enabled: true syncwave: \u0026#39;0\u0026#39; namespace: name: openshift-compliance create: true subscription: channel: release-0.1 approval: Automatic operatorName: compliance-operator source: redhat-operators sourceNamespace: openshift-marketplace operatorgroup: create: true notownnamespace: true It is executed during Sync Wave 0 and defines if a Namespace (openshift-compliance) shall be created (true) and the specification of the Operator which you need to know upfront:\nchannel: Defines which channel shall be used. Some operators offer different channels.\napproval: Either Automatic or Manual … defines if the Operator shall be updated automatically or requires an approval.\noperatorName: the actual name of the Operator (compliance-operator)\nsource: Where does this Operator come from (redhat-operator)\nsourceNamespace: In this case openshift-marketplace\nYou can fetch these values by looking at the Packagemanifest:\noc get packagemanifest compliance-operator -o yaml Status Checker Configuration This part is handed over to the Sub-Chart \u0026#34;helper-status-checker\u0026#34;\u0026#34;. The main values here are the operatorName and the namespace where the Operator is installed.\nWhat is not visible here is the Sync Wave, which is per default set to 1 inside the Helm Chart. If you need to overwrite it, it can be configured in this section as well.\nhelper-status-checker: enabled: true (1) # use the value of the currentCSV (packagemanifest) but WITHOUT the version !! operatorName: compliance-operator (2) # where operator is installed namespace: name: openshift-compliance (3) serviceAccount: create: true name: \u0026#34;sa-compliance\u0026#34; (4) 1 Is the status checker enabled or is it not. 2 The name of the operator as it is reported by the value currentCSV inside the packageManifest 3 The namespace where the Operator has been installed. 4 The name of the ServiceAccount that is created temporarily. The operatorName is sometimes different than the Operator name required for helper-operator chart. Here it seems the value of the currentCSV must be used but without the version number. (The Job will look up the version itself) Operator CRD configuration The final section of the values file manages the configuration for the Operator itself. This section does not use a Sub Chart. Instead, the variables are used in the Main-Chart. In this example, the ScanSettingBinding will be configured during Sync Wave 3, which is all we need to basic functionality.\ncompliance: scansettingbinding: enabled: true syncwave: \u0026#39;3\u0026#39; (1) profiles: (2) - name: ocp4-cis-node - name: ocp4-cis scansetting: default 1 Define the Sync Wave. This value must be higher than the Sync Wave of the helper-status-checker 2 ScanSettingBinding configuration. Two profiles are used in this example. Synchronizing Argo CD Basic Application in Argo CD before it is synced:\nFigure 2. Argo CD: Application Sync Wave 0: Synchronization has started. Namespace and Subscription are deployed.\nFigure 3. Argo CD: Synchronization is started (Sync Wave 0) Sync Wave 1: Status Checker Job has started and tries to verify the Operator.\nFigure 4. Argo CD: Status Checker Job started (Sync Wave 1) The Log output of the Operator. You can see that the status switches from Pending to Installing to Succeeded.\nFigure 5. Argo CD: Log of the Status Checker Pod After Sync Wave 3, the whole Application has been synchronized and the Checker Job has been removed.\nFigure 6. Argo CD: Compliance Operator is fully deployed "},{"uri":"https://blog.stderr.at/tags/cert-manager/","title":"Cert Manager","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/certificates/","title":"Certificates","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ssl/","title":"SSL","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2023/02/ssl-certificate-management-for-openshift-on-aws/","title":"SSL Certificate Management for OpenShift on AWS","tags":["oc","OpenShift","OCP","SSL","Certificates","Cert Manager"],"description":"Using cert-manager to automatically issue new certificates","content":" Finally, after a long time on my backlog, I had some time to look into the Cert-Manager Operator and use this Operator to automatically issue new SSL certificates. This article shall show step-by-step how to create a certificate request and use this certificate for a Route and access a service via your Browser. I will focus on the technical part, using a given domain on AWS Route53.\nIntroduction After a new OpenShift Cluster has been deployed, self-signed certificates are used to access the Routes (for example the console) and the API. Typically, an application is exposed to the world using the schema \u0026lt;app-name\u0026gt;-\u0026lt;namespace-name\u0026gt;.apps.\u0026lt;clusterdomain\u0026gt;.\nWe will try to create a certificate for a specific application with a custom domain name and for the cluster domains: *.apps.clusterdomain and api.clusterdomain.\nThe domain is already available and delegated to AWS Route53. As certificate authority, I am using Let’s Encrypt.\nWe will install 2 operators:\nCert Manager: to issue new certificates.\nCert Utils Operator: injects the certificate into a Route object.\nThe Cert Utils Operator can provide additional information for a certificate and monitors the expiration date. Here we mainly use it to automatically inject Route objects by defining specific annotations. Prerequisites OpenShift cluster with a user that has privileges to install Operators.\nA domain hosted for example at Route 53\nCredentials for your Cloud Provider (AWS)\nDeploy an Example Application Let’s use the super complex demo application bookimport.\noc new-project bookimport oc apply -f https://raw.githubusercontent.com/tjungbauer/book-import/master-no-pre-post/book-import/deployment.yaml -n bookimport oc apply -f https://raw.githubusercontent.com/tjungbauer/book-import/master-no-pre-post/book-import/service.yaml -n bookimport oc expose service book-import -n bookimport oc get route -n bookimport The last command will print you an URL which, copied into the browser, will open our application:\nFigure 1. Application Book Import using HTTP As you can see in the address line the connection is not secured (Nicht sicher in German) and my domain is *.apps.ocp.aws.ispworld.at\nConfigure an AWS user for accessing Route 53 On AWS I have currently 2 Zones, the public aws.ispworld.at and a private zone, created by the OpenShift Installer.\nFigure 2. Domain Zones Before you can manage your domains a user with appropriate privileges must be created.\nStore the following in the file policy.json. This will allow a user to perform DNS Upgrades.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;route53:GetChange\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:route53:::change/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;route53:ChangeResourceRecordSets\u0026#34;, \u0026#34;route53:ListResourceRecordSets\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:route53:::hostedzone/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53:ListResourceRecordSets\u0026#34;, \u0026#34;route53:ListHostedZonesByName\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Apply the new policy to AWS and store the ARN into a variable:\naws iam create-policy --policy-name AllowDNSUpdates --policy-document file://policy.json export POLICY_ARN=$(aws iam list-policies --query \u0026#39;Policies[?PolicyName==`AllowDNSUpdates`].Arn\u0026#39; --output text) Create the user route53-openshift and assign the policy to that user:\naws iam create-user --user-name route53-openshift aws iam attach-user-policy --policy-arn $POLICY_ARN --user-name route53-openshift Finally, create the access key and store the AccessKeyId and the SecretAccessKey for later use:\naws iam create-access-key --user-name route53-openshift --output json { \u0026#34;AccessKey\u0026#34;: { \u0026#34;UserName\u0026#34;: \u0026#34;route53-openshift\u0026#34;, \u0026#34;AccessKeyId\u0026#34;: \u0026#34;XXXXXXXXXXXXXX\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;XXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;CreateDate\u0026#34;: \u0026#34;2023-02-15T12:34:06+00:00\u0026#34; } } Installing Operators to OpenShift We will install 2 Operators to our cluster:\ncert-manager Operator for Red Hat OpenShift\nCert Utils Operator\nSimply search both on OLM and install them keeping the default values.\nThe Cert Utils Operator is a Community Operator. Figure 3. Operators This will install the Cert-Manager into the namespace openshift-cert-manager\nConfigure the Cert-Manager Operator Before we can issue a certificate, we need to create a secret with our AWS SecretAccessKey (see above):\noc create secret generic prod-route53-credentials-secret --from-literal secret-access-key=\u0026#34;XXXXXXXXXXXXXXXXXXX\u0026#34; -n openshift-cert-manager As next step, we create a ClusterIssuer that will be available cluster-wide using Let’s Encrypt as certificate authority:\nThe connection to Let’s Encrypt is using the productive API. If you would like to use the staging environment instead, change the server URL to https://acme-staging-v02.api.letsencrypt.org/directory apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: email: your@email.com (1) preferredChain: \u0026#39;\u0026#39; privateKeySecretRef: name: letsencrypt-account-key server: \u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39; solvers: - dns01: route53: accessKeyID: XXXXXXXXXXXXXX (2) region: eu-central-1 (3) secretAccessKeySecretRef: key: secret-access-key name: prod-route53-credentials-secret (4) selector: dnsZones: - your-domain (5) 1 Change your email address 2 Use the AccessKeyId created above 3 Using AWS you need to define a region 4 The name of the secret created during the step before 5 Your public domain, for example aws.ispworld.at. Once created the ClusterIssuer should switch to the status \u0026#34;Ready\u0026#34;\noc describe clusterissuer letsencrypt-prod Status: ... Conditions: Last Transition Time: 2023-02-16T13:54:49Z Message: The ACME account was registered with the ACME server Observed Generation: 1 Reason: ACMEAccountRegistered Status: True Type: Ready OPTIONAL: When using private Domains or Firewalls As you can see in one of the images above, I have two domains:\naws.ispworld.at\nocp.aws.ispworld.at\nThe first one is marked as public, that means everybody can resolve names. The second one is set to private and only define VPCs (in this case the cluster itself) can resolve hostnames.\nIn case of the following error:\nE0216 15:27:29.513080 1 controller.go:163] cert-manager/challenges \u0026#34;msg\u0026#34;=\u0026#34;re-queuing item due to error processing\u0026#34; \u0026#34;error\u0026#34;=\u0026#34;failed to determine Route 53 hosted zone ID: zone not found in Route 53 for domain _acme-challenge.bookimport.apps.ocp.aws.ispworld.at.\u0026#34; \u0026#34;key\u0026#34;=\u0026#34;bookimport/bookimport-cert-jbmh6-2173685137-2399596362\u0026#34; Add the following into ClusterManager\noc edit CertManager.operator.openshift.io/cluster unsupportedConfigOverrides: controller: args: - --v=2 - --cluster-resource-namespace=$(POD_NAMESPACE) - --leader-election-namespace=kube-system - --dns01-recursive-nameservers-only - --dns01-recursive-nameservers=ns-362.awsdns-45.com:53,ns-930.awsdns-52.net:53 (1) 1 List of nameserver the PUBLIC domain is hosted on. The Operator will then try to resolve the names using the specified nameserver only.\nIssue a new certificate At this step, we can create a Certificate:\napiVersion: cert-manager.io/v1 kind: Certificate metadata: name: bookimport-cert (1) namespace: bookimport (2) spec: dnsNames: - bookimport.apps.ocp.aws.ispworld.at (3) issuerRef: kind: ClusterIssuer name: letsencrypt-prod (4) secretName: bookimport.apps.ocp.aws.ispworld.at-certificate (5) 1 Name of the certificate objects 2 Application namespace 3 List of domain names 4 Issuer that shall be used 5 Name of the Secret that will be created and hold the certificate information Create a Route After a while the certificate will be Ready:\noc get certificate/bookimport-cert -n bookimport NAME READY SECRET AGE bookimport-cert True bookimport.apps.ocp.aws.ispworld.at-certificate 87m Now we can create a Route object to configure the IngressController. The important part here is the annotation, which will tell the Cert Utils Operator to automatically inject the certificate.\nkind: Route apiVersion: route.openshift.io/v1 metadata: name: bookimport-tls namespace: bookimport annotations: cert-utils-operator.redhat-cop.io/certs-from-secret: bookimport.apps.ocp.aws.ispworld.at-certificate (1) spec: host: bookimport.apps.ocp.aws.ispworld.at (2) to: kind: Service name: book-import weight: 100 tls: termination: edge port: targetPort: web wildcardPolicy: None 1 Annotation that points to the Secret which stored the certificate. The values of this Secret will be automatically injected into this Route object. 2 The hostname for our Route As you can see, the Browser will show no warning when opening the URL.\nFigure 4. Book Import using HTTPS Cluster Default Certificates During a cluster deployment, OpenShift will create self-signed certificates for its API and for the default IngressController *.apps.clusterdomain.\nUsually, we want to change them as well. So why not use the Cert-Manager to issue the appropriate certificates?\nDefault IngressController For the default IngressController I create a certificate request with 2 domain names: the wildcard and the base domain (just to be sure, actually the wildcard should be enough)\napiVersion: cert-manager.io/v1 kind: Certificate metadata: name: router-certificate namespace: openshift-ingress (1) spec: dnsNames: - apps.ocp.aws.ispworld.at (2) - \u0026#39;*.apps.ocp.aws.ispworld.at\u0026#39; issuerRef: kind: ClusterIssuer name: letsencrypt-prod secretName: router-certificate (3) 1 The default IngressController runs in the namespace openshift-ingress 2 List of domains 3 Name of the Secret that will be created once the Certificate has been approved. After a while, the certificate request should be Ready again. In the namespace openshift-ingress a Secret will be available with the name router-certificate\nAPI For the API URL we do the same. This time it is stored in the namepsace openshift-config\napiVersion: cert-manager.io/v1 kind: Certificate metadata: name: api-certificate namespace: openshift-config spec: dnsNames: - api.ocp.aws.ispworld.at issuerRef: kind: ClusterIssuer name: letsencrypt-prod secretName: api-certificate It is possible to create one certificate with all required Domainnames. Just be sure that the Secret is available in the appropriate Namespace. Patching API Server and IngressController As a final step we need to patch the IngressController and the API server so they will use the correct Secrets with the officially signed certificates.\n# IngressController oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch=\u0026#39;{\u0026#34;spec\u0026#34;: { \u0026#34;defaultCertificate\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;router-certificate\u0026#34; }}}\u0026#39; # API Server oc patch apiserver cluster --type=merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;servingCerts\u0026#34;: {\u0026#34;namedCertificates\u0026#34;: [{\u0026#34;names\u0026#34;: [\u0026#34;api.ocp.aws.ispworld.at\u0026#34;], \u0026#34;servingCertificate\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;api-certificate\u0026#34;}}]}}}\u0026#39; (1) 1 Be sure to use the correct URL for the API This will restart a bunch of services. Once everything is up and running again (your can watch using the command watch oc get co), the correct certificate will be shown in the browser:\nFigure 5. UI or via curl:\ncurl -v https://api.ocp.aws.ispworld.at:6443 * Connected to api.ocp.aws.ispworld.at (13.52.208.31) port 6443 [...] * Server certificate: * subject: CN=api.ocp.aws.ispworld.at * start date: Feb 16 15:11:36 2023 GMT * expire date: May 17 15:11:35 2023 GMT * subjectAltName: host \u0026#34;api.ocp.aws.ispworld.at\u0026#34; matched cert\u0026#39;s \u0026#34;api.ocp.aws.ispworld.at\u0026#34; * issuer: C=US; O=Let\u0026#39;s Encrypt; CN=R3 * SSL certificate verify ok. Summary Now with these steps, it is possible to issue new Certificates. Of course, there I many more options to configure a certificate. I encourage everybody to read the official documentation of the Cert Manager.\nEspecially, if you are interested in the whole certificate Certificate Lifecycle\n"},{"uri":"https://blog.stderr.at/openshift/2022/11/using-serversideapply-with-argocd/","title":"Using ServerSideApply with ArgoCD","tags":["oc","kubectl","OpenShift","OCP","GitOps","Argo CD"],"description":"Using ServerSideApply with ArgoCD","content":" „If it is not in GitOps, it does not exist“ - However, managing objects partially only by Gitops was always an issue, since ArgoCD would like to manage the whole object. For example, when you tried to work with node labels and would like to manage them via Gitops, you would need to put the whole node object into ArgoCD. This is impractical since the node object is very complex and typically managed by the cluster. There were 3rd party solutions (like the patch operator), that helped with this issue.\nHowever, with the Kubernetes feature Server-Side Apply this problem is solved. Read further to see a working example of this feature.\nWhat is Server-Side Apply (SSA) Quoting from Kuberneted Documentation:\nServer-Side Apply helps users and controllers manage their resources through declarative configurations. Clients can create and modify their objects declaratively by sending their fully specified intent.\nA fully specified intent is a partial object that only includes the fields and values for which the user has an opinion. That intent either creates a new object or is combined, by the server, with the existing object.\nIn other words: you can send a snippet of an object to the cluster and the cluster will eventually combine everything on the server and not validate on the client side first. All you need is a way to identify the object. Usually, the name and maybe the namespace too.\nSSA and ArgoCD When it comes to GitOps the implementation of SSA is quite new. However, it is important to note, that (managed field) conflicts are currently not handled by ArgoCD. Instead, ArgoCD forces a change and overrides everything, even if the field is managed by somebody else. This might be improved in the future. Nevertheless …​ let’s test the feature.\nPrerequisites The support of the Server-Side Apply feature is currently available in the latest version of ArgoCD. This means, that the channel of the openshift-gitops operator must be changed to \u0026#34;latest\u0026#34;, which will deploy openshift-gitops version 1.6\nA new stable version will arrive soon. :)\nNode Labelling Chart In this example, I would like to use a Helm chart that will try to set two different labels on 2 nodes. This is a very easy example to demonstrate the feature.\nAs a Helm chart, I have prepared the following: https://github.com/tjungbauer/openshift-clusterconfig-gitops/tree/main/clusters/management-cluster/node-configuration\nThe values for this chart are straightforward: per node, a list of custom labels is defined.\nhelper-server-side-apply: nodes: (1) - name: ip-10-0-233-237.us-west-1.compute.internal (2) enabled: true custom_labels: (3) environment: \u0026#39;Production\u0026#39; gpu: false - name: ip-10-0-193-67.us-west-1.compute.internal enabled: true custom_labels: environment: \u0026#39;Test\u0026#39; gpu: true 1 List of nodes 2 Node name as OpenShift knows the node (oc get nodes) 3 List of labels that should be added to the node: here environment and gpu The Chart is using a sub-chart called helper-server-side-apply. The source can be found at the Helm Repository The output of this Helm Chart will be the following:\n# Source: node-labels/charts/helper-server-side-apply/templates/node.yaml kind: Node apiVersion: v1 metadata: name: \u0026#34;ip-10-0-233-237.us-west-1.compute.internal\u0026#34; (1) labels: gitops.ownedBy: openshift-gitops helm.sh/chart: helper-server-side-apply-1.0.3 app.kubernetes.io/name: helper-server-side-apply app.kubernetes.io/instance: release-name app.kubernetes.io/managed-by: Helm environment: \u0026#34;Production\u0026#34; (2) gpu: \u0026#34;false\u0026#34; (3) --- # Source: node-labels/charts/helper-server-side-apply/templates/node.yaml kind: Node apiVersion: v1 metadata: name: \u0026#34;ip-10-0-193-67.us-west-1.compute.internal\u0026#34; labels: gitops.ownedBy: openshift-gitops helm.sh/chart: helper-server-side-apply-1.0.3 app.kubernetes.io/name: helper-server-side-apply app.kubernetes.io/instance: release-name app.kubernetes.io/managed-by: Helm environment: \u0026#34;Test\u0026#34; gpu: \u0026#34;true\u0026#34; 1 The name of the node and our identifier 2 The first label we set 3 The second label we set This is not a full definition of a Node object. The only things defined are the node name and the labels. (Besides the customer labels we would like to add, some default labels are added automatically.) ArgoCD Application So we have a Helm chart in Git. Perfect, but to automate everything with Gitops we need to create the object Application. For example the following:\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: node-labelling namespace: openshift-gitops spec: destination: namespace: default server: \u0026#39;https://kubernetes.default.svc\u0026#39; info: - name: Description value: Deploy Node Labels project: default source: helm: valueFiles: - values.yaml path: clusters/management-cluster/node-configuration (1) repoURL: \u0026#39;https://github.com/tjungbauer/openshift-clusterconfig-gitops\u0026#39; targetRevision: main syncPolicy: syncOptions: - ServerSideApply=true (2) - Validate=false (3) 1 Path and URL of the node labelling Helm chart 2 Must be set to true to enable SSA 3 Must be set to false to skip schema validation The two syncOptions are important to set. Since the yaml output might not pass the validation, the schema validation should be disabled. This will create the following application in ArgoCD:\nFigure 1. Argo CD: Application Syncing the Application When you now synchronize the ArgoCD application, ArgoCD will take the yaml and will tell Kubernetes (or OpenShift) to perform a Server-Side Apply. This will result in the following yaml for the node:\nkind: Node apiVersion: v1 metadata: name: ip-10-0-193-67.us-west-1.compute.internal labels: beta.kubernetes.io/os: linux app.kubernetes.io/instance: node-labelling [...] node-role.kubernetes.io/worker: \u0026#39;\u0026#39; gitops.ownedBy: openshift-gitops [...] environment: Test [...] That’s it …​ all the magic is done.\n"},{"uri":"https://blog.stderr.at/tags/day-2/","title":"Day-2","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/secrets/","title":"Secrets","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2022/08/secrets-management-vault-on-openshift/","title":"Secrets Management - Vault on OpenShift","tags":["OCP","Day-2","OpenShift","Vault","Secrets"],"description":"Secrets Management with HashiCorp Vault","content":" Sensitive information in OpenShift or Kubernetes is stored as a so-called Secret. The management of these Secrets is one of the most important questions, when it comes to OpenShift. Secrets in Kubernetes are encoded in base64. This is not an encryption format. Even if etcd is encrypted at rest, everybody can decode a given base64 string which is stored in the Secret.\nFor example: The string Thomas encoded as base64 is VGhvbWFzCg==. This is simply a masked plain text and it is not secure to share these values, especially not on Git. To make your CI/CD pipelines or Gitops process secure, you need to think of a secure way to manage your Secrets. Thus, your Secret objects must be encrypted somehow. HashiCorp Vault is one option to achieve this requirement.\nHashiCorp Vault HashiCorp Vault is a solution to solve our Secret management problem. It stores and encrypts our sensitive information at rest and in transit and enables you to create fine grained access controls (ACL). This defines who has access to a specific secret. Application A should only get access to secret A and so on. However, that is just the tip of the iceberg. Vault can do much more. If you are interested in further details, check out the introduction video by Armon, the Co-Founder of Hashicorp Vault at: https://learn.hashicorp.com/tutorials/vault/getting-started-intro?in=vault/getting-started\nFor this article we will keep it simple and cover:\nInstalling HashiCorp Vault to OpenShift\nIntegrating the plugin \u0026#34;Kubernetes Authentication\u0026#34; to access the Secrets\nAccessing a static secret stored in HashiCorp Vault by an example application\nInstalling Vault The easiest way to install Vault is by using the supported Helm chart.\nAdd the Helm repository to your repo:\nhelm repo add hashicorp https://helm.releases.hashicorp.com Update the repository to get the latest updates.\nhelm repo update Before we install Vault, we need to create a values file to set certain variables. Let’s create a file called \u0026#34;overwrite-values.yaml\u0026#34; with the following content\nglobal: openshift: true server: ha: enabled: true replicas: 3 raft: enabled: true This will tell HashiCorp Vault the environment we are going to install is (OpenShift), enables high availability with 3 replicas and enables RAFT (see below for some introduction).\nFinally, let us deploy HashiCorp Vault into the namespace vault using the values file we have created in the previous step:\nhelm upgrade --install vault hashicorp/vault --values bootstrap/vault/overwrite-values.yaml --namespace=vault --create-namespace This will start the agent-injector and 3 vault-pods:\noc get pods -n vault NAME READY STATUS RESTARTS AGE vault-0 0/1 Running 0 36s vault-1 0/1 Running 0 36s vault-2 0/1 Running 0 36s vault-agent-injector-74c848f67b-sq4dq 1/1 Running 0 37s vault agent injector: detecting applications with annotations that require vault agent which will get injected\nvault-0: vault server\nThe vault servers remain in not-ready state until Vault has been unsealed by you.\nWhen you check the logs of one of the pods you will see:\noc logs vault-0 -n vault ... 2022-08-11T12:31:22.497Z [INFO] core: security barrier not initialized 2022-08-11T12:31:22.497Z [INFO] core: seal configuration missing, not initialized Vault always starts uninitialized and sealed, to protect the secrets. You need to give at least three different unseal-keys in order to be able to use Vault.\nVault’s seal mechanism uses Shamir’s secret sharing. This is a manual process to secure the cluster if it restarts. You can use auto-unseal for specific cloud providers to bypass the manual requirement. Raft Storage During the deployment we have enabled Raft which is the integrated HA storage for Vault. Vault can use several different styles of storage backends, but when it comes to HA and Kubernetes, Raft is easiest one since it has no other dependencies, and it is well known inside OpenShift. Etcd is using Raft as well. It is a distributed Consensus Algorithm where multiple members form a cluster and elect one leader. The leader has the responsibility to replicate everything to the followers. Since this leader election requires a majority an odd number of cluster members must be available to ensure there is a minimum number left if a member is failing.\nInitialize and Unseal Vault As mentioned, the Vault Pods are not fully available yet, since Vault it currently sealed and cannot be used. The first thing to do is to initialize and unseal it.\nThe following commands will login into one Pod and execute commands from there. Let’s get the status of our Vault fist:\noc -n vault exec -it vault-0 -- vault operator init -status This will return the message: Vault is not initialized\nThe following command will initialize Vault for further usage:\noc exec -ti -n vault vault-0 -- vault operator init -format=json \u0026gt; unseal.json This will create the file unseal.json locally on your machine. Keep this file secure It contains by default 5 key shards and the root token you will need to unseal Vault and authenticate as root.\nKeep this file secure, it contains keys to unseal Vault and the root_token to authenticate against. Never share this file. It will look like this:\n{ \u0026#34;unseal_keys_b64\u0026#34;: [ \u0026#34;key_1\u0026#34;, \u0026#34;key_2\u0026#34;, \u0026#34;key_3\u0026#34;, \u0026#34;key_4\u0026#34;, \u0026#34;key_5\u0026#34; ], \u0026#34;unseal_keys_hex\u0026#34;: [ \u0026#34;key_hex_1\u0026#34;, \u0026#34;key_hex_2\u0026#34;, \u0026#34;key_hex_3\u0026#34;, \u0026#34;key_hex_4\u0026#34;, \u0026#34;key_hex_5\u0026#34; ], \u0026#34;unseal_shares\u0026#34;: 5, \u0026#34;unseal_threshold\u0026#34;: 3, \u0026#34;recovery_keys_b64\u0026#34;: [], \u0026#34;recovery_keys_hex\u0026#34;: [], \u0026#34;recovery_keys_shares\u0026#34;: 5, \u0026#34;recovery_keys_threshold\u0026#34;: 3, \u0026#34;root_token\u0026#34;: \u0026#34;root.token\u0026#34; } With the initialization in place Vault is put into a sealed mode. This means Vault cannot decrypt secrets at this moment. To unseal Vault you need the unseal key, which is split into multiple shards using Shamir’s secret sharing. A certain number of individual shards (default 3) must be provided to reconstruct the unseal key.\nTo unseal Vault lets login to our Pod \u0026#34;vault-0\u0026#34; and unseal it. Use the following command and provide one of the keys:\noc exec -ti -n vault vault-0 -- vault operator unseal Unseal Key (will be hidden): Key Value --- ----- Seal Type shamir Initialized true (1) Sealed true (2) Total Shares 5 Threshold 3 Unseal Progress 1/3 (3) Unseal Nonce 08b01535-be15-e865-251c-f948ed0661c9 Version 1.11.2 Build Date 2022-07-29T09:48:47Z Storage Type raft HA Enabled true 1 Vault is initialized 2 Vault is still sealed 3 The unseal progress: Currently 1 out of 3 keys have been provided Use the same command another 2 times using different keys to complete the unseal process:\nAt the end the following output should be shown:\nUnseal Key (will be hidden): Key Value --- ----- Seal Type shamir Initialized true Sealed false (1) Total Shares 5 Threshold 3 Version 1.11.2 Build Date 2022-07-29T09:48:47Z Storage Type raft Cluster Name vault-cluster-f7402e5b (2) Cluster ID aff648f0-b3a2-1fdd-12f6-492842b08b2b HA Enabled true HA Cluster https://vault-0.vault-internal:8201 HA Mode active (3) Active Since 2022-08-16T07:20:45.828215961Z Raft Committed Index 36 Raft Applied Index 36 1 Vault is now unsealed 2 Name of our cluster 3 High availability is enabled Vault-0 is now initialized, but there are 2 other members in our HA cluster which must be added. Let vault-1 and vault-2 join the cluster and perform the same unseal process as previously: use 3 different keys:\noc exec -ti vault-1 -n vault -- vault operator raft join http://vault-0.vault-internal:8200 # 3 times.... oc exec -ti vault-1 -n vault -- vault operator unseal oc exec -ti vault-2 -n vault -- vault operator raft join http://vault-0.vault-internal:8200 # 3 times... oc exec -ti vault-2 -n vault -- vault operator unseal Verify Vault Cluster To verify if the Raft cluster has successfully been initialized, run the following.\nFirst, login using the root_token, that was created above, on the vault-0 pod.\noc exec -ti vault-0 -n vault -- vault login Token (will be hidden): \u0026lt;root_token\u0026gt; oc exec -ti vault-0 -n vault -- vault operator raft list-peers This should return:\nNode Address State Voter ---- ------- ----- ----- 16ec7490-f621-42ea-976d-5f054cfaeecc vault-0.vault-internal:8201 leader true 60ba2885-432a-c7d3-d280-a824f0acce42 vault-1.vault-internal:8201 follower true bcc4f551-79bc-47e5-d01b-97fc12d1afa5 vault-2.vault-internal:8201 follower true As you can see Vault-0 is the leader while the other two members are followers.\nConfigure Kubernetes Authentication There are multiple ways how an application can interact with Vault. One example is to use Tokens. This is quite easy but has the disadvantage that it does require additional steps of managing the life cycle of such token, moreover they might be shared, which is not what we want.\nHashiCorp Vault supports different authentication methods. One of which is the Kubernetes Auth Method that must be enabled before we can use. The Kubernetes Auth Method makes use of Jason Web Tokens (JWT)s that are bound to a Service Account. When we tell Vault that a Service Account is fine to authenticate, then a Deployment using this account is able to authenticate and request Secrets.\nVault has a plugin ecosystem, which allows to enable certain plugins. To enable Kubernetes Auth Method use the following process:\nLogin vault-0 pods oc exec -it vault-0 -n vault — /bin/sh\nexecute the command: vault auth enable kubernetes which returns: Success! Enabled kubernetes auth method at: kubernetes/\nSet up the Kubernetes configuration to use Vault’s service account JWT.\nthe address to the OpenShift API (KUBERNETES_PORT_443_TCP_ADDR) is automatically available via an environment variable. vault write auth/kubernetes/config issuer=\u0026#34;\u0026#34; \\ token_reviewer_jwt=\u0026#34;$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\u0026#34; \\ kubernetes_host=\u0026#34;https://$KUBERNETES_PORT_443_TCP_ADDR:443\u0026#34; \\ kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt Success! Data written to: auth/kubernetes/config With this step authentication against OpenShift is enabled.\nConfigure a Secret With the Kubernetes Auth Method in place we can configure a secret to test our setup. We will use an example application called expenses that has a MySQL database. The static password to bootstrap this database shall be stored in Vault. A plugin called key-value secrets engine will be used to achieve this.\nThere are other plugins that are specifically designed to automatically rotate secrets. For example, it is possible to dynamically create user credentials für MySQL.\nYou can list available engines by using the command: oc -n vault exec -it vault-0 — vault secrets list There are currently two versions of this key/value engine:\nKV Version 1: does not versionize the key/values, thus updates will overwrite the old values.\nKV Version 2: does versionize the key/value pairs\nIn our example we will use version 2.\nLike the authentication method, we need to enable the secrets engine:\noc -n vault exec -it vault-0 -- vault secrets enable \\ -path=expense/static \\ (1) -version=2 \\ (2) kv (3) 1 API path where our secrets are stored 2 Version 2 3 name of our engine You can list the enabled engines with the following command:\noc -n vault exec -it vault-0 -- vault secrets list Path Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_f1e955f9 per-token private secret storage expense/static/ kv kv_6db09e5d n/a identity/ identity identity_ca05e6ab identity store (1) sys/ system system_e34a76c3 system endpoints used for control, policy and debugging 1 Enabled KV secrets engine using the path expense/static/ Now lets put a secret into our store. We will store our super-secure MySQL password into expense/static/mysql\nMYSQL_DB_PASSWORD=mysuperpassword$ oc -n vault exec -it vault-0 -- vault kv put expense/static/mysql db_login_password=${MYSQL_DB_PASSWORD} This command will store the key db_login_password with the database as value. We can get the secret by calling:\noc -n vault exec -it vault-0 -- vault kv get expense/static/mysql ====== Secret Path ====== expense/static/data/mysql (1) ======= Metadata ======= Key Value --- ----- created_time 2022-08-17T06:08:05.839663508Z custom_metadata \u0026lt;nil\u0026gt; deletion_time n/a destroyed false version 1 ========== Data ========== Key Value --- ----- db_login_password mysuperpassword$ (2) 1 The data path of our secret 2 our password Configuring policies The Secret is now stored at expense/static/mysql but there is no policy in place. Everybody who is authenticated and is calling this path will get to see the secrets. Luckily, one or more policies can be assigned to the authentication method. A policy defines capabilities that allow you to perform certain actions.\nThe following capabilities are known:\ncreate - to create new data\nread - to read data\ndelete - to delete data\nlist - to list data\nPolicies can be written either in JSON or HCL (HashiCorp Configuration Language). Let’s create a file with the following content:\npath \u0026#34;expense/static/data/mysql\u0026#34; { capabilities = [\u0026#34;read\u0026#34;, \u0026#34;list\u0026#34;] } KV Version2 stores the secrets in a path with the prefix data/ This will limit my access to read and list only.\nWrite the policy:\ncat my-policy.hcl | oc -n vault exec -it vault-0 -- vault policy write expense-db-mysql - Next, we are going to bind the Vault secret to a service account and a namespace. Both objects will be created later, when we deploy the application.\noc -n vault exec -it vault-0 -- vault write auth/kubernetes/role/expense-db-mysql \\ (1) bound_service_account_names=expense-db-mysql \\ (2) bound_service_account_namespaces=expenses \\ (3) policies=expense-db-mysql \\ (4) ttl=1h (5) 1 Path or our new role 2 Name of the service account we will create and that will be used by the application 3 Name of the namespace we will create 4 Name of the policy we created earlier 5 The token is valid for 1 hour, after this period the service account must re-authenticate Let’s start an Application Now we will create our MySQL application into the namespace expenses. Use the following command to create the namespace and the application containing the objects Deployment, ServiceAccount (expense-db-mysql) and Service.\nSee at the Github Page for a full yaml specification of the three objects. oc new-project expenses oc apply -f https://raw.githubusercontent.com/joatmon08/vault-argocd/part-1/database/deployment.yaml The deployment will start a Pod with a sidecar container vault-agent. This sidecar is automatically created and must not be defined inside the Deployment specification. Instead, some annotations in the Deployment define what the container should be automatically injected and also where to find our secret:\nannotations: vault.hashicorp.com/agent-inject: \u0026#34;true\u0026#34; (1) vault.hashicorp.com/role: \u0026#34;expense-db-mysql\u0026#34; (2) vault.hashicorp.com/agent-inject-secret-db: \u0026#34;expense/static/data/mysql\u0026#34; (3) vault.hashicorp.com/agent-inject-template-db: | (4) {{ with secret \u0026#34;expense/static/data/mysql\u0026#34; -}} export MYSQL_ROOT_PASSWORD=\u0026#34;{{ .Data.data.db_login_password }}\u0026#34; {{- end }} ... spec: serviceAccountName: expense-db-mysql (5) 1 Defines that the vault-agent side car container shall be automatically injected. This is the most important annotation. 2 Name of the role that was created created previously 3 The agent will inject the data from expense/static/data/mysql and stores it in a file db The file name is everything that comes after vault.hashicorp.com/agent-inject-secret- 4 Configuration…​ the template that defines how the secret will be rendered 5 The service account name we bound our secret to, using the Consul language. In this case the MySQL password is simply exported The vault-agent is requesting the database password from Vault and provides it to the application where it is stored at /vault/secrets/db\noc -n expenses exec -it $(oc get pods -l=app=expense-db-mysql -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) -c expense-db-mysql -- cat /vault/secrets/db # output export MYSQL_ROOT_PASSWORD=\u0026#34;mysuperpassword$\u0026#34; The Deployment sources this file when it starts and MySQL will take this information to configure itself.\nTIP: Using vault CLI on your local environment All above commands that are dealing with Vault commands, first login to a pod and then execure the commands from there.\nIf you have the Vault CLI installed on your local machine, you can open a port forwarding to your Vault cluster at OpenShift and execute the commands locally:\noc port-forward -n vault svc/vault 8200 export VAULT_ADDR=http://localhost:8200 vault login ... Thanks Thanks to the wonderful Rosemary Wang and her Github repository: https://github.com/joatmon08/vault-argocd/tree/part-1\nAlso check out the Youtube Video: GitOps Guide to the Galaxy (Ep 31) | GitOps With Vault Part 1 in which Rosemary and Christian discuss this setup\n"},{"uri":"https://blog.stderr.at/tags/vault/","title":"Vault","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2022/04/overview-of-red-hats-multi-cloud-gateway-noobaa/","title":"Overview of Red Hat&#39;s Multi Cloud Gateway (Noobaa)","tags":[],"description":"","content":" This is my personal summary of experimenting with Red Hat\u0026#39;s Multi Cloud Gateway (MCG) based on the upstream Noobaa project. MCG is part of Red Hat\u0026#39;s OpenShift Data Foundation (ODF). ODF bundles the upstream projects Ceph and Noobaa.\nOverview Noobaa, or the Multicloud Gateway (MCG), is a S3 based data federation tool. It allows you to use S3 backends from various sources and\nsync replicate or simply use existing S3 buckets. Currently the following sources, or backing stores are supported:\nAWS S3 Azure Blob Google Cloud Storage Any other S3 compatible storage, for example\nCeph Minio Noobaa also supports using a local file system as a backing store for S3.\nThe main purpose is to provide a single API endpoint for applications using various S3 backends.\nOne of the main features of Noobaa is the storage pipeline. With a standard Noobaa S3 bucket, when storing a new Object Noobaa executes the following steps:\nChunking of the Object De-duplication Compression and Encryption This means that data stored in public cloud S3 offerings is automatically encrypted. Noobaa also supports using Hashicorp Vault for storing and retrieving encryption keys.\nIf you need to skip the storage pipeline, Noobaa also supports namespace buckets. For example these type of buckets allow you to write directly to AWS S3 and retrieve Objects via Noobaa. Or it could be used to migrate buckets from one cloud provider to another.\nNoobaa also has support for triggering JavaScript based function when\ncreating new objects reading existing objects deleting objects Setup With OpenShift Plus or an OpenShift Data Foundation subscription you can use the OpenShift Data Foundation Operator.\nFor testing Noobaa we used the standalone installation method without setting up Ceph storage (see here). OpenShift was running in AWS for testing.\nIf you would like to use the upstream version you can use the Noobaa operator (https://github.com/noobaa/noobaa-operator). This is what the OpenShift Data Foundation (ODF) is using as well.\nCommand line interface Noobaa also comes with a command line interface noobaa. It\u0026#39;s available via an ODF subscription or can be installed separately. See the noobaa-operator readme for more information.\nResources Before using an S3 object store with Noobaa we need to create so called Resources. This can be done via the Noobaa user interface or via the command line. For example the following commands create a new Resource using an AWS S3 bucket as a backing store\n# create an S3 bucket in eu-north-1 aws s3api create-bucket \\ --region eu-north-1 \\ --bucket tosmi-eu-north-1 \\ --create-bucket-configuration LocationConstraint=eu-north-1 # create an S3 bucket in eu-north-1 aws s3api create-bucket \\ --region eu-west-1 \\ --bucket tosmi-eu-west-1 \\ --create-bucket-configuration LocationConstraint=eu-west-1 # create Noobaa backing store using the tosmi-eu-north-1 bucket above noobaa backingstore create aws-s3 \\ --region eu-north-1 \\ --target-bucket tosmi-eu-north-1 aws-eu-north # create Noobaa backing store using the tosmi-eu-west-1 bucket above noobaa backingstore create aws-s3 \\ --region eu-west-1 \\ --target-bucket tosmi-eu-west-1 aws-eu-west Or if we would like to use Azure blob\n# create two resource groups for storage az group create --location northeurope -g mcg-northeurope # create two storage accounts az storage account create --name mcgnortheurope -g mcg-northeurope --location northeurope --sku Standard_LRS --kind StorageV2 # create containers for storing blobs az storage container create --account-name mcgnortheurope -n mcg-northeurope # list storage account keys for noobaa az storage account list az storage account show -g mcg-northeurope -n mcgnortheurope az storage account keys list -g mcg-westeurope -n mcgwesteurope az storage account keys list -g mcg-northeurope -n mcgnortheurope noobaa backingstore create \\ azure-blob azure-northeurope \\ --account-key=\u0026#34;\u0026lt;the key\u0026gt;\u0026#34; \\ --account-name=mcgnortheurope \\ --target-blob-container=mcg-northeurope Using\nnoobaa backingstore list we are able to confirm that our stores were created successfully.\nBuckets After creating the backend stores we are able to create Buckets and define the layout of backends.\nThere are two ways how to create buckets, either directly via the Noobaa UI, or using Kubernetes (K8s) objects.\nWe will focus on using K8s objects in this post.\nRequired K8s objects The Noobaa operator provides the following Custom Resource Definitions:\nBackingStore: we already created BackingStores in the Resources section BucketClass: a bucket class defines the layout of our bucket (single, mirrored or tiered) StorageClass: a standard K8s StorageClass referencing the BucketClass ObjectBucketClaim: A OBC or ObjectBucketClaim creates the bucket for us in Noobaa. Additionally the Noobaa operator creates a ConfigMap and a Secret with the same name as the Bucket, storing access details (ConfigMap) and credentials (Secret) for accessing the bucket. BucketClass Let\u0026#39;s create a example BucketClass which mirrors objects between the AWS S3 buckets eu-west-1 and eu-north-1.\napiVersion: noobaa.io/v1alpha1 kind: BucketClass metadata: labels: app: noobaa name: aws-mirrored-bucket-class namespace: openshift-storage spec: placementPolicy: tiers: - backingStores: - aws-eu-north - aws-eu-west placement: Mirror So we are defining a BucketClass aws-mirrored-bucket-class that has the following placement policy:\nA single tier with one backing store The backing store uses two AWS buckets\naws-eu-north aws-eu-west The placement policy is mirror, so all objects uploaded to buckets using this BucketClass will be mirrored between aws-eu-north and aws-eu-west. A BucketClass could have multiple tiers, moving cold data transparently to a lower tier, but let\u0026#39;s keep this simple.\nStorageClass After creating our BucketClass we are now able to define a standard K8s StorageClass:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: description: Provides Mirrored Object Bucket Claims (OBCs) in AWS name: aws-mirrored-openshift-storage.noobaa.io parameters: bucketclass: aws-mirrored-bucket-class provisioner: openshift-storage.noobaa.io/obc reclaimPolicy: Delete volumeBindingMode: Immediate This StorageClass uses our BucketClass aws-mirrored-bucket-class as a backend. All buckets created leveraging this StorageClass will mirror data between aws-eu-north and aws-eu-west (see the previous chapter).\nObjectBucketClaim Finally we are able to create ObjectBucketClaims for projects requiring object storage. An ObjectBucketClaim is similar to an PersistentVolumeClaim. Every time a claim is created the Noobaa operator will create a corresponding S3 bucket for us.\nLet\u0026#39;s start testing this out by creating a new OpenShift project\noc new-project obc-test Now we define a ObjectBucketClaim to create a new bucket for our application:\napiVersion: objectbucket.io/v1alpha1 kind: ObjectBucketClaim metadata: labels: app: noobaa name: aws-mirrored-claim spec: generateBucketName: aws-mirrored storageClassName: aws-mirrored-openshift-storage.noobaa.io We use the StorageClass created in the previous step. This will create\na S3 Bucket in the requested StorageClass a ConfigMap storing access information a Secret storing credentials for accessing the S3 Bucket For testing we will upload some data via s3cmd and use a pod to monitor data within the bucket.\nLet\u0026#39;s do the upload with s3cmd, we need the following config file:\n[default] check_ssl_certificate = False check_ssl_hostname = False access_key = \u0026lt;access key\u0026gt; secret_key = \u0026lt;secret key\u0026gt; host_base = s3-openshift-storage.apps.ocp.aws.tntinfra.net host_bucket = %(bucket).s3-openshift-storage.apps.ocp.aws.tntinfra.net Of course you must change host-base according to your cluster name. It\u0026#39;s a route in the openshift-storage namespace:\noc get route -n openshift-storage s3 -o jsonpath=\u0026#39;{.spec.host}\u0026#39; You can extract the access and secret key from the K8s secret via:\noc extract secret/aws-mirrored-claim --to=- Copy the access key and the secret key to the s3 command config file (we\u0026#39;ve called our config noobaa-s3.cfg). Now we can list all available buckets via:\n$ s3cmd ls -c noobaa-s3.cfg 2022-04-22 13:56 s3://aws-mirrored-c1087a17-5c84-4c62-9f36-29081a6cf5a4 Now we are going to upload a sample file:\n$ s3cmd -c noobaa-s3.cfg put simple-aws-mirrored-obc.yaml s3://aws-mirrored-c1087a17-5c84-4c62-9f36-29081a6cf5a4 upload: \u0026#39;simple-aws-mirrored-obc.yaml\u0026#39; -\u0026gt; \u0026#39;s3://aws-mirrored-c1087a17-5c84-4c62-9f36-29081a6cf5a4/simple-aws-mirrored-obc.yaml\u0026#39; [1 of 1] 226 of 226 100% in 0s 638.18 B/s done We can also list available files via\ns3cmd -c noobaa-s3.cfg ls s3://aws-mirrored-c1087a17-5c84-4c62-9f36-29081a6cf5a4 2022-04-22 13:57 226 s3://aws-mirrored-c1087a17-5c84-4c62-9f36-29081a6cf5a4/simple-aws-mirrored-obc.yaml Our we could use a Pod to list available files from within OpenShift. Note how we use the ConfigMap and the Secret the Noobaa operater created for us, when we created the ObjectBucketClaim:\napiVersion: batch/v1 kind: Job metadata: name: s3-test-job spec: template: metadata: name: s3-pod spec: containers: - image: d3fk/s3cmd:latest name: s3-pod env: - name: BUCKET_NAME valueFrom: configMapKeyRef: name: aws-mirrored-claim key: BUCKET_NAME - name: BUCKET_HOST valueFrom: configMapKeyRef: name: aws-mirrored-claim key: BUCKET_HOST - name: BUCKET_PORT valueFrom: configMapKeyRef: name: aws-mirrored-claim key: BUCKET_PORT - name: AWS_ACCESS_KEY_ID valueFrom: secretKeyRef: name: aws-mirrored-claim key: AWS_ACCESS_KEY_ID - name: AWS_SECRET_ACCESS_KEY valueFrom: secretKeyRef: name: aws-mirrored-claim key: AWS_SECRET_ACCESS_KEY command: - /bin/sh - -c - \u0026#39;s3cmd --host $BUCKET_HOST --host-bucket \u0026#34;%(bucket).$BUCKET_HOST\u0026#34; --no-check-certificate ls s3://$BUCKET_NAME\u0026#39; restartPolicy: Never That\u0026#39;s all for now. If time allows we are going to write a follow up blog post on\nReplicating Buckets and Functions "},{"uri":"https://blog.stderr.at/java/2022-02-25-jpa-disconnected-entity/","title":"Adventures in Java Land: JPA disconnected entities","tags":[],"description":"","content":" An old man tries to refresh his Java skills and does DO378. He fails spectacularly at the first real example but learns a lot on the way.\nThe exception There is this basic example where you build a minimal REST API for storing speaker data in a database. Quarkus makes this quite easy. You just have to define your database connection properties in resources/application.properties and off you go developing your Java Quarkus REST service:\nquarkus.datasource.db-kind=h2 quarkus.datasource.jdbc.url=jdbc:h2:mem:default quarkus.datasource.username=admin quarkus.hibernate-orm.database.generation=drop-and-create quarkus.hibernate-orm.log.sql=true So next we define our entity class to be stored in the database. I will skip the import statements and any other code not relevant for this post.\n// import statements skipped @Entity public class Speaker extends PanacheEntity { public UUID uuid; public String nameFirst; public String nameLast; public String organization; @JsonbTransient public String biography; public String picture; public String twitterHandle; // Constructors, getters and setters, toString and other methods skipped .... } We define an entity Speaker which extends the PanacheEntity class. Panache is a thin wrapper around Hibernate providing convince features. For example the base class PanacheEntity defines a autoincrement Id column for us. This inherited Id column is of importance for understanding the problem ahead of us.\nSo next you define your SpeakerService class which uses the entity. Once again I will skip the imports and any code not relevant for understanding the problem:\n// imports omitted @ApplicationScoped public class SpeakerService { // other code omitted public Speaker create(Speaker speaker) { speaker.persist(); return speaker; } We focus on the create method here because the call to speaker.persist() was the reason for all the headache.\nBut we are still in coding mode and last but not least we define our SpeakerResource class, again everything not relevant for understanding the problem was removed:\n// import statements omitted @Path(\u0026#34;/speaker\u0026#34;) @Produces(MediaType.APPLICATION_JSON) @Consumes(MediaType.APPLICATION_JSON) public class SpeakerResource { @Inject SpeakerService service; // other code omitted @POST @Transactional public Speaker create(Speaker newSpeaker) { service.create(newSpeaker); return newSpeaker; } } The root path for our SpeakerResource is /speaker. We inject the SpeakerService and define a method create() for creating a Speaker. We would like to be able to send @Post requests to this endpoint and Jsonb or Jackson, whichever we currently prefer, will deserialize the JSON body in a Speaker object for us.\nSplendid, time to switch from coding mode to testing.\nWe launch that Quarkus application in developer mode\nmvn quarkus:dev Quarkus is so friendly and provides a swagger-ui in dev mode for testing our endpoint. Super duper lets call the create() endpoint via Swagger:\nBecause we are lazy we accept the default Swagger provides for us and just click Execute.\nBOOM, 500 internal server error. And a beautiful Java exception:\norg.jboss.resteasy.spi.UnhandledException: javax.persistence.PersistenceException: org.hibernate.PersistentObjectException: detached entity passed to persist: org.acme.conference.speaker.Speaker What? Detached entity what does this mean and why?\nEnlightenment Behind the scenes Hibernate uses a so called EntityManager for managing entities. An Entity can be in the following states when managed by Hibernate:\nNEW: The entity object was just created and is not persisted to the database MANAGED: The entity is managed by a running Session and all changes to the entity will be propagated to the database. After call to entitymanager.persist() or in our case newSpeaker.persist() the entity is stored in the database and in the managed state. REMOVED: The entity is removed from the database. And finally DETACHED: The Entity was detached from the EntityManager, e.g. by calling entitymanager.detach() or entitymanager.close(). See this blog for a way better explanation what is going on with entity states.\nOk, cool but why the hell is our Speaker entity in the DETACHED state? It was just created and never saved to the database before!\nAfter checking the database (was empty), I started my Java debugger of choice (IntellJ, but use whatever fit\u0026#39;s your needs. I\u0026#39;m to old for IDE vs Editor and Editor vs Editor wars).\nSo looking at the Speaker entity before calling persist() revealed the following:\nThe Speaker object passed into create() has an Id of 0 and all the internal Hibernate fields are set to null. So this seems to indicate that this Speaker object is currently not attached to an EntityManager session. This might explain the DETACHED state.\nI started playing around with EntityManager and calling merge() on the speaker object. The code looked like this:\n@ApplicationScoped public class SpeakerService { @Inject EntityManager em; // lots of code skipped public Speaker create(Speaker speaker) { var newSpeaker = em.merge(speaker); newSpeaker.persist(); return speaker; } Looking at the newSpeaker object returned by calling entitymanager.merge() in the debugger revealed the following:\nnewSpeaker has an Id of 1 (hm, why no 0?) and some those special Hibernate fields starting with $$ have a value assigned. So for me this indicates that the object is now managed by an EntityManager session and in the MANAGED state.\nAnd the Id, already assigned to the original Speaker object, de-serialized form JSON is actually the reason for the beautiful exception above.\nExplanation So after a little bit of internet search magic I found an explanation for the exception:\nIf an Id is already assigned to an entity object, Hibernate assumes that this is an entity in the DETACHED state (if the Id is auto-generated). For an entity to be persisted to the database it has to be transferred in the MANAGED state by calling entitymanager.merge()\nFor more information see the Hibernate documentation.\nWe can only call persist() if the object is in the transient state, to quote the Hibernate documentation:\ntransient: the entity has just been instantiated and is not associated with a persistence context. It has no persistent representation in the database and typically no identifier value has been assigned (unless the assigned generator was used).\nAnd reading on we also get explanation for the detached state:\ndetached: the entity has an associated identifier but is no longer associated with a persistence context (usually because the persistence context was closed or the instance was evicted from the context)\nJust removing the Id from the POST request will solve the issue and the example started to work.\nThis is also why the Id column is different in the Speaker object (deserialized from JSON) and newSpeaker object (create by calling entitymanager.merge()). The Speaker Id got passed in from JSON, and has nothing to do with the auto generated primary key Id within our database. After calling entitymanager.merge() the entity is actually associated with a database session and the Id is auto generated.\nSo maybe this is basic stuff, but it took me quite a few hours to understand what was going on.\nMaybe this is also a bad example. Should one expose the Id if it is auto generated and only used internally? Or the code just needs to handle that case… But this needs me more learning about API design.\n"},{"uri":"https://blog.stderr.at/categories/java/","title":"Java","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/etcd/2022-01-29-automatedetcdbackup/","title":"Automated ETCD Backup","tags":["OCP","Day-2","OpenShift","etcd"],"description":"Create ETCD backups using cronjobs","content":" Securing ETCD is one of the major Day-2 tasks for a Kubernetes cluster. This article will explain how to create a backup using OpenShift Cronjob.\nThere is absolutely no warranty. Verify your backups regularly and perform restore tests. Prerequisites The following is required:\nOpenShift Cluster 4.x\nIntegrated Storage, might be NFS or anything. Best practice would be a RWX enabled storage.\nConfigure Project \u0026amp; Cronjob Create the following objects in OpenShift. This fill create:\nA Project called ocp-etcd-backup\nA PersistentVolumeClaim to store the backups. Change to your appropriate StorageClass and accessMode\nA ServiceAccount called openshift-backup\nA dedicated ClusterRole which is able to start (debug pods)\nA ClusterRoleBinding between the created ServiceAccount and the customer ClusterRole\nA 2nd ClusterRoleBinding, which gives our ServiceAccount the permission to start privileged containers. This is required to start a debug pod on a control plane node.\nA CronJob which performs the backup …​ see Callouts for inline explanations.\nA helm chart, which would create these objects below, can be found at: https://github.com/tjungbauer/ocp-auto-backup. This is probably a better way to manage the variables via the values.yaml file. kind: Namespace apiVersion: v1 metadata: name: ocp-etcd-backup annotations: openshift.io/description: Openshift Backup Automation Tool openshift.io/display-name: Backup ETCD Automation openshift.io/node-selector: \u0026#39;\u0026#39; spec: {} --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: etcd-backup-pvc namespace: ocp-etcd-backup spec: accessModes: - ReadWriteOnce (1) resources: requests: storage: 100Gi storageClassName: gp2 volumeMode: Filesystem --- kind: ServiceAccount apiVersion: v1 metadata: name: openshift-backup namespace: ocp-etcd-backup --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-etcd-backup rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - \u0026#34;nodes\u0026#34; verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: - \u0026#34;pods\u0026#34; - \u0026#34;pods/log\u0026#34; verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;watch\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: openshift-backup subjects: - kind: ServiceAccount name: openshift-backup namespace: ocp-etcd-backup roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-etcd-backup --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: etcd-backup-scc-privileged roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:openshift:scc:privileged subjects: - kind: ServiceAccount name: openshift-backup namespace: ocp-etcd-backup --- kind: CronJob apiVersion: batch/v1 metadata: name: cronjob-etcd-backup namespace: ocp-etcd-backup labels: purpose: etcd-backup spec: schedule: \u0026#39;*/5 * * * *\u0026#39; (2) startingDeadlineSeconds: 200 concurrencyPolicy: Forbid suspend: false jobTemplate: metadata: creationTimestamp: null spec: backoffLimit: 0 template: metadata: creationTimestamp: null spec: nodeSelector: node-role.kubernetes.io/master: \u0026#39;\u0026#39; (3) restartPolicy: Never activeDeadlineSeconds: 200 serviceAccountName: openshift-backup schedulerName: default-scheduler hostNetwork: true terminationGracePeriodSeconds: 30 securityContext: {} containers: - resources: requests: cpu: 300m memory: 250Mi terminationMessagePath: /dev/termination-log name: etcd-backup command: (4) - /bin/bash - \u0026#39;-c\u0026#39; - \u0026gt;- oc get no -l node-role.kubernetes.io/master --no-headers -o name | grep `hostname` | head -n 1 | xargs -I {} -- oc debug {} -- bash -c \u0026#39;chroot /host sudo -E /usr/local/bin/cluster-backup.sh /home/core/backup\u0026#39; ; echo \u0026#39;Moving Local Master Backups to target directory (from /home/core/backup to mounted PVC)\u0026#39;; mv /home/core/backup/* /etcd-backup/; echo \u0026#39;Deleting files older than 30 days\u0026#39; ; find /etcd-backup/ -type f -mtime +30 -exec rm {} \\; securityContext: privileged: true runAsUser: 0 imagePullPolicy: IfNotPresent volumeMounts: - name: temp-backup mountPath: /home/core/backup (5) - name: etcd-backup mountPath: /etcd-backup (6) terminationMessagePolicy: FallbackToLogsOnError image: registry.redhat.io/openshift4/ose-cli serviceAccount: openshift-backup volumes: - name: temp-backup hostPath: path: /home/core/backup type: \u0026#39;\u0026#39; - name: etcd-backup persistentVolumeClaim: claimName: etcd-backup-pvc dnsPolicy: ClusterFirst tolerations: - operator: Exists effect: NoSchedule - operator: Exists effect: NoExecute successfulJobsHistoryLimit: 5 failedJobsHistoryLimit: 5 1 RWO is used here, since I have no other available storage on my test cluster. 2 How often shall the job be executed. Here, every 5 minutes. 3 Bind the job to \u0026#34;Master\u0026#34; nodes. 4 Command to be executed…​ It fetches the actual local master nodename and starts a debugging Pod there. The backup script is called and moves the backup to /home/core/backup which is a folder on the control plane itself. The move command will move the backups from the local folder to the actual backup target volume. Finally, it will remove backups older than 30 days. 5 Mounted /home/core/backup on the master nodes, here the command will store the backups before they are moved 6 Target destination for the etcd backup on the mounted PVC Start a Job If you do not want to wait until the CronJob is triggered, you can manually start the Job using the following commands:\noc create job backup --from=cronjob/cronjob-etcd-backup -n ocp-etcd-backup This will start a Pod which will do the backup:\nStarting pod/ip-10-0-196-187us-east-2computeinternal-debug ... To use host binaries, run `chroot /host` found latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-15 found latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10 found latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-9 found latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-3 etcdctl is already installed {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:1638199790.980932,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:119\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;created temporary db file\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/home/core/backup/snapshot_2021-11-29_152949.db.part\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2021-11-29T15:29:50.991Z\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;clientv3/maintenance.go:200\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;opened snapshot stream; downloading\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:1638199790.9912837,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:127\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;fetching snapshot\u0026#34;,\u0026#34;endpoint\u0026#34;:\u0026#34;https://10.0.196.187:2379\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2021-11-29T15:29:53.306Z\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;clientv3/maintenance.go:208\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;completed snapshot read; closing\u0026#34;} Snapshot saved at /home/core/backup/snapshot_2021-11-29_152949.db {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:1638199793.3482974,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:142\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;fetched snapshot\u0026#34;,\u0026#34;endpoint\u0026#34;:\u0026#34;https://10.0.196.187:2379\u0026#34;,\u0026#34;size\u0026#34;:\u0026#34;180 MB\u0026#34;,\u0026#34;took\u0026#34;:2.367303503} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:1638199793.348459,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:152\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;saved\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/home/core/backup/snapshot_2021-11-29_152949.db\u0026#34;} {\u0026#34;hash\u0026#34;:1180914745,\u0026#34;revision\u0026#34;:10182252,\u0026#34;totalKey\u0026#34;:19360,\u0026#34;totalSize\u0026#34;:179896320} snapshot db and kube resources are successfully saved to /home/core/backup Removing debug pod ... Moving Local Master Backups to target directory (from /home/core/backup to mounted PVC) Verifying the Backup Let’s start a dummy Pod which can access the PVC to verify if the backup is really there.\napiVersion: v1 kind: Pod metadata: name: verify-etcd-backup spec: containers: - name: verify-etcd-backup image: registry.access.redhat.com/ubi8/ubi command: [\u0026#34;sleep\u0026#34;, \u0026#34;3000\u0026#34;] volumeMounts: - name: etcd-backup mountPath: /etcd-backup volumes: - name: etcd-backup persistentVolumeClaim: claimName: etcd-backup-pvc Logging into that Pod will show the available backups stored at /etcd-backup which is the mounted PVC.\noc rsh -n ocp-etcd-backup verify-etcd-backup ls -la etcd-backup total 1406196 drwxr-xr-x. 3 root root 4096 Nov 29 17:00 . dr-xr-xr-x. 1 root root 25 Nov 29 17:06 .. drwx------. 2 root root 16384 Nov 29 15:21 lost+found -rw-------. 1 root root 179896352 Nov 29 15:21 snapshot_2021-11-29_152150.db -rw-------. 1 root root 179896352 Nov 29 15:29 snapshot_2021-11-29_152949.db -rw-------. 1 root root 179896352 Nov 29 15:32 snapshot_2021-11-29_153159.db -rw-------. 1 root root 179896352 Nov 29 15:36 snapshot_2021-11-29_153618.db -rw-------. 1 root root 179896352 Nov 29 15:55 snapshot_2021-11-29_155513.db -rw-------. 1 root root 179896352 Nov 29 16:00 snapshot_2021-11-29_160020.db -rw-------. 1 root root 179896352 Nov 29 16:55 snapshot_2021-11-29_165521.db -rw-------. 1 root root 179896352 Nov 29 17:00 snapshot_2021-11-29_170020.db -rw-------. 1 root root 89875 Nov 29 15:21 static_kuberesources_2021-11-29_152150.tar.gz -rw-------. 1 root root 89875 Nov 29 15:29 static_kuberesources_2021-11-29_152949.tar.gz -rw-------. 1 root root 89875 Nov 29 15:32 static_kuberesources_2021-11-29_153159.tar.gz -rw-------. 1 root root 89875 Nov 29 15:36 static_kuberesources_2021-11-29_153618.tar.gz -rw-------. 1 root root 89875 Nov 29 15:55 static_kuberesources_2021-11-29_155513.tar.gz -rw-------. 1 root root 89875 Nov 29 16:00 static_kuberesources_2021-11-29_160020.tar.gz -rw-------. 1 root root 89875 Nov 29 16:55 static_kuberesources_2021-11-29_165521.tar.gz -rw-------. 1 root root 89875 Nov 29 17:00 static_kuberesources_2021-11-29_170020.tar.gz "},{"uri":"https://blog.stderr.at/tags/etcd/","title":"Etcd","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/annotations/","title":"Annotations","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/environments/","title":"Environments","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ingress/","title":"Ingress","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ingresscontroller/","title":"IngressController","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/labels/","title":"Labels","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/nodeselector/","title":"NodeSelector","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/pod-placement/","title":"Pod Placement","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/pod-placement/","title":"Pod Placement","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/labels_environmets/2022-01-12-creatingenvironment/","title":"Working with Environments","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector","Environments","Ingress","IngressController","Labels","Annotations"],"description":"Create separate environments on one OpenShift cluster","content":" Imagine you have one OpenShift cluster and you would like to create 2 or more environments inside this cluster, but also separate them and force the environments to specific nodes, or use specific inbound routers. All this can be achieved using labels, IngressControllers and so on. The following article will guide you to set up dedicated compute nodes for infrastructure, development and test environments as well as the creation of IngressController which are bound to the appropriate nodes.\nPrerequisites Before we start we need an OpenShift cluster of course. In this example we have a cluster with typical 3 control plane nodes (labelled as master) and 7 compute nodes (labelled as worker)\noc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-149-168.us-east-2.compute.internal Ready worker 13h v1.21.1+6438632 # \u0026lt;-- will become infra ip-10-0-154-244.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # \u0026lt;-- will become infra ip-10-0-158-44.us-east-2.compute.internal Ready worker 15m v1.21.1+6438632 # \u0026lt;-- will become infra ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-188-198.us-east-2.compute.internal Ready worker 13h v1.21.1+6438632 # \u0026lt;-- will become worker-test ip-10-0-191-9.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # \u0026lt;-- will become worker-test ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-195-201.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # \u0026lt;-- will become worker-dev ip-10-0-199-235.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # \u0026lt;-- will become worker-dev We will use the 7 nodes to create the environments for:\nInfrastructure Services (3 nodes) - will be labelled as infra\nDevelopment Environment (2 nodes) - will be labelled as worker-dev\nTest Environment (2 nodes) - will be labelled as worker-test\nTo do this, we will label the nodes and create dedicated roles for them.\nCreate Nodes Labels and MachineConfigPools Let’s create a maschine config pool for the different environments.\nThe pool inherits the configuration from worker nodes by default, which means that any new update on the worker configuration will also update custom labeled nodes. Or in other words: it is possible to remove the worker label from the custom pools.\nCreate the following objects:\n# Infrastructure apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: infra spec: machineConfigSelector: matchExpressions: - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,infra]} (1) maxUnavailable: 1 (2) nodeSelector: matchLabels: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; (3) paused: false --- # Worker-DEV apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: worker-dev spec: machineConfigSelector: matchExpressions: - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-dev]} nodeSelector: matchLabels: node-role.kubernetes.io/worker-dev: \u0026#34;\u0026#34; --- # Worker-TEST apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: worker-test spec: machineConfigSelector: matchExpressions: - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-test]} nodeSelector: matchLabels: node-role.kubernetes.io/worker-test: \u0026#34;\u0026#34; 1 User worker and infra so that the default worker configuration gets applied during upgrades 2 Whenever an update happens, do only 1 at a time 3 This pool is valid for nodes which are labelled as infra Now let’s label our nodes. First add the new, additional label:\n# Add the label \u0026#34;infra\u0026#34; oc label node ip-10-0-149-168.us-east-2.compute.internal ip-10-0-154-244.us-east-2.compute.internal ip-10-0-158-44.us-east-2.compute.internal node-role.kubernetes.io/infra= # Add the label \u0026#34;worker-dev\u0026#34; oc label node ip-10-0-195-201.us-east-2.compute.internal ip-10-0-199-235.us-east-2.compute.internal node-role.kubernetes.io/worker-dev= # Add the label \u0026#34;worker-test\u0026#34; oc label node ip-10-0-188-198.us-east-2.compute.internal ip-10-0-191-9.us-east-2.compute.internal node-role.kubernetes.io/worker-test= This will result in:\nNAME STATUS ROLES AGE VERSION ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-149-168.us-east-2.compute.internal Ready infra,worker 13h v1.21.1+6438632 ip-10-0-154-244.us-east-2.compute.internal Ready infra,worker 20m v1.21.1+6438632 ip-10-0-158-44.us-east-2.compute.internal Ready infra,worker 19m v1.21.1+6438632 ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-188-198.us-east-2.compute.internal Ready worker,worker-test 13h v1.21.1+6438632 ip-10-0-191-9.us-east-2.compute.internal Ready worker,worker-test 20m v1.21.1+6438632 ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-195-201.us-east-2.compute.internal Ready worker,worker-dev 20m v1.21.1+6438632 ip-10-0-199-235.us-east-2.compute.internal Ready worker,worker-dev 20m v1.21.1+6438632 We can remove the worker label from the worker-dev and worker-test nodes now.\nKeep the label \u0026#34;worker\u0026#34; for the infra nodes as this is the default worker label which is used when no nodeselector is in use. You can use any other node, just keep in mind that per default new applications will be started on nodes with the labels \u0026#34;worker\u0026#34;. As an alternative, you can also define a cluster-wide default node selector. oc label node ip-10-0-195-201.us-east-2.compute.internal ip-10-0-199-235.us-east-2.compute.internal node-role.kubernetes.io/worker- oc label node ip-10-0-188-198.us-east-2.compute.internal ip-10-0-191-9.us-east-2.compute.internal node-role.kubernetes.io/worker- The final node labels will look like the following:\nNAME STATUS ROLES AGE VERSION ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-149-168.us-east-2.compute.internal Ready infra,worker 13h v1.21.1+6438632 ip-10-0-154-244.us-east-2.compute.internal Ready infra,worker 22m v1.21.1+6438632 ip-10-0-158-44.us-east-2.compute.internal Ready infra,worker 21m v1.21.1+6438632 ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-188-198.us-east-2.compute.internal Ready worker-test 13h v1.21.1+6438632 ip-10-0-191-9.us-east-2.compute.internal Ready worker-test 21m v1.21.1+6438632 ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-195-201.us-east-2.compute.internal Ready worker-dev 21m v1.21.1+6438632 ip-10-0-199-235.us-east-2.compute.internal Ready worker-dev 22m v1.21.1+6438632 Since the custom pools (infra, worker-test and worker-dev) inherit their configuration from the default worker pool, no changes on the files on the nodes themselves are triggered at this point. Create Custom Configuration Let’s test our setup by deploying a configuration on specific nodes. The following MaschineConfig objects will create a file at /etc/myfile on the nodes labelled either infra, worker-dev or worker_test. Dependent on the node role the content of the file will vary.\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: infra (1) name: 55-infra spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:,infra (2) filesystem: root mode: 0644 path: /etc/myfile (3) --- apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: worker-dev name: 55-worker-dev spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:,worker-dev filesystem: root mode: 0644 path: /etc/myfile --- apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: worker-test name: 55-worker-test spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:,worker-test filesystem: root mode: 0644 path: /etc/myfile 1 Valid for node with the role xyz 2 Content of the file 3 File to be created Since this is a new configuration, all nodes will get reconfigured.\nNAME STATUS ROLES AGE VERSION ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-149-168.us-east-2.compute.internal Ready,SchedulingDisabled infra,worker 13h v1.21.1+6438632 ip-10-0-154-244.us-east-2.compute.internal Ready infra,worker 28m v1.21.1+6438632 ip-10-0-158-44.us-east-2.compute.internal Ready infra,worker 27m v1.21.1+6438632 ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-188-198.us-east-2.compute.internal Ready worker-test 13h v1.21.1+6438632 ip-10-0-191-9.us-east-2.compute.internal NotReady,SchedulingDisabled worker-test 27m v1.21.1+6438632 ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-195-201.us-east-2.compute.internal Ready worker-dev 27m v1.21.1+6438632 ip-10-0-199-235.us-east-2.compute.internal NotReady,SchedulingDisabled worker-dev 28m v1.21.1+6438632 Wait until all nodes are ready and test the configuration by verifying the content of /etc/myfile:\n### # infra nodes: ### oc get pods -n openshift-machine-config-operator -l k8s-app=machine-config-daemon --field-selector \u0026#34;spec.nodeName=ip-10-0-149-168.us-east-2.compute.internal\u0026#34; NAME READY STATUS RESTARTS AGE machine-config-daemon-f85kd 2/2 Running 6 16h # Get file content oc rsh -n openshift-machine-config-operator machine-config-daemon-f85kd chroot /rootfs cat /etc/myfile Defaulted container \u0026#34;machine-config-daemon\u0026#34; out of: machine-config-daemon, oauth-proxy infra ### #worker-dev: ### oc get pods -n openshift-machine-config-operator -l k8s-app=machine-config-daemon --field-selector \u0026#34;spec.nodeName=ip-10-0-195-201.us-east-2.compute.internal\u0026#34; NAME READY STATUS RESTARTS AGE machine-config-daemon-s6rr5 2/2 Running 4 3h5m # Get file content oc rsh -n openshift-machine-config-operator machine-config-daemon-s6rr5 chroot /rootfs cat /etc/myfile Defaulted container \u0026#34;machine-config-daemon\u0026#34; out of: machine-config-daemon, oauth-proxy worker-dev ### # worker-test: ### oc get pods -n openshift-machine-config-operator -l k8s-app=machine-config-daemon --field-selector \u0026#34;spec.nodeName=ip-10-0-188-198.us-east-2.compute.internal\u0026#34; NAME READY STATUS RESTARTS AGE machine-config-daemon-m22rf 2/2 Running 6 16h # Get file content oc rsh -n openshift-machine-config-operator machine-config-daemon-m22rf chroot /rootfs cat /etc/myfile Defaulted container \u0026#34;machine-config-daemon\u0026#34; out of: machine-config-daemon, oauth-proxy worker-test The file /etc/myfile exists on all nodes and depending on their role the files have a different content.\nBind an Application to a Specific Environment The following will label the nodes with a specific environment and will deploy an example application, which should only be executed on the appropriate nodes.\nLet’s label the nodes with environment=worker-dev and environment=worker-test:\noc label node ip-10-0-195-201.us-east-2.compute.internal ip-10-0-199-235.us-east-2.compute.internal environment=worker-dev oc label node ip-10-0-188-198.us-east-2.compute.internal ip-10-0-191-9.us-east-2.compute.internal environment=worker-test Create a namespace for the example application\noc new-project bookinfo Create an annotation and a label for the namespace. The annotation will make sure that the application will only be started on nodes with the same label. The label will be later used for the IngressController setup.\noc annotate namespace bookinfo environment=worker-dev oc annotate namespace bookinfo openshift.io/node-selector: environment=worker-test oc label namespace bookinfo environment=worker-dev Deploy the example application. In this article the sample application of Istio was used:\noc apply -f https://raw.githubusercontent.com/istio/istio/release-1.11/samples/bookinfo/platform/kube/bookinfo.yaml This will start the application on worker-dev` nodes only, because the annotation in the namespace was created accordingly.\noc get pods -n bookinfo -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES details-v1-86dfdc4b95-v8zfv 1/1 Running 0 9m19s 10.130.2.17 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; productpage-v1-658849bb5-8gcl7 1/1 Running 0 7m17s 10.128.4.21 ip-10-0-195-201.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ratings-v1-76b8c9cbf9-cc4js 1/1 Running 0 9m19s 10.130.2.19 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v1-58b8568645-mbgth 1/1 Running 0 7m44s 10.128.4.20 ip-10-0-195-201.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v2-5d8f8b6775-qkdmz 1/1 Running 0 9m19s 10.130.2.21 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v3-666b89cfdf-8zv8w 1/1 Running 0 9m18s 10.130.2.22 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Create Dedicated IngressController IngressController are responsible to bring the traffic into the cluster. OpenShift comes with one default controller, but it is possible to create more in order to use different domains and separate the incoming traffic to different nodes.\nBind the default ingress controller to the infra labeled nodes, so we can be sure that the default router pods are executed only on these nodes:\noc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch=\u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;nodePlacement\u0026#34;:{\u0026#34;nodeSelector\u0026#34;: {\u0026#34;matchLabels\u0026#34;:{\u0026#34;node-role.kubernetes.io/infra\u0026#34;:\u0026#34;\u0026#34;}}}}}\u0026#39; The pods will get restarted, to be sure they are running on infra:\noc get pods -n openshift-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES router-default-78f8dd6f69-dbtbv 0/1 ContainerCreating 0 2s \u0026lt;none\u0026gt; ip-10-0-149-168.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-78f8dd6f69-wwpgb 0/1 ContainerCreating 0 2s \u0026lt;none\u0026gt; ip-10-0-158-44.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-7bbbc8f9bd-vfh84 1/1 Running 0 22m 10.129.4.6 ip-10-0-158-44.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-7bbbc8f9bd-wggrx 1/1 Terminating 0 19m 10.128.2.8 ip-10-0-149-168.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Create the following IngressController objects for worker-dev and worker-test. Replace with the domain of your choice\napiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: ingress-worker-dev namespace: openshift-ingress-operator spec: domain: worker-dev.\u0026lt;yourdomain\u0026gt; (1) endpointPublishingStrategy: type: HostNetwork httpErrorCodePages: name: \u0026#39;\u0026#39; namespaceSelector: matchLabels: environment: worker-dev (2) nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker-dev: \u0026#39;\u0026#39; (3) replicas: 3 tuningOptions: {} unsupportedConfigOverrides: null --- apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: ingress-worker-test namespace: openshift-ingress-operator spec: domain: worker-test.\u0026lt;yourdomain\u0026gt; endpointPublishingStrategy: type: HostNetwork httpErrorCodePages: name: \u0026#39;\u0026#39; namespaceSelector: matchLabels: environment: worker-test nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker-test: \u0026#39;\u0026#39; replicas: 3 tuningOptions: {} unsupportedConfigOverrides: null 1 Domainname which is used by this Controller 2 Namespace selector …​ namespaces with such label will be handled by this IngressController 3 Node Placement …​ This Controller should run on nodes with this label/role This will spin up additional router pods on the collect labelled nodes:\noc get pods -n openshift-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES router-default-78f8dd6f69-dbtbv 1/1 Running 0 8m7s 10.128.2.11 ip-10-0-149-168.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-78f8dd6f69-wwpgb 1/1 Running 0 8m7s 10.129.4.10 ip-10-0-158-44.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-ingress-worker-dev-76b65cf558-mspvb 1/1 Running 0 113s 10.130.2.13 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-ingress-worker-dev-76b65cf558-p2jpg 1/1 Running 0 113s 10.128.4.12 ip-10-0-195-201.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-ingress-worker-test-6bbf9967f-4whfs 1/1 Running 0 113s 10.131.2.13 ip-10-0-191-9.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-ingress-worker-test-6bbf9967f-jht4w 1/1 Running 0 113s 10.131.0.8 ip-10-0-188-198.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Verify Ingress Configuration To test our new ingress router lets create a route object for our example application:\nkind: Route apiVersion: route.openshift.io/v1 metadata: name: productpage namespace: bookinfo spec: host: productpage-bookinfo.worker-dev.\u0026lt;yourdomain\u0026gt; to: kind: Service name: productpage weight: 100 port: targetPort: http wildcardPolicy: None --- kind: Route apiVersion: route.openshift.io/v1 metadata: name: productpage-worker-test namespace: bookinfo spec: host: productpage-bookinfo.worker-test.\u0026lt;yourdomain\u0026gt; to: kind: Service name: productpage weight: 100 port: targetPort: http wildcardPolicy: None Be sure that the name is resolvable and a load balancer is configured accordingly Verify that the router pod has the correct configuration in the file haproxy.config :\noc get pods -n openshift-ingress NAME READY STATUS RESTARTS AGE router-default-78f8dd6f69-dbtbv 1/1 Running 0 95m router-default-78f8dd6f69-wwpgb 1/1 Running 0 95m router-ingress-worker-dev-76b65cf558-mspvb 1/1 Running 0 88m router-ingress-worker-dev-76b65cf558-p2jpg 1/1 Running 0 88m router-ingress-worker-test-6bbf9967f-4whfs 1/1 Running 0 88m router-ingress-worker-test-6bbf9967f-jht4w 1/1 Running 0 88m Verify the content of the haproxy configuration for one of the worker-dev router\noc rsh -n openshift-ingress router-ingress-worker-dev-76b65cf558-mspvb cat haproxy.config | grep productpage backend be_http:bookinfo:productpage server pod:productpage-v1-658849bb5-8gcl7:productpage:http:10.128.4.21:9080 10.128.4.21:9080 cookie 3758caf21badd7e4f729209173eece08 weight 256 Compare with worker-test router\noc rsh -n openshift-ingress router-ingress-worker-test-6bbf9967f-jht4w cat haproxy.config | grep productpage --\u0026gt; Empty result, this router is not configured with that route. Compare with default router:\nbackend be_http:bookinfo:productpage server pod:productpage-v1-658849bb5-8gcl7:productpage:http:10.128.4.21:9080 10.128.4.21:9080 cookie 3758caf21badd7e4f729209173eece08 weight 256 Why does this happen? Why are the default router and the router for worker-dev configured? This happens because it is the default router and we must explicitly tell it to ignore certain labels.\nModify the default IngressController\noc edit ingresscontroller.operator default -n openshift-ingress-operator Add the following\nnamespaceSelector: matchExpressions: - key: environment operator: NotIn values: - worker-dev - worker-test This will tell the default IngressController to ignore selectors on worker-dev and worker-test\nWait a few seconds until the route pods have been restarted:\noc get pods -n openshift-ingress NAME READY STATUS RESTARTS AGE router-default-744998df46-8lh4t 1/1 Running 0 2m32s router-default-744998df46-hztgf 1/1 Running 0 2m31s router-ingress-worker-dev-76b65cf558-mspvb 1/1 Running 0 96m router-ingress-worker-dev-76b65cf558-p2jpg 1/1 Running 0 96m router-ingress-worker-test-6bbf9967f-4whfs 1/1 Running 0 96m router-ingress-worker-test-6bbf9967f-jht4w 1/1 Running 0 96m And test again\noc rsh -n openshift-ingress router-default-744998df46-8lh4t cat haproxy.config | grep productpage --\u0026gt; empty result At this point the new router feels responsible. Be sure to have a load balancer configured correctly. Appendix Bind other infra-workload to infrastructure nodes:\nInternal Registry oc patch configs.imageregistry.operator.openshift.io/cluster -n openshift-image-registry --type=merge --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;nodeSelector\u0026#34;:{\u0026#34;node-role.kubernetes.io/infra\u0026#34;:\u0026#34;\u0026#34;}}}\u0026#39; OpenShift Monitoring Workload Create the following file and apply it.\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; cluster-monitoring-config-cm.yaml apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; prometheusOperator: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; grafana: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; telemeterClient: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; EOF oc create -f cluster-monitoring-config-cm.yaml "},{"uri":"https://blog.stderr.at/categories/advanced-cluster-security/","title":"Advanced Cluster Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/acs/2021-12-11-acsauth/","title":"Advanced Cluster Security - Authentication","tags":["ACS","Advanced Cluster Security","OpenShift","Security","Keycloak","Authentication","SSO","Stackrox"],"description":"Red Hat Advanced Cluster Security - Authentication","content":" Red Hat Advanced Cluster Security (RHACS) Central is installed with one administrator user by default. Typically, customers request an integration with existing Identity Provider(s) (IDP). RHACS offers different options for such integration. In this article 2 IDPs will be configured as an example. First OpenShift Auth and second Red Hat Single Sign On (RHSSO) based on Keycloak\nPrerequisites OpenShift 4 Cluster\nAdvanced Cluster Security v3.66+\nRed Hat SSO Operator installed\nWhile RHSSO will be installed during this article, only default and example values are used. These are by no means examples for a production system. Introduction Advanced Cluster Security comes with several default roles, which can be assigned to users:\nSystem role Description Admin\nThis role is targeted for administrators. Use it to provide read and write access to all resources.\nAnalyst\nThis role is targeted for a user who cannot make any changes, but can view everything. Use it to provide read-only access for all resources.\nContinuous Integration\nThis role is targeted for CI (continuous integration) systems and includes the permission set required to enforce deployment policies.\nNone\nThis role has no read and write access to any resource. You can set this role as the minimum access role for all users.\nSensor Creator\nRed Hat Advanced Cluster Security for Kubernetes uses this role to automate new cluster setups. It includes the permission set to create Sensors in secured clusters.\nScope Manager\nThis role includes the minimum permissions required to create and modify access scopes.\nIt is possible to create custom roles. Configure RHACS Authentication: OpenShift Auth It is assumed that RHACS is already installed and login to the Central UI is available. Login to your RHACS and select “Platform Configuration” \u0026gt; “Access Control”\nFrom the drop down menu Add auth provider select OpenShift Auth\nFigure 1. ACS Auth Provider Enter a Name for your provider and select a default role which is assigned to any user who can authenticate.\nIt is recommended to select the role None, so new accounts will have no privileges in RHACS.\nWith Rules you can assign roles to specific users, based on their userid, name, mail address or groups.\nFor example the user with the name poweruser gets the role Admin assigned.\nVerify Authentication with OpenShift Auth Logout from the Central UI and reload the browser.\nSelect from the drop down OpenShift Auth\nFigure 2. ACS Login Try to login with a valid OpenShift user. Depending on the Rules which have been defined during previous steps the appropriate permissions should be assigned. For example: If you login as user poweruser the role Admin is assigned.\nConfigure Red Hat Single Sign On The following steps will create some basic example objects to an existing RHSSO or Keycloak to test the authentication at RHACS. Skip to step #5 if you have Keycloak already up and running and would like to reuse an existing client.\nThe RHSSO operator (or Keycloak) is installed at the namespace single-sign-on.\nCreate an instance of Keycloak\napiVersion: keycloak.org/v1alpha1 kind: Keycloak metadata: name: example-keycloak namespace: single-sign-on spec: externalAccess: enabled: true instances: 1 Create a Realm This will create a Realm called Basic\napiVersion: keycloak.org/v1alpha1 kind: KeycloakRealm metadata: name: example-keycloakrealm namespace: single-sign-on spec: instanceSelector: matchLabels: app: sso realm: displayName: Basic Realm enabled: true id: basic realm: basic Login into Red Hat SSO Get the route to your RHSSO instance:\noc get route keycloak -n single-sign-on --template=\u0026#39;{{ .spec.host }}\u0026#39; # keycloak-single-sign-on.apps.cluster-29t8z.29t8z.sandbox677.opentlc.com and log into the Administration Interface.\nExtract the admin password for Keycloak\nThe secret name is build from \u0026#34;credential\u0026#34;\u0026lt;keycloak-instance-name\u0026gt;\noc extract secret/credential-example-keycloak -n single-sign-on --to=- # ADMIN_PASSWORD \u0026lt;you password\u0026gt; # ADMIN_USERNAME admin Be sure to select your Realm (Basic in our case), goto Clients and select a ClientID.\nIn this example we select account\nFigure 3. ACS Login Of course you can create or use any other Client. Enable the option Implicit Flow\nGet the Issuer URL from your realm. This is typically your: https://\u0026lt;KEYCLOAK_URL\u0026gt;/auth/realms/\u0026lt;REALM_NAME\u0026gt;;\nFor Example: https://keycloak-single-sign-on.apps.cluster-29t8z.29t8z.sandbox677.opentlc.com/auth/realms/basic\nCreate Test Users In RHSSO create 2 user accounts to test the authentication later.\nGoto Users and create the users:\nUser: acsadmin\nFirst Name: acsadmin\nUser: user1\nFirst Name: user 1\nYou can set any other values for these users. However, be sure to set a password for both, after they have been created.\nConfigure RHACS Authentication: RHSSO It is assumed that RSACS is already installed and login to the Central UI is available. Login to your RHACS and select “Platform Configuration” \u0026gt; “Access Control”\nFrom the drop down menu Add auth provider select OpenID Connect\nEnter a “Name” for your provider i.e. “Single Sign On”\nLeave the “Callback Mode” to the “Auto-Select” setting\nEnter your Issuer URL\nAs Client ID enter account (or the ClientID you would like to use)\nLeave the Client Secret empty and select the checkbox Do not use Client Secret which is good enough for our tests.\nRemember the two callback URL from the blue box. They must be configured in Keycloak.\nSelect a default role which is assigned to any user who can authenticate.\nIt is recommended to select the role None, so new accounts will have no privileges in RHACS.\nWith Rules you can assign roles to specific users, based on their userid, name, mail address or groups.\nFor example the user with the name acsadmin (which have been created previously in our RHSSO) gets the role Admin assigned.\nThe final settings are depict in the following image:\nFigure 4. ACS Login Continue RHSSO Configuration What is left to do is the configuration of redirect URLs. These URLs are shown in the ACS Authentication Provider configuration (see blue field in the image above)\nLog back into RHSSO and select “Clients” \u0026gt; “account”\nInto Valid Redirect URLs enter the two URLs which you saved from the blue box in the RHACS configuration.\nTroubleshoot: Test Login In RHACS you can test the login to you SSO.\nGoto \u0026#34;Platform Configuration\u0026#34; \u0026gt; \u0026#34;Access Control\u0026#34;\nClick the button \u0026#34;Test login\u0026#34;\nA popup will appear which asks you to enter SSO credentials. The connection to RHSSO will be validated:\nFigure 5. ACS Test SSO Verify Authentication with OpenShift Auth Logout from the Central UI and reload the browser.\nSelect from the drop down Single Sign On\nFigure 6. ACS Login SSO Try to login with a valid SSO user. Depending on the Rules which have been defined during previous steps the appropriate permissions should be assigned. For example: If you login as user acsadmin the role Admin is assigned.\n"},{"uri":"https://blog.stderr.at/tags/authentication/","title":"Authentication","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/keycloak/","title":"Keycloak","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/security/","title":"Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/sso/","title":"SSO","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/stackrox/","title":"Stackrox","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ansible/","title":"Ansible","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/ansible/","title":"Ansible","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/2021/11/ansible-style-guide/","title":"Ansible Style Guide","tags":["Ansible","Controller","Automation Controller","Style","StyleGuide","Best Practices","Playbook","Role"],"description":"Howto write and debug Ansible playbooks","content":" You should always follow the Best Practices and Ansible Lint rules defined by the Ansible documentation when developing playbooks.\nAlthough very basic, the Best Practices document gives a few guidelines to be able to carry out well-structured playbooks and roles, it contains recommendations that evolve with the project, so it is recommended to review it regularly. It is advisable to review the organization of content in Ansible.\nThe Ansible Lint documentation shows us through this tool the syntax rules that will be checked in the testing of roles and playbooks, the rules that will be checked are indicated in this document in their respective section.\nThis style guide is meant as a base of playbook development. It is not the ultimate truth. Always verify and apply appropriate company rules. There are many additional references available, such as: https://github.com/whitecloud/ansible-styleguide or https://github.com/redhat-cop/automation-good-practices General Recommendations Treat your Ansible content like code: Any playbook, role, inventory or collection should be versionized using Git or a similar tool.\nIterate: start with basic playbook and refactor later\nUse multiple Git repositories: on per role/collection, separate inventory from other repositories\nUse human-meaningful names in inventory files\ngroup hosts in inventory files\nuse a consistent Directory Structure\nOptimize Playbook Execution Disable facts gathering if it is not required.\nTry not to use: ansible_facts[‘hostname’] (or ‘nodename’)\nTry to use inventory_hostname and inventory_hostname_short instead\nIt is possible to selectively gather facts (gather_subnet)\nConsider caching facts\nIncrease Parallelism\nAnsible runs 1st task on every host, then 2nd task on every host …\nUse “forks” (default is 5) to control how many connections can be active (load will increase, try first with conservative values)\nWhile testing forks analize Enable CPU and Memory Profiling\nIf you use the module \u0026#34;copy\u0026#34; for large files: do not use “copy” module. Use “synchronize” module instead (based on rsync)\nUse lists when ever a modules support it (i.e. yum) Instead of\ntasks: - name: Ensure the packages are installed yum: name: \u0026#34;{{ item }}\u0026#34; state: present loop: - httpd - mod_ssl - httpd-tools - mariadb-server - mariadb - php - php-mysqlnd use\ntasks: - name: Ensure the packages are installed yum: state: present name: - httpd - mod_ssl - httpd-tools - mariadb-server - mariadb - php - php-mysqlnd Optimize SSH Connections\nin ansible.cfg set\nControlMaster\nControlPersist\nPreferredAuthentications\nEnabling Pipelining\nNot enabled by default\nreduces number of SSH operations\nrequires to disable requiertty in sudo options\nName Tasks and Plays Every task or play should be explicitly named in a way that the purpose is easily understood and can be referred to if the playbook has to be restarted from a certain task.\nFor example the following is hard to read and debug:\nPLAY [localhost] ******************************** TASK [include_vars] ******************************** ok: [localhost] TASK [yum] ******************************** ok: [localhost] While with naming it is easier to follow what is happening:\nPLAY [Create a new virtual machine] ******************************** TASK [Include vmware-credentials] ******************************** ok: [localhost] TASK [Install required packages with yum] ******************************** ok: [localhost] # bad # set another variable - set_fact: my_second_var: \u0026#34;{{ my_var }}\u0026#34; # good - name: set another variable set_fact: my_second_var: \u0026#34;{{ my_var }}\u0026#34; Reason Better understanding what is currently happening in a play and better possibility to debug.\nVariables in Task Names Include as much information as necessary to explain the purpose of a task. Make usage of variables inside a task name to create dynamic output messages.\n#bad - name: \u0026#39;Change status\u0026#39; service: enabled: true name: \u0026#39;httpd\u0026#39; state: \u0026#39;{{ state }}\u0026#39; become: true #good - name: \u0026#39;Change status of httpd to {{ state }}\u0026#39; service: enabled: true name: \u0026#39;httpd\u0026#39; state: \u0026#39;{{ state }}\u0026#39; become: true Reason This will help to easily understand log outputs of playbooks.\nOmitting Unnecessary Information While name tasks in a playbook, do not include the name of the role which is currently executed, since Ansible will do this automatically.\nReason Avoiding the same output twice on the console will prevent confusions.\nNames All the newly created Ansible roles should follow the name convention using dashes if necessary: [company]-[action]-[function/technology]\n# bad lvm # good mycompany-setup-lvm Reason If using roles from Ansible Galaxy, it will keep consistency about which roles are created internally.\nUse Modules instead of command or shell Before using the command or shell module, verify if there is already a module available which can avoid the usage of raw shell command.\n# bad - name: install httpd tasks: - command: \u0026#34;yum install httpd\u0026#34; # good - name: install packages tasks: - name: \u0026#39;install httpd\u0026#39; yum: name: \u0026#39;httpd\u0026#39; state: \u0026#39;present\u0026#39; Reason While raw command could be seen as a security risk in general, another reason to avoid them is the loss of immutability of the ansible playbooks or roles. Ansible cannot verify if a command has been already executed before or not and will therefore execute it every time the playbook is running.\nDocumenting a Task/Play Every playbook, role or task should start with a documentation why this code has been written and what it does and should include an example usage if applicable. The comment should be followed by ---` with no blank lines around it, to indicate the actual start of the yaml definition\n#bad - name: \u0026#39;Change httpd status\u0026#39; service: enabled: true name: \u0026#39;httpd\u0026#39; state: \u0026#39;{{ state }}\u0026#39; become: true #good # Example usage: ansible-playbook -e state=started playbook.yml # This playbook changes the state of the httpd daemon --- - name: \u0026#39;Change httpd status\u0026#39; service: enabled: true name: \u0026#39;httpd\u0026#39; state: \u0026#39;{{ state }}\u0026#39; become: true Reason This common programmatic practice helps to quickly understand the purpose and usage of a playbook or role.\nLint rule Yamllint document-start rule\nEnd a File Files should be ended with a newline.\nReason This is common Unix best practice which avoids any prompt misalignment when printing files in a terminal.\nLint rule Yamllint new-line-at-end-of-file rule\nQuotes Strings should be quoted, while quotes for booleans (e.g. true/false) or integers (i.g. 42) should be avoided.\nDo NOT quote:\nhosts: targets (e.g. hosts: databases rather than hosts: ‘databases’)\ninclude_tasks: and include_roles: target file names\ntask and role names\nregistered variables\nnumber values\nboolean values\nIt is possible to use single or double quotes. In this document single quotes are used, as it seems to be more common. Double quotes are often seen in European countries, especially German speaking countries. The most important thing is to stick to one style. # bad - name: \u0026#39;start robot named my-robot\u0026#39; service: name: my-robot state: started enabled: true become: true # good - name: \u0026#39;start robot named my-robot\u0026#39; service: name: \u0026#39;my-robot\u0026#39; state: \u0026#39;started\u0026#39; enabled: true become: true # double quotes w/ nested single quotes - name: \u0026#39;start all robots\u0026#39; service: name: \u0026#39;{{ item[\u0026#34;robot_name\u0026#34;] }}\u0026#39; state: \u0026#39;started\u0026#39; enabled: true with_items: \u0026#39;{{ robots }}\u0026#39; become: true # double quotes to escape characters - name \u0026#39;print some text on two lines\u0026#39; debug: msg: \u0026#34;This text is on\\ntwo lines\u0026#34; # folded scalar style - name: \u0026#39;robot infos\u0026#39; debug: msg: \u0026gt; Robot {{ item[\u0026#39;robot_name\u0026#39;] }} is {{ item[\u0026#39;status\u0026#39;] }} and in {{ item[\u0026#39;az\u0026#39;] }} availability zone with a {{ item[\u0026#39;curiosity_quotient\u0026#39;] }} curiosity quotient. with_items: robots # folded scalar when the string has nested quotes already - name: \u0026#39;print some text\u0026#39; debug: msg: \u0026gt; \u0026#34;I haven\u0026#39;t the slightest idea,\u0026#34; said the Hatter. # do not quote booleans/numbers - name: \u0026#39;download google homepage\u0026#39; get_url: dest: \u0026#39;/tmp\u0026#39; timeout: 60 url: \u0026#39;https://google.com\u0026#39; validate_certs: true # variables example 1 - name: \u0026#39;set a variable\u0026#39; set_fact: my_var: \u0026#39;test\u0026#39; # variables example 2 - name: \u0026#39;print my_var\u0026#39; debug: var: my_var when: ansible_os_family == \u0026#39;Darwin\u0026#39; # variables example 3 - name: \u0026#39;set another variable\u0026#39; set_fact: my_second_var: \u0026#39;{{ my_var }}\u0026#39; Lint rule Yamllint quoted-strings rule\nSudo Use the new become syntax when designating that a task needs to be run with sudo privileges\n#bad - name: \u0026#39;template client.json to /etc/sensu/conf.d/\u0026#39; template: dest: \u0026#39;/etc/sensu/conf.d/client.json\u0026#39; src: \u0026#39;client.json.j2\u0026#39; sudo: true # good - name: \u0026#39;template client.json to /etc/sensu/conf.d/\u0026#39; template: dest: \u0026#39;/etc/sensu/conf.d/client.json\u0026#39; src: \u0026#39;client.json.j2\u0026#39; become: true Reason Using sudo was deprecated at Ansible version 1.9.1\nLint rule Ansible-lint E103 rule\nHosts Declarations Host sections be defined in such a way that it follows this general order:\nhost declaration\nhost options in alphabetical order\npre_tasks\nroles\ntasks\n# example - hosts: \u0026#39;webservers\u0026#39; remote_user: \u0026#39;centos\u0026#39; vars: tomcat_state: \u0026#39;started\u0026#39; pre_tasks: - name: \u0026#39;set the timezone to America/Boise\u0026#39; lineinfile: dest: \u0026#39;/etc/environment\u0026#39; line: \u0026#39;TZ=America/Boise\u0026#39; state: \u0026#39;present\u0026#39; become: true roles: - { role: \u0026#39;tomcat\u0026#39;, tags: \u0026#39;tomcat\u0026#39; } tasks: - name: \u0026#39;start the tomcat service\u0026#39; service: name: \u0026#39;tomcat\u0026#39; state: \u0026#39;{{ tomcat_state }}\u0026#39; Reason A global definition about the order of these items, will create an easy and consistent human readable code.\nTasks Declarations A task should be defined in such a way that it follows this general order:\ntask name\ntags\ntask map declaration (e.g. service:)\ntask parameters in alphabetical order (remember to always use multi-line map syntax)\nloop operators (e.g. with_items)\ntask options in alphabetical order (e.g. become, ignore_errors, register)\n# example - name: \u0026#39;create some ec2 instances\u0026#39; tags: \u0026#39;ec2\u0026#39; ec2: assign_public_ip: true image: \u0026#39;ami-c7d092f7\u0026#39; instance_tags: Name: \u0026#39;{{ item }}\u0026#39; key_name: \u0026#39;my_key\u0026#39; with_items: \u0026#39;{{ instance_names }}\u0026#39; ignore_errors: true register: ec2_output when: ansible_os_family == \u0026#39;Darwin\u0026#39; Reason A global definition about the order of these items, will create an easy and consistent human readable code.\nInclude Declaration For include statements, make sure to quote filenames and only use blank lines between include statements if they are multi-line (e.g. they have tags).\n# bad - include: other_file.yml - include: \u0026#39;second_file.yml\u0026#39; - include: third_file.yml tags=third # good - include: \u0026#39;other_file.yml\u0026#39; - include: \u0026#39;second_file.yml\u0026#39; - include: \u0026#39;third_file.yml\u0026#39; tags: \u0026#39;third\u0026#39; Reason Using such syntax, will create an easy and consistent human readable code.\nBooleans Use true/false instead of yes/no (or 1/0).\n# bad - name: \u0026#39;start httpd\u0026#39; service: name: \u0026#39;httpd\u0026#39; state: \u0026#39;restarted\u0026#39; enabled: 1 become: \u0026#39;yes\u0026#39; # good - name: \u0026#39;start httpd\u0026#39; service: name: \u0026#39;httpd\u0026#39; state: \u0026#39;restarted\u0026#39; enabled: true become: true Reason Ansible can read boolean values in many different ways, like: True/False, true/false, yes/no, 1/0. It makes sense to stick to one option, which is then equally used in any playbook or role. It is recommended to use true/false since Java and Javascript are using the same values for boolean values.\nLint rule Yamllint truthy rule\nRequired config: truthy: {allowed-values: [\u0026#34;true\u0026#34;, \u0026#34;false\u0026#34;]}`\nKey Value Pairs Only one space should be used after the colon, when defining key/value pairs.\n# bad - name : \u0026#39;start httpd\u0026#39; service: name : \u0026#39;httpd\u0026#39; state : \u0026#39;restarted\u0026#39; enabled : true become : true # good - name: \u0026#39;start httpd\u0026#39; service: name: \u0026#39;httpd\u0026#39; state: \u0026#39;restarted\u0026#39; enabled: true become: true Reason It increases human readability and reduces changeset collisions for version control.\nLint rule Yamllint colons rule\nRequired config: colons: {max-spaces-before: 0, max-spaces-after: 1}\nUsing Map Syntax Always use the map syntax for better readability.\n# bad - name: \u0026#39;create conf.d directory\u0026#39; file: \u0026#39;path=/etc/httpd/conf.d/ state=directory mode=0755 owner=httpd group=httpd\u0026#39; become: true - name: \u0026#39;copy mod_ssl.conf to /etc/httpd/conf.d\u0026#39; copy: \u0026#39;dest=/etc/httpd/conf.d/ src=mod_ssl.conf\u0026#39; become: true # good - name: \u0026#39;create conf.d directory\u0026#39; file: group: \u0026#39;httpd\u0026#39; mode: \u0026#39;0755\u0026#39; owner: \u0026#39;httpd\u0026#39; path: \u0026#39;/etc/httpd/conf.d\u0026#39; state: \u0026#39;directory\u0026#39; become: true - name: \u0026#39;copy mod_ssl.conf to /etc/httpd/conf.d\u0026#39; copy: dest: \u0026#39;/etc/httpd/conf.d/\u0026#39; src: \u0026#39;mod_ssl.conf\u0026#39; become: true Reason It increases human readability and reduces changeset collisions for version control.\nSpacing You should have blank lines between two host blocks, between two task blocks, and between host and include blocks. When indenting, you should use 2 spaces to represent sub-maps, and multi-line maps should start with a -. For a more in-depth example of how spacing (and other things) please take a look at the example below\n# Example: ansible-playbook --ask-become-pass --ask-vault-pass style.yml # # This is a sample Ansible script to showcase all of our style decisions. # Pay close attention to things like spacing, where we use quotes, etc. # The only thing you can ignore is where comments are, except this first comment: # It\u0026#39;s generally a good idea to include some good information like sample usage # at the beginning of your file, so that someone can run head on the script # to see what they should do. # # A good rule of thumb on quoting is to quote anything that represents a value # that does not represent either a primitive type, or something within the # playbook; e.g. do not quote integers, booleans, variable names, boolean logic # Variable names still need to be quoted when they are module parameters for # Ansible to properly resolve them. # You should also always have single quotes around the outer string, and # double quotes on the inside. # If for some reason this is not possible or it would require escaping quotes # (which you should avoid if you can), use the scalar string operator (shown # in this playbook). # # Directory structure style: # Your directory structure should match the structure described by the Ansible # developers: http://docs.ansible.com/ansible/playbooks_best_practices.html # # --- # # - include: \u0026#39;role_name.yml\u0026#39; # become: true # only if every task in the role requires super user # # The self-named yml file contains all of the actual role tasks. # # Header comments are followed by blank line, then --- to signify start of YAML, # then another blank line, then the script. --- - hosts: \u0026#39;localhost\u0026#39; tasks: - name: \u0026#39;fail if someone tries to run this\u0026#39; fail: msg: \u0026#39;this playbook was not meant to actually be ran. just inspect the source!\u0026#39; - include: \u0026#39;first_include.yml\u0026#39; # quote filenames - include: \u0026#39;second_include.yml\u0026#39; # no blank line needed between includes without tags - include: \u0026#39;third_include.yml\u0026#39; # includes with tags should have blank lines between tags: \u0026#39;third_include\u0026#39; - include: \u0026#39;fourth_include.yml\u0026#39; tags: \u0026#39;fourth_include\u0026#39; - hosts: \u0026#39;tag_environment_samplefruit\u0026#39; remote_user: \u0026#39;centos\u0026#39; # options in alphabetical order vars: sample_str: \u0026#39;dood\u0026#39; # use snake_case for variable names sample_bool: true # do not quote booleans or integers sample_int: 42 vars_files: - \u0026#39;group_vars/secrets.yml\u0026#39; pre_tasks: # then pre_tasks, roles, tasks - name: \u0026#39;this runs a command that involves both single and double quotes\u0026#39; command: \u0026gt; echo \u0026#34;I can\u0026#39;t even\u0026#34; args: chdir: \u0026#39;/tmp\u0026#39; - name: \u0026#39;this command just involves double quotes\u0026#39; command: \u0026#39;echo \u0026#34;Hey man\u0026#34;\u0026#39; roles: - { role: \u0026#39;sample_role\u0026#39;, tags: \u0026#39;sample_role\u0026#39; } # use this format for role listing tasks: - name: \u0026#39;get list of directory permissions in /tmp\u0026#39; command: \u0026#39;ls -l /tmp\u0026#39; register: tmp_listing # do not quote variable names when registering # A task should be defined in the following order: # name # tags # module # module arguments, alphabetical # loop operator (e.g. with_items, with_fileglob) # other options, alphabetical (e.g. become, ignore_errors, when) - name: \u0026#39;a more complicated task to show where everything goes: touch all items from /tmp\u0026#39; tags: \u0026#39;debug\u0026#39; # tags go immediately after name file: path: \u0026#39;{{ item }}\u0026#39; # use path for single file actions, dest/src for multi file actions state: \u0026#39;touch\u0026#39; # arguments go in alphabetical order with_items: tmp_listing.stdout_lines # loop things go immediately after module # the rest of the task options are in alphabetical order become: true # try to keep become only on the tasks that need it. If every task in a host uses become, then move it up to the host options ignore_errors: true when: ansible_os_family == \u0026#39;Darwin\u0026#39; and tmp_listing.stdout_lines | length \u0026gt; 1 - name: \u0026#39;some modules can have maps in their maps (woah man)\u0026#39; ec2: assign_public_ip: true group: [\u0026#39;wca_ssh\u0026#39;, \u0026#39;wca_tomcat\u0026#39;] image: \u0026#39;ami-c7d092f7\u0026#39; instance_tags: Name: \u0026#39;instance\u0026#39; service_tomcat: \u0026#39;\u0026#39; key_name: \u0026#39;ops\u0026#39; - hosts: \u0026#39;tag_environment_secondfruit\u0026#39; tasks: - name: \u0026#39;this task has multiple tags\u0026#39; tags: [\u0026#39;tagme\u0026#39;, \u0026#39;tagmetoo\u0026#39;] set_fact: mr_fact: \u0026#39;w\u0026#39; - name: \u0026#39;perform an action\u0026#39; action: ec2_facts delegate_to: \u0026#39;localhost\u0026#39; # newline at end of file Reason This produces nice looking code that is easy to read.\nLint rule Yamllint empty-lines rule\nVariable Names Names of variables should be as expressive as possible. snake_case for names will help to make the code human readable. The prefix should contain the name of the role.\n# bad - name: \u0026#39;set some facts\u0026#39; set_fact: myBoolean: true int: 20 MY_STRING: \u0026#39;test\u0026#39; # good - name: \u0026#39;set some facts\u0026#39; set_fact: rolename_my_boolean: true rolename_my_int: 20 rolename_my_string: \u0026#39;test\u0026#39; Reason Ansible uses snake_case for module names so it makes sense to extend this convention to variable names. Perfixing the variable with the role name, makes it immediately obvious where it is used.\nJinja Variables Use spaces around Jinja variable names to increase readability.\n# bad - name: set some facts set_fact: my_new_var: \u0026#34;{{my_old_var}}\u0026#34; # good - name: set some facts set_fact: my_new_var: \u0026#34;{{ my_old_var }}\u0026#34; Reason A proper definition for how to create Jinja variables produces consistent and easily readable code.\nLint rule Ansible-lint E206 rule\nComparing Do not compare to literal True/False. Use when: var rather than when: var == True (or conversely when: not var).\nDo not compare it to empty strings. Use when: var rather than when: var != \u0026#34;\u0026#34; (or conversely when: not var rather than when: var == \u0026#34;\u0026#34;)\n# bad - name: validate required variables fail: msg: \u0026#34;No value specified for \u0026#39;{{ item }}\u0026#39;\u0026#34; when: (vars[item] is undefined) or (vars[item] is defined and vars[item] | trim == \u0026#34;\u0026#34;) with_items: \u0026#34;{{ appd_required_variables }}\u0026#34; - name: Create an user and add to the global group include_tasks: user.yml when: - username is defined - username != \u0026#34;\u0026#34; # good - name: Validate required variables fail: msg: \u0026#34;No value specified for \u0026#39;{{ item }}\u0026#39;\u0026#34; when: (vars[item] is undefined) or (vars[item] is defined and not vars[item] | trim == \u0026#34;\u0026#34;) with_items: \u0026#34;{{ appd_required_variables }}\u0026#34; - name: Create an user and add to the global group include_tasks: user.yml when: - username is defined - username Reason Avoid code complexity using quotes and standardize the way literals and empty strings are used\nLint rule Ansible-lint E601 rule\nDelegation Do not use local_action, use delegate_to: localhost\n# bad - name: Send summary mail local_action: module: mail subject: \u0026#34;Summary Mail\u0026#34; to: \u0026#34;{{ mail_recipient }}\u0026#34; body: \u0026#34;{{ mail_body }}\u0026#34; run_once: true # good - name: Send summary mail mail: subject: \u0026#34;Summary Mail\u0026#34; to: \u0026#34;{{ mail_recipient }}\u0026#34; body: \u0026#34;{{ mail_body }}\u0026#34; delegate_to: localhost run_once: true Reason Avoid complexity, standardization, flexibility and code readability. The module and its parameters are easy to read and can be delegated even to a third party server.\nLint rule Ansible-lint E504 rule\nPlaybook File Extension All Ansible Yaml files should have a .yml extension (and NOT .YML, .yaml etc).\n# bad ~/tasks.yaml # good ~/tasks.yml Reason Ansible tooling (like ansible-galaxy init) creates files with a .yml extension. Also, the Ansible documentation website references files with a .yml extension several times. Because of this, it is normal in the Ansible community to use a .yml extension for all Ansible YAML files.\nLint rule Ansible-lint E205 rule\nTemplate File Extension All Ansible Template files should have a .j2 extension.\n# bad ~/template.conf # good ~/template.conf.j2 Reason Ansible Template files will usually have the .j2 extension, which denotes the Jinja2 templating engine used.\nVaults All Ansible Vault files should have a .vault extension (and NOT .yml, .YML, .yaml etc).\n# bad ~/secrets.yml # good ~/secrets.vault Reason It is easier to control unencrypted files automatically for the specific .vault extension.\nDebug and Comments Do not overuse debug and comments in final code as much as possible. Use task and role names to explain what the task or role does. Use the verbose option under ansible for debugging purposes. If debug is used, assign a verbosity option. This will display the message only on certain debugging levels.\n# bad - name: print my_var debug: var: my_var when: ansible_os_family == \u0026#34;Darwin\u0026#34; # good - name: print my_var debug: var: my_var verbosity: 2 when: ansible_os_family == \u0026#34;Darwin\u0026#34; Reason It will keep clean code and consistency avoiding extra debug and comments. Extra debug will spend extra time when running the playbook or role.\nUse Modules copy or templates instead of linefile or blockfile Instead of using the modules linefile and blockfile, which manage changes inside a file, it should be tried to use the modules copy or template instead, which will manage the whole file. For better future proof template should be preferred over copy.\nReason When using linefile/blockfile only a single line or a part of a file is managed. It is not easy to remember which part exactly is managed and which is not. Using the template module the whole file is managed by Ansible and there is no confusion about different parts of a file. Moreover, regular expressions can be avoided, which are often used using linefile.\nUse Module synchronize Instead of copy for Large Files Copying large files takes significantly longer than syncing it. The synchronize modules which are based on rsync can increase the time moving large files from one node to another (or even on the same node).\nDo not Show Sensitive Data in Ansible Output When using the template module and there are passwords or other sensitive data in the file, use the no_log option to hide this information.\nReason For obvious reasons the output of sensitive data on the screen (and logfile) should be prohibited.\nUse Block-Module Block can help to organize the code and can enable rollbacks.\n- block: copy: src: critical.conf dest: /etc/critical/crit.conf service: name: critical state: restarted rescue: command: shutdown -h now Reason Using blocks groups critical tasks together and allows better management when a single task of this block fails.\nEnable CPU and Memory Profiling Enabling profiling is extremely useful when testing forks or to analyse memory and cpu consumption in general. It will present a summary of used CPU/memory for the whole play and per task.\nTo enable it:\nCreate a new control group - which is required for CPU and memory profiling\ncgcreate -a root:root -t root:root -g cpuacct,memory,pids:ansible_profile Configure ansible.cfg\ncallback_whitelist=cgroup_perf_recap [callback_cgroup_perf_recap] control_group=ansible_profile Execute the playbook using cgexec\ncgexec -g cpuacct,memory,pids:ansible_profile ansible-playbook x.yaml Analyse the usage\nMemory Execution Maximum: 11146.29MB cpu Execution Maximum: 112.08% pids Execution Maximum: 35.00 memory: lab : creating network (b42e9945-0dc7-20b1-09d1-00000000000a): 11097.35MB ….. Enable Task and Role Profiling The following can be enabled as well, to help debugging playbooks and roles:\n[defaults] callback_whitelist = profile_tasks, profile_role, timer Timer: duration of playbook execution (activated by default) profile_tasks/role: displays execution time per tasks/role\nThis will generate an output like the following:\nDirectory Structure A consistent directory structure is important to easily understand all playbooks and roles which are written. Ansible knows many different folder structures, any can be used. However, it is important to stick to one structure.\nThe following is an example. Not all folders are usually used and working with collections will change such structure a little bit.\n. ├── ansible.cfg ├── ansible_modules ├── group_vars │ ├── webservers │ └── all ├── hosts │ ├── webserver01 │ └── webserver02 ├── host_vars ├── modules ├── playbooks │ └── ansible-cmdb.yml └── roles ├── requirements.yml ├── galaxy └── dev-sec.ssh-hardening └── auditd ├── files │ ├── auditd.conf │ ├── audit.yml ├── handlers │ └── main.yml ├── meta │ └── main.yml └── tasks └── main.yml "},{"uri":"https://blog.stderr.at/tags/automation-controller/","title":"Automation Controller","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/best-practices/","title":"Best Practices","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/controller/","title":"Controller","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/playbook/","title":"Playbook","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/role/","title":"Role","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/style/","title":"Style","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/styleguide/","title":"StyleGuide","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/azure/2021-10-29-private-aro/","title":"Stumbling into Azure Part II: Setting up a private ARO cluster","tags":[],"description":"","content":" In Part I of our blog post we covered setting up required resources in Azure. Now we are finally going to set up a private cluster. Private\nAs review from Part I here is our planned setup, this time including the ARO cluster.\nAzure Setup The diagram below depicts our planned setup:\nOn the right hand side can see the resources required for our lab:\na virtual network (vnet 192.168.128.0/19). This vnet will be split into 3 separate subnets a master subnet (192.168.129.0/24) holding the ARO control plane nodes a node subnet (192.168.130.0/24) holding ARO worker nodes and finally a subnet call GatewaySubnet where we are going to deploy our Azure VPN gateway (called a vnet-gateway) The subnet where the Azure VPN gateway is located needs to have the name GatewaySubnet. Otherwise creating the Azure VPN gateway will fail.\nwe also need a publicIP resource that we are going to connect to our vnet-gateway (the VPN gateway) and finally a local-gateway resource that tells the vnet-gateway which networks are reachable on the left, in our case the Hetzner server. Creating the private Azure Red Hat OpenShift cluster Register required resource providers\naz provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait First we are going to set some environment variable. Those variables are used in the upcoming commands:\nexport RESOURCEGROUP=aro-rg export CLUSTER=\u0026#34;aro1\u0026#34; export GATWAY_SUBNET=\u0026#34;192.168.128.0/24\u0026#34; export MASTER_SUBNET=\u0026#34;192.168.129.0/24\u0026#34; export WORKER_SUBNET=\u0026#34;192.168.130.0/24\u0026#34; export HETZNER_VM_NETWORKS=\u0026#34;10.0.0.0/24 192.168.122.0/24 172.16.100.0/24\u0026#34; Disable subnet private endpoint policies\naz network vnet subnet update \\ --name master-subnet \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --disable-private-link-service-network-policies true Create a private DNS zone for our cluster\naz network private-dns zone create -n private2.tntinfra.net -g aro-rg Create the cluster\naz aro create \\ --resource-group $RESOURCEGROUP \\ --name $CLUSTER \\ --vnet aro-vnet \\ --master-subnet master-subnet \\ --worker-subnet worker-subnet \\ --apiserver-visibility Private \\ --ingress-visibility Private \\ --domain private.tntinfra.net # --pull-secret @pull-secret.txt # [OPTIONAL] After successful cluster creating add DNS entry for the API and Ingress\nQuery the Azure API for the API server IP and the ingress IP addresses:\naz aro show -n aro1 -g aro-rg --query \u0026#39;{api:apiserverProfile.ip, ingress:ingressProfiles[0].ip}\u0026#39; Example output\nApi Ingress ------------- --------------- 192.168.129.4 192.168.130.254 Add entries to Azure private DNS\naz network private-dns record-set a add-record -g aro-rg -z private.tntinfra.net -a \u0026#34;192.168.129.4\u0026#34; -n api az network private-dns record-set a add-record -g aro-rg -z private.tntinfra.net -a \u0026#34;192.168.130.254\u0026#34; -n \u0026#34;*.apps\u0026#34; List entries to verify configuration\naz network private-dns record-set a list -g aro-rg -z private.tntinfra.net Output:\nName ResourceGroup Ttl Type AutoRegistered Metadata ------ --------------- ----- ------ ---------------- ---------- api aro-rg 3600 A False *.apps aro-rg 3600 A False List cluster credentials after successful setup\naz aro list-credentials \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP Get the console URL\naz aro show \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \u0026#34;consoleProfile.url\u0026#34; -o tsv DNS, curl this works, dunno why?\ndig @192.168.129.7 console-openshift-console.apps.xm7rdz4r.westeurope.aroapp.io use curl to access the internal API and see if it works:\ncurl -kv https://192.168.129.4:6443 Additional Resources Build an Azure site-to-site VPN for DevTest Create a virtual network with a Site-to-Site VPN connection using CLI Libreswan: Disable rp_filter for IPsec Libreswan: NAT and IPsec not working Libreswan: Subnet to subnet VPN "},{"uri":"https://blog.stderr.at/ansible/2021/10/automation-controller-and-ldap-authentication/","title":"Automation Controller and LDAP Authentication","tags":["Ansible","Controller","Automation Controller","LDAP","Authentication"],"description":"Enable LDAP authentication and create and example LDAP server","content":" The following article shall quickly, without huge background information, deploy an Identity Management Server (based on FreeIPA) and connect this IDM to an existing Automation Controller so authentication can be tested and verified based on LDAP.\nInstall FreeIPA Run the following command to deploy and configure the IPA Server:\nyum module enable idm:DL1\nyum distro-sync\nyum -y module install idm:DL1/server\nInstall the server by calling the command ipa-server-install. This will start an interactive installation modus which requires the basic information about the IPA server. The following uses tower.local as base domain\nDo you want to configure integrated DNS (BIND)? [no]: Server host name [node01.tower.local]: Please confirm the domain name [tower.local]: Please provide a realm name [TOWER.LOCAL]: Directory Manager password: \u0026lt;enter password\u0026gt; Password (confirm): \u0026lt;enter password\u0026gt; IPA admin password: \u0026lt;enter password\u0026gt; Password (confirm): \u0026lt;enter password\u0026gt; Do you want to configure chrony with NTP server or pool address? [no]: Continue to configure the system with these values? [no]: yes Once all information have been provided the installation/configuration process starts. This will take a while…​\nBe sure that the hostname, here node01.tower.local, is resolvable, at least from the Tower/Controller node and the node you are accessing the FreeIPA UI. You can use your local hosts file or a real domain name for that. Login to IPA server via Command Line For user admin use: kinit admin\nCreate a Binduser (BindDN) The Binduser (or BindDN) will be used by the Controller to authenticate the Controller against the LDAP server.\nCreate the actual user\nipa user-add --first=”BindUser” --last=”None” --password binduser Output:\nPassword: Enter Password again to verify: ------------------ Added user \u0026#34;binduser\u0026#34; ------------------ User login: binduser First name: ”BindUser” Last name: ”None” Full name: ”BindUser” ”None” Display name: ”BindUser” ”None” Initials: ”” Home directory: /home/binduser GECOS: ”BindUser” ”None” Login shell: /bin/sh Principal name: binduser@TOWER.LOCAL Principal alias: binduser@TOWER.LOCAL User password expiration: 20211015133112Z Email address: binduser@tower.local UID: 1573400003 GID: 1573400003 Password: True Member of groups: ipausers Kerberos keys available: True Assign the new user to the admin group\nipa group-add-member admins --users=binduser Output:\nGroup Name: admins Description: Account administrators group GID: 1573400000 Member users: admin, binduser ----------------------------------- Number of members added 1 ----------------------------------- Create a 2nd User to test the authentication later\nipa user-add --first=”User” --last=”Name” --password user1 Enable LDAP Auth in Automation Controller Login to Automation Controller ad go to \u0026#34;Settings \u0026gt; LDAP Settings \u0026gt; Default\u0026#34;\nadd a new connection:\nLDAP Service URI: ldap://node01.tower.local:389\nLDAP Bind Password: \u0026lt;password of user binduser\u0026gt;`\nLDAP Group Type: MemberDNGroupType\nLDAP Bind DN: uid=binduser,cn=users,cn=accounts,dc=tower,dc=local\nLDAP User Search:\n[ \u0026#34;cn=users,cn=accounts,dc=tower,dc=local\u0026#34;, \u0026#34;SCOPE_SUBTREE\u0026#34;, \u0026#34;(uid=%(user)s)\u0026#34; ] LDAP Group Search:\n[ \u0026#34;cn=groups,cn=accounts,dc=tower,dc=local\u0026#34;, \u0026#34;SCOPE_SUBTREE\u0026#34;, \u0026#34;(objectClass=posixgroup)\u0026#34; ] The configuration should look like the following image:\nFigure 1. Automation Controller LDAP Authentication Verify Login with user1 You can now test the login using user1. If it does not work, check the following files for errors:\nTower Node: /var/log/tower/tower.log\nIPA Node: /var/log/dirsrv/slapd-TOWER-LOCAL/access\nThe login should work, but since the user1 is not assigned to any Team/Organization inside the Automation Controller, no privileges are granted. The user can do nothing. Automatically assign permissions 2 roles can be automatically assigned to authenticated users:\nSuper User\nAuditor\nTo test this, 2 groups will be created in the LDAP server and a new user will be assigned to one of the groups.\nCreate the group for super users: ipa group-add tower_administrators\nCreate the group for auditors: ipa group-add tower_auditors\nCreate a new user: ipa user-add --first=”User” --last=”Name” --password user2\nAssign the user to one the the groups: ipa group-add-member tower_administrators --users=user2\nModify the Controller LDAP configuration and set LDAP User Flags by Group. This will assing any member of tower_administrators to is_superuser for example.\n{ \u0026#34;is_superuser\u0026#34;: [ \u0026#34;cn=tower_administrators,cn=groups,cn=accounts,dc=tower,dc=local\u0026#34; ], \u0026#34;is_system_auditor\u0026#34;: [ \u0026#34;cn=tower_auditors,cn=groups,cn=accounts,dc=tower,dc=local\u0026#34; ] } Test the authentication and authorization with the user2. This user should now gain super admin permissions.\nAllow Users From Specific Groups Only Not all LDAP users shall be able to authenticate. Only users, which are member of a specific group, shall be able to authenticate.\nCreate a 3rd user: ipa user-add --first=”User” --last=”Name” --password user3\nModify the LDAP Configuration in Automation Controller and set LDAP Require Groups:\n\u0026#34;cn=towerusers,cn=groups,cn=accounts,dc=tower,dc=local\u0026#34; Add the group toweruser: ipa group-add towerusers\nAssign the user user3 to that group: ipa group-add-member towerusers --users=user3\nAt this state only user3 will be able to login. In order to allow the other users as well, all must be assigned to the group towerusers\nipa group-add-member towerusers --users=user3 ipa group-add-member towerusers --users=user1 Additional Configuration It is possible to automatically map users to Controller Organization. I did not fully test this, but the following is an example:\n{ \u0026#34;LDAP Organization\u0026#34;: { \u0026#34;admins\u0026#34;: \u0026#34;cn=engineering_admins,ou=groups,dc=example,dc=com\u0026#34;, \u0026#34;remove_admins\u0026#34;: false, \u0026#34;users\u0026#34;: [ \u0026#34;cn=engineering,ou=groups,dc=example,dc=com\u0026#34;, \u0026#34;cn=sales,ou=groups,dc=example,dc=com\u0026#34;, \u0026#34;cn=it,ou=groups,dc=example,dc=com\u0026#34; ], \u0026#34;remove_users\u0026#34;: false }, \u0026#34;LDAP Organization 2\u0026#34;: { \u0026#34;admins\u0026#34;: [ \u0026#34;cn=Administrators,cn=Builtin,dc=example,dc=com\u0026#34; ], \u0026#34;remove_admins\u0026#34;: false, \u0026#34;users\u0026#34;: true, \u0026#34;remove_users\u0026#34;: false } } "},{"uri":"https://blog.stderr.at/tags/ldap/","title":"LDAP","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/quay/","title":"Quay","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/azure/2021-10-17-s2s-vpn/","title":"Stumbling into Azure Part I: Building a site-to-site VPN tunnel for testing","tags":[],"description":"","content":" So we want to play with ARO (Azure Red Hat OpenShift) private clusters. A private cluster is not reachable from the internet (surprise) and is only reachable via a VPN tunnel from other networks.\nThis blog post describes how we created a site-to-site VPN between a Hetzner dedicated server running multiple VM\u0026#39;s via libvirt and Azure.\nAn upcoming blog post is going to cover the setup of the private ARO cluster.\nAzure Setup The diagram below depicts our planned setup:\nOn the right hand side can see the resources required for our lab:\na virtual network (vnet 192.168.128.0/19). This vnet will be split into 3 separate subnets a master subnet (192.168.129.0/24) holding the ARO control plane nodes a node subnet (192.168.130.0/24) holding ARO worker nodes and finally a subnet call GatewaySubnet where we are going to deploy our Azure VPN gateway (called a vnet-gateway) The subnet where the Azure VPN gateway is located needs to have the name GatewaySubnet. Otherwise creating the Azure VPN gateway will fail.\nwe also need a publicIP resource that we are going to connect to our vnet-gateway (the VPN gateway) and finally a local-gateway resource that tells the vnet-gateway which networks are reachable on the left, in our case the Hetzner server. Creating the required Azure resources First we are going to set some environment variable. Those variables are used in the upcoming commands:\nexport RESOURCEGROUP=aro-rg export GATWAY_SUBNET=\u0026#34;192.168.128.0/24\u0026#34; export MASTER_SUBNET=\u0026#34;192.168.129.0/24\u0026#34; export WORKER_SUBNET=\u0026#34;192.168.130.0/24\u0026#34; export HETZNER_VM_NETWORKS=\u0026#34;10.0.0.0/24 192.168.122.0/24 172.16.100.0/24\u0026#34; Next create a VNET resource holding our sub networks:\naz network vnet create \\ --resource-group $RESOURCEGROUP \\ --name aro-vnet \\ --address-prefixes 192.168.128.0/18 Create the GatewaySubnet subnet\naz network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name GatewaySubnet \\ --address-prefixes $GATEWAY_SUBNET Create the master subnet\naz network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name master-subnet \\ --address-prefixes $MASTER_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Create the worker subnet\naz network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name worker-subnet \\ --address-prefixes $WORKER_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Create a public IP resource\naz network public-ip create \\ --name GatewayIP \\ --resource-group $RESOURCEGROUP \\ --allocation-method Dynamic Create a local-gateway resource\naz network local-gateway create \\ --name playground \\ --resource-group $RESOURCEGROUP \\ --local-address-prefixes $HETZNER_VM_NETWORKS \\ --gateway-ip-address 95.217.42.98 Create a vnet-gateway resource (takes around 30 minutes)\naz network vnet-gateway create \\ --name vpn-gateway \\ --public-ip-address GatewayIP \\ --resource-group $RESOURCEGROUP \\ --vnet aro-vnet \\ --gateway-type Vpn \\ --vpn-type RouteBased \\ --sku Basic \\ --no-wait Define a vpn-connection\naz network vpn-connection create \\ --name VNet1toSite2 \\ --resource-group $RESOURCEGROUP \\ --vnet-gateway1 vpn-gateway \\ --local-gateway2 playground \\ --location westeurope \\ --shared-key thepassword Required iptables (nf tables) hacks for libvirt Skip NAT rules if the destination network is in Azure and the client network deploy via libvirt iptables -I LIBVIRT_PRT 2 -t nat -d 192.168.129.0/24 -j RETURN iptables -I LIBVIRT_PRT 2 -t nat -d 192.168.130.0/24 -j RETURN Skip NAT rules if the destination network is in Azure and the client is connected via tailscale iptables -I ts-postrouting 1 -t nat -d 192.168.129.0/24 -j RETURN iptables -I ts-postrouting 1 -t nat -d 192.168.130.0/24 -j RETURN Libreswan setup on CentOS Stream Install the Libreswan packages\ndnf install libreswan Create a Azure configuration for Libreswan in ~/etc/ipsec.d/azure.conf\nconn masterSubnet also=azureTunnel leftsubnet=192.168.129.0/24 rightsubnet=172.16.100.0/24 auto=start conn workerSubnet also=azureTunnel leftsubnet=192.168.130.0/24 rightsubnet=172.16.100.0/24 auto=start conn azureTunnel authby=secret auto=start dpdaction=restart dpddelay=30 dpdtimeout=120 ike=aes256-sha1;modp1024 ikelifetime=3600s ikev2=insist keyingtries=3 pfs=yes phase2alg=aes128-sha1 left=51.137.113.44 leftsubnets=192.168.128.0/24 right=%defaultroute rightsubnets=172.16.100.0/24 salifetime=3600s type=tunnel ipsec-interface=yes Create a Libreswan secrets file for Azure in /etc/ipsec.d/azure.secrets:\n%any %any : PSK \u0026#34;abc123\u0026#34; Enable and start the IPsec service\nsystemctl enable --now ipsec We had to explicitly load the IPsec configuration via\nipsec addconn --config /etc/ipsec.d/azure.conf azureTunnel Libreswan IPSEC debugging tips Check the state of the IPsec systemd service\nsystemctl status ipsec Check the full log of the IPsec systemd service\njournalctl -e -u ipsec Check the state of the tunnels with the ipsec command line tool\nipsec status Check for the following lines\n000 Total IPsec connections: loaded 5, active 2 000 000 State Information: DDoS cookies not required, Accepting new IKE connections 000 IKE SAs: total(1), half-open(0), open(0), authenticated(1), anonymous(0) 000 IPsec SAs: total(2), authenticated(2), anonymous(0) 000 000 #130: \u0026#34;azureTunnel/1x1\u0026#34;:500 STATE_V2_ESTABLISHED_CHILD_SA (IPsec SA established); EVENT_SA_REKEY in 2003s; newest IPSEC; eroute owner; isakmp#131; idle; 000 #130: \u0026#34;azureTunnel/1x1\u0026#34; esp.56cf4304@51.137.113.44 esp.6f49e8d3@95.217.42.98 tun.0@51.137.113.44 tun.0@95.217.42.98 Traffic: ESPin=0B ESPout=0B! ESPmax=0B 000 #129: \u0026#34;masterSubnet/0x0\u0026#34;:500 STATE_V2_ESTABLISHED_CHILD_SA (IPsec SA established); EVENT_SA_REKEY in 1544s; newest IPSEC; eroute owner; isakmp#131; idle; 000 #129: \u0026#34;masterSubnet/0x0\u0026#34; esp.6e81e8da@51.137.113.44 esp.6f72bbc8@95.217.42.98 tun.0@51.137.113.44 tun.0@95.217.42.98 Traffic: ESPin=0B ESPout=0B! ESPmax=0B 000 #131: \u0026#34;masterSubnet/0x0\u0026#34;:500 STATE_V2_ESTABLISHED_IKE_SA (established IKE SA); EVENT_SA_REKEY in 2121s; newest ISAKMP; idle; IPsec specifies properties of connections via security associations (SA). The parent SA is describes the IKEv2 connections, the child SA is the ESP (encapsulated security payload) connection.\nCheck IPsec transformation policies\nip xfrm policy Check the state of IPsec transformation policies\nip xfrm state Check for dropped packages on the IPsec interface (ipsec1 in our case)\nip -s link show dev ipsec1 Additonal Resources Build an Azure site-to-site VPN for DevTest Create a virtual network with a Site-to-Site VPN connection using CLI Libreswan: Disable rp_filter for IPsec Libreswan: NAT and IPsec not working Libreswan: Subnet to subnet VPN "},{"uri":"https://blog.stderr.at/quay/2021-09-07-quay-upgrade-3.4/","title":"Stumbling into Quay: Upgrading from 3.3 to 3.4 with the quay-operator","tags":[],"description":"","content":" We had the task of answering various questions related to upgrading Red Hat Quay 3.3 to 3.4 and to 3.5 with the help of the quay-operator.\nThankfully (sic!) everything changed in regards to the Quay operator between Quay 3.3 and Quay 3.4.\nSo this is a brain dump of the things to consider.\nOperator changes With Quay 3.4 the operator was completely reworked and it basically changed from opinionated to very opinionated. The upgrade works quite well but you have to be aware about the following points:\nThe name of the custom resource changed from QuayEcosystem to QuayRegistry The configHostname, used for providing the quay configuration UI, is no longer configurable The password for the configuration UI is always regenerated after a configuration re-deployment The volume size of the PostgreSQL PVC will change to 50G In my test cluster I was using a 10G Ceph block device and the StorageClass did not support volume expansion. So my upgrade stopped at this point and I had to allow volume expansion in the storage class.\nA horizontal pod autoscaler is also deploy during the upgrade. The default is to scale automatically to 20 pods, you might reconsider this… With the Quay operator version 3.5 the operator is monitoring all namespaces for custom resources and needs to be installed in the openshift-operators namespace, this is how we upgraded Quay to 3.5 including the operator:\nchange the quay operator channel to 3.5 trigger an upgrade now Quay gets upgraded to version 3.5 after the Quay upgrade you need to reinstall the operator:\ndeinstall the quay operator, Quay is not affected by this reinstall the Quay operator (3.5) in all-namespaces the re-installation of the operator triggers a quay deployment, all Quay pods are restarted! You have to manually cleanup old\npostgres-config-secrets and quay-config-bundle\u0026#39;s Backup and restore considerations Red Hat is working on providing documentation on how to backup and restore Quay in various scenarios. There\u0026#39;s an open task for this that provides more information https://issues.redhat.com/browse/PROJQUAY-2242.\n"},{"uri":"https://blog.stderr.at/tags/sealed-secret/","title":"Sealed Secret","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2021/09/secure-your-secrets-with-sealed-secrets/","title":"Secure your secrets with Sealed Secrets","tags":["Storage","OpenShift","OCP","Sealed Secret"],"description":"Using Sealed Secrets to encrypt your secrets and be able to upload them to Git","content":" Working with a GitOps approach is a good way to keep all configurations and settings versioned and in sync on Git. Sensitive data, such as passwords to a database connection, will quickly come around. Obviously, it is not a idea to store clear text strings in a, maybe even public, Git repository. Therefore, all sensitive information should be stored in a secret object. The problem with secrets in Kubernetes is that they are actually not encrypted. Instead, strings are base64 encoded which can be decoded as well. Thats not good …​ it should not be possible to decrypt secured data. Sealed Secret will help here…​\nSealed Secrets by Bitnami[1] is one option to create real, encrypted secrets. It contains two parts:\nA cluster-side controller / operator, which decrypts the secrets server-side on OpenShift installed in a dedicated namespace usually called sealed secrets.\nkubeseal - a client-side command line tool\nPrerequisites An OpenShift 4 cluster with cluster-admin permissions.\nSealed Secrets Operator Goto OperatorHub and search for Sealed Secrets (This is a Community Operator)\nFigure 1. Search Sealed Secrets in OperatorHub Install the operator, using the default settings, into the namespace sealed-secrets\nFigure 2. Installed Sealed Secret Operator Install the CRD SealedSecretController Install the following object. For now the default values can be used.\napiVersion: bitnami.com/v1alpha1 kind: SealedSecretController metadata: name: controller (1) namespace: sealed-secrets spec: networkPolicy: false nodeSelector: {} podLabels: {} resources: {} affinity: {} securityContext: fsGroup: \u0026#39;\u0026#39; runAsUser: \u0026#39;\u0026#39; rbac: create: true pspEnabled: false crd: create: true keep: true ingress: annotations: {} enabled: false hosts: - chart-example.local path: /v1/cert.pem tls: [] serviceAccount: create: true name: \u0026#39;\u0026#39; image: pullPolicy: IfNotPresent repository: \u0026gt;- quay.io/bitnami/sealed-secrets-controller@sha256:8e9a37bb2e1a6f3a8bee949e3af0e9dab0d7dca618f1a63048dc541b5d554985 secretName: sealed-secrets-key tolerations: [] controller: create: true priorityClassName: \u0026#39;\u0026#39; podAnnotations: {} 1 Be aware of the name of the controller OBJECT (name: controller). It is used lated as part of the actual controller name Install the command line tool kubeseal The kubeseal binary can be easily installed using either\non Mac: brew install kubeseal or\non Linux:\nwget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.16.0/kubeseal-linux-amd64 -O kubeseal install -m 755 kubeseal /usr/local/bin/ Testing Sealed Secrets Create a new project oc new-project myproject\nCreate a secret\necho -n \u0026#34;my_super_secret_string\u0026#34; \\ | kubectl create secret generic mypasswords --dry-run=client --from-file=password=/dev/stdin -o json \\ | kubeseal --controller-namespace=sealed-secrets --controller-name=controller-sealed-secrets --format json \u0026gt; mysealedsecret.json (1) 1 The switches --controller-namespace define the namespace where the operator is installed, --controller-name is a combination of the SealedSecretController object name and the name of the namespace The password=my_super_secret_string is created and piped into kubeseal which is using the controller, where the server created a certificate for encryption, to create an encrypted json file mysealedsecret.json. It is important to note, that the actually Kubernetes secret object is not created at this stage.\nThe file mysealedsecret.json is encrypted now and it is safe to store this file on Github.\nIt looks like this:\n{ \u0026#34;kind\u0026#34;: \u0026#34;SealedSecret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;bitnami.com/v1alpha1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mypasswords\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;myproject\u0026#34;, (1) \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;spec\u0026#34;: { \u0026#34;template\u0026#34;: { \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mypasswords\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;myproject\u0026#34;, (1) \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: null }, \u0026#34;encryptedData\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;AgBsSZVcTfzfNFI7ZlCsH3/4b3L7m52/O9f70pMtn1myPWHeY1QJFoxpWkH0tWosfeIoko+iB0kCyFk/iJEYSvd31zgnr90hv4e2qVtEBmm6n5B7V40ZERdiy2Cz7UXakUKDdhTjA0BTjcf0f0b2FRDenGxCHJB7cyOVGOZ36jF6IdP2k6kbsZXklti/4MXK7oskDXGzU7rTsESK0ttk5uQgrpfWrhaUip5+Db5vcG1OlHhMJ7In3NlNr0mbl+YiXsKKDNvyw9T14L3rlfvHz1xe0lIqC72i5LSCarpGoSKNOr+Sev9+b/+no6P4VDPuSLORbwVXlP5kt+8xnpZJIEqnetwhr78dt8F3xmjXVBZncdwKk22Y/b9L+uUKWPAvOT78khpUIHQPo9dV/nmz1ldvu58fCFL4TjOOtyTBcUPD3qQJp+sEXgy63l8hEaMXuLUlk+srSnJfMtwkFhl0CG2fKsg4CsQoZlvq5oKOl50sujg3Trv4W9qVVCYHA7BUXEj6J0DxjOCqSQixHRr7Z7JqIyhhdLYdHwMH80scsIb6Ok7keC82v1yae770NWWxJJ4M7Ieb2ERzgwy825gkdq9nx9I6fVxYJkkZlpKKoTvL0uno4sKjC1yQjCgW1vpiZeLIJO2f9TpvVdK2nrag0/gXPMboAL2BGnMPMwjR7OZm+iHq3NXNKiIV1aWRO4wkd/spWziLjOpeS7T1k9w4XxoACwv3g4it\u0026#34; } } } 1 The sealed secret will be created in your project Upload the sealed secret oc create -f mysealedsecret.json\nVerify Secret The object SealedSecret is created:\noc get SealedSecret NAME AGE mypasswords 3s The SealedSecretController will decrypt the and store the secret in the namespace. This can take a few seconds:\noc get secret mypasswords NAME TYPE DATA AGE mypasswords Opaque 1 25s Extract the secret and verify that your string has been stored as \u0026#34;normal\u0026#34; secret\noc extract secret/mypasswords --to=- # password my_super_secret_string Updating or appending new values The process for updateing or appending a secret is similar. The only difference is that a new value for the key string is new.\n# Updaing string echo -n \u0026#34;my_NEW_super_secret_string\u0026#34; \\ | kubectl create secret generic mypasswords --dry-run=client --from-file=password=/dev/stdin -o json \\ | kubeseal --controller-namespace=sealed-secrets --controller-name=controller-sealed-secrets --format json --merge-into mysealedsecret.json # Appending echo -n \u0026#34;my_appended_string\u0026#34; \\ | kubectl create secret generic mypasswords --dry-run=client --from-file=appendedstring=/dev/stdin -o json \\ | kubeseal --controller-namespace=sealed-secrets --controller-name=controller-sealed-secrets --format json --merge-into mysealedsecret.json Be sure that you are in the namespace you want to install the secret Upload the sealed secret oc apply -f mysealedsecret.json and extract it again to validate:\noc extract secret/mypasswords --to=- # appendedstring my_appended_string # password my_NEW_super_secret_string Sources [1]: Bitname Readme on Github\n"},{"uri":"https://blog.stderr.at/tags/storage/","title":"Storage","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/affinity/","title":"Affinity","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/anti-affinity/","title":"Anti-Affinity","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/descheduler/","title":"Descheduler","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/","title":"Introduction","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector","Affinity","Anti-Affinity"],"description":"Introduction to Pod Placement","content":" Pod scheduling is an internal process that determines placement of new pods onto nodes within the cluster. It is probably one of the most important tasks for a Day-2 scenario and should be considered at a very early stage for a new cluster. OpenShift/Kubernetes is already shipped with a default scheduler which schedules pods as they get created accross the cluster, without any manual steps.\nHowever, there are scenarios where a more advanced approach is required, like for example using a specifc group of nodes for dedicated workload or make sure that certain applications do not run on the same nodes. Kubernetes provides different options:\nControlling placement with node selectors\nControlling placement with pod/node affinity/anti-affinity rules\nControlling placement with taints and tolerations\nControlling placement with topology spread constraints\nThis series will try to go into the detail of the different options and explains in simple examples how to work with pod placement rules. It is not a replacement for any official documentation, so always check out Kubernetes and or OpenShift documentations.\nPod Placement Series NodeSelector\nPod Affinity and Anti Affinity\nNode Affinity\nTaints and Tolerations\nTopology Spread Constraints\nDescheduler\nPrerequisites The following prerequisites are used for all examples. Let’s image that our cluster (OpenShift 4) has 4 compute nodes\noc get node --selector=\u0026#39;node-role.kubernetes.io/worker\u0026#39; NAME STATUS ROLES AGE VERSION compute-0 Ready worker 7h1m v1.19.0+d59ce34 compute-1 Ready worker 7h1m v1.19.0+d59ce34 compute-2 Ready worker 7h1m v1.19.0+d59ce34 compute-3 Ready worker 7h1m v1.19.0+d59ce34 An example application (from the catalog Django + Postgres) has been deployed in the namespace podtesting. It contains by default 1 pod for a PostGresql database and one pod for a frontend web application.\noc get pods -n podtesting -o wide | grep Running django-psql-example-1-h6kst 1/1 Running 0 20m 10.130.2.97 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-1-4pcm4 1/1 Running 0 21m 10.131.0.51 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Without any configuration the OpenShift scheduler will try to spread the pods evenly accross the cluster.\nLet’s increase the replica of the web frontend:\noc scale --replicas=4 dc/django-psql-example -n podtesting Eventually 4 additional pods will be started accross the compute nodes of the cluster:\noc get pods -n podtesting -o wide | grep Running django-psql-example-1-842fl 1/1 Running 0 2m7s 10.131.0.65 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-h6kst 1/1 Running 0 24m 10.130.2.97 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-pxhlv 1/1 Running 0 2m7s 10.128.2.13 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-xms7x 1/1 Running 0 2m7s 10.129.2.10 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-1-4pcm4 1/1 Running 0 26m 10.131.0.51 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; As you can see, the scheduler already tries to spread the pods evenly, so that every worker node will host one frontend pod.\nHowever, let’s try to apply a more advanced configuration for the pod placement, starting with the Pod Placement - NodeSelector\n"},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-29-node-affinity/","title":"Node Affinity","tags":["OCP","Day-2","OpenShift","Pod Placement","Affinity"],"description":"Placeing Pods Using the Node Affinity rules","content":" Node Affinity allows to place a pod to a specific group of nodes. For example, it is possible to run a pod only on nodes with a specific CPU or disktype. The disktype was used as an example for the nodeSelector and yes …​ Node Affinity is conceptually similar to nodeSelector but allows a more granular configuration.\nPod Placement Series Please check out other ways of pod placements:\nExpand me... NodeSelector\nPod Affinity and Anti Affinity\nNode Affinity\nTaints and Tolerations\nTopology Spread Constraints\nDescheduler\nUsing Node Affinity Currently two types of affinity settings are known:\nrequiredDuringSchedulingIgnoreDuringExecuption (short required) - a hard requirement which must be met before a pod can be scheduled\npreferredDuringSchedulingIgnoredDuringExecution (short preferred) - a soft requirement the scheduler tries to meet, but does not guarantee it\nBoth types can be specified. In such case the node must first meet the required rule and then attempt to meet the preferred rule. Preparing node labels Remember the prerequisites explained in the . Pod Placement - Introduction. We have 4 compute nodes and an example web application up and running. Before we start with affinity rules we need to label all nodes. Let’s create 2 zones (east and west) for our compute nodes.\nYou can skip this, if these labels are still set. Figure 1. Node Zones oc label nodes compute-0 compute-1 topology.kubernetes.io/zone=east oc label nodes compute-2 compute-3 topology.kubernetes.io/zone=west Configure node affinity rule Like pod affinity the node affinity is defined on the pod specification:\nkind: DeploymentConfig apiVersion: apps.openshift.io/v1 metadata: name: django-psql-example namespace: podtesting [...] spec: [...] template: [...] spec: [...] affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - west In this example the pods are started only on nodes of the zone \u0026#34;West\u0026#34;. Since the value is an array, multiple zones can be defined letting the web application be executed on West and East for example. With this setup you can control on which node a specific application shall be executed. For example: you have a group of nodes which provide a GPU and your GPU application must be started only on this group of nodes.\nLike with pod affinity you can combine required and preferred settings.\nWhat happened to Node Anti-Affinity? Unlike Pod Anti-Affinity, there is no concept to define a node Anti-Affinity. Instead you can use the NotIn and DoesNotExist operators to achieve this bahaviour.\nCleanup As cleanup simply remove the affinity specification from the DeploymentConf. The node labels can stay as they are since they do not hurt.\nSummary This concludes the quick overview of the node affinity. Further information can be found at Node Affinity\n"},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/","title":"NodeSelector","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector"],"description":"Placeing Pods Using the NodeSelector","content":" One of the easiest ways to tell your Kubernetes cluster where to put certain pods is to use a nodeSelector specification. A nodeSelector defines a key-value pair and are defined inside the specification of the pods and as a label on one or multiple nodes (or machine set or machine config). Only if selector matches the node label, the pod is allowed to be scheduled on that node.\nKubernetes distingushes between 2 types of selectors:\ncluster-wide node selectors: defined by the cluster administrators and valid for the whole cluster\nproject node selectors: to place new pods inside projects into specific nodes.\nPod Placement Series Please check out other ways of pod placements:\nExpand me... NodeSelector\nPod Affinity and Anti Affinity\nNode Affinity\nTaints and Tolerations\nTopology Spread Constraints\nDescheduler\nUsing nodeSelector As previously described, we have a cluster with an example application scheduled accross the worker nodes evenly by the scheduler.\noc get pods -n podtesting -o wide | grep Running django-psql-example-1-842fl 1/1 Running 0 2m7s 10.131.0.65 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-h6kst 1/1 Running 0 24m 10.130.2.97 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-pxhlv 1/1 Running 0 2m7s 10.128.2.13 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-xms7x 1/1 Running 0 2m7s 10.129.2.10 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-1-4pcm4 1/1 Running 0 26m 10.131.0.51 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; However, our 4 compute nodes are assembled with different hardware specification and are using different harddisks (sdd vs hdd).\nFigure 1. Nodes with Different Specifications Since our web application must run on fast disks must configure the cluster to schedule the pods on nodes with SSD only.\nTo start using nodeSelectors we first label our nodes accordingly:\ncompute-0 and compute-1 are faster nodes with an SSD attached.\ncompute-2 and compute-2 have a HDD attached.\noc label nodes compute-0 compute-1 disktype=ssd (1) oc label nodes compute-2 compute-3 disktype=hdd 1 as key we are using disktype As crosscheck we can list nodes with a specific label:\noc get nodes -l disktype=ssd NAME STATUS ROLES AGE VERSION compute-0 Ready worker 7h32m v1.19.0+d59ce34 compute-1 Ready worker 7h31m v1.19.0+d59ce34 oc get nodes -l disktype=hdd NAME STATUS ROLES AGE VERSION compute-2 Ready worker 7h32m v1.19.0+d59ce34 compute-3 Ready worker 7h32m v1.19.0+d59ce34 If no matching label is found, the pod cannot be scheduled. Therefore, always label the nodes first. The 2nd step is to add the node selector to the specification of the pod. In our example we are using a DeploymentConfig, so let’s add it there:\noc patch dc django-psql-example -n podtesting --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;nodeSelector\u0026#34;:{\u0026#34;disktype\u0026#34;:\u0026#34;ssd\u0026#34;}}}}}\u0026#39; This adds the nodeSelector into: spec/template/spec\nnodeSelector: disktype: ssd Kubernetes will now trigger a restart of the pods on the supposed nodes.\noc get pods -n podtesting -o wide | grep Running django-psql-example-3-4j92k 1/1 Running 0 42s 10.129.2.7 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-3-d7hsd 1/1 Running 0 42s 10.129.2.8 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-3-fkbfm 1/1 Running 0 14m 10.128.2.18 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-3-psskb 1/1 Running 0 14m 10.128.2.17 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; As you can see, only nodes with a SSD (compute-0 and compute-1) are being used.\nControlling pod placement with project-wide selector Adding a nodeSelector to a deployment seems fine…​ until somebody forgets to add it. Then the pods would be started anywhere the scheduler finds suitable. Therefore, it might make sense to use a project-wide node selector, which will automatically be applied on all pods on that project. The project selector is added by the cluster administrator to the Namespace object (no matter what the OpenShift documentation says in it’s example) as openshift.io/node-selector parameter.\nLet’s remove our previous configuration and add the setting to our namespace podtesting:\nCleanup\nRemove the nodeSelector from the deployment configuration and wait until all pods have been reshuffeld\noc patch dc django-psql-example -n podtesting --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/nodeSelector\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;disktype=ssd\u0026#34; }]\u0026#39; Add the label to the project\noc annotate ns/podtesting openshift.io/node-selector=\u0026#34;disktype=ssd\u0026#34; The OpenShift scheduler will now spread the Pods accross compute-0 or compute-1 again, but not on compute-2 or 3.\nWe can prove that by stressing our cluster (and nodes) a little bit and scale our frontend application to 10:\noc get pods -n podtesting -o wide | grep Running django-psql-example-4-2jn2l 1/1 Running 0 27s 10.128.2.8 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-6g7ks 1/1 Running 0 7m47s 10.129.2.23 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-752nm 1/1 Running 0 7m47s 10.128.2.7 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-c5jvm 1/1 Running 0 27s 10.129.2.4 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-f5kwg 1/1 Running 0 27s 10.129.2.5 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-g7bcs 1/1 Running 0 7m47s 10.129.2.24 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-h5tgb 1/1 Running 0 27s 10.129.2.6 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-spvpp 1/1 Running 0 28s 10.128.2.5 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-v9qwj 1/1 Running 0 7m48s 10.129.2.22 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-zgwcv 1/1 Running 0 27s 10.128.2.6 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; As you can see compute-0 and compute-1 are the only nodes which are used.\nWell-Known Labels nodeSelector is one of the easiest ways to control where an application shall be started. Working with labels is therefore very important as soon as workload shall be added to the cluster. Kubernetes reserves some labels which can be leveraged and some are already predefined on the nodes, for example:\nbeta.kubernetes.io/arch=amd64\nkubernetes.io/hostname=compute-0\nkubernetes.io/os=linux\nnode-role.kubernetes.io/worker=\nnode.openshift.io/os_id=rhcos\nA list of all known can be found at: [1]\nTwo of them I would like to mention here, since they might become very important when designing the placement of pods:\ntopology.kubernetes.io/zone\ntopology.kubernetes.io/region\nWith these two labels you can create availability zones for your cluster. A zone can be seen a logical failure domain and a cluster is typically spanned across multiple zones. This could be a rack in a data center for example, hardware which is sharing the same switch or simply different data centers. Zones are seen as independent to each other.\nA region is made up of one or more zones. A cluster is usually not spanned across multiple region.\nKubernetes makes a few assumptions about the structure of zones and regions:\nregions and zones are hierarchical: zones are strict subsets of regions and no zone can be in 2 regions\nzone names are unique across regions; for example region \u0026#34;africa-east-1\u0026#34; might be comprised of zones \u0026#34;africa-east-1a\u0026#34; and \u0026#34;africa-east-1b\u0026#34;\nCleanup This concludes the chapter about nodeSelectors. For the next chapter of the Pod Placement Series (Pod Affinity and Anti Affinity) we need to cleanup our configuration.\nScale the frontend down to 2\noc scale --replicas=2 dc/django-psql-example -n podtesting Remove the label from the namespace\noc annotate ns/podtesting openshift.io/node-selector- (1) 1 The minus at the end defines that this annotation shall be removed And, just to be sure if you have not done this before, remove the nodeSelector from the DeploymentConfig\noc patch dc django-psql-example -n podtesting --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/nodeSelector\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;disktype=ssd\u0026#34; }]\u0026#39; Sources [1]: Well-Known Labels, Annotations and Taints\n"},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-28-affinity/","title":"Pod Affinity/Anti-Affinity","tags":["OCP","Day-2","OpenShift","Pod Placement","Affinity","Anti-Affinity"],"description":"Placeing Pods Using the Affinity rules","content":" While noteSelector provides a very easy way to control where a pod shall be scheduled, the affinity/anti-affinity feature, expands this configuration with more expressive rules like logical AND operators, constraints against labels on other pods or soft rules instead of hard requirements.\nThe feature comes with two types:\npod affinity/anti-affinity - allows constrains against other pod labels rather than node labels.\nnode affinity - allows pods to specify a group of nodes they can be placed on\nPod Placement Series Please check out other ways of pod placements:\nExpand me... NodeSelector\nPod Affinity and Anti Affinity\nNode Affinity\nTaints and Tolerations\nTopology Spread Constraints\nDescheduler\nPod Affinity and Anti-Affinity Affinity and Anti-Affinity controls the nodes on which a pod should (or should not) be scheduled based on labels on Pods that are already scheduled on the node. This is a different approach than nodeSelector, since it does not directly take the node labels into account. That said, one example for such setup would be: You have dedicated nodes for developement and production workload and you have to be sure that pods of dev or prod applications do not run on the same node.\nAffinity and Anti-Affinity are shortly defined as:\nPod affinity - tells scheduler to put a new pod onto the same node as other pods (selection is done using label selectors)\nFor example:\nI want to run where this other labelled pod is already running (Pods from same service shall be running on same node.)\nPod Anti-Affinity - prevents the scheduler to place a new pod onto the same nodes with pods with the same labels\nFor example:\nI definitely do not want to start a pod where this other pod with the defined label is running (to prevent that all Pods of same service are running in the same availability zone.)\nAs described in the official Kubernetes documention two things should be considered:\nThe affinity feature requires processing which can slow down the cluster. Therefore, it is not recommended for cluster with more than 100 nodes.\nThe feature requires that all nodes are consistently labelled (topologyKey). Unintended behaviour might happen if labels are missing.\nUsing Affinity/Anti-Affinity Currently two types of pod affinity/anti-affinity are known:\nrequiredDuringSchedulingIgnoreDuringExecuption (short required) - a hard requirement which must be met before a pod can be scheduled\npreferredDuringSchedulingIgnoredDuringExecution (short preferred) - a soft requirement the scheduler tries to meet, but does not guarantee it\nBoth types can be defined in the same specification. In such case the node must first meet the required rule and then attempt based on best effort to meet the preferred rule. Preparing node labels Remember the prerequisites explained in the . Pod Placement - Introduction. We have 4 compute nodes and an example web application up and running. Before we start with affinity rules we need to label all nodes. Let’s create 2 zones (east and west) for our compute nodes using the well-known label topology.kubernetes.io/zone\nFigure 1. Node Zones oc label nodes compute-0 compute-1 topology.kubernetes.io/zone=east oc label nodes compute-2 compute-3 topology.kubernetes.io/zone=west Configure pod affinity rule In our example we have one database pod and multiple web application pods. Let’s image we would like to always run these pods in the same zone.\nThe pod affinity defines that a pod can be scheduled onto a node ONLY if that node is in the same zone as at least one already-running pod with a certain label.\nThis means we must first label the postgres pod accordingly. Let’s labels the pod with security=zone1\noc patch dc postgresql -n podtesting --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/metadata/labels/security\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;zone1\u0026#34; }]\u0026#39; After a while the postgres pod is restarted on one of the 4 compute nodes (since we did not specify in which zone this single pod shall be started) - here compute-1 (zone == east):\noc get pods -n podtesting -o wide | grep Running postgresql-5-6v5h6 1/1 Running 0 119s 10.129.2.24 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; As a second step, the deployment configuration of the web application must be modified. Remember, we want to run web application pods only on nodes located in the same zone as the postgres pods. In our example this would be either compute-0 or compute-1.\nModify the config accordingly:\nkind: DeploymentConfig apiVersion: apps.openshift.io/v1 [...] namespace: podtesting spec: [...] template: [...] spec: [...] affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security (1) operator: In (2) values: - zone1 (3) topologyKey: topology.kubernetes.io/zone (4) 1 The key of the label of a pod which is already running on that node is \u0026#34;security\u0026#34; 2 As operator \u0026#34;In\u0026#34; is used the postgres pod must have a matching key (security) containing the value (zone1). Other options like \u0026#34;NotIn\u0026#34;, \u0026#34;DoesNotExist\u0026#34; or \u0026#34;Exact\u0026#34; are available as well 3 The value must be \u0026#34;zone1\u0026#34; 4 As topology the topology.kubernetes.io/zone is used. The application can be deployed on nodes with the same label Setting this (and maybe scaling the replicas up a little bit) will start all frontend pods either on compute-0 or on compute-1.\nIn other words: On nodes of the same zone, where the postgres pod with the label security=zone1 is running.\noc get pods -n podtesting -o wide | grep Running django-psql-example-13-4w6qd 1/1 Running 0 67s 10.128.2.58 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-655dj 1/1 Running 0 67s 10.129.2.28 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-9d4pj 1/1 Running 0 67s 10.129.2.27 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-bdwhb 1/1 Running 0 67s 10.128.2.61 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-d4jrw 1/1 Running 0 67s 10.128.2.57 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-dm9qk 1/1 Running 0 67s 10.128.2.60 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-ktmfm 1/1 Running 0 67s 10.129.2.25 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-ldm56 1/1 Running 0 77s 10.128.2.55 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-mh2f5 1/1 Running 0 67s 10.129.2.29 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-qfkhq 1/1 Running 0 67s 10.129.2.26 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-v88qv 1/1 Running 0 67s 10.128.2.56 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-vfgf4 1/1 Running 0 67s 10.128.2.59 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-5-6v5h6 1/1 Running 0 3m18s 10.129.2.24 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Configure pod anti-affinity rule For now the database pod and the web application pod are running on nodes of the same zone. However, somebody is asking us to configure it vice versa: the web application should not run in the same zone as postgresql.\nHere we can use the Anti-Affinity feature.\nAs an alternative, it would also be possible to change the operator in the affinity rule from \u0026#34;In\u0026#34; to \u0026#34;NotIn\u0026#34; kind: DeploymentConfig apiVersion: apps.openshift.io/v1 [...] namespace: podtesting spec: [...] template: [...] spec: [...] affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security (1) operator: In (2) values: - zone1 (3) topologyKey: topology.kubernetes.io/zone (4) This will force the web application pods to run only on \u0026#34;west\u0026#34; zone nodes.\ndjango-psql-example-16-4n9h5 1/1 Running 0 40s 10.131.1.53 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-blf8b 1/1 Running 0 29s 10.130.2.63 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-f9plb 1/1 Running 0 29s 10.130.2.64 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-tm5rm 1/1 Running 0 28s 10.131.1.55 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-x8lbh 1/1 Running 0 29s 10.131.1.54 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-zb5fg 1/1 Running 0 28s 10.130.2.65 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-5-6v5h6 1/1 Running 0 18m 10.129.2.24 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Combining required and preferred affinities It is possible to combine requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution. In such case the required affinity MUST be met, while the preferred affinity is tried to be met. The following examples combines these two types in an affinity and anti-affinity specification.\nThe podAffinity block defines the same as above: schedule the pod on a node of the same zone, where a pod with the label security=zone1 is running. The podAntiAffinity defines that the pod should not be started on a node if that node has a pod running with the label security=zone2. However, the scheduler might decide to do so as long the podAffinity rule is met.\nspec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - zone1 topologyKey: topology.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 (1) podAffinityTerm: labelSelector: matchExpressions: - key: security operator: In values: - zone2 topologyKey: topology.kubernetes.io/zone 1 The weight field is used by the scheduler to create a scoring. The higher the scoring the more preferred is that node. topologyKey It is important to understand the topologyKey setting. This is the key for the node label. If an affinity rule is met, Kubernetes will try to find suitable nodes which are labelled with the topologyKey. All nodes must be labelled consistently, otherwise unintended behaviour might occur.\nAs described in the Kubernetes documentation at Pod Affinity and Anit-Affinity, the topologyKey has some constraints:\nQuote Kubernetes:\nFor pod affinity, empty topologyKey is not allowed in both requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution.\nFor pod anti-affinity, empty topologyKey is also not allowed in both requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution.\nFor requiredDuringSchedulingIgnoredDuringExecution pod anti-affinity, the admission controller LimitPodHardAntiAffinityTopology was introduced to limit topologyKey to kubernetes.io/hostname. If you want to make it available for custom topologies, you may modify the admission controller, or disable it.\nExcept for the above cases, the topologyKey can be any legally label-key.\nEnd of quote\nCleanup As cleanup simply remove the affinity specification from the DeploymentConf. The node labels can stay as they are since they do not hurt.\nSummary This concludes the quick overview of the pod affinity. The next chapter will discuss Node Affinity rules, which allows affinity based on node specifications.\n"},{"uri":"https://blog.stderr.at/tags/taints/","title":"Taints","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/","title":"Taints and Tolerations","tags":["OCP","Day-2","OpenShift","Pod Placement","Taints","Tolerations"],"description":"Placeing Pods Using Taints and Tolerations","content":" While Node Affinity is a property of pods that attracts them to a set of nodes, taints are the exact opposite. Nodes can be configured with one or more taints, which mark the node in a way to only accept pods that do tolerate the taints. The tolerations themselves are applied to pods, telling the scheduler to accept a taints and start the workload on a tainted node.\nA common use case would be to mark certain nodes as infrastructure nodes, where only specific pods are allowed to be executed or to taint nodes with a special hardware (i.e. GPU).\nPod Placement Series Please check out other ways of pod placements:\nExpand me... NodeSelector\nPod Affinity and Anti Affinity\nNode Affinity\nTaints and Tolerations\nTopology Spread Constraints\nDescheduler\nUnderstanding Taints and Tolerations Matching taints and tolerations is defined by a key/value pair and a taint effect.\nFor example, a node can be tainted as:\nspec: [...] template: [...] spec: taints: - effect: NoExecute key: key1 value: value1 [...] And a pod can have the matching toleration:\nspec: [...] template: [...] spec: tolerations: - key: \u0026#34;key1\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value1\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 3600 [...] This means that the pod is tolerating the taint of the node with the pair key1=value1.\nParameter: effect Above example is using as effect NoExecute. The following effects are possible:\nNoSchedule - new pods are not scheduled, existing pods remain\nPreferNoSchedule - new pods are not preferred but can still be scheduled but the scheduler tries to avoid that, existing pods remain\nNoExecute - new pods are not scheduled, existing pods (without matching toleration) are removed! The setting tolerationSeconds is used to define a maximum time until a pod is allowed to stay.\nParameter: operator For the operator two options are possible:\nExists - simply checks if a key exists and key and effect matches. No value should be configured in this case.\nEqual (default) - with this option key, value and effect must match exactly.\nConfiguring Taints and Tolerations Our compute-0 node is a special node with a GPU installed. We would like that our web application is running on this node and ONLY our web application is running on that node. To achieve this, we will taint the node accordingly with the key/value gpu=enabled and the effect NoExecute and configure a toleration to the DeploymentConfig of our web fronted.\nFigure 1. Tainting Node Always configure tolerations before you taint a node. Otherwise the scheduler might not be able to start a pod until it has been configured to tolerate the taints. First we will set the toleration to the DeploymentConfig:\noc edit dc/django-psql-example -n podtesting And add a toleration for the key/value pair, the effect NoExecute and a tolerationSeconds of X seconds.\nspec: [...] template: [...] spec: tolerations: - key: \u0026#34;gpu\u0026#34; value: \u0026#34;enabled\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 30 (1) 1 I am setting the tolerationSeconds to 30 seconds to get it done quicker. Before we taint our node, let’s check which pods are currently running there:\noc get pods -A -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,STATE:.status.phase | grep compute-0 | grep Running tuned-hrvl4 compute-0 Running dns-default-ln9s4 compute-0 Running node-resolver-qk24n compute-0 Running node-ca-lxrvf compute-0 Running ingress-canary-fv5w6 compute-0 Running machine-config-daemon-558v4 compute-0 Running grafana-78ccdb8c9d-8rqsp compute-0 Running node-exporter-rn8h4 compute-0 Running prometheus-adapter-66976bf759-fxdcd compute-0 Running multus-additional-cni-plugins-29qgq compute-0 Running multus-mkd87 compute-0 Running network-metrics-daemon-64hrw compute-0 Running network-check-target-l6l7n compute-0 Running ovnkube-node-hrtln compute-0 Running django-psql-example-24-c599b compute-0 Running django-psql-example-24-l7znv compute-0 Running django-psql-example-24-zpg77 compute-0 Running Multiple different pods are running here, as well as two of our web frontend (django-psql-example)\nThe other django-psql-example pods are started accross the cluster:\noc get pods -n podtesting -o wide oc get pods -n podtesting -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES django-psql-example-25-8ppxc 1/1 Running 0 21s 10.128.0.61 master-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-8rv76 1/1 Running 0 21s 10.130.2.92 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-9m8k2 1/1 Running 0 21s 10.129.0.67 master-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-fhvxg 1/1 Running 0 31s 10.128.2.126 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-kqgwz 0/1 Running 0 21s 10.130.0.71 master-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-m66nn 1/1 Running 0 21s 10.130.0.70 master-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-ntjqb 1/1 Running 0 21s 10.128.0.60 master-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-nxxqh 1/1 Running 0 21s 10.130.2.93 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-p8nbz 1/1 Running 0 21s 10.131.1.183 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-ttr9g 1/1 Running 0 21s 10.129.2.57 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-xn4fp 1/1 Running 0 21s 10.129.2.56 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-xpqf4 1/1 Running 0 21s 10.128.2.127 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-xwmwv 1/1 Running 0 21s 10.131.1.184 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; To taint the node we can simply execute the following command. Be sure to use the same values as in the toleration:\noc adm taint nodes compute-0 gpu=enabled:NoExecute This will create the following specification in the node object:\nspec: [...] taints: - effect: NoExecute key: gpu value: enabled OpenShift will allow pods, which are not tolerating the taints, to keep on running for 30 seconds. After that, these pods will be evicted and started elsewhere.\nWhen we check after the tolerationSeconds time has passed which pods are running on the node compute-0, we will see that most pods have disappeared, except the pods for the webapplication and pods which are part of DaemonSets. (DaemonSets are defined to run on all or specific nodes and are not evicted. The cluster-DaemonSets are tolerating everything)\noc get pods -A -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,STATE:.status.phase | grep compute-0 | grep Running tuned-hrvl4 compute-0 Running node-resolver-qk24n compute-0 Running node-ca-lxrvf compute-0 Running machine-config-daemon-558v4 compute-0 Running node-exporter-rn8h4 compute-0 Running multus-additional-cni-plugins-29qgq compute-0 Running multus-mkd87 compute-0 Running network-metrics-daemon-64hrw compute-0 Running network-check-target-l6l7n compute-0 Running ovnkube-node-hrtln compute-0 Running django-psql-example-25-8dzqh compute-0 Running django-psql-example-25-9p4sb compute-0 Running django-psql-example-25-pqbvn compute-0 Running django-psql-example-25-sb6hr compute-0 Running Removing taints Taints can be simply removed with the oc command added a trailing -\noc adm taint nodes compute-0 gpu=enabled:NoExecute- Built-in Taints - Taint Nodes by Condition Several taints are built into OpenShift and are set during certain events (aka Taint Nodes by Condition) and cleared when the condition is resolved.\nThe following list is quoted from the OpenShift documentation and provides a list of taints which are automatically set. For example, when a node becomes unavailable:\nnode.kubernetes.io/not-ready: The node is not ready. This corresponds to the node condition Ready=False.\nnode.kubernetes.io/unreachable: The node is unreachable from the node controller. This corresponds to the node condition Ready=Unknown.\nnode.kubernetes.io/out-of-disk: The node has insufficient free space on the node for adding new pods. This corresponds to the node condition OutOfDisk=True.\nnode.kubernetes.io/memory-pressure: The node has memory pressure issues. This corresponds to the node condition MemoryPressure=True.\nnode.kubernetes.io/disk-pressure: The node has disk pressure issues. This corresponds to the node condition DiskPressure=True.\nnode.kubernetes.io/network-unavailable: The node network is unavailable.\nnode.kubernetes.io/unschedulable: The node is unschedulable.\nnode.cloudprovider.kubernetes.io/uninitialized: When the node controller is started with an external cloud provider, this taint is set on a node to mark it as unusable. After a controller from the cloud-controller-manager initializes this node, the kubelet removes this taint.\nDepending on the condition, the node will either have the effect NoSchedule, which means no new pods will be started there (unless a toleration is configured) or the effect NoExecute, which will evict pods with no tolerations from the node. Typical examples for NoSchedule condition would be: memory-pressure or disk-pressure. If a node is unreachable or not-ready, then it will be automatically tainted with the effect NoExecute. tolerationSeconds (default 300) will be respected.\nTolerating all taints Tolerations can be configured to tolerate all possible taints. In such case no value or key is configured and operator: \u0026#34;Exists\u0026#34; is used:\nspec: [...] template: [...] spec: tolerations: - operator: “Exists” Some cluster daemonsets (i.e. tuned) are configured this way. Cleanup Remove the taint from the node:\noc adm taint nodes compute-0 gpu=enabled:NoExecute- And remove the toleration specification from the DeploymentConfig\nspec: [...] template: [...] spec: tolerations: - key: \u0026#34;gpu\u0026#34; value: \u0026#34;enabled\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 30 (1) "},{"uri":"https://blog.stderr.at/tags/tolerations/","title":"Tolerations","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/topology-spread-constraints/","title":"Topology Spread Constraints","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/","title":"Topology Spread Constraints","tags":["OCP","Day-2","OpenShift","Pod Placement","Topology Spread Constraints"],"description":"Placeing Pods Using Topology Spread Constraints","content":" Topology spread constraints is a new feature since Kubernetes 1.19 (OpenShift 4.6) and another way to control where pods shall be started. It allows to use failure-domains, like zones or regions or to define custom topology domains. It heavily relies on configured node labels, which are used to define topology domains. This feature is a more granular approach than affinity, allowing to achieve higher availability.\nPod Placement Series Please check out other ways of pod placements:\nExpand me... NodeSelector\nPod Affinity and Anti Affinity\nNode Affinity\nTaints and Tolerations\nTopology Spread Constraints\nDescheduler\nUnderstanding Topology Spread Constraints Imagine, like for pod affinity, we have two zones (east and west) in our 4 compute node cluster.\nFigure 1. Node Zones These zones are defined by node labels, which can be created with the following commands:\noc label nodes compute-0 compute-1 topology.kubernetes.io/zone=east oc label nodes compute-2 compute-3 topology.kubernetes.io/zone=west Now let’s scale our web application to 3 pods. The OpenShift scheduler will try to evenly spread the pods accross the cluster:\noc get pods -n podtesting -o wide django-psql-example-31-jrhsr 0/1 Running 0 8s 10.130.2.112 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-31-q66hk 0/1 Running 0 8s 10.131.1.219 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-31-xv7jc 0/1 Running 0 8s 10.128.3.115 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Figure 2. Running Pods If a new pod is started the scheduler may try to start it on compute-1. However, this is done based on best effort. The scheduler does not guarantee that and may try to start it on one of the nodes in zone \u0026#34;West\u0026#34;. With the configuration topologySpreadConstraints this can be controlled and incoming pods can only be scheduled on a node of zone \u0026#34;East\u0026#34; (either on compute-0 or compute-1).\nConfigure TopologySpreadContraint Let’s configure our topology by adding the following into the DeploymentConfig:\nspec: [...] template: [...] topologySpreadConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: name: django-psql-example This defines the following parameter:\nmaxSkew - defines the degree to which pods may be unevenly distributed (must be greater than zero). Depending on the whenUnsatisfiable parameter the bahaviour differs:\nwhenUnsatisfiable == DoNotSchedule: would not schedule if the maxSkew is is not met.\nwhenUnsatisfiable == ScheduleAnyway: the scheduler will still schedule and gives the topology which would decrease the skew a better score\ntopologyKey - key of node lables\nwhenUnsatisfiable - defines what to do with a pod if the spread constraint is not met. Can be either DoNotSchedule (default) or ScheduleAnyway\nlableSelector - used to find matching pods. Found pods are considered to be part of the topology domain. In our example we simply use a default label of the application: name: django-psql-example\nIf now a 4th pod shall be started the topologySpreadConstraints will allow this pod on one of the nodes of zone=east (either compute-0 or compute-1). It cannot be scheduled on nodes in zone=west since it would violate the maxSkew: 1\nPod distribution would be 1 in zone east and 3 on zone west. --\u0026gt; the skew would then be 3 - 1 = 2 which is not equal to 1 Configure multiple TopologySpreadContraint It is possible to configure multiple TopologySpreadConstraints. In such a case all contrains must meet the requirement (logical AND). For example:\nspec: [...] template: [...] topologySpreadConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: name: django-psql-example - maxSkew: 1 topologyKey: kubernetes.io/hostname whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: name: django-psql-example The first part is identical to our first example and would allow the schedule to start a pod in topology.kubernetes.io/zone: east (compute-0 or compute-1). The second configuration defines not a zone but a node hostname. Now the scheduler can only deploy into zone=east AND onto node=compute-1\nConflicts with multiple TopologySpreadConstraints If you use multiple constraints conflicts are possible. For example, you have a 3 node cluster and the 2 zones east and west. In such cases the maxSkew might be increased or the whenUnsatisfiable might be set to ScheduleAnyway\nSee https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#example-multiple-topologyspreadconstraints for further information.\nCleanup Remove the topologySpreadConstraint\nspec: [...] template: [...] topologySpreadConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: name: django-psql-example "},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-09-01-descheduler/","title":"Using Descheduler","tags":["OCP","Day-2","OpenShift","Descheduler"],"description":"Evicting pods using the Descheduler","content":" Descheduler is a new feature which is GA since OpenShift 4.7. It can be used to evict pods from nodes based on specific strategies. The evicted pod is then scheduled on another node (by the Scheduler) which is more suitable.\nThis feature can be used when:\nnodes are under/over-utilized\npod or node affinity, taints or labels have changed and are no longer valid for a running pod\nnode failures\npods have been restarted too many times\nPod Placement Series Please check out other ways of pod placements:\nExpand me... NodeSelector\nPod Affinity and Anti Affinity\nNode Affinity\nTaints and Tolerations\nTopology Spread Constraints\nDescheduler\nDescheduler Profiles The following descheduler profiles are known and one or multiple can be configured for the Descheduler. Each profiles enables certain strategies which the Descheduler is leveraging. Since the strategies names are more or less self explaining, I did not add their full description here. Instead detailed information can be found at: Descheduler Profiles\nAffinityAndTaints - removes pods that violates affinity and anti-affinity rules or taints\nRemovePodsViolatingInterPodAntiAffinity\nRemovePodsViolatingNodeAffinity\nRemovePodsViolatingNodeTaints\nTopologyAndDuplicates - evicts pods which are not evenly spreaded or which are violating the topology domain\nRemovePodsViolatingTopologySpreadConstraint\nRemoveDuplicates\nLifecycleAndUtilization - evicts long-running pods to balance resource usage of nodes\nRemovePodsHavingTooManyRestarts - Pods that are restarted more than 100 times\nLowNodeUtilization - removes pods from overutilized nodes.\nA node is considered underutilized if its usage is below 20% for all thresholds (CPU, memory, and number of pods).\nA node is considered overutilized if its usage is above 50% for any of the thresholds (CPU, memory, and number of pods).\nPodLifeTime - evicts pods that are too old\nDescheduler mechanism The following rules are followed by the Descheduler to ensure that eviction of pods does not go wild. Therefore the following pods will never be evicted:\npods in openshift-* or kube-system namespaces\npods with priorityClassName equal to system-cluster-critical or system-node-critical\npods which cannot be recreated, for example: static or stand-alone pods/jobs or pods without a replication controller or replica set\npods of a daemon set\npods with local storage\npods which are violating the pod disruption budget\nInstalling the Descheduler The Descheduler is not installed by default and must be installed after the cluster has been initiated. This is done by installed the Kube Descheduler Operator.\nFirst we create a separate namespace for our operator, including a label:\noc adm new-project openshift-kube-descheduler-operator oc label ns/openshift-kube-descheduler-operator openshift.io/cluster-monitoring=true Then we search for the Kube Descheduler operator and install it, using the newly created namespace:\nFigure 1. Install Descheduler Operator After a few moments the operator will be installed.\nYou can now create a Descheduler instance either via UI (wizard) or by using the following specification:\napiVersion: operator.openshift.io/v1 kind: KubeDescheduler metadata: name: cluster namespace: openshift-kube-descheduler-operator spec: deschedulingIntervalSeconds: 3600 (1) logLevel: Normal (2) managementState: Managed operatorLogLevel: Normal (3) profiles: (4) - AffinityAndTaints - TopologyAndDuplicates - LifecycleAndUtilization 1 Defines the time interval the descheduler is running. Default is 3600 seconds 2 Defines logging for overall component. Can be Normal, Debug, Trace or TraceAll 3 Defines logging for the operator itself. Can be Normal, Debug, Trace or TraceAll 4 Enables on or multiple profiles the Descheduler should consider "},{"uri":"https://blog.stderr.at/tags/ansible-tower/","title":"Ansible Tower","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/2021/07/ansible-tower-and-downloading-collections/","title":"Ansible Tower and downloading collections","tags":["Ansible","Ansible Tower"],"description":"Getting Ansible Tower to download collections can be hard","content":" Every wondered why Ansible Tower does not start downloading required collections when you synchronize a project? Here are the stumbling blocks we discovered so far:\nWrong name for requirements.yml When downloading collections Ansible Tower searches for a file requirements.yml in the collections directory.\nBe careful with the file extension: requirements.yml has to end with the extension .yml and not .yaml.\nCollections download is disabled in Ansible Tower Within Ansible Tower there is a setting called ENABLE COLLECTION(S) DOWNLOAD under Settings/Jobs. This has to be set to true, which is also the default.\nNo Ansible Galaxy credential defined for the organization Last but not least an Ansible Galaxy credential needs to be defined for the organization where the project is defined. With the default installation of Ansible Tower, when the sample playbooks are installed there is a credential called Ansible Galaxy defined. You need to assign this credential to the organization.\nIf you skip installing the sample playbooks, no Ansible Galaxy credential will be defined for you and you have to create it manually.\nHow does this actually work? Ansible Tower uses a Python virtual environment for running Ansible. The default environment is installed in /var/lib/awx/venv/awx. You can also create custom environments, see Using virtualenv with Ansible Tower.\nIn the default setup the following files define how collections are downloaded:\nlib/python3.6/site-packages/awx/main/tasks.py\nlib/python3.6/site-packages/awx/playbooks/project_update.yml\ntask.py task.py defines various internal tasks Tower has to run on various occasions. For example in line number 1930 (Ansible Tower 3.8.3) the task RunProjectUpdate gets defined. This is the task Tower has to run whenever a project update is required.\nIn our case the function build_extra_vars_file (line 2083 with Ansible Tower 3.8.3) defines the variable galaxy_creds_are_defined only if the organization has a galaxy credential defined (line 2099 Ansible Tower 3.8.3).\nLine 2120 (Ansible Tower 3.8.3) finally defines the Ansible extra variable collections_enabled depending on galaxy_creds_are_defined.\nproject_update.yml So task.py defines the extra variable collections_enabled (see above). Finally the playbook project_update.yml consumes this extra variable and only downloads collections if collections_enabled is set to true, see the block string at line 192 (Ansible Tower 3.8.3) in `project_update.yml.\nSo long and thanks for all the fish!\n"},{"uri":"https://blog.stderr.at/tags/hardening/","title":"Hardening","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/compliance/2021/07/oc-compliance-command-line-plugin/","title":"oc compliance command line plugin","tags":["Hardening","OpenShift","OCP","security","compliance","plugin"],"description":"Using the oc client plugin os-compliance","content":" As described at Compliance Operator the Compliance Operator can be used to scan the OpenShift cluster environment against security benchmark, like CIS. Fetching the actual results might be a bit tricky tough.\nWith OpenShift 4.8 plugins to the oc command are allowed. One of these plugin os oc compliance, which allows you to easily fetch scan results, re-run scans and so on. Let’s install and try it out.\nInstallation An oc plugin must be deployed into the same directory as the oc command itself.\nThe following describes the building and installation of the plugin.\nYou need Go installed on your node. Clone the Git repository:\ngit clone https://github.com/openshift/oc-compliance.git Build and install the plugin\nmake; make install go build -o ./bin/oc-compliance ./cmd which oc | xargs dirname | xargs -n1 cp ./bin/oc-compliance The plugin allows the use of oc compliance\noc compliance You must specify a sub-command. Usage: oc-compliance [flags] oc-compliance [command] Available Commands: bind Creates a ScanSettingBinding for the given parameters controls Get a report of what controls you\\\u0026#39;re complying with fetch-fixes Download the fixes/remediations fetch-raw Download raw compliance results help Help about any command rerun-now Force a re-scan for one or more ComplianceScans view-result View a ComplianceCheckResult Flags: -h, --help help for oc-compliance Use \u0026#34;oc-compliance [command] --help\u0026#34; for more information about a command. Fetch Raw Results Without the oc-compliance plugin it was required to manually spin up a Pod and download the results from this Pod, where the PV is mounted. Now, with a simple command we can select the ScanSettingBinding and define an output folder. For example:\noc compliance fetch-raw \u0026lt;object-type\u0026gt; \u0026lt;object-name\u0026gt; -o \u0026lt;output-path\u0026gt; Assuming the the compliance operator was configured as in the previous article, we have the ScanSettingBinding called cis-compliance:\noc compliance fetch-raw scansettingbindings cis-compliance -n openshift-compliance -o /tmp/ This starts downloading the result archives into /tmp\nFetching results for cis-compliance scans: ocp4-cis-node-worker, ocp4-cis-node-master, ocp4-cis Fetching raw compliance results for pod \u0026#39;raw-result-extractor-fxbw8\u0026#39;.Fetching raw compliance results for scan \u0026#39;ocp4-cis-node-worker\u0026#39;......... The raw compliance results are avaliable in the following directory: /tmp/ocp4-cis-node-worker Fetching raw compliance results for pod \u0026#39;raw-result-extractor-kqrw5\u0026#39;.Fetching raw compliance results for scan \u0026#39;ocp4-cis-node-master\u0026#39;..... The raw compliance results are avaliable in the following directory: /tmp/ocp4-cis-node-master Fetching raw compliance results for pod \u0026#39;raw-result-extractor-pfrgk\u0026#39;.Fetching raw compliance results for scan \u0026#39;ocp4-cis\u0026#39;.. The raw compliance results are avaliable in the following directory: /tmp/ocp4-cis ls -la /tmp/ocp4-cis* /tmp/ocp4-cis: total 172 drwx------ 2 root root 4096 Jul 30 16:05 . drwxrwxrwt. 18 root root 4096 Jul 30 16:05 .. -rw-r--r-- 1 root root 166676 Jul 30 16:05 ocp4-cis-api-checks-pod.xml.bzip2 /tmp/ocp4-cis-node-master: total 504 drwx------ 2 root root 4096 Jul 30 16:05 . drwxrwxrwt. 18 root root 4096 Jul 30 16:05 .. -rw-r--r-- 1 root root 168256 Jul 30 16:05 ocp4-cis-node-master-master-0-pod.xml.bzip2 -rw-r--r-- 1 root root 165716 Jul 30 16:05 ocp4-cis-node-master-master-1-pod.xml.bzip2 -rw-r--r-- 1 root root 166945 Jul 30 16:05 ocp4-cis-node-master-master-2-pod.xml.bzip2 /tmp/ocp4-cis-node-worker: total 1112 drwx------ 2 root root 4096 Jul 30 16:05 . drwxrwxrwt. 18 root root 4096 Jul 30 16:05 .. -rw-r--r-- 1 root root 154943 Jul 30 16:05 ocp4-cis-node-worker-compute-0-pod.xml.bzip2 -rw-r--r-- 1 root root 154903 Jul 30 16:05 ocp4-cis-node-worker-compute-1-pod.xml.bzip2 -rw-r--r-- 1 root root 154939 Jul 30 16:05 ocp4-cis-node-worker-compute-2-pod.xml.bzip2 -rw-r--r-- 1 root root 154890 Jul 30 16:05 ocp4-cis-node-worker-compute-3-pod.xml.bzip2 -rw-r--r-- 1 root root 168175 Jul 30 16:05 ocp4-cis-node-worker-master-0-pod.xml.bzip2 -rw-r--r-- 1 root root 165603 Jul 30 16:05 ocp4-cis-node-worker-master-1-pod.xml.bzip2 -rw-r--r-- 1 root root 166914 Jul 30 16:05 ocp4-cis-node-worker-master-2-pod.xml.bzip2 Re-Run Scans Sometimes it is necessary to re-run scans. This can be done by annotating the appropriate scan as described at: Performing a Rescan\nWith the oc plugin you can simply trigger a re-scan with a single command:\noc compliance rerun-now scansettingbindings \u0026lt;name of scanbinding\u0026gt; For example:\noc compliance rerun-now scansettingbindings cis-compliance Example output:\nRerunning scans from \u0026#39;cis-compliance\u0026#39;: ocp4-cis-node-worker, ocp4-cis-node-master, ocp4-cis Re-running scan \u0026#39;openshift-compliance/ocp4-cis-node-worker\u0026#39; Re-running scan \u0026#39;openshift-compliance/ocp4-cis-node-master\u0026#39; Re-running scan \u0026#39;openshift-compliance/ocp4-cis\u0026#39; With the command oc get compliancescan -n openshift-compliance you can check when the scan has been done:\nNAME PHASE RESULT ocp4-cis RUNNING NOT-AVAILABLE ocp4-cis-node-master RUNNING NOT-AVAILABLE ocp4-cis-node-worker AGGREGATING NOT-AVAILABLE View Results on CLI Once a scan process has finished you can verify the check results quick and easy using the command line:\noc get ComplianceCheckResult -A This prints for example:\nNAMESPACE NAME STATUS SEVERITY [...] openshift-compliance ocp4-cis-audit-log-forwarding-enabled FAIL medium [...] The view-result can print a human readable output, for example:\noc compliance view-result ocp4-cis-audit-log-forwarding-enabled -n openshift-compliance Example:\n+----------------------+-----------------------------------------------------------------------------------------+ | KEY | VALUE | +----------------------+-----------------------------------------------------------------------------------------+ | Title | Ensure that Audit Log | | | Forwarding Is Enabled | +----------------------+-----------------------------------------------------------------------------------------+ | Status | FAIL | +----------------------+-----------------------------------------------------------------------------------------+ | Severity | medium | +----------------------+-----------------------------------------------------------------------------------------+ | Description | OpenShift audit works at the | | | API server level, logging | | | all requests coming to the | | | server. Audit is on by default | | | and the best practice is | | | to ship audit logs off the | | | cluster for retention. The | | | cluster-logging-operator is | | | able to do this with the | | | | | | | | | | | | ClusterLogForwarders | | | | | | | | | | | | resource. The forementioned resource can be configured to logs to different third party | | | systems. For more information on this, please reference the official documentation: | | | https://docs.openshift.com/container-platform/4.6/logging/cluster-logging-external.html | +----------------------+-----------------------------------------------------------------------------------------+ | Rationale | Retaining logs ensures the | | | ability to go back in time to | | | investigate or correlate any | | | events. Offloading audit logs | | | from the cluster ensures that | | | an attacker that has access | | | to the cluster will not be | | | able to tamper with the logs | | | because of the logs being | | | stored off-site. | +----------------------+-----------------------------------------------------------------------------------------+ | Instructions | Run the following command: | | | | | | oc get clusterlogforwarders | | | instance -n openshift-logging | | | -ojson | jq -r | | | \u0026#39;.spec.pipelines[].inputRefs | | | | contains([\u0026#34;audit\u0026#34;])\u0026#39; | | | | | | The output should return true. | +----------------------+-----------------------------------------------------------------------------------------+ | CIS-OCP Controls | 1.2.23 | +----------------------+-----------------------------------------------------------------------------------------+ | NIST-800-53 Controls | AC-2(12), AU-6, AU-6(1), | | | AU-6(3), AU-9(2), SI-4(16), | | | AU-4(1), AU-11, AU-7, AU-7(1) | +----------------------+-----------------------------------------------------------------------------------------+ | Available Fix | No | +----------------------+-----------------------------------------------------------------------------------------+ | Result Object Name | ocp4-cis-audit-log-forwarding-enabled | +----------------------+-----------------------------------------------------------------------------------------+ | Rule Object Name | ocp4-audit-log-forwarding-enabled | +----------------------+-----------------------------------------------------------------------------------------+ | Remediation Created | No | +----------------------+-----------------------------------------------------------------------------------------+ "},{"uri":"https://blog.stderr.at/tags/plugin/","title":"Plugin","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/compliance/2021/07/compliance-operator/","title":"Compliance Operator","tags":["Hardening","OpenShift","OCP","security","compliance","Compliance Operator"],"description":"Hardening OpenShift using the Compliance Operator","content":" OpenShift comes out of the box with a highly secure operating system, called Red Hat CoreOS. This OS is immutable, which means that no direct changes are done inside the OS, instead any configuration is managed by OpenShift itself using MachineConfig objects. Nevertheless, hardening certain settings must still be considered. Red Hat released a hardening guide (CIS Benchmark) which can be downloaded at https://www.cisecurity.org/.\nHowever, an automated way to perform such checks would be nice too. To achieve this the Compliance Operator can be leveraged, which runs an OpenSCAP check to create reports of the clusters is compliant or as the official documentation describes:\nThe Compliance Operator lets OpenShift Container Platform administrators describe the desired compliance state of a cluster and provides them with an overview of gaps and ways to remediate them. The Compliance Operator assesses compliance of both the Kubernetes API resources of OpenShift Container Platform, as well as the nodes running the cluster. The Compliance Operator uses OpenSCAP, a NIST-certified tool, to scan and enforce security policies provided by the content.\nThis article shall show how to quickly install the operator and retrieve the first result. It is not a full documentation, which is written by other people at: Compliance Operator, especially remediation is not covered here.\nAs prerequisites we have:\nInstalled OpenShift 4.6+ cluster\nThe Compliance Operator is available for Red Hat Enterprise Linux CoreOS (RHCOS) deployments only. Install the Compliance Operator The easiest way to deploy the Compliance Operator is by searching the OperatorHub which is available inside OpenShift.\nFigure 1. Install Compliance Operator Keep the default settings and wait until the operator has been installed.\nFigure 2. Install Compliance Operator Custom Resources (CRDs) The operator brings a ton of new CRDs into the system:\nScanSetting …​ defines when and on which roles (worker, master …​) a check shall be executed. It also defines a persistent volume (PV) to store the scan results. Two ScanSettings are created during the installation:\ndefault: just scans without automatically apply changes\ndefault-auto-apply: can automatically remediate without extra steps\nScanSettingBinding …​ binds one or more profiles to a scan\nProfile …​ Represent different compliance benchmarks with a set of rules. For this blog we will use CIS Benchmark profiles\nProfileBundle …​ Bundles a security image, which is later used by Profiles.\nRule …​ Rules which are used by profiles to verify the state of the cluster.\nTailoredProfile …​ Customized profile\nComplianceScan …​ scans which have been performed\nComplianceCheckResult …​ The results of a scan. Each ComplianceCheckResult represents the result of one compliance rule check\nComplianceRemediation …​ If a rule ca be remediated automatically, this object is created.\nCreate a ScanBinding object The first step to do is to create a ScanBiding objects. (We reuse the default ScanSetting)\nLet’s create the following object, which is using the profiles ocp4-cis and ocp4-cis-node\napiVersion: compliance.openshift.io/v1alpha1 kind: ScanSettingBinding metadata: name: cis-compliance profiles: - name: ocp4-cis-node (1) kind: Profile apiGroup: compliance.openshift.io/v1alpha1 - name: ocp4-cis (2) kind: Profile apiGroup: compliance.openshift.io/v1alpha1 settingsRef: name: default (3) kind: ScanSetting apiGroup: compliance.openshift.io/v1alpha1 1 use the profile ocp4-cis-node 2 use the profile ocp4-cis 3 reference to the default scansetting As soon as the object is created the cluster is scan is started. The objects ComplianceSuite and ComplianceScan are created automatically and will eventually reach the phase \u0026#34;DONE\u0026#34; when the scan is completed.\nThe following command will show the results of the scans\noc get compliancescan -n openshift-compliance NAME PHASE RESULT ocp4-cis DONE NON-COMPLIANT ocp4-cis-node-master DONE NON-COMPLIANT ocp4-cis-node-worker DONE INCONSISTENT Three different checks have been done. One overall cluster check and 2 separated for master and worker nodes.\nAs we used the default ScanSetting the next check will run a 1 am.\nProfiles The operator comes with a set of standard profiles which represent different compliance benchmarks.\nTo view available profiles:\noc get profiles.compliance -n openshift-compliance NAME AGE ocp4-cis 28m ocp4-cis-node 28m ocp4-e8 28m ocp4-moderate 28m rhcos4-e8 28m rhcos4-moderate 28m Each profile contains a description which explains the intention and a list of rules which used in this profile.\nFor example the profile \u0026#39;ocp4-cis-node\u0026#39; used above is containing:\noc get profiles.compliance -n openshift-compliance -oyaml ocp4-cis-node # Output description: This profile defines a baseline that aligns to the Center for Internet Security® Red Hat OpenShift Container Platform 4 Benchmark™, V0.3, currently unreleased. This profile includes Center for Internet Security® Red Hat OpenShift Container Platform 4 CIS Benchmarks™ content. Note that this part of the profile is meant to run on the Operating System that Red Hat OpenShift Container Platform 4 runs on top of. This profile is applicable to OpenShift versions 4.6 and greater. [...] name: ocp4-cis-node namespace: openshift-compliance [...] rules: - ocp4-etcd-unique-ca - ocp4-file-groupowner-cni-conf - ocp4-file-groupowner-controller-manager-kubeconfig - ocp4-file-groupowner-etcd-data-dir - ocp4-file-groupowner-etcd-data-files - ocp4-file-groupowner-etcd-member - ocp4-file-groupowner-etcd-pki-cert-files - ocp4-file-groupowner-ip-allocations [...] Like the profiles the different rules can be inspected:\noc get rules.compliance -n openshift-compliance ocp4-file-groupowner-etcd-member -o jsonpath=\u0026#39;{\u0026#34;Title: \u0026#34;}{.title}{\u0026#34;\\nDescription: \\n\u0026#34;}{.description}\u0026#39; # Output Title: Verify Group Who Owns The etcd Member Pod Specification File Description: To properly set the group owner of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml , run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml Profile Customization Sometimes is it required to modify (tailor) a profile to fit specific needs. With the TailoredProfile object it is possible to enable or disable rules.\nIn this blog, I just want to share a quick example from the official documentaiton: https://docs.openshift.com/container-platform/4.7/security/compliance_operator/compliance-operator-tailor.html\nThe following TailoredProfile disables 2 rules and sets a value for another rule:\napiVersion: compliance.openshift.io/v1alpha1 kind: TailoredProfile metadata: name: nist-moderate-modified spec: extends: rhcos4-moderate title: My modified NIST moderate profile disableRules: - name: rhcos4-file-permissions-node-config rationale: This breaks X application. - name: rhcos4-account-disable-post-pw-expiration rationale: No need to check this as it comes from the IdP setValues: - name: rhcos4-var-selinux-state rationale: Organizational requirements value: permissive Working with scan results Once a scan finished you probably want to see what the status of the scan is.\nAs you sse above the cluster failed to be compliant.\noc get compliancescan -n openshift-compliance NAME PHASE RESULT ocp4-cis DONE NON-COMPLIANT ocp4-cis-node-master DONE NON-COMPLIANT ocp4-cis-node-worker DONE INCONSISTENT Retrieving results via oc command List all results which can be remediated automatically:\noc get compliancecheckresults -l \u0026#39;compliance.openshift.io/check-status=FAIL,compliance.openshift.io/automated-remediation\u0026#39; -n openshift-compliance NAME STATUS SEVERITY ocp4-cis-api-server-encryption-provider-cipher FAIL medium ocp4-cis-api-server-encryption-provider-config FAIL medium Further information about remediation can be found at: Compliance Operator Remediation List all results which cannot be remediated automatically and must be fixed manually instead:\noc get compliancecheckresults -l \u0026#39;compliance.openshift.io/check-status=FAIL,!compliance.openshift.io/automated-remediation\u0026#39; -n openshift-compliance NAME STATUS SEVERITY ocp4-cis-audit-log-forwarding-enabled FAIL medium ocp4-cis-file-permissions-proxy-kubeconfig FAIL medium ocp4-cis-node-master-file-groupowner-ip-allocations FAIL medium ocp4-cis-node-master-file-groupowner-openshift-sdn-cniserver-config FAIL medium ocp4-cis-node-master-file-owner-ip-allocations FAIL medium ocp4-cis-node-master-file-owner-openshift-sdn-cniserver-config FAIL medium ocp4-cis-node-master-kubelet-configure-event-creation FAIL medium ocp4-cis-node-master-kubelet-configure-tls-cipher-suites FAIL medium ocp4-cis-node-master-kubelet-enable-protect-kernel-defaults FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-imagefs-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-imagefs-inodesfree FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-memory-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-nodefs-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-nodefs-inodesfree FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-imagefs-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-imagefs-inodesfree FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-memory-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-nodefs-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-nodefs-inodesfree FAIL medium ocp4-cis-node-worker-file-groupowner-ip-allocations FAIL medium ocp4-cis-node-worker-file-groupowner-openshift-sdn-cniserver-config FAIL medium ocp4-cis-node-worker-file-owner-ip-allocations FAIL medium ocp4-cis-node-worker-file-owner-openshift-sdn-cniserver-config FAIL medium ocp4-cis-node-worker-kubelet-configure-event-creation FAIL medium ocp4-cis-node-worker-kubelet-configure-tls-cipher-suites FAIL medium ocp4-cis-node-worker-kubelet-enable-protect-kernel-defaults FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-imagefs-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-imagefs-inodesfree FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-memory-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-nodefs-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-nodefs-inodesfree FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-imagefs-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-imagefs-inodesfree FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-memory-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-nodefs-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-nodefs-inodesfree FAIL medium Retrieving RAW results Let’s first retrieve the raw result of the scan. For each of the ComplianceScans a volume claim (PVC) is created to store he results. We can use a Pod to mount the volume to download the scan results.\nThe following PVC have been created on our example:\noc get pvc -n openshift-compliance NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE ocp4-cis Bound pvc-cc026ae3-2f42-4e19-bc55-016c6dd31d22 1Gi RWO managed-nfs-storage 4h17m ocp4-cis-node-master Bound pvc-3bd47c5e-2008-4759-9d53-ba41b568688d 1Gi RWO managed-nfs-storage 4h17m ocp4-cis-node-worker Bound pvc-77200e5f-0f15-410c-a4ee-f2fb3e316f84 1Gi RWO managed-nfs-storage 4h17m Now we can create a Pod which mounts all PVCs at once:\napiVersion: \u0026#34;v1\u0026#34; kind: Pod metadata: name: pv-extract namespace: openshift-compliance spec: containers: - name: pv-extract-pod image: registry.access.redhat.com/ubi8/ubi command: [\u0026#34;sleep\u0026#34;, \u0026#34;3000\u0026#34;] volumeMounts: (1) - mountPath: \u0026#34;/workers-scan-results\u0026#34; name: workers-scan-vol - mountPath: \u0026#34;/masters-scan-results\u0026#34; name: masters-scan-vol - mountPath: \u0026#34;/ocp4-scan-results\u0026#34; name: ocp4-scan-vol volumes: (2) - name: workers-scan-vol persistentVolumeClaim: claimName: ocp4-cis-node-worker - name: masters-scan-vol persistentVolumeClaim: claimName: ocp4-cis-node-master - name: ocp4-scan-vol persistentVolumeClaim: claimName: ocp4-cis 1 mount paths 2 volumesclaims to mount This creates a Pod with the PVCs mounted inside:\nsh-4.4# ls -la | grep scan drwxrwxrwx. 3 root root 4096 Jul 20 05:20 master-scan-results drwxrwxrwx. 3 root root 4096 Jul 20 05:20 ocp4-scan-results drwxrwxrwx. 3 root root 4096 Jul 20 05:20 workers-scan-results We can download the result-files to our local machine for further auditing. Therefore, we create the folder scan_results in which we copy everything:\nmkdir scan-results; cd scan-results oc -n openshift-compliance cp pv-extract:ocp4-scan-results ocp4-scan-results/. oc -n openshift-compliance cp pv-extract:workers-scan-results workers-scan-results/. oc -n openshift-compliance cp pv-extract:masters-scan-results masters-scan-results/. This will download several bzip2 archives for the appropriate scan result.\nOnce done, you can delete the \u0026#34;download pod\u0026#34; using: oc delete pod pv-extract -n openshift-compliance\nWork wth RAW results So above section described the download of the bzip2 files but what to do with it? First, you can import it into a tool which is able to read openScap reports. Or, secondly, you can use the oscap command to create a html output.\nWe have downloaded the following files:\n./ocp4-scan-results/0/ocp4-cis-api-checks-pod.xml.bzip2 ./masters-scan-results/0/ocp4-cis-node-master-master-0-pod.xml.bzip2 ./masters-scan-results/0/ocp4-cis-node-master-master-2-pod.xml.bzip2 ./masters-scan-results/0/ocp4-cis-node-master-master-1-pod.xml.bzip2 ./workers-scan-results/0/ocp4-cis-node-worker-compute-0-pod.xml.bzip2 ./workers-scan-results/0/ocp4-cis-node-worker-compute-1-pod.xml.bzip2 ./workers-scan-results/0/ocp4-cis-node-worker-compute-3-pod.xml.bzip2 ./workers-scan-results/0/ocp4-cis-node-worker-compute-2-pod.xml.bzip2 To create the html output (be sure that open-scap is installed on you host):\nmkdir html oscap xccdf generate report ocp4-scan-results/0/ocp4-cis-api-checks-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-api-checks.html oscap xccdf generate report masters-scan-results/0/ocp4-cis-node-master-master-0-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-node-master-master-0.html oscap xccdf generate report masters-scan-results/0/ocp4-cis-node-master-master-1-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-node-master-master-1.html oscap xccdf generate report masters-scan-results/0/ocp4-cis-node-master-master-2-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-node-master-master-2.html oscap xccdf generate report workers-scan-results/0/ocp4-cis-node-worker-compute-0-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-node-worker-compute-0.html ... The resulted html files are too big to be show here, but some snippets should give an overview:\nTo view the html output as an example I have linked the html files:\nOCP4 - CIS\nExample Master Node Results\nExample Worker Node Results\nOverall Scoring of the result:\nFigure 3. Scoring A list if passed or failed checks:\nFigure 4. Scan Result list Scan details with a link to the CIS Benchmark section and further explainations on how to fix the issue:\nFigure 5. Scan details Performing a rescan If it is necessary to run a rescan, the ComplianceScan object is simply annotated with:\noc annotate compliancescans/\u0026lt;scan_name\u0026gt; compliance.openshift.io/rescan= If default-auto-apply is enabled, remediation which changes MachineConfigs will trigger a cluster reboot. "},{"uri":"https://blog.stderr.at/tags/block-devices/","title":"Block Devices","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2021/02/understanding-rwo-block-device-handling-in-openshift/","title":"Understanding RWO block device handling in OpenShift","tags":["Storage","OpenShift","OCP","Block devices"],"description":"A basic introduction into block device usage","content":" In this blog post we would like to explore OpenShift / Kubernetes block device handling. We try to answer the following questions:\nWhat happens if multiple pods try to access the same block device?\nWhat happens if we scale a deployment using block devices to more than one replica?\nAnd finally we want to give a short, high level overview about how the container storage interface (CSI) actually works.\nA block device provides Read-Write-Once (RWO) storage. This basically means a local file system mounted by a single node. Do not confuse this with a cluster (CephFS, GlusterFS) or network file system (NFS). These file systems provide Read-Write-Many (RWX) storage mountable on more than one node. Test setup For running our tests we need the following resources\nA new namespace/project for running our tests\nA persistent volume claim (PVC) to be mounted in our test pods\nTwo pods definitions for mounting the PVC\nStep 1: Creating a new namespace/project To run our test cases we created a new project with OpenShift\noc new-project blockdevices Step 2: Defining a block PVC Our cluster is running the rook operator (https://rook.io) and provides a ceph-block storage class for creating block devices:\n$ oc get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate false 4d14h Let’s take a look a the details of the storage class:\n$ oc get sc -o yaml ceph-block apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-block parameters: clusterID: rook-ceph csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph csi.storage.k8s.io/fstype: ext4 (1) csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph imageFeatures: layering imageFormat: \u0026#34;2\u0026#34; pool: blockpool provisioner: rook-ceph.rbd.csi.ceph.com reclaimPolicy: Delete volumeBindingMode: Immediate 1 So whenever we create a PVC using this storage class the Ceph provisioner will also create an EXT4 file system on the block device. To test block device handling we create the following persistent volume claim (PVC):\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: block-claim spec: accessModes: - ReadWriteOnce (1) resources: requests: storage: 1Gi storageClassName: ceph-block 1 The access mode is set to ReadWriteOnce (RWO), as block devices oc create -f pvc.yaml $ oc get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE block-claim Bound pvc-bd68be5d-c312-4c31-86a8-63a0c22de844 1Gi RWO ceph-block 91s To test our shiny new block device we are going to use the following three pod definitions:\nblock-pod-a apiVersion: v1 kind: Pod metadata: labels: run: block-pod-a name: block-pod-a spec: containers: - image: registry.redhat.io/ubi8/ubi:8.3 name: block-pod-a command: - sh - -c - \u0026#39;df -h /block \u0026amp;\u0026amp; findmnt /block \u0026amp;\u0026amp; sleep infinity\u0026#39; volumeMounts: - name: blockdevice mountPath: /block volumes: - name: blockdevice persistentVolumeClaim: claimName: block-claim block-pod-b apiVersion: v1 kind: Pod metadata: labels: run: block-pod-b name: block-pod-b spec: affinity: podAntiAffinity: (1) requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: run operator: In values: - block-pod-a topologyKey: kubernetes.io/hostname containers: - image: registry.redhat.io/ubi8/ubi:8.3 name: block-pod-b command: - sh - -c - \u0026#39;df -h /block \u0026amp;\u0026amp; findmnt /block \u0026amp;\u0026amp; sleep infinity\u0026#39; volumeMounts: - name: blockdevice mountPath: /block volumes: - name: blockdevice persistentVolumeClaim: claimName: block-claim 1 We use an AntiAffinity rule for making sure that block-pod-b runs on a different node than block-pod-a. block-pod-c apiVersion: v1 kind: Pod metadata: labels: run: block-pod-c name: block-pod-c spec: affinity: podAffinity: (1) preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: run operator: In values: - block-pod-a topologyKey: kubernetes.io/hostname containers: - image: registry.redhat.io/ubi8/ubi:8.3 name: block-pod-c command: - sh - -c - \u0026#39;df -h /block \u0026amp;\u0026amp; findmnt /block \u0026amp;\u0026amp; sleep infinity\u0026#39; volumeMounts: - name: blockdevice mountPath: /block volumes: - name: blockdevice persistentVolumeClaim: claimName: block-claim 1 We use an Affinity rule for making sure that block-pod-c runs on the same node as block-pod-a. In our first test we want to make sure that both pods are running on separate cluster nodes. So we create block-pod-a and block-pod-b:\n$ oc create -f block-pod-a.yml $ oc create -f block-pod-b.yml After a few seconds we can check the state of our pods:\n$ oc get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES block-pod-a 1/1 Running 0 46s 10.130.6.4 infra02.lan.stderr.at \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; block-pod-b 0/1 ContainerCreating 0 16s \u0026lt;none\u0026gt; infra01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Hm, block-pod-b is in the state ContainerCreating, let’s check the events. Also note that it is running on another node (infra01) then block-pod-a (infra02).\n10s Warning FailedAttachVolume pod/block-pod-b Multi-Attach error for volume \u0026#34;pvc-bd68be5d-c312-4c31-86a8-63a0c22de844\u0026#34; Volume is already used by pod(s) block-pod-a Ah, so because of our block device with RWO access mode and block-pod-b running on separate cluster node, OpenShift or K8s can’t attach the volume to our block-pod-b.\nBut let’s try another test and let’s create a third pod block-pod-c that should run on the same node as block-pod-a:\n$ oc create -f block-pod-c.yml Now let’s check the status of block-pod-c:\n$ oc get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES block-pod-a 1/1 Running 0 6m49s 10.130.6.4 infra02.lan.stderr.at \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; block-pod-b 0/1 ContainerCreating 0 6m19s \u0026lt;none\u0026gt; infra01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; block-pod-c 1/1 Running 0 14s 10.130.6.5 infra02.lan.stderr.at \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Oh, block-pod-c is running on node infra02 and mounted the RWO volume. Let’s check the events for block-pod-c:\n3m6s Normal Scheduled pod/block-pod-c Successfully assigned blockdevices/block-pod-c to infra02.lan.stderr.at 2m54s Normal AddedInterface pod/block-pod-c Add eth0 [10.130.6.5/23] 2m54s Normal Pulled pod/block-pod-c Container image \u0026#34;registry.redhat.io/ubi8/ubi:8.3\u0026#34; already present on machine 2m54s Normal Created pod/block-pod-c Created container block-pod-c 2m54s Normal Started pod/block-pod-c Started container block-pod-c When we compare this with the events for block-pod-a:\n9m41s Normal Scheduled pod/block-pod-a Successfully assigned blockdevices/block-pod-a to infra02.lan.stderr.at 9m41s Normal SuccessfulAttachVolume pod/block-pod-a AttachVolume.Attach succeeded for volume \u0026#34;pvc-bd68be5d-c312-4c31-86a8-63a0c22de844\u0026#34; 9m34s Normal AddedInterface pod/block-pod-a Add eth0 [10.130.6.4/23] 9m34s Normal Pulled pod/block-pod-a Container image \u0026#34;registry.access.redhat.com/ubi8/ubi:8.3\u0026#34; already present on machine 9m34s Normal Created pod/block-pod-a Created container block-pod-a 9m34s Normal Started pod/block-pod-a Started container block-pod-a So the AttachVolume.Attach message is missing in the events for block-pod-c. Because the volume is already attached to the node, interesting.\nEven with RWO block device volumes it is possible to use the same volume in multiple pods if the pods a running on the same node. I was not aware of this possibility and always had the believe with an RWO block device only one pod can access the volume. That’s the problem with believing :-)\nThanks or reading this far.\n"},{"uri":"https://blog.stderr.at/tags/kubernetes/","title":"Kubernetes","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2021/01/writing-operator-using-ansible/","title":"Writing Operator using Ansible","tags":["Operator","OpenShift","OCP","Kubernetes"],"description":"Example of an Kubernetes Operator based on Ansible","content":" This quick post shall explain, without any fancy details, how to write an Operator based on Ansible. It is assumed that you know what purpose an Operator has.\nAs a short summary: Operators are a way to create custom controllers in OpenShift or Kubernetes. It watches for custom resource objects and creates the application based on the parameters in such custom resource object. Often written in Go, the SDK supports Ansible, Helm and (new) Java as well.\nIn this example we will install Gogs, a painless self-hosted Git services.\nAs general prerequisites we have:\nInstalled OpenShift 4.6+ cluster (could be Minicube)\nPossibility to execute Ansible scripts and oc/kubectl commands\nCommands: make, docker (or podman)\nInstall Operator SDK As explained at https://sdk.operatorframework.io/docs/installation/ the following prerequisites must be met prior installing the SDK at least:\nDocker v17.03+ or podman v1.9.3+ or buildah v1.7+\nOpenShift CLU v4.6+\nKubernetes/OpenShift cluster\nAccess to container registry, for example quay.io\nOptional: Go v1.13+ (for Operators based on Golang)\nAnsible v2.9.0+\nFollowing the instructions of the SDK documentation, the operator-sdk command will be installed.\nCreating the Operator To begin we create a new folder for the Operator and initialize the Operator project.\nmkdir gogs-operator cd gogs-operator operator-sdk init --plugins=ansible --domain=example.com.at operator-sdk create api --group gogs --version=v1alpha1 --kind Gogs --generate-playbook This will create a new project structure with the following parameters:\n--plugin: Type of Operator (Ansible or Helm)\n--domain: Defines the api endpoint together with group and version.\n--group: Usually short product name\n--version: Defines version of API endpoint\nThe folder structure which will be created automatically looks as follows:\n. |-- Dockerfile |-- Makefile |-- PROJECT |-- config | |-- crd | | |-- bases | | | |-- gogs.example.com.at_gogs.yaml | | |-- kustomization.yaml | |-- default | | |-- kustomization.yaml | | |-- manager_auth_proxy_patch.yaml | |-- manager | | |-- kustomization.yaml | | |-- manager.yaml | |-- prometheus | | |-- kustomization.yaml | | |-- monitor.yaml | |-- rbac | | |-- auth_proxy_client_clusterrole.yaml | | |-- auth_proxy_role.yaml | | |-- auth_proxy_role_binding.yaml | | |-- auth_proxy_service.yaml | | |-- gogs_editor_role.yaml | | |-- gogs_viewer_role.yaml | | |-- kustomization.yaml | | |-- leader_election_role.yaml | | |-- leader_election_role_binding.yaml | | |-- role.yaml | | |-- role_binding.yaml | |-- samples | | |-- gogs_v1alpha1_gogs.yaml | | |-- kustomization.yaml | |-- scorecard | | |-- bases | | | |-- config.yaml | | |-- kustomization.yaml | | |-- patches | | |-- basic.config.yaml | | |-- olm.config.yaml | |-- testing | |-- debug_logs_patch.yaml | |-- kustomization.yaml | |-- manager_image.yaml | |-- pull_policy | |-- Always.yaml | |-- IfNotPresent.yaml | |-- Never.yaml |-- molecule | |-- default | | |-- converge.yml | | |-- create.yml | | |-- destroy.yml | | |-- kustomize.yml | | |-- molecule.yml | | |-- prepare.yml | | |-- tasks | | | |-- gogs_test.yml | | |-- verify.yml | |-- kind | |-- converge.yml | |-- create.yml | |-- destroy.yml | |-- molecule.yml |-- playbooks | |-- gogs.yml |-- requirements.yml |-- roles |-- watches.yaml The watches.yaml file maps Custom Resources (identified by Group, Version, and Kind [GVK]) to Ansible Roles and Playbooks. It tells the Operator where to find the actual Ansible playbook.\n--- # Use the \u0026#39;create api\u0026#39; subcommand to add watches to this file. - version: v1alpha1 group: gogs.example.com.at kind: Gogs playbook: playbooks/gogs.yml # +kubebuilder:scaffold:watch Other files, especially inside playbooks and roles are created as placeholders. These files (or folders) are waiting for you to add the Ansible logic.\nDefining Roles and Playbook With the folder structure above, a playbook and different roles can be created in order to tell the Operator what it needs to do.\nSince the Operator will constantly watch for changes, all tasks must be idempotent In our example we will try to install Gogs, a Git service. It contains a Postgres database system and a webservice. To use some example roles and not fully start from scratch let’s clone the following repository and copy the folders to our Operator.\ncd .. https://github.com/tjungbauer/ansible-operator-roles cd gogs-operator # Remove placeholder rm -Rf roles/ # Copy Postgres deployment role cp -R ../ansible-operator-roles/roles/postgresql-ocp ./roles # Copy Gogs Deplyoment role cp -R ../ansible-operator-roles/roles/gogs-ocp ./roles When we examine the folder, we see 2 typical Ansible roles. The simple purpose is, to create all required OpenShift objects, like Deployment, Route, Service and so on, fully automated by the Operator.\n|-- playbooks | |-- gogs.yaml |-- roles |-- gogs-ocp | |-- README.adoc | |-- defaults | | |-- main.yml | |-- meta | | |-- main.yml | |-- tasks | | |-- main.yml | |-- templates | |-- config_map.j2 | |-- deployment.j2 | |-- persistent_volume_claim.j2 | |-- route.j2 | |-- service.j2 | |-- service_account.j2 |-- postgresql-ocp |-- README.adoc |-- defaults | |-- main.yml |-- meta | |-- main.yml |-- tasks | |-- main.yml |-- templates |-- deployment.j2 |-- persistent_volume_claim.j2 |-- secret.j2 |-- service.j2 Copy (or create) the following playbook under playbooks/gogs.yaml. As you can see there are 2 tasks: the first one will create the postgres application, the seconds one the Gogs service.\n--- # Persistent Gogs deployment playbook. # # The Playbook expects the following variables to be set in the CR: # (Note that Camel case gets converted by the ansible-operator to Snake case) # - PostgresqlVolumeSize # - GogsVolumeSize # - GogsSSL # The following variables come from the ansible-operator # - ansible_operator_meta.namespace # - ansible_operator_meta.name (from the name of the CR) - hosts: localhost gather_facts: no tasks: - name: Set up PostgreSQL include_role: name: ../roles/postgresql-ocp (1) vars: (2) _postgresql_namespace: \u0026#34;{{ ansible_operator_meta.namespace }}\u0026#34; _postgresql_name: \u0026#34;postgresql-gogs-{{ ansible_operator_meta.name }}\u0026#34; _postgresql_database_name: \u0026#34;gogsdb\u0026#34; _postgresql_user: \u0026#34;gogsuser\u0026#34; _postgresql_password: \u0026#34;gogspassword\u0026#34; _postgresql_volume_size: \u0026#34;{{ postgresql_volume_size|d(\u0026#39;4Gi\u0026#39;) }}\u0026#34; _postgresql_image: \u0026#34;{{ postgresql_image|d(\u0026#39;registry.redhat.io/rhscl/postgresql-10-rhel7\u0026#39;) }}\u0026#34; _postgresql_image_tag: \u0026#34;{{ postgresql_image_tag|d(\u0026#39;latest\u0026#39;) }}\u0026#34; _postgresql_size: 1 - name: Set Gogs Service name to default value set_fact: gogs_service_name: \u0026#34;gogs-{{ ansible_operator_meta.name }}\u0026#34; when: gogs_service_name is not defined - name: Set up Gogs include_role: name: ../roles/gogs-ocp (3) vars: (4) _gogs_namespace: \u0026#34;{{ ansible_operator_meta.namespace }}\u0026#34; _gogs_name: \u0026#34;{{ gogs_service_name }}\u0026#34; _gogs_ssl: \u0026#34;{{ gogs_ssl|d(False)|bool }}\u0026#34; _gogs_route: \u0026#34;{{ gogs_route | d(\u0026#39;\u0026#39;) }}\u0026#34; _gogs_image_tag: \u0026#34;{{ gogs_image_tag | d(\u0026#39;latest\u0026#39;) }}\u0026#34; _gogs_volume_size: \u0026#34;{{ gogs_volume_size|d(\u0026#39;4Gi\u0026#39;) }}\u0026#34; _gogs_postgresql_service_name: \u0026#34;postgresql-gogs-{{ ansible_operator_meta.name }}\u0026#34; _gogs_postgresql_database_name: gogsdb _gogs_postgresql_user: gogsuser _gogs_postgresql_password: gogspassword _gogs_size: 1 1 Path to Postgres Role 2 Parameters for Postgres service 3 Path to Gogs Role 4 Parameters for Gogs service Operator Permissions The Operator will require correct permissions in order to create objects like Routes or Services in OpenShift. The SDK automatically created a default role.yaml which can be modified. Open the file config/rbac/role.yaml and add permissions for:\nfor apiGroups \u0026#34;\u0026#34;\nservices\nroutes\nperistentvlumeclaims\nserviceaccounts\nconfigmaps\nfor apiGroups: route.operanshift.io the resource routes\nAt the end, the role.yaml should look like this:\n--- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: manager-role rules: ## ## Base operator rules ## - apiGroups: - \u0026#34;\u0026#34; resources: - secrets - pods - pods/exec - pods/log - services - routes - configmaps - persistentvolumeclaims - serviceaccounts verbs: - create - delete - get - list - patch - update - watch - apiGroups: - apps resources: - deployments - daemonsets - replicasets - statefulsets verbs: - create - delete - get - list - patch - update - watch ## ## Rules for gogs.example.com.at/v1alpha1, Kind: Gogs ## - apiGroups: - gogs.example.com.at resources: - gogs - gogs/status - gogs/finalizers verbs: - create - delete - get - list - patch - update - watch - apiGroups: - route.openshift.io resources: - routes verbs: - create - update - delete - get - list - watch - patch - apiGroups: - route.openshift.io resources: - routes verbs: - create - update - delete - get - list - watch - patch Building and Deploy the Operator Now it is time to build the Operator and push it to a repository. In this example a repository was created at quay.io and is called gogs-operator. The SDK will automatically create a Makefile during the initialization, which we will use now.\nThe Makefile is prepared for docker. If you use podman some modifications must be done first. Run the command sed -i \u0026#39;s/docker/podman/g\u0026#39; Makefile to replace all docker commands inside the Makefile. The next commands will build, push, install and deploy the Operator. Before we start we must be logged in to you Registry of choice (i.e. docker login …​) as well as into our OpenShift cluster. Moreover, it is required that the IMG environment variable is exported with the correct value.\nBuild the Operator and push into the registry\n# export IMG, be sure that the correct tag is used export IMG=quay.io/tjungbau/gogs-operator:v1.0.0 # Build and push into registry make podman-build podman-push podman build . -t quay.io/tjungbau/gogs-operator:v1.0.0 STEP 1: FROM quay.io/operator-framework/ansible-operator:v1.3.0 STEP 2: COPY requirements.yml ${HOME}/requirements.yml --\u0026gt; Using cache 4f84e7064b066c2cac5179b56490a0ef85591170c501ec8a480b617d6e91cff3 STEP 3: RUN ansible-galaxy collection install -r ${HOME}/requirements.yml \u0026amp;\u0026amp; chmod -R ug+rwx ${HOME}/.ansible --\u0026gt; Using cache 2a3a5d44451a45a4c38e1c314e8887c6c45f2551cbef87ef0d1ce518c1969c0d STEP 4: COPY watches.yaml ${HOME}/watches.yaml --\u0026gt; Using cache 642f8361a7b358b89d2e4e5211c1c7a1e22488c53bba0bf1ba2ba275fd56ee69 STEP 5: COPY roles/ ${HOME}/roles/ --\u0026gt; Using cache 93c1af8782bad84d8b81d2d2294c405caab70e2d01c232440f7eb8e5001746c1 STEP 6: COPY playbooks/ ${HOME}/playbooks/ --\u0026gt; Using cache 1cdeee1456ac67d70d4233b0f9ed8052465aaa2cded6bd8ae962dfcc848e5b92 STEP 7: COMMIT quay.io/tjungbau/gogs-operator:v1.0.0 --\u0026gt; 1cdeee1456a 1cdeee1456ac67d70d4233b0f9ed8052465aaa2cded6bd8ae962dfcc848e5b92 podman push quay.io/tjungbau/gogs-operator:v1.0.0 Getting image source signatures Copying blob d5ca8c3b3d34 skipped: already exists Copying blob 4b036ae478b7 skipped: already exists Copying blob 5cfcd0621ffc skipped: already exists Copying blob c6f3d1432bd0 skipped: already exists Copying blob 92538e92de29 skipped: already exists Copying blob eb7bf34352ca skipped: already exists Copying blob 80c43a11288f done Copying blob 803eb2035c9a done Copying blob 40d943ae1834 done Copying blob f4d9024614ee done Copying blob 5143a36c6002 done Copying blob 5050e1080446 skipped: already exists Copying config 1cdeee1456 done Writing manifest to image destination Copying config 1cdeee1456 [--------------------------------------] 0.0b / 6.2KiB Writing manifest to image destination Writing manifest to image destination Storing signatures Install the CRD into OpenShift\n# Install the custom resource definition make install /root/projects/gogs-operator/bin/kustomize build config/crd | kubectl apply -f - customresourcedefinition.apiextensions.k8s.io/gogs.gogs.example.com.at created Deploy the Operator and all required objects into OpenShift\n# Deploy the Operator into OpenShift make deploy cd config/manager \u0026amp;\u0026amp; /root/projects/gogs-operator/bin/kustomize edit set image controller=quay.io/tjungbau/gogs-operator:v1.0.0 /root/projects/gogs-operator/bin/kustomize build config/default | kubectl apply -f - namespace/gogs-operator-system created customresourcedefinition.apiextensions.k8s.io/gogs.gogs.example.com.at unchanged role.rbac.authorization.k8s.io/gogs-operator-leader-election-role created clusterrole.rbac.authorization.k8s.io/gogs-operator-manager-role created clusterrole.rbac.authorization.k8s.io/gogs-operator-metrics-reader created clusterrole.rbac.authorization.k8s.io/gogs-operator-proxy-role created rolebinding.rbac.authorization.k8s.io/gogs-operator-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-proxy-rolebinding created service/gogs-operator-controller-manager-metrics-service created deployment.apps/gogs-operator-controller-manager created This will create a new project in OpenShift called gogs-operator-system. Here, the Operator is running and waiting that somebody creates a CRD of the kind Gogs. Once this happens the Operator will execute the playbooks and therefore create a Postgres and a Gogs pod.\n# Operator Namespace oc get pods -n gogs-operator-system NAME READY STATUS RESTARTS AGE gogs-operator-controller-manager-6747bb6c6-s8794 2/2 Running 0 6m8s Using the Operator Now we need to create a CRD of the kind Gogs. This will happen in a new project, where the Gogs service shall be hosted.\nCreate a new OpenShift project\noc new-project gogs Verify the sample resource\ncat config/samples/gogs_v1alpha1_gogs.yaml apiVersion: gogs.example.com.at/v1alpha1 kind: Gogs metadata: name: gogs-sample spec: foo: bar Apply the sample resource\noc apply -f config/samples/gogs_v1alpha1_gogs.yaml -n gogs This will create two services:\npostgresql\nGogs\nThe Operator will be responsible to roll out all required objects. This includes the Deployments for the container, the Openshift service and the route.\noc get all -n gogs NAME READY STATUS RESTARTS AGE pod/gogs-gogs-sample-57778fd76-ghg8j 1/1 Running 0 74s pod/postgresql-gogs-gogs-sample-bbc49b794-mnltb 1/1 Running 0 115s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/gogs-gogs-sample ClusterIP 172.30.47.31 \u0026lt;none\u0026gt; 3000/TCP 80s service/postgresql-gogs-gogs-sample ClusterIP 172.30.47.158 \u0026lt;none\u0026gt; 5432/TCP 117s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/gogs-gogs-sample 1/1 1 1 74s deployment.apps/postgresql-gogs-gogs-sample 1/1 1 1 115s NAME DESIRED CURRENT READY AGE replicaset.apps/gogs-gogs-sample-57778fd76 1 1 1 74s replicaset.apps/postgresql-gogs-gogs-sample-bbc49b794 1 1 1 115s NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/gogs-gogs-sample gogs-gogs-sample-gogs.apps.ocp.ispworld.at gogs-gogs-sample \u0026lt;all\u0026gt; None At the end, all Pods are alive, ready and are fully controlled by the Operator. We can access the Gogs web interface via the route and start using our own Git service.\nUpdating Operator While the Operator is running fine now, at some point you might want to do some changes. For example, let’s run the Gog service with a replica of 3.\nPerform the following actions:\nSet the variable _gogs_size to 3 in playbooks/gogs.yml\nBuild and push the new version\nexport IMG=quay.io/tjungbau/gogs-operator:v1.0.8 make podman-build podman-push podman build . -t quay.io/tjungbau/gogs-operator:v1.0.8 STEP 1: FROM quay.io/operator-framework/ansible-operator:v1.3.0 STEP 2: COPY requirements.yml ${HOME}/requirements.yml --\u0026gt; Using cache 4f84e7064b066c2cac5179b56490a0ef85591170c501ec8a480b617d6e91cff3 STEP 3: RUN ansible-galaxy collection install -r ${HOME}/requirements.yml \u0026amp;\u0026amp; chmod -R ug+rwx ${HOME}/.ansible --\u0026gt; Using cache 2a3a5d44451a45a4c38e1c314e8887c6c45f2551cbef87ef0d1ce518c1969c0d STEP 4: COPY watches.yaml ${HOME}/watches.yaml --\u0026gt; Using cache 642f8361a7b358b89d2e4e5211c1c7a1e22488c53bba0bf1ba2ba275fd56ee69 STEP 5: COPY roles/ ${HOME}/roles/ --\u0026gt; Using cache 55785493e215d933ef7a93fe000afa6fbb088d87eeffcdddeea4e7fd1896f5b5 STEP 6: COPY playbooks/ ${HOME}/playbooks/ STEP 7: COMMIT quay.io/tjungbau/gogs-operator:v1.0.8 --\u0026gt; bb9d6a995d0 bb9d6a995d059eab7758f9ac17d3ce12f8759518e231f77d32a4b820e4b14396 podman push quay.io/tjungbau/gogs-operator:v1.0.8 Getting image source signatures Copying blob 5cfcd0621ffc skipped: already exists Copying blob d5ca8c3b3d34 skipped: already exists Copying blob eb7bf34352ca skipped: already exists Copying blob 4b036ae478b7 skipped: already exists Copying blob c6f3d1432bd0 skipped: already exists Copying blob 92538e92de29 skipped: already exists Copying blob 41e53e538a36 done Copying blob 5050e1080446 skipped: already exists Copying blob 40d943ae1834 skipped: already exists Copying blob 803eb2035c9a skipped: already exists Copying blob 80c43a11288f skipped: already exists Copying blob ee0361a14e3b skipped: already exists Copying config bb9d6a995d done Writing manifest to image destination Copying config bb9d6a995d [--------------------------------------] 0.0b / 6.2KiB Writing manifest to image destination Writing manifest to image destination Storing signatures Deploy the new version\nmake deploy cd config/manager \u0026amp;\u0026amp; /root/projects/gogs-operator/bin/kustomize edit set image controller=quay.io/tjungbau/gogs-operator:v1.0.8 /root/projects/gogs-operator/bin/kustomize build config/default | kubectl apply -f - namespace/gogs-operator-system unchanged customresourcedefinition.apiextensions.k8s.io/gogs.gogs.example.com.at unchanged role.rbac.authorization.k8s.io/gogs-operator-leader-election-role unchanged clusterrole.rbac.authorization.k8s.io/gogs-operator-manager-role unchanged clusterrole.rbac.authorization.k8s.io/gogs-operator-metrics-reader unchanged clusterrole.rbac.authorization.k8s.io/gogs-operator-proxy-role unchanged rolebinding.rbac.authorization.k8s.io/gogs-operator-leader-election-rolebinding unchanged clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-manager-rolebinding unchanged clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-proxy-rolebinding unchanged service/gogs-operator-controller-manager-metrics-service unchanged deployment.apps/gogs-operator-controller-manager configured The Operator will restart with a new version. After a while the changes will take affect and 3 Gogs pods will run.\noc get pods -n gogs NAME READY STATUS RESTARTS AGE gogs-gogs-sample-57778fd76-4m98m 1/1 Running 0 12m gogs-gogs-sample-57778fd76-5hrdn 1/1 Running 0 6m23s gogs-gogs-sample-57778fd76-xgh2f 1/1 Running 0 6m24s postgresql-gogs-gogs-sample-bbc49b794-z84wt 1/1 Running 0 13m What Else? - References Above example is a very quick overview about what can be done. There are many other options. You can create Operators using Go or Helm.\nThe best starting points are the following websites:\nCertified Operator Build Guide\nOperator SDK Documentation\n"},{"uri":"https://blog.stderr.at/tags/grafana/","title":"Grafana","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/thanos/","title":"Thanos","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2020/12/thanos-querier-vs-thanos-querier/","title":"Thanos Querier vs Thanos Querier","tags":["Grafana","Thanos","OpenShift","OCP"],"description":"How to leverage different Thanos Querier services.","content":" OpenShift comes per default with a static Grafana dashboard, which will present cluster metrics to cluster administrators. It is not possible to customize this Grafana instance.\nHowever, many customers would like to create their own dashboards, their own monitoring and their own alerting while leveraging the possibilities of OpenShift at the same time and without installing a completely separated monitoring stack.\nSo how can you create your own queries? How can you visualize them on custom dashboards, without the need to install Prometheus or Alertmanager a second time?\nThe solution is simple: Since OpenShift 4.5 (as TechPreview) and since OpenShift 4.6 (as GA) the default monitoring stack of OpenShift has been extended to support monitoring of user-defined projects. This additional configuration will help to observe your own projects.\nIn this article we will see how to deploy the Grafana operator and what the possible issues can occur, when simply connecting Grafana to the OpenShift monitoring.\nOverview As a developer in OpenShift, you can create an application which provides your custom statistics of your application at the endpoint /metrics. Here the example from the official OpenShift documentation:\n# HELP http_requests_total Count of all HTTP requests # TYPE http_requests_total counter http_requests_total{code=\u0026#34;200\u0026#34;,method=\u0026#34;get\u0026#34;} 4 http_requests_total{code=\u0026#34;404\u0026#34;,method=\u0026#34;get\u0026#34;} 2 # HELP version Version information about this binary # TYPE version gauge version{version=\u0026#34;v0.1.0\u0026#34;} 1 This metric can then be viewed inside OpenShift in the developer view under the menu Monitoring. If you go to \u0026#34;Monitoring \u0026gt; Metrics\u0026#34; and select \u0026#34;Custom Query\u0026#34; from the drop down, you can enter, for example, the following PromQL query:\nsum(rate(http_requests_total[2m])) The following graph will be the result:\nFigure 1. Custom Query This is great! But …​ what happens if a customer would like to see his very own super fancy Grafana dashboard? You cannot change the cluster dashboard. However, you can install your own Grafana instance and one way to do so is using the Custom Grafana Operator.\nBefore we begin Before we start the following should be prepared already:\nOpenShift 4.5+\nEnabled user-define workload monitoring\nA project with user-defined workload monitoring. This is explained in the official documentation at https://docs.openshift.com/container-platform/4.6/monitoring/enabling-monitoring-for-user-defined-projects.html.\nDuring this blog, we will use the namespace ns1 with a custom metric\nDeploy Custom Grafana Operator As for any community operator the following must be considered:\nCommunity Operators are operators which have not been vetted or verified by Red Hat. Community Operators should be used with caution because their stability is unknown. Red Hat provides no support for Community Operators. The community Grafana operator must be deployed to its own namespace, for example grafana. Create this namespace first (oc new-project grafana) and search and intall the Grafana Operator from the OperatorHub. You can use the default values, just be sure to select the wanted namespace.\nAfter a few minutes, the operator should be available:\nFigure 2. Installed Community Grafana Operator Setup Grafana Operator Before we can use Grafana to draw beautiful images it must be configured. We need to create an instance of Grafana. Ideally, OpenShift OAuth is already leveraged, to avoid the need to creating user account manually, inside Grafana.\nOAuth requires some objects, which must be created before the actual Grafana instance. The following YAMLs are taken from the operator documentation. Create the following inside the Grafana namespace:\nSession secret for the proxy …​ change the password!!\na cluster role grafana-proxy\na cluster role binding for the role\na config map injecting trusted CA bundles\napiVersion: v1 data: session_secret: Y2hhbmdlIG1lCg== kind: Secret metadata: name: grafana-k8s-proxy type: Opaque --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: grafana-proxy rules: - apiGroups: - authentication.k8s.io resources: - tokenreviews verbs: - create - apiGroups: - authorization.k8s.io resources: - subjectaccessreviews verbs: - create --- apiVersion: authorization.openshift.io/v1 kind: ClusterRoleBinding metadata: name: grafana-proxy roleRef: name: grafana-proxy subjects: - kind: ServiceAccount name: grafana-serviceaccount namespace: grafana userNames: - system:serviceaccount:grafana:grafana-serviceaccount --- apiVersion: v1 kind: ConfigMap metadata: labels: config.openshift.io/inject-trusted-cabundle: \u0026#34;true\u0026#34; name: ocp-injected-certs Now you can create the following instance under: \u0026#34;Installed Operators \u0026gt; Grafana Operator \u0026gt; Grafana \u0026gt; Create Grafana \u0026gt; YAML View\u0026#34; (or, as an alternative, via the CLI)\napiVersion: integreatly.org/v1alpha1 kind: Grafana metadata: name: grafana-oauth namespace: grafana spec: config: (1) auth: disable_login_form: false disable_signout_menu: true auth.anonymous: enabled: false auth.basic: enabled: true log: level: warn mode: console security: (2) admin_password: secret admin_user: root secrets: - grafana-k8s-tls - grafana-k8s-proxy client: preferService: true dataStorage: (3) accessModes: - ReadWriteOnce class: managed-nfs-storage size: 10Gi containers: (4) - args: - \u0026#39;-provider=openshift\u0026#39; - \u0026#39;-pass-basic-auth=false\u0026#39; - \u0026#39;-https-address=:9091\u0026#39; - \u0026#39;-http-address=\u0026#39; - \u0026#39;-email-domain=*\u0026#39; - \u0026#39;-upstream=http://localhost:3000\u0026#39; - \u0026#39;-tls-cert=/etc/tls/private/tls.crt\u0026#39; - \u0026#39;-tls-key=/etc/tls/private/tls.key\u0026#39; - \u0026gt;- -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token - \u0026#39;-cookie-secret-file=/etc/proxy/secrets/session_secret\u0026#39; - \u0026#39;-openshift-service-account=grafana-serviceaccount\u0026#39; - \u0026#39;-openshift-ca=/etc/pki/tls/cert.pem\u0026#39; - \u0026#39;-openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\u0026#39; - \u0026#39;-openshift-ca=/etc/grafana-configmaps/ocp-injected-certs/ca-bundle.crt\u0026#39; - \u0026#39;-skip-auth-regex=^/metrics\u0026#39; - \u0026gt;- -openshift-sar={\u0026#34;namespace\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;verb\u0026#34;: \u0026#34;get\u0026#34;} image: \u0026#39;quay.io/openshift/origin-oauth-proxy:4.8\u0026#39; name: grafana-proxy ports: - containerPort: 9091 name: grafana-proxy resources: {} volumeMounts: - mountPath: /etc/tls/private name: secret-grafana-k8s-tls readOnly: false - mountPath: /etc/proxy/secrets name: secret-grafana-k8s-proxy readOnly: false ingress: enabled: true targetPort: grafana-proxy termination: reencrypt service: annotations: service.alpha.openshift.io/serving-cert-secret-name: grafana-k8s-tls ports: - name: grafana-proxy port: 9091 protocol: TCP targetPort: grafana-proxy serviceAccount: annotations: serviceaccounts.openshift.io/oauth-redirectreference.primary: \u0026gt;- {\u0026#34;kind\u0026#34;:\u0026#34;OAuthRedirectReference\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;reference\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;Route\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;grafana-route\u0026#34;}} configMaps: - ocp-injected-certs dashboardLabelSelector: - matchExpressions: - key: app operator: In values: - grafana 1 Some default settings, which can be modified if required 2 A default administrative user 3 A datastore to use a persistent volume. Other options would be to use ephemeral storage, or another database. This might be especially important, if you would like HA for your Grafana. 4 Container arguments, most important the openshift-sar line which is important for the OAuth After a few moments, the operator will pick up the change and creates a Grafana pod.\nAdding a Data Source The next step is to connect your custom Grafana to Prometheus, or actually to the Thanos Querier. To do so, you will need to add a role to the Grafana service account and to create a CRD GrafanaDataSource.\nAt this moment, we will work with the cluster role cluster-monitoring-view. The problem this might bring is discussed later.\nAdd the role to the Grafana serviceaccount\noc adm policy add-cluster-role-to-user cluster-monitoring-view -z grafana-serviceaccount Retrieve the token of the service account\noc serviceaccounts get-token grafana-serviceaccount -n grafana Create the following Grafana Data Source, either via UI or via CLI. Be sure to change \u0026lt;TOKEN\u0026gt; with the token from step #2.\napiVersion: integreatly.org/v1alpha1 kind: GrafanaDataSource metadata: name: prometheus-grafanadatasource namespace: grafana spec: datasources: - access: proxy editable: true isDefault: true jsonData: httpHeaderName1: Authorization timeInterval: 5s tlsSkipVerify: true name: Prometheus secureJsonData: httpHeaderValue1: \u0026gt;- Bearer \u0026lt;TOKEN\u0026gt; (1) type: prometheus url: \u0026#39;https://thanos-querier.openshift-monitoring.svc.cluster.local:9091\u0026#39; (2) name: prometheus-grafanadatasource.yaml 1 enter token from step #2 2 Thanos default querier URL…​. this might cause problems (see below) The operator will now restart the Grafana pod to add the newest changes, which should not take more than a few seconds. Grafana can be used now. Dashboards can be created …​ but lets run some tests with PromQL queries instead.\nLet’s Test Log in to your Grafana using OAuth and a cluster administrator.\nYou could also use a non cluster administrator, if the user is able to GET the services of the Grafana namespace. The reason is the following line in the Grafana CRD: -openshift-sar={\u0026#34;namespace\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;services\u0026#34;,\u0026#34;verb\u0026#34;: \u0026#34;get\u0026#34;} which defines, that OAuth will work for everybody who can get the service. This might be changed according to personal needs, but for this test it is good enough. Then use the credentials for the admin account, which have been defined while creating the Grafana instance.\nYou will be logged in now and since there are no Dashboards, lets go to Explore to enter some custom PromQL queries, for instance our example from above:\nsum(rate(http_requests_total[2m])) Figure 3. First Query This is looking good.\nLet’s give it another try and sort by namespaces.\nsum(rate(http_requests_total[2m])) by (namespace) Figure 4. Second Query - showing internal namespace What is this? I see a namespace which is actually meant for the cluster (openshift-monitoring).\nLet’s try another query using a different metric:\nsum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate) by (namespace) Figure 5. Third Query - shows even more namespaces Ok, so we have access to all namespaces on the cluster.\nWhy do I see all namespaces? What does this mean? Well, it means that we have access to all namespaces of the cluster. We see everything. This makes sense, since we assign the cluster role \u0026#34;cluster-monitoring-view\u0026#34; to the serviceaccount of Grafana. But what if we want to show only objects from a specific namespace? If we want, for example, give the developers the possibility to create their own dashboards, without having view access to the whole cluster.\nThe first test might be to remove the cluster-monitoring-view privileges from the Grafana serviceaccount. This will lead to an error on Grafana itself, since it cannot access the Thanos Querier, which we configured with: https://thanos-querier.openshift-monitoring.svc.cluster.local:9091\nHow does the Openshift WebUI actually work, when you are a developer and would like to search one of the above queries. Let’s try that:\nFigure 6. Query using the OpenShift UI It works! It shows the namespace of the developer and only this namespace. When you inspect the actual network traffic, you will see that OpenShift automatically adds the URL parameter namespace=ns1 to the request URL:\nhttps://your-cluster/api/prometheus-tenancy/api/v1/query?namespace=ns1\u0026amp;query=sum%28node_namespace_pod_container%3Acontainer_cpu_usage_seconds_total%3Asum_rate%29+by+%28namespace%29 This is good information, let’s try this using the Grafana Data Source.\nIt is currently not possible to perform this configuration using the GrafanaDataSource CRD. Instead, it must be done directly at the Grafana Dashboard configuration. There is an open ticket at: https://github.com/integr8ly/grafana-operator/issues/309 Login to Grafana as administrator and switch to \u0026#34;Configuration \u0026gt; Data Source \u0026gt; Prometheus \u0026gt;\u0026#34;. At the very bottom add namespace=ns1 to the Custom query parameters\nFigure 7. Configure Grafana Data Source At this point the Grafana serviceaccount has cluster_monitoring_view privileges. As you can see in the following image, this configuration did not help.\nFigure 8. Query after Data Source has manually been modified Thanos Querier vs. Thanos Querier To summarize, in the OpenShift UI everything works, but when using the Grafana dashboard, we see all namespaces from the cluster. Let’s try to find out how OpenShift does this.\nWhen we check the Thanos services we will see 3 ports:\nports: - name: web protocol: TCP port: 9091 targetPort: web - name: tenancy protocol: TCP port: 9092 targetPort: tenancy - name: tenancy-rules protocol: TCP port: 9093 targetPort: tenancy-rules Currently we configured port 9091, but there is another one, which is called tenancy, maybe this is what we need? Let’s try it:\nChange the CRD GrafanaDataSource to use port 9092 (instead of 9091). This will restart the pod and remove the custom query parameter we configured earlier.\nRemove the cluster-role\noc adm policy remove-cluster-role-from-user cluster-monitoring-view -z grafana-serviceaccount The serviceaccount of Grafana, must be able to view the project we want to show in the dashboards. Therefore, allow the Grafana serviceaccount to view the project ns1:\noc adm policy add-role-to-user view system:serviceaccount:grafana:grafana-serviceaccount -n ns1 Log into Grafana as administrator and manually change the Data Source and add namespace=ns1 to the setting Custom query parameters\nRerun the Query …​ as you see you will now see one namespace only.\nFigure 9. Query with Thanos Querier on port 9092 What happened? So what actually happened here? We have two ports for our Thanos Querier which are important: 9091 and 9092.\nWhen we check the Deployment of the Thanos Querier for these ports we will see:\nFor the port 9091 it looks like the following:\nspec: [...] containers: [...] - resources: [...] ports: - name: web containerPort: 9091 protocol: TCP [...] args: [...] - \u0026#39;-openshift-sar={\u0026#34;resource\u0026#34;: \u0026#34;namespaces\u0026#34;, \u0026#34;verb\u0026#34;: \u0026#34;get\u0026#34;}\u0026#39; There is an OAuth setting which says: you have to have the privilege to GET the objects \u0026#34;namespace\u0026#34;.\nThe only cluster role which has exactly this privilege and which is also mentioned by the official OpenShift documentation is cluster-monitoring-view\n- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-monitoring-view rules: - apiGroups: - \u0026#34;\u0026#34; resources: - namespaces verbs: - get As we have seen above, this will show you all namespaces available on the cluster.\nWhen you check port 9092 there is no such OAuth configuration. This service is actually in front of the container kube-rbac-proxy. It does not require OAuth, but instead the namespace URL parameter.\nDetails can be found at: https://github.com/openshift/enhancements/blob/master/enhancements/monitoring/user-workload-monitoring.md\nIn short the whole setup looks like this:\nFigure 10. Thanos interconnecting containers While port 9091 goes directly to Thanos it will require that you have the cluster-monitoring-view role. Port 9092 does not require this, but instead you MUST send the URL parameter namespace=.\nSummary While both options are valid, some considerations must be done when using the Grafana Operator.\nCurrently the URL parameter can be set in Grafana directly only. The operator will ignore it. The ticket in the project shall address this, but is not yet implemented: https://github.com/integr8ly/grafana-operator/issues/309\nThe URL parameter setting will be gone, when the Grafana pods is restarted, which might lead to a problem.\nWhile the Grafana serviceaccount does not require cluster permissions, it will require permission to view the appropriate namespace\nAll above also means, that you actually would need to create a new DataSource for every project you want to monitor. I was not able to find a way, to send multiple namespaces in the URL parameter.\nIs it useful to leverage the Grafana operator then at all? Probably yes, since Operators are the future and it is actively developed. Nevertheless, it is always possible to deploy Grafana manually.\n"},{"uri":"https://blog.stderr.at/openshift/2020/08/gitops-argo-cd/","title":"GitOps - Argo CD","tags":["Gitops","OpenShift","OCP","ArgoCD"],"description":"Using Argo CD to manage OpenShift resources","content":" Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. GitOps itself uses Git pull request to manager infrastructure and application configuration.\nLet’s try to install and use a simple usecase in order to demonstrate the basic possibilities.\nWithout going into the very detail, typical GitOps usecases are:\nApply configurations from Git\nDetect, (auto-)sync and notify configuration drifts\nManage multiple clusters and keep the configuration equal\n…​\nand much more. Further information about the theory behind can be found at https://www.openshift.com/blog/introduction-to-gitops-with-openshift.\nIn short: there is no reason why not to use GitOps and to leverage tools like Argo CD to manage configurations. In this tutorial, the Argo CD operator gets installed and a simple use case is shown to demonstrate the possibilities of this software.\nAs for architectural overview of Argo CD, please read the official documentation: https://argoproj.github.io/argo-cd/operator-manual/architecture/, which explains very well the core components. No need to rewrite it here.\nPrerequisites You need an Openshift 4 cluster. :)\nWatch the video at: https://demo.openshift.com/en/latest/argocd/\nInstall Argo CD operator Before you begin, create a new project:\noc new-project argocd In this project the operator will be deployed.\nLook for the Operatorhub in the OpenShift WebUI and \u0026#34;argocd\u0026#34; and select the \u0026#34;Argo CD Community\u0026#34; operator. Subscribe to this operator. Just be sure that the newly created project is selected. Other settings can stay as default.\nFigure 1. Argo CD: Operator This will install the operator. You can monitor this process by clicking \u0026#34;Installed Operators\u0026#34;. After a while it should switch from \u0026#34;Installing\u0026#34; to \u0026#34;Succeeded\u0026#34;.\nDeploy ArgoCD instance Select the installed operator \u0026#34;Argo CD\u0026#34;, select the tab \u0026#34;ArgoCD\u0026#34; and hit the button \u0026#34;Create ArgoCD\u0026#34;\nEnter the following yaml:\napiVersion: argoproj.io/v1alpha1 kind: ArgoCD metadata: name: argocd namespace: argocd spec: dex: image: quay.io/redhat-cop/dex openShiftOAuth: true version: v2.22.0-openshift rbac: policy: | g, argocdadmins, role:admin scopes: \u0026#39;[groups]\u0026#39; server: route: enabled: true This yaml extends the default example by:\nusing OpenShift authentication\nAllow all users from the group \u0026#34;argocdadmins\u0026#34; admin permissions inside Argo CD\ncreate a route to access argocd web interface\nOnce this configuration is created, the operator will automatically start to roll out the different pods, which are required. No worries, it will take quite long until everything is up and running.\nCreate a new group and assign a user to it In the ArgoCD resource we have defined the group argocdadmins and all users in this group will get administrator privileges in Argo CD. This group must be created and in addition we assign the user admin to it.\nFor example with the following commands:\noc adm groups new argocdadmins oc adm groups add-users argocdadmins admin Login to Argo CD Now it is time to login to Argo CD. Just fetch the route which was created by the operator (for example with: oc get routes -n argocd).\nOn the login page select \u0026#34;Login via OpenShift\u0026#34; and enter the credentials of the user you would like to use. (well, the one which you can admin permissions in the step above).\nFigure 2. Argo CD: Login Screen This will open the Argo CD Interface.\nFirst test with Argo CD Let’s create an application in Argo CD to demonstrate the possibilities about application management with GitOps. We will use a simple application which draws a blue (or green) box in your browser.\nClick on the button \u0026#34;Create App\u0026#34; and enter the following parameters:\nName: bgd\nProject: default (This is the project inside Argo CD, not OpenShift)\nSync Policy: Can stay at manual for now\nRepository URL: https://github.com/tjungbauer/gitops-examples (This is a fork of christianh814/gitops-examples)\nRevision: master\nPath: bgd/\nCluster: https://kubernetes.devault.svc (This is the local default cluster Argo CD created. Other Clusters may be defined)\nNamespace: bgd (This is the OpenShift namespace which will be created)\nAt the end, it should look like this:\nFigure 3. Argo CD: Create an Application Press the \u0026#34;Create\u0026#34; button and your application is ready to be synchronized. Since no synchronization happens yet, Argo CD will complain that the application is out of sync.\nSync application Since we set the Sync Policy to manual, the synchronization process must be started, guess what, manually. Click on the \u0026#34;Sync\u0026#34; button and Argo CD will open a side panel, which shows the resources are out of sync and other options.\nFigure 4. Argo CD: Sync an Application One notable option is the \u0026#34;Prune\u0026#34; setting. By selecting this, changes which have been done directly on OpenShift, are removed and replaced by the ones which are stored at Git.\nThis is a very good option, to force everyone to follow the GitOps process :) Press the \u0026#34;Synchronize\u0026#34; button and select the application. As you see the sync process has started and after a while, all resources are synced to OpenShift.\nFigure 5. Argo CD: Application Syncing Figure 6. Argo CD: Application Synced Verifying objects Now that Argo CD says that the application has been synchronized, we should check the objects, which have been created in OpenShift.\nAs you can see in the Git repository, there are 4 objects which should exist now:\na namespace (bgd)\na deployment\na service\na route\nFigure 7. Argo CD: Git Repo To verify the existence either check via the WebUI or simply try:\noc get all -n bgd NAME READY STATUS RESTARTS AGE pod/bgd-6b9b64d94d-5fqdg 1/1 Running 0 6m2s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/bgd ClusterIP 172.30.233.30 \u0026lt;none\u0026gt; 8080/TCP 6m7s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/bgd 1/1 1 1 6m4s NAME DESIRED CURRENT READY AGE replicaset.apps/bgd-6b9b64d94d 1 1 1 6m3s NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/bgd bgd-bgd.apps.ocp.example.test bgd 8080 None Obviously, the namespace exists and with it also the other objects, which hae been synchronized.\nWhen you now open the route http://bgd-bgd.apps.ocp.example.test in your browser, you will see a nice blue box.\nFigure 8. Argo CD: The Blue Box As you can see all objects have been synchronized and the application has been deployed correctly. The source of truth is in Git and all changes should be done there.\nI want a green box So you want a green box? Maybe you think of doing this:\nModify the Deployment and change the environment COLOR from blue to green:\n... spec: containers: - name: bgd image: \u0026#39;quay.io/redhatworkshops/bgd:latest\u0026#39; env: - name: COLOR value: green # change from blue to green ... This will trigger a re-deployment and …​ fine …​ you have a green box:\nFigure 9. Argo CD: The Green Box But is this the correct way to do that? NO, it is not. Argo CD will immediately complain that the application is out of sync.\nFigure 10. Argo CD: Out of Sync When you sync the application it will end up with a blue box again.\nFigure 11. Argo CD: The Blue Box But you really really want a green box? Fair enough, the correct way would be to change the deployment configuration on Git. Simply change the file bgd/bgd-deployment.yaml and set the COLOR to green:\n... spec: containers: - image: quay.io/redhatworkshops/bgd:latest name: bgd env: - name: COLOR value: \u0026#34;green\u0026#34; resources: {} Again Argo CD will complain that it is out of sync.\nFigure 12. Argo CD: Git Update By synchronizing the changes, it will deploy the latest version found at Git and …​ yes, you have a green box now (When deployment on OpenShift side has finished).\nFigure 13. Argo CD: The Green Box "},{"uri":"https://blog.stderr.at/tags/container-security/","title":"Container Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/05/enable-automatic-route-creation/","title":"Enable Automatic Route Creation","tags":["Istio","Service Mesh","OpenShift","OCP","Route"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 13 - Automatic route creation","content":" Red Hat Service Mesh 1.1 allows you to enable a \u0026#34;Automatic Route Creation\u0026#34; which will take care about the routes for a specific Gateway. Instead of defining * for hosts, a list of domains can be defined. The Istio OpenShift Routing (ior) synchronizes the routes and creates them inside the Istio namespace. If a Gateway is deleted, the routes will also be removed again.\nThis new features makes the manual creation of the route obsolete, as it was explained here: Openshift 4 and Service Mesh 4 - Ingress with custom domain\nEnable Automatic Route Creation Before this feature can be used, it must be enabled. To do so the ServiceMeshContolPlace, typically found in the namespace istio-system must be modified. Add the line ior_enabled: true to the istio-ingressgate configuration.\n... spec: istio: gateways: istio-egressgateway: autoscaleEnabled: false istio-ingressgateway: autoscaleEnabled: false ior_enabled: true ... Verify current service Let’s check our tutorial application, if it is still working.\noc project tutorial export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) curl $GATEWAY_URL/customer customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 30 Let’s review and remove the current used Gateway. As you can see the hosts is set to \u0026#39;*\u0026#39;\noc get istio-io oc get gateway.networking.istio.io/customer-gateway -o yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;networking.istio.io/v1alpha3\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Gateway\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;customer-gateway\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;tutorial\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;selector\u0026#34;:{\u0026#34;istio\u0026#34;:\u0026#34;ingressgateway\u0026#34;},\u0026#34;servers\u0026#34;:[{\u0026#34;hosts\u0026#34;:[\u0026#34;*\u0026#34;],\u0026#34;port\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;http\u0026#34;,\u0026#34;number\u0026#34;:80,\u0026#34;protocol\u0026#34;:\u0026#34;HTTP\u0026#34;}}]}} creationTimestamp: \u0026#34;2020-05-13T07:52:20Z\u0026#34; generation: 1 name: customer-gateway namespace: tutorial resourceVersion: \u0026#34;41370056\u0026#34; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/tutorial/gateways/customer-gateway uid: 96e82ed9-e870-493c-941f-bfa83c892b94 spec: selector: istio: ingressgateway servers: - hosts: - \u0026#39;*\u0026#39; port: name: http number: 80 protocol: HTTP Create a new Gateway First let’s remove the current Gateway\noc delete gateway.networking.istio.io/customer-gateway Now lets create a new Gateway, but this time we define some names for the hosts section:\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; Gateway-ior.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: customer-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - www.example.com - svc.example.com EOF oc apply -f Gateway-ior.yaml -n tutorial When you now check the routes, 2 new routes have been added:\noc get routes -n istio-system NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD ... tutorial-customer-gateway-kmqrl www.example.com istio-ingressgateway http2 None tutorial-customer-gateway-ks7q7 svc.example.com istio-ingressgateway http2 None To test the connectivity, you need to be sure that the hosts, used in the Gateway, are resolvable. If they are then you can access your service:\ncurl www.example.com/customer customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 31 curl svc.example.com/customer customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 32 "},{"uri":"https://blog.stderr.at/tags/istio/","title":"Istio","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/quay/2020-05-13-quay-tutorial1/","title":"Red Hat Quay Registry - Overview and Installation","tags":["Quay","Registry","OpenShift","Container Security"],"description":"Red Hat Quay Registry - Overview and Installation","content":" Red Hat Quay is an enterprise-quality container registry, which is responsible to build, scan, store and deploy containers. The main features of Quay include:\nHigh Availability\nSecurity Scanning (with Clair)\nRegistry mirroring\nDocker v2\nContinuous integration\nand much more.\nQuay can be installed as:\nStandalone single service\nHigh Availibility service\nOn OpenShift with an Operator\nOn OpenShift without an Operator.\nFor the following Quay Tutorial Quay version 3.3 on OpenShift 4.3/4.4 with an Operator is used. Moreover, a local storage is used, to make the deployment easier and independent to a storage provider. In any case, be sure that for a production environment a \u0026#34;real\u0026#34; storage is used.\nFurther details can be found at the official Red Hat documentation at: Deploy Red Hat Quay on OpenShift with an Operator\nQuay Architecture Several components are used to build Quay\nDatabase: used to store metadata (not images)\nRedis: stores live builder logs\nQuay container registry: Runs Quay as a service\nTwo types of storages are supported:\nPublic cloud storage, like Amazon S3, Google Cloud Storage\nPrivate cloud storage, S3 compliant Object Store, Like Ceph RADOS or OpenStack Swift.\nSince I perform the example setup in a limited lab environment, I used local storage. This is not supported for production installations. Installation This example installation, covers the configuration of all credentials, but it ignores the configuration of a storage engine. Instead it is using local storage.\nCreate a new project and install the Quay Operator Note: Before you begin create a new project called \u0026#34;quay\u0026#34;.\nFrom the OpenShift console goto Operators \u0026gt; OperationHub and search for \u0026#34;Red Hat Quay Operator\u0026#34;. Be sure NOT to select the community version. Install the Operator using the following settings:\nInstallation Mode: select the namespace \u0026#34;quay\u0026#34;\nLeave the other settings as default.\nFigure 1. Operator Installation Once you select \u0026#34;Subscribe\u0026#34;, the installation process will take a few minutes. At the end, the operator pods should run inside your namespace and is ready to bse used.\nFigure 2. Operator Running Configure Quay To configure Quay the Custom Resource Definition \u0026#34;QuayEcosystem\u0026#34; must be defined. Below example shows a ready to use example to test the deployment. Several secrets are used in this QuayEcosystem which must be created first. If they are not specified in the QuayEcosystem, then default values would be used. However, it always makes sense to specify passwords :)\nCreate a Pull Secret to pull from Quay.io Check the article Accessing Red Hat Quay to find the appropriate credentials you need to fetch images from quay.io. Find the section \u0026#34;Red Hat Quay v3 on OpenShift\u0026#34;, copy the secret into the file redhat_secret.yaml and create the object in OpenShift:\noc create -f redhat_secret.yaml Create Quay Superuser Credentials The Quay superuser will be able to manage other users or projects. Create the secret by defining username, password and e-mail address:\nThe password must have a minimum length of 8 characters. oc create secret generic quay-credentials \\ --from-literal=superuser-username=\u0026lt;username\u0026gt; \\ --from-literal=superuser-password=\u0026lt;password\u0026gt; \\ --from-literal=superuser-email=\u0026lt;email\u0026gt; Create Quay Configuration Credentials The Quay configuration is done, by a separate pod, with a separate accessible route. To set the password, create the following secret:\nThe password must have a minimum length of 8 characters. oc create secret generic quay-config-app \\ --from-literal=config-app-password=\u0026lt;password\u0026gt; Specify database credentials As next let’s create the credentials for the database:\noc create secret generic \u0026lt;secret_name\u0026gt; \\ --from-literal=database-username=\u0026lt;username\u0026gt; \\ --from-literal=database-password=\u0026lt;password\u0026gt; \\ --from-literal=database-root-password=\u0026lt;root-password\u0026gt; \\ --from-literal=database-name=\u0026lt;database-name\u0026gt; It is also possible to use and existing database. To configure this, create the secret as described and add the server parameter, containing the hostname, to the QuayEcosystem definition+ Setting Redis password By default, the operator would install Redis without any password. To specify a password, create the following secret:\noc create secret generic quay-redis-password \\ --from-literal=password=\u0026lt;password\u0026gt; Create QuayEcosystem Resource With all the secrets created above, it is time to create the QuayEcosystem. Once it is defined, the operator will automatically start all required services.\nThe following is an example, using the different secret names (The names should be self explaining) In addition, the following has been defined:\nvolumeSize = 10GI for the database\nkeepConfigDeployment to false, this will remove the configuration pod after the deployment.\nhostname: to reach the Quay registry under a defined hostname (otherwise a default name would be created)\nClair container scanning is enabled\napiVersion: redhatcop.redhat.io/v1alpha1 kind: QuayEcosystem metadata: name: quayecosystem spec: quay: imagePullSecretName: redhat-quay-pull-secret superuserCredentialsSecretName: quay-credentials configSecretName: quay-config-app deploymentStrategy: Recreate skipSetup: false keepConfigDeployment: false externalAccess: hostname: quay.apps.ocp.ispworld.at database: volumeSize: 10Gi credentialsSecretName: quay-database-credential registryBackends: - name: local local: storagePath: /opt/quayregistry redis: credentialsSecretName: quay-redis-password imagePullSecretName: redhat-quay-pull-secret clair: enabled: true imagePullSecretName: redhat-quay-pull-secret Quay WebUI Once the Quay Operator has deployed all containers, you should see one route (or 2 if you kept Configuration Deployment Container) and can access your Quay installation.\nFigure 3. Quay WebUI Optional: Disable self account creation Many customers want to disable the \u0026#34;Create Account\u0026#34; link on the login page (see Figure #3), to prevent that anybody could create a new account. To remove this option, the configuration pod must run.\nVerify if the Configuration pod is running If the following does not return anything, then the container is not running:\noc get routes -n quay | grep config If this is the case, modify the resource QuayEcosystem to enable the Configuration UI.\nEdit:\noc edit QuayEcosystem/quayecosystem and set \u0026#34;KeepConfigDeployment\u0026#34; to true:\nquay: [...] keepConfigDeployment: true After a few minutes another pod, called \u0026#34;quayecosystem-quay-config\u0026#34; will be started and a new route is created:\noc get routes -n quay | grep config quayecosystem-quay-config quayecosystem-quay-config-quay.apps.ocp.ispworld.at quayecosystem-quay-config 8443 passthrough/Redirect None Configure Account Creation and Anonymous Access Login to the Configuration Web Interface with the credentials you specified during the deployment and scroll down to the section \u0026#34;Access Settings\u0026#34;.\nFigure 4. Quay Configuration There remove the checkbox from:\nAnonymous Access\nUser Creation\nand save and build the configuration.\nThis will trigger a change on the Quay pod. After it has been recreated (this will take a few minutes), the feature to create a new account is removed from the Login page.\nWorking with Quay The following quick steps through Quay are the steps of the Quay tutorial, which can be seen at the Quay WebUI at the \u0026#34;Tutorial\u0026#34; tab.\nLogin via Docker CLI To login via docker CLI simply use:\ndocker login \u0026lt;your selected hostname for quay\u0026gt; Docker expects a valid certificate. Such certificate could be added to the definition of QuayEcosystem. However, I did not create a certificate for this lab. To allow untrusted certificates, on a Mac, simply download the certificates (For Chrome: you can drag and drop the certificate from the browser to your Desktop, for Firefox, you need to open the Options menu and export the certificates.). After that double click both certificates (the root and the site certificate), which will install them on you local Keychain. Open the Keychain on you Mac, find the appropriate certificates and set both to \u0026#34;Always trust\u0026#34; Create an example container The next step to create a new image is to create a container. For this example the busybox base image is used.\ndocker run busybox echo \u0026#34;fun\u0026#34; \u0026gt; newfile This will pull the latest image of busybox and create a container:\nUnable to find image \u0026#39;busybox:latest\u0026#39; locally latest: Pulling from library/busybox d9cbbca60e5f: Pulling fs layer d9cbbca60e5f: Verifying Checksum d9cbbca60e5f: Download complete d9cbbca60e5f: Pull complete Digest: sha256:836945da1f3afe2cfff376d379852bbb82e0237cb2925d53a13f53d6e8a8c48c Status: Downloaded newer image for busybox:latest With \u0026#34;docker ps\u0026#34; the running container is shown. Remember the Container ID for further steps. In this case fc3e9bb1e9da.\ndocker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fc3e9bb1e9da busybox \u0026#34;echo fun\u0026#34; 3 minutes ago Exited (0) 3 minutes ago relaxed_proskuriakova Create the image Once a container has terminated in Docker, the next step is to commit the container to an image. To do so we will use \u0026#34;docker commit\u0026#34; command. As name for the repository I took superapp.\ndocker commit fc3e9bb1e9da quay.apps.ocp.ispworld.at/quay/superapp Push the image to Red Hat Quay The final step is to push the image to our repository, where it will be stored for future use.\ndocker push quay.apps.ocp.ispworld.at/quay/superapp Figure 5. Quay Repository Test Container Security Scanner Clair is used to scan containers about possible security risks. It imports vulnerability data permanently from a known source and creates a list of threats for an image.\nTo test such scanning, we pull the \u0026#34;Universal Base Image RHEL 7\u0026#34; from Red Hat. I am using version 7.6 since this is already quite old and we expect some known vulnerabilities for this image.\nFirst you need to login to Red Hat Registry:\ndocker login registry.redhat.io Username: \u0026lt;Username\u0026gt; Password: \u0026lt;Password\u0026gt; Login Succeeded Then let’s pull the UBI Image 7.6 (instead of the latest)\ndocker pull registry.redhat.io/ubi7/ubi:7.6 Before we can push it to our Quay registry, we need to tag it:\ndocker tag registry.redhat.io/ubi7/ubi:7.6 quay.apps.ocp.ispworld.at/quay/ubi7:7.6 If we now check the local images, we see that there are two UBI images.\ndocker images REPOSITORY TAG IMAGE ID CREATED SIZE quay.apps.ocp.ispworld.at/quay/ubi7 7.6 247ee58855fd 10 months ago 204MB registry.redhat.io/ubi7/ubi 7.6 247ee58855fd 10 months ago 204MB Now it is possible to push the image to the Quay repository.\ndocker push quay.apps.ocp.ispworld.at/quay/ubi7:7.6 Finally the image is available inside our Registry and Clair will queue it for a security scan.\nOnce the scan is finished, possible found issues are shown under the \u0026#34;Repository Tags\u0026#34;.\nFigure 6. Clair Security Scanning When you click on then, you will see a detailed result page, with all vulnerabilities found:\nFigure 7. Clair Security Scanning Result "},{"uri":"https://blog.stderr.at/tags/registry/","title":"Registry","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/route/","title":"Route","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/service-mesh/","title":"Service Mesh","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/service-mesh/","title":"Service Mesh","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/authorization/","title":"Authorization","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/05/authorization-rbac/","title":"Authorization (RBAC)","tags":["Istio","Service Mesh","OpenShift","OCP","Authorization","Security"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 12 - Authorization Role Based Access Control (RBAC)","content":" Per default all requests inside a Service Mesh are allowed, which can be a problem security-wise. To solve this, authorization, which verifies if the user is allowed to perform a certain action, is required. Istio’s authorization provides access control on mesh-level, namespace-level and workload-level.\nWith the resource AuthorizationPolicy granular policies can be defined. These policies are loaded to and verified by the Envoy Proxy which then authorizes a request.\nImplicit enablement To enable authorization the only thing you need is to do is to define the AuthorizationPolicy. If the resource is not defined, then no access control will be used, instead any traffic is allowed. If AuthorizationPolicy is applied to a workload, then by default any traffic is denied unless it is explicitly allowed.\nThis is applicable to Service Mesh version 1.1+ Preparing Environment The following steps will configure an example Role Based Access Control (RBAC). It will start from scratch. If you just want to quickly configure the authorization and have anything else in place, you can start form here: Configure Authentication Policy\nCreate a new project\noc new-project tutorial Be sure that a Service Mesh Member Roll exists for this new project\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; memberroll.yaml apiVersion: maistra.io/v1 kind: ServiceMeshMemberRoll metadata: name: default spec: members: - tutorial EOF oc apply -f memberroll.yaml -n istio-system Clone and install the example application\ngit clone https://github.com/redhat-developer-demos/istio-tutorial/ istio-tutorial oc apply -f istio-tutorial/customer/kubernetes/Deployment.yml -n tutorial oc apply -f istio-tutorial/customer/kubernetes/Service.yml -n tutorial oc expose service customer oc apply -f istio-tutorial/preference/kubernetes/Deployment.yml -n tutorial oc apply -f istio-tutorial/preference/kubernetes/Service.yml -n tutorial oc apply -f istio-tutorial/recommendation/kubernetes/Deployment.yml -n tutorial oc apply -f istio-tutorial/recommendation/kubernetes/Service.yml -n tutorial Wait until all pods are running. There should be 2 containers for all pods:\noc get pods -w Create Gateway and VirtualService\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; Gateway_VirtualService.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: customer-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: customer-gateway spec: hosts: - \u0026#34;*\u0026#34; gateways: - customer-gateway http: - match: - uri: prefix: /customer rewrite: uri: / route: - destination: host: customer port: number: 8080 EOF oc apply -f Gateway_VirtualService.yaml -n tutorial Verify if the application is working You can either use the run.sh from previous tutorials, or simply try the following curl\nexport GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;); echo $GATEWAY_URL curl $GATEWAY_URL/customer This should return the following line:\ncustomer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1 Optionally check the connection from inside the customer container\nget pods name and enter it:\noc get pods NAME READY STATUS RESTARTS AGE customer-6948b8b959-dhsm9 2/2 Running 0 177m preference-v1-7fdb89c86b-dvzs9 2/2 Running 0 177m recommendation-v1-69db8d6c48-cjcpn 2/2 Running 0 177m Connect into the container pod and try to reach the different microservices\noc rsh customer-6948b8b959-dhsm9 sh-4.4$ curl customer:8080 customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 2 sh-4.4$ curl preference:8080 preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3 sh-4.4$ curl recommendation:8080 recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 4 Configure Authentication Policy Enabling User-End authentication\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; authentication-policy.yaml apiVersion: \u0026#34;authentication.istio.io/v1alpha1\u0026#34; kind: \u0026#34;Policy\u0026#34; metadata: name: \u0026#34;customerjwt\u0026#34; spec: targets: - name: customer - name: preference - name: recommendation origins: - jwt: issuer: \u0026#34;testing@secure.istio.io\u0026#34; jwksUri: \u0026#34;https://gist.githubusercontent.com/lordofthejars/7dad589384612d7a6e18398ac0f10065/raw/ea0f8e7b729fb1df25d4dc60bf17dee409aad204/jwks.json\u0026#34; principalBinding: USE_ORIGIN EOF oc apply -f authentication-policy.yaml -n tutorial Access should be denied after a few seconds\ncurl $GATEWAY_URL/customer Origin authentication failed.% Use token to authenticate\ntoken=$(curl https://gist.githubusercontent.com/lordofthejars/a02485d70c99eba70980e0a92b2c97ed/raw/f16b938464b01a2e721567217f672f11dc4ef565/token.simple.jwt -s) curl -H \u0026#34;Authorization: Bearer $token\u0026#34; $GATEWAY_URL/customer This will result in a correct response\ncustomer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 5 Configure Role Based Access Control (RBAC) Create the resource AuthorizationPolicy\nThis is a new resources, supported since Service Mesh 1.1. It will allow GET method when the role equals to \u0026#34;customer\u0026#34;\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; AuthorizationPolicy.yaml apiVersion: \u0026#34;security.istio.io/v1beta1\u0026#34; kind: \u0026#34;AuthorizationPolicy\u0026#34; metadata: name: \u0026#34;customer\u0026#34; spec: rules: - to: - operation: methods: [\u0026#34;GET\u0026#34;] when: - key: request.auth.claims[role] values: [\u0026#34;customer\u0026#34;] EOF oc apply -f AuthorizationPolicy.yaml -n tutorial Get a token for the role and retry to connect to the service,\ntoken=$(curl https://gist.githubusercontent.com/lordofthejars/f590c80b8d83ea1244febb2c73954739/raw/21ec0ba0184726444d99018761cf0cd0ece35971/token.role.jwt -s) curl -H \u0026#34;Authorization: Bearer $token\u0026#34; $GATEWAY_URL/customer This results in:\ncustomer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 8 Let’s verify the setting and change the AuthorizationPolicy. This will break the authorization, since the token provides roles=customer and we set the Policy to \u0026#34;whereistherole\u0026#34;\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; AuthorizationPolicy-Hack.yaml apiVersion: \u0026#34;security.istio.io/v1beta1\u0026#34; kind: \u0026#34;AuthorizationPolicy\u0026#34; metadata: name: \u0026#34;customer\u0026#34; spec: rules: - to: - operation: methods: [\u0026#34;GET\u0026#34;] when: - key: request.auth.claims[role] values: [\u0026#34;whereistherole\u0026#34;] EOF oc replace -f AuthorizationPolicy-Hack.yaml -n tutorial If you now try to access the service, with the token, which provides \u0026#34;customer\u0026#34; as role, it will lead to an error:\ntoken=$(curl https://gist.githubusercontent.com/lordofthejars/f590c80b8d83ea1244febb2c73954739/raw/21ec0ba0184726444d99018761cf0cd0ece35971/token.role.jwt -s) curl -H \u0026#34;Authorization: Bearer $token\u0026#34; $GATEWAY_URL/customer RBAC: access denied "},{"uri":"https://blog.stderr.at/general/2020/05/basic-usage-of-git/","title":"Basic usage of git","tags":["git","github"],"description":"Using git for contributing to projects hosted on github.com","content":" This is a very short and hopefully simple introduction on how to use Git when you would like to contribute to projects hosted on github.com. The same workflow should also work for projects on gitlab.com.\nIntroduction There is this fancy mega application hosted on github called megaapp that you would like to contribute to. It’s perfect but there’s just this little feature missing to make it even more perfect.\nThis is how we would tackle this.\nrocket science ahead Glossary Term Definition fork\nA (personal) copy of a repository you created on github or gitlab.\nupstream\nWhen creating forks of repositories on github or gitlab, the original repository hosting the project\nindex\nThe staging area git uses before you can commit to a repository\nremote repository\nA repository hosted on a server shared by developers\nlocal repository\nA local copy of a repository stored on you machine.\nStep 1: Fork the repository on github.com Login to you Github account and navigate to the project you would like to fork, megaapp in our example.\nClick on the the fork button, as depicted in the image below:\nIf you are a member of several projects on github.com, github is going to ask you into which project you would like to clone this repository.\nAfter selecting the project or your personal account, github is going to clone the repository into the project you selected. For this example I’m going to use my personal github account \u0026#34;tosmi\u0026#34;.\nStep 2: Clone the repository to you workstation Next we are going to clone our fork from Step 1: Fork the repository on github.com to our workstation and start working on the new feature.\nAfter forking the upstream project you are redirect to your personal copy of the project. Click on the \u0026#34;Clone or download\u0026#34; button and select the link. You can choose between SSH and HTTPS protocols for downloading the project. We are going to use SSH.\nCopy the link into a terminal and execute the git clone command:\n$ git clone git@github.com:tosmi/megaapp.git Step 3: Create a feature branch for your new fancy feature Change into the directory of the project you downloaded in Step 2: Clone the repository to you workstation\ncd megaapp Now we create a feature branch with a short name that describes our new feature:\ngit checkout -b tosmi/addoption Because we would like to add a new option to megaapp we call this feature branch addoption.\nWe are also prefixing the feature branch with our github username so that it is clear for the upstream project maintainer(s) who is contributing this.\nHow you name you branches is opinionated, so we would search for upstream project guidelines and if there are none maybe look at some existing pull request how other people are naming there branches. If we find no clue upstream we sticking with \u0026lt;github username\u0026gt;/\u0026lt;branch name\u0026gt;.\nWe can now start adding our mega feature to the project.\nStep 4: Add you changes to the Git index Before we can commit our changes, we have to place the changes made in the so called index or staging area:\n$ git add \u0026lt;path to file you have changed\u0026gt; If we would like to place all of our changes onto the index we could execute\n$ git add -A Step 5: Commit your changes After adding our changes to the Git index we can commit with\n$ git commit This will open our favorite editor and we can type a commit message. The first line should be a short description of our change, probably not longer than 70 to 80 characters. After two newlines we can enter a detailed explanation of your changes.\nThis is an example commit message\nAdded a `version` option to output the current version of megaapp This change introduces a `version` option to megaapp. The purpose is to output the current version of megaapp for users. This might be helpful when users open a bug report so we can see what version is affected. After saving the message and we have successfully created a commit.\nRemember this is now only stored in the local copy of the repository! We still have to push our changes to github. There is also the option to add the commit comment directly on the command line\n$ git commit -m \u0026#39;Added a `version` option to output the current version of megaapp This change introduces a `version` option to megaapp. The purpose is to output the current version of megaapp for users. This might be helpful when users open a bug report so we can see what version is affected.\u0026#39; Step 6: Pushing our local changes to our forked repo on github.com We execute\n$ git push to push our local changes to the forked repository hosted on github.com.\nStep 7: Creating a pull request on github.com We navigate to our personal project page of the forked repository on github. For the fork we are using in this example this is http://github.com/tosmi/megaapp.\nGithub is going to show us a button \u0026#34;Compare \u0026amp; pull request\u0026#34;:\nAfter clicking on that button we are able to review the changes we would like to include in this pull request.\nIf we are happy with our changes we click on \u0026#34;Create pull request\u0026#34;. The upstream owner of the repository will get notified and we can see our open pull request on the upstream project page under \u0026#34;Pull requests\u0026#34;.\nIf there are CI test configured for that project they will start to run and we can see if our pull request is going to pass all test configured.\nRebasing to current upstream if required Sometimes a upstream project maintainer asks you to rebase your work on the current upstream master branch. The following steps explain the basic workflow.\nFirst we are going to create a new remote location of our repository called upstream. Upstream points to the upstream project repository. We will not push to this location, in most cases this is not possible because you do not have write access to a remote upstream repository. It is just used for pulling upstream changes in our forked repository.\nExecute the following commands to add the upstream repository as a new remote location and display all remote locations currently defined.\n$ git remote add upstream https://github.com/rhatservices/megaapp.git $ git remote -v origin git@github.com:tosmi/megaapp.git (fetch) origin git@github.com:tosmi/megaapp.git (push) upstream https://github.com/rhatservices/megaapp.git (fetch) upstream https://github.com/rhatservices/megaapp.git (push) As we hopefully implemented our new feature in feature branch, we can pull changes from the upstream master branch into our local copy of the master branch. Remember we are using a feature branch and master should be kept clean from local changes.\n$ git checkout master Switched to branch \u0026#39;master\u0026#39; Your branch is up to date with \u0026#39;origin/master\u0026#39;. So now we have this older copy of the upstream master branch checked out and we would like to update it to the latest and greatest from the upstream master branch.\n$ git pull upstream master remote: Enumerating objects: 10, done. remote: Counting objects: 100% (10/10), done. remote: Compressing objects: 100% (3/3), done. remote: Total 6 (delta 2), reused 6 (delta 2), pack-reused 0 Unpacking objects: 100% (6/6), 630 bytes | 157.00 KiB/s, done. From https://github.com/rhatservices/megaapp * branch master -\u0026gt; FETCH_HEAD * [new branch] master -\u0026gt; upstream/master Updating 4d8584e..ddfd077 Fast-forward cmd/megaapp/main.go | 2 ++ cmd/megaapp/rule.go | 20 ++++++++++++++++++++ 2 files changed, 22 insertions(+) create mode 100644 cmd/megaapp/rule.go With the pull command above you pulled all changes from the upstream master branch into you local copy of master. Just to be sure let’s display all available branches, local and remote ones.\nBranches with a name remote/\u0026lt;remote name\u0026gt;/\u0026lt;branch name\u0026gt; are remote branches that git knows about. Origin points to our forked repository and is also the default location for push operations.\n$ git branch -a master * tosmi/megafeature remotes/origin/HEAD -\u0026gt; origin/master remotes/origin/master remotes/origin/tosmi/megafeature remotes/upstream/master So finally to rebase our feature branch to the upstream master branch we first need to checkout our feature branch via\n$ git checkout tosmi/megafeature Now we are able to rebase our changes to upstream master. Git basically pulls in all changes from the master branch and re-applies the changes we did in our feature branch.\ngit rebase upstream/master Successfully rebased and updated refs/heads/tosmi/megafeature. There might be merge conflicts when git tries to apply you changes from your feature branch. You have to fix those changes, git add the fixed files and execute git rebase continue. Luckily this is not the case for your megafeature.\nAs we have successfully rebased our feature branch to upstream master we can now try to push changes made to our forked github repository.\n$ git push To github.com:tosmi/megaapp.git ! [rejected] tosmi/megafeature -\u0026gt; tosmi/megafeature (non-fast-forward) error: failed to push some refs to \u0026#39;git@github.com:tosmi/megaapp.git\u0026#39; hint: Updates were rejected because the tip of your current branch is behind hint: its remote counterpart. Integrate the remote changes (e.g. hint: \u0026#39;git pull ...\u0026#39;) before pushing again. hint: See the \u0026#39;Note about fast-forwards\u0026#39; in \u0026#39;git push --help\u0026#39; for details. Oh, this fails of course! The reason is that our local feature branch and the remote feature branch have a different commit history. The remote feature branch is missing the commits from master that we applied when rebasing on the current master branch.\nSo let’s try again, this time using the --force-with-lease option. You could also use -f or --force but --force-with-lease will stop you if someone else (our you) has modified the remote feature branch meanwhile. If you push with -f or --force anyways you might loose changes.\n$ git push --force-with-lease Enumerating objects: 5, done. Counting objects: 100% (5/5), done. Delta compression using up to 8 threads Compressing objects: 100% (3/3), done. Writing objects: 100% (3/3), 295 bytes | 295.00 KiB/s, done. Total 3 (delta 2), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To github.com:tosmi/megaapp.git + acf66a3...39357b2 tosmi/megafeature -\u0026gt; tosmi/megafeature (forced update) But as no one modified the remote feature branch while we did our rebase the force push goes through.\nOur merge request (if we opened one already) is now updated to the latest upstream master branch and merging our feature should be a breeze. You might notify the upstream project maintainer that you feature branch is up to date and ready for merging\nUsing git’s interactive rebase to change you commit history When working with upstream projects it might be that a project maintainer requests that you rework your git history before he is willing to merge your changes. For example this could be that case if you have plenty of commits with very small changes (e.g. fixed typos).\nThe general rule is that one commit should implement one change. This is not a hard rule, but usually works.\nLet’s look at an example. For the implementation of our new feature that we would like to bring upstream we have the following commit history\n$ git log --oneline 0a5221d (HEAD -\u0026gt; tosmi/megafeature) fixed typo 0e60d12 update README bf2ef3c update We have updated README.md in the repository but there a three commits for this little change. Before bringing this upstream in our pull request, we would like to convert those three commits into a single one and also make the commit message a little more meaningful.\nWe execute the following command to start reworking our commit history\n$ git rebase -i Git will drop us into our beloved editor (vi in this case), under Linux you could change the editor git uses by modifying the $EDITOR environment variable. We are going to see the following output:\npick bf2ef3c update pick 0e60d12 update README pick 0a5221d fixed typo # Rebase 39357b2..0a5221d onto 39357b2 (3 commands) # # Commands: # p, pick \u0026lt;commit\u0026gt; = use commit # r, reword \u0026lt;commit\u0026gt; = use commit, but edit the commit message # e, edit \u0026lt;commit\u0026gt; = use commit, but stop for amending # s, squash \u0026lt;commit\u0026gt; = use commit, but meld into previous commit # f, fixup \u0026lt;commit\u0026gt; = like \u0026#34;squash\u0026#34;, but discard this commit\u0026#39;s log message # x, exec \u0026lt;command\u0026gt; = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with \u0026#39;git rebase --continue\u0026#39;) # d, drop \u0026lt;commit\u0026gt; = remove commit # l, label \u0026lt;label\u0026gt; = label current HEAD with a name # t, reset \u0026lt;label\u0026gt; = reset HEAD to a label # m, merge [-C \u0026lt;commit\u0026gt; | -c \u0026lt;commit\u0026gt;] \u0026lt;label\u0026gt; [# \u0026lt;oneline\u0026gt;] # . create a merge commit using the original merge commit\u0026#39;s # . message (or the oneline, if no original merge commit was # . specified). Use -c \u0026lt;commit\u0026gt; to reword the commit message. # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # Git automatically selected commit id bf2ef3c as the basis for our rebase. We could also have specified the commit id where we would like to start our rebase operation e.g.\ngit rebase -i bf2ef3c In our editor of choice we can now tell git what it should do with the selected commits. Please go ahead and read the helpfull explanation text in comments (prefixed with \u0026#39;#\u0026#39;) to get a better understanding of the operations supported.\nIn our case we would like to squash the last commits. So we change the lines with pick to squash until it looks like the following:\npick bf2ef3c update squash 0e60d12 update README squash 0a5221d fixed typo We would like to squash commits 0a5221d and 0e60d12 onto commit bf2ef3c. Keep in mind that git actually reverses the order of commits. So 0a5221d is the last commit we added.\nIf we save the file and quit our editor (I’m using vi here), git drops us into another buffer where we can finally modify the commits\nThis is a combination of 3 commits. # This is the 1st commit message: update # This is the commit message #2: update README # This is the commit message #3: fixed typo # Please enter the commit message for your changes. Lines starting # with \u0026#39;#\u0026#39; will be ignored, and an empty message aborts the commit. # # Date: Mon May 18 15:46:37 2020 +0200 # # interactive rebase in progress; onto 39357b2 # Last commands done (3 commands done): # squash 0e60d12 update README # squash 0a5221d fixed typo # No commands remaining. # You are currently rebasing branch \u0026#39;tosmi/megafeature\u0026#39; on \u0026#39;39357b2\u0026#39;. # # Changes to be committed: # modified: README.md # We can see all three commit message and we are going to modify those messages until we are happy\n# This is a combination of 3 commits. # This is the 1st commit message: updated README.md to megafeature as we added megafeature, it makes sense to include a short note about it also in README.md # Please enter the commit message for your changes. Lines starting # with \u0026#39;#\u0026#39; will be ignored, and an empty message aborts the commit. # # Date: Mon May 18 15:46:37 2020 +0200 # # interactive rebase in progress; onto 39357b2 # Last commands done (3 commands done): # squash 0e60d12 update README # squash 0a5221d fixed typo # No commands remaining. # You are currently rebasing branch \u0026#39;tosmi/megafeature\u0026#39; on \u0026#39;39357b2\u0026#39;. # # Changes to be committed: # modified: README.md # When we are happy with new commit message we just save and quit our editor. Git will now rewirte the history and when we take look at the commit history again we will see our changes:\n$ git log --oneline 91d1ae2 (HEAD -\u0026gt; tosmi/megafeature) updated README.md to megafeature 39357b2 (origin/tosmi/megafeature) added a mega feature ddfd077 (upstream/master, master) added rule command 4d8584e (origin/master, origin/HEAD) Update README.md eb6ccbc Create README.md 60fcabc start using cobra for argument parsing 5140ed0 import .gitignore d2b55d1 import a simple Makefile 2ecb412 initial import We only have commit 91d1ae2 now , which includes all three changes from the commits before.\nRewriting the history of a repository is a dangerous operation. Especially when you are working in a team. It is not advised to change the history of commits that got already pushed to a remote location. Otherwise your teammates will get confused next time they try to push or pull from the shared repository. So it’s OK to change the commit history of a feature branch that only you are using, but be careful when working on branches more than one developer is using.\n"},{"uri":"https://blog.stderr.at/tags/git/","title":"Git","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/github/","title":"Github","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/bookinfo/","title":"Bookinfo","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/deploy-example-bookinfo-application/","title":"Deploy Example Bookinfo Application","tags":["Istio","Service Mesh","OpenShift","OCP","Bookinfo","Example"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 11 - Deploy Example Bookinfo Application","content":" To test a second application, a bookinfo application shall be deployed as an example.\nThe following section finds it’s origin at:\nIstio - Bookinfo Application\nOpenShift 4 - Example Application\nThe Bookinfo application displays information about a book, similar to a single catalog entry of an online book store. Displayed on the page is a description of the book, book details (ISBN, number of pages, and other information), and book reviews. The Bookinfo application consists of these microservices: * The productpage microservice calls the details and reviews microservices to populate the page. * The details microservice contains book information. * The reviews microservice contains book reviews. It also calls the ratings microservice. * The ratings microservice contains book ranking information that accompanies a book review. There are three versions of the reviews microservice: * Version v1 does not call the ratings Service. * Version v2 calls the ratings Service and displays each rating as one to five black stars. * Version v3 calls the ratings Service and displays each rating as one to five red stars. The end-to-end architecture of the application is shown below. Figure 1. Bookinfo Application End2End Overview To use the bookinfo application inside service mesh, no code changes are required. Instead an Envoy proxy is added as a sidecar container to all containers (product, review, details) which intercepts the traffic.\nInstallation Let’s start right away:\nCreate a new project\noc new-project bookinfo Add the new project to our Service Mesh\napiVersion: maistra.io/v1 kind: ServiceMeshMember metadata: name: default namespace: bookinfo spec: controlPlaneRef: name: basic-install namespace: istio-system Create the application\noc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-1.1/samples/bookinfo/platform/kube/bookinfo.yaml Create the Gateway and the VirtuaService\noc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-1.1/samples/bookinfo/networking/bookinfo-gateway.yaml Check if the services and pods are up and running\noc get svc,pods NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/details ClusterIP 172.30.178.172 \u0026lt;none\u0026gt; 9080/TCP 7m16s service/productpage ClusterIP 172.30.78.96 \u0026lt;none\u0026gt; 9080/TCP 7m13s service/ratings ClusterIP 172.30.154.12 \u0026lt;none\u0026gt; 9080/TCP 7m15s service/reviews ClusterIP 172.30.138.174 \u0026lt;none\u0026gt; 9080/TCP 7m14s NAME READY STATUS RESTARTS AGE pod/details-v1-d7db4d55b-mwzsk 2/2 Running 0 7m14s pod/productpage-v1-5f598fbbf4-svkbc 2/2 Running 0 7m11s pod/ratings-v1-85957d89d8-v2lrs 2/2 Running 0 7m11s pod/reviews-v1-67d9b4bcc-x6s2v 2/2 Running 0 7m11s pod/reviews-v2-67b465c497-zpz6z 2/2 Running 0 7m11s pod/reviews-v3-7bd659b757-j6rwn 2/2 Running 0 7m11s Verify that application is accessible Export the Gateway URL into a variable\nexport GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) Verify if the productpage is accessible\ncurl -s http://${GATEWAY_URL}/productpage | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; \u0026lt;title\u0026gt;Simple Bookstore App\u0026lt;/title\u0026gt; You can also access the Productpage in your browser. When you reload the page several times, you will see different results for the Reviews. This comes due to 3 different versions: one without any rating, one with black stars and one with red stars. http://${GATEWAY_URL}/productpage\nFigure 2. Bookinfo Application Adding default Destination Rule oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-1.1/samples/bookinfo/networking/destination-rule-all-mtls.yaml This will add default routing to all endpoints with same weight. As you can see in Kiali, the Reviews microservice is contacted equally.\nFigure 3. Kiali: Bookinfo Application Feel free to play with other DestinationRules to controll your traffic.\n"},{"uri":"https://blog.stderr.at/tags/example/","title":"Example","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/2020/04/ansible-azure-resource-manager-example/","title":"Ansible - Azure Resource Manager Example","tags":["Ansible","Azure"],"description":"Using Ansible Azure Resource Manager to create a Virtual Machine","content":" Using Ansible Resource Manager with an ARM template and a simple Ansible playbook to deploy a Virtual Machine with Disk, virtual network, public IP and so on.\nIntroduction Source: [1]\nIn order to deploy a Virtual Machine and all depended resources using the Azure Resource Manager (ARM) template with Ansible, you will need three things:\nThe ARM Template\nThe parameters you want to use\nThe Ansible playbook\nAll can be found below. Store them and simply call:\nansible-playbook Azure/create_azure_deployment.yml it will take several minutes, until everything has been deployment in Azure. Instead of using a json file locally you can upload the template file (as well as a parameters file) to a version control system and use it from there. Additional Resources Ansible Playbook: Create-Azure-Deplyoment.yml --- - name: Get facts of a VM hosts: localhost connection: local become: false gather_facts: false tasks: #- name: Destroy Azure Deploy # azure_rm_deployment: # resource_group: tju-ResourceGroup # name: tju-testDeployment # state: absent - name: Create Azure Resource Group deployment azure_rm_deployment: state: present resource_group_name: tju-ResourceGroup name: tju-testDeployment #template_link: \u0026#39;\u0026lt;YOUR RAW Github template file\u0026gt;\u0026#39; #parameters_link: \u0026#39;\u0026lt;YOUR RAW Github parameters file\u0026gt;\u0026#39; template: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;ResourceManagerTemplate.json\u0026#39;) }}\u0026#34; parameters: projectName: value: tjuProject location: value: \u0026#34;East US\u0026#34; adminUsername: value: tjungbauer adminPublicKey: value: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/Users/tjungbauer/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; operatingSystem: value: CentOS operatingSystemPublisher: value: OpenLogic operatingSystemSKU: value: \u0026#39;7.1\u0026#39; vmSize: value: Standard_D2s_v3 register: azure - name: Add new instance to host group add_host: hostname: \u0026#34;{{ item[\u0026#39;ips\u0026#39;][0].public_ip }}\u0026#34; groupname: azure_vms loop: \u0026#34;{{ azure.deployment.instances }}\u0026#34; ResourceManagerTemplate.json { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;projectName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies a name for generating resource names.\u0026#34; } }, \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the location for all resources.\u0026#34; } }, \u0026#34;adminUsername\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies a username for the Virtual Machine.\u0026#34; } }, \u0026#34;adminPublicKey\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the SSH rsa public key file as a string. Use \\\u0026#34;ssh-keygen -t rsa -b 2048\\\u0026#34; to generate your SSH key pairs.\u0026#34; } }, \u0026#34;operatingSystem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the Operating System. i.e. CentOS\u0026#34; } }, \u0026#34;operatingSystemPublisher\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the publisher. i.e. OpenLogic\u0026#34; } }, \u0026#34;operatingSystemSKU\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the version of the OS. i.e. 7.1\u0026#34; } }, \u0026#34;vmSize\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the the VM size. i.e. Standard_D2s_v3\u0026#34; } } }, \u0026#34;variables\u0026#34;: { \u0026#34;vNetName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-vnet\u0026#39;)]\u0026#34;, \u0026#34;vNetAddressPrefixes\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;vNetSubnetName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;vNetSubnetAddressPrefix\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;vmName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-vm\u0026#39;)]\u0026#34;, \u0026#34;publicIPAddressName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-ip\u0026#39;)]\u0026#34;, \u0026#34;networkInterfaceName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-nic\u0026#39;)]\u0026#34;, \u0026#34;networkSecurityGroupName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-nsg\u0026#39;)]\u0026#34;, \u0026#34;networkSecurityGroupName2\u0026#34;: \u0026#34;[concat(variables(\u0026#39;vNetSubnetName\u0026#39;), \u0026#39;-nsg\u0026#39;)]\u0026#34; }, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/networkSecurityGroups\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;networkSecurityGroupName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;securityRules\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ssh_rule\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Locks inbound down to ssh default port 22.\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;Tcp\u0026#34;, \u0026#34;sourcePortRange\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;destinationPortRange\u0026#34;: \u0026#34;22\u0026#34;, \u0026#34;sourceAddressPrefix\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;destinationAddressPrefix\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;access\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;priority\u0026#34;: 123, \u0026#34;direction\u0026#34;: \u0026#34;Inbound\u0026#34; } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/publicIPAddresses\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;publicIPAddressName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;publicIPAllocationMethod\u0026#34;: \u0026#34;Dynamic\u0026#34; }, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Basic\u0026#34; } }, { \u0026#34;comments\u0026#34;: \u0026#34;Simple Network Security Group for subnet [variables(\u0026#39;vNetSubnetName\u0026#39;)]\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/networkSecurityGroups\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-08-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;networkSecurityGroupName2\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;securityRules\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;default-allow-22\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;priority\u0026#34;: 1000, \u0026#34;access\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;direction\u0026#34;: \u0026#34;Inbound\u0026#34;, \u0026#34;destinationPortRange\u0026#34;: \u0026#34;22\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;Tcp\u0026#34;, \u0026#34;sourceAddressPrefix\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;sourcePortRange\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;destinationAddressPrefix\u0026#34;: \u0026#34;*\u0026#34; } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/virtualNetworks\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vNetName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkSecurityGroups\u0026#39;, variables(\u0026#39;networkSecurityGroupName2\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;addressSpace\u0026#34;: { \u0026#34;addressPrefixes\u0026#34;: [ \u0026#34;[variables(\u0026#39;vNetAddressPrefixes\u0026#39;)]\u0026#34; ] }, \u0026#34;subnets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnetName\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;addressPrefix\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnetAddressPrefix\u0026#39;)]\u0026#34;, \u0026#34;networkSecurityGroup\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkSecurityGroups\u0026#39;, variables(\u0026#39;networkSecurityGroupName2\u0026#39;))]\u0026#34; } } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/networkInterfaces\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;networkInterfaceName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;Microsoft.Network/publicIPAddresses\u0026#39;, variables(\u0026#39;publicIPAddressName\u0026#39;))]\u0026#34;, \u0026#34;[resourceId(\u0026#39;Microsoft.Network/virtualNetworks\u0026#39;, variables(\u0026#39;vNetName\u0026#39;))]\u0026#34;, \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkSecurityGroups\u0026#39;, variables(\u0026#39;networkSecurityGroupName\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;ipConfigurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ipconfig1\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;privateIPAllocationMethod\u0026#34;: \u0026#34;Dynamic\u0026#34;, \u0026#34;publicIPAddress\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/publicIPAddresses\u0026#39;, variables(\u0026#39;publicIPAddressName\u0026#39;))]\u0026#34; }, \u0026#34;subnet\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/virtualNetworks/subnets\u0026#39;, variables(\u0026#39;vNetName\u0026#39;), variables(\u0026#39;vNetSubnetName\u0026#39;))]\u0026#34; } } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-10-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vmName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkInterfaces\u0026#39;, variables(\u0026#39;networkInterfaceName\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;hardwareProfile\u0026#34;: { \u0026#34;vmSize\u0026#34;: \u0026#34;[parameters(\u0026#39;vmSize\u0026#39;)]\u0026#34; }, \u0026#34;osProfile\u0026#34;: { \u0026#34;computerName\u0026#34;: \u0026#34;[variables(\u0026#39;vmName\u0026#39;)]\u0026#34;, \u0026#34;adminUsername\u0026#34;: \u0026#34;[parameters(\u0026#39;adminUsername\u0026#39;)]\u0026#34;, \u0026#34;linuxConfiguration\u0026#34;: { \u0026#34;disablePasswordAuthentication\u0026#34;: true, \u0026#34;ssh\u0026#34;: { \u0026#34;publicKeys\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;[concat(\u0026#39;/home/\u0026#39;, parameters(\u0026#39;adminUsername\u0026#39;), \u0026#39;/.ssh/authorized_keys\u0026#39;)]\u0026#34;, \u0026#34;keyData\u0026#34;: \u0026#34;[parameters(\u0026#39;adminPublicKey\u0026#39;)]\u0026#34; } ] } } }, \u0026#34;storageProfile\u0026#34;: { \u0026#34;imageReference\u0026#34;: { \u0026#34;publisher\u0026#34;: \u0026#34;[parameters(\u0026#39;operatingSystemPublisher\u0026#39;)]\u0026#34;, \u0026#34;offer\u0026#34;: \u0026#34;[parameters(\u0026#39;operatingSystem\u0026#39;)]\u0026#34;, \u0026#34;sku\u0026#34;: \u0026#34;[parameters(\u0026#39;operatingSystemSKU\u0026#39;)]\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;osDisk\u0026#34;: { \u0026#34;createOption\u0026#34;: \u0026#34;fromImage\u0026#34; } }, \u0026#34;networkProfile\u0026#34;: { \u0026#34;networkInterfaces\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkInterfaces\u0026#39;, variables(\u0026#39;networkInterfaceName\u0026#39;))]\u0026#34; } ] } } } ], \u0026#34;outputs\u0026#34;: { \u0026#34;adminUsername\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;adminUsername\u0026#39;)]\u0026#34; } } } Sources [1]: azure_rm_deployment – Create or destroy Azure Resource Manager template deployments\n"},{"uri":"https://blog.stderr.at/tags/azure/","title":"Azure","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2020/04/openshift-pipelines-tekton-introduction/","title":"OpenShift Pipelines - Tekton Introduction","tags":["Pipelines","OpenShift","OCP","Tekton"],"description":"Using OpenShift 4.x and OpenShift Pipelines using Tekton for your CI/CD process","content":" OpenShift Pipelines is a cloud-native, continuous integration and delivery (CI/CD) solution for building pipelines using Tekton. Tekton is a flexible, Kubernetes-native, open-source CI/CD framework that enables automating deployments across multiple platforms (Kubernetes, serverless, VMs, etc) by abstracting away the underlying details. [1]\nOpenShift Pipelines features Source: [1]\nStandard CI/CD pipeline definition based on Tekton\nBuild images with Kubernetes tools such as S2I, Buildah, Buildpacks, Kaniko, etc\nDeploy applications to multiple platforms such as Kubernetes, serverless and VMs\nEasy to extend and integrate with existing tools\nScale pipelines on-demand\nPortable across any Kubernetes platform\nDesigned for microservices and decentralized teams\nIntegrated with the OpenShift Developer Console\nPrerequisites OpenShift 4.x cluster. Try yourself at https://try.openshift.com\nOptional: Tekton CLI - Optional for now, since you could do everything via UI as well.\noc/kubectl CLI or WebUI\nThe Tekton CLI is optional in case you prefer to do everything via the OpenShift WebUI. However, below examples make use of the Tekton CLI and I personally would recommend to at least install it. (Like oc client it is good to have a CLI option as well). In any case, all described action can be done directly via the WebUI as well. Basic Concepts Tekton makes use of several custom resources (CRD).\nThese CRDs are:\nTask: each step in a pipeline is a task, while a task can contain several steps itself, which are required to perform a specific task. For each Task a pod will be allocated and for each step inside this Task a container will be used. This helps in better scalability and better performance throughout the pipeline process.\nPipeline: is a series of tasks, combined to work together in a defined (structured) way\nTaskRun: is the result of a Task, all combined TaskRuns are used in the PipelineRun\nPipelineRun: is the actual execution of a whole Pipeline, containing the results of the pipeline (success, failed…​)\nPipelines and Tasks should be generic and never define possible variables, like input git repository, directly in their definition. For this, the concept of PipelineResources has been created, which defines these parameters and which are used during a PipelineRun.\nInstallation The OpenShift Pipeline is an operator which can e installed using the following yaml:\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-OpenShift-Pipelines.yaml apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: openshift-pipelines-operator namespace: openshift-operators spec: channel: dev-preview name: openshift-pipelines-operator source: community-operators sourceNamespace: openshift-marketplace EOF oc create -f deploy-OpenShift-Pipelines.yaml As an alternative, you can also use the WebUI to rollout the operator:\nSearch for \u0026#34;OpenShift Pipeline\u0026#34; under OperatorHub and install it\nSelect:\nAll Namespaces: since the operator needs to watch for Tekton Custom Resources across all namespaces.\nChannel: Dev Preview\nApprove Strategy: Automatic\nPrepare a tutorial project To test our OpenShift Pipelines, we need to deploy an example application. This application let’s you vote what pet you like more: Cats or Dogs? It contais of a backend and a frontend part, which both will be deployed in a namespace.\nLet’s first create a new project:\noc new-project pipelines-tutorial The OpenShift Pipeline operator will automatically create a pipeline serviceaccount with all required permissions to build and push an image and which is used by PipelineRuns:\noc get sa pipeline NAME SECRETS AGE pipeline 2 15s Create a Task A Task is the smallest block of a Pipeline which by itself can contain one or more steps which are executed in order to process a specific element. For each Task a pod is allocated and each step is running in a container inside this pod. Tasks are reusable by other Pipelines. Input and Output specifications can be used to interact with other Tasks.\nLet’s create two tasks Source: Pipeline-Tutorial\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-Example-Tasks.yaml apiVersion: tekton.dev/v1alpha1 kind: Task metadata: name: apply-manifests spec: inputs: resources: - {type: git, name: source} params: - name: manifest_dir description: The directory in source that contains yaml manifests type: string default: \u0026#34;k8s\u0026#34; steps: - name: apply image: quay.io/openshift/origin-cli:latest workingDir: /workspace/source command: [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;] args: - |- echo Applying manifests in $(inputs.params.manifest_dir) directory oc apply -f $(inputs.params.manifest_dir) echo ----------------------------------- --- apiVersion: tekton.dev/v1alpha1 kind: Task metadata: name: update-deployment spec: inputs: resources: - {type: image, name: image} params: - name: deployment description: The name of the deployment patch the image type: string steps: - name: patch image: quay.io/openshift/origin-cli:latest command: [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;] args: - |- oc patch deployment $(inputs.params.deployment) --patch=\u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{ \u0026#34;containers\u0026#34;:[{ \u0026#34;name\u0026#34;: \u0026#34;$(inputs.params.deployment)\u0026#34;, \u0026#34;image\u0026#34;:\u0026#34;$(inputs.resources.image.url)\u0026#34; }] }}}}\u0026#39; EOF oc create -f deploy-Example-Tasks.yaml Verify that the two tasks have been created using the Tekton CLI:\ntkn task ls NAME AGE apply-manifests 52 seconds ago update-deployment 52 seconds ago Create a Pipeline A pipeline is a set of Tasks, which should be executed in a defined way to achieve a specific goal.\nThe example Pipeline below uses two resources:\ngit-repo: defines the Git-Source\nimage: Defines the target at a repository\nIt first uses the Task buildah, which is a standard Task the OpenShift operator created automatically. This task will build the image. The resulted image is pushed to an image registry, defined in the output parameter. After that our created tasks apply-manifest and update-deployment are executed. The execution order of these tasks is defined with the runAfter Parameter in the yaml definition.\nThe Pipeline should be re-usable accross multiple projects or environments, thats why the resources (git-repo and image) are not defined here. When a Pipeline is executed, these resources will get defined. cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-Example-Pipeline.yaml apiVersion: tekton.dev/v1alpha1 kind: Pipeline metadata: name: build-and-deploy spec: resources: - name: git-repo type: git - name: image type: image params: - name: deployment-name type: string description: name of the deployment to be patched tasks: - name: build-image taskRef: name: buildah kind: ClusterTask resources: inputs: - name: source resource: git-repo outputs: - name: image resource: image params: - name: TLSVERIFY value: \u0026#34;false\u0026#34; - name: apply-manifests taskRef: name: apply-manifests resources: inputs: - name: source resource: git-repo runAfter: - build-image - name: update-deployment taskRef: name: update-deployment resources: inputs: - name: image resource: image params: - name: deployment value: $(params.deployment-name) runAfter: - apply-manifests EOF oc create -f deploy-Example-Pipeline.yaml Verify that the Pipeline has been created using the Tekton CLI:\ntkn pipeline ls NAME AGE LAST RUN STARTED DURATION STATUS build-and-deploy 3 seconds ago --- --- --- --- Trigger Pipeline After the Pipeline has been created, it can be triggered to execute the Tasks.\nCreate PipelineResources Since the Pipeline is generic, we need to define 2 PipelineResources first, to execute a Pipepline. Our example application contains a frontend (vote-ui) AND a backend (vote-api), therefore 4 PipelineResources will be created. (2 times git repository to clone the source and 2 time output image)\nQuick overview:\nui-repo: will be used as git_repo in the Pipepline for the Frontend\nui-image: will be used as image in the Pipeline for the Frontend\napi-repo: will be used as git_repo in the Pipepline for the Backend\napi-image: will be used as image in the Pipeline for the Backend\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-Example-PipelineResources.yaml apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: ui-repo spec: type: git params: - name: url value: http://github.com/openshift-pipelines/vote-ui.git --- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: ui-image spec: type: image params: - name: url value: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-ui:latest --- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: api-repo spec: type: git params: - name: url value: http://github.com/openshift-pipelines/vote-api.git --- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: api-image spec: type: image params: - name: url value: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-api:latest EOF oc create -f deploy-Example-PipelineResources.yaml The resources can be listed with:\ntkn resource ls NAME TYPE DETAILS api-repo git url: http://github.com/openshift-pipelines/vote-api.git ui-repo git url: http://github.com/openshift-pipelines/vote-ui.git api-image image url: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-api:latest ui-image image url: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-ui:latest Execute Pipelines We start a PipelineRune for the backend and frontend of our application.\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-Example-PipelineRun.yaml apiVersion: tekton.dev/v1alpha1 kind: PipelineRun metadata: name: build-deploy-api-pipelinerun spec: pipelineRef: name: build-and-deploy resources: - name: git-repo resourceRef: name: api-repo - name: image resourceRef: name: api-image params: - name: deployment-name value: vote-api --- apiVersion: tekton.dev/v1alpha1 kind: PipelineRun metadata: name: build-deploy-ui-pipelinerun spec: pipelineRef: name: build-and-deploy resources: - name: git-repo resourceRef: name: ui-repo - name: image resourceRef: name: ui-image params: - name: deployment-name value: vote-ui EOF oc create -f deploy-Example-PipelineRun.yaml The PipelineRuns can be listed with\ntkn pipelinerun ls NAME STARTED DURATION STATUS build-deploy-api-pipelinerun 3 minutes ago --- Running build-deploy-ui-pipelinerun 3 minutes ago --- Running Moreover, the logs can be viewed with the following command and select the appropriate PipelineRun:\ntkn pipeline logs -f ? Select pipelinerun: [Use arrows to move, type to filter] \u0026gt; build-deploy-api-pipelinerun started 2 minutes ago build-deploy-ui-pipelinerun started 2 minutes ago Checking your application Now our Pipeline built and deployed the voting application, where you can vote if you prefere cats or dogs (Cats or course :) )\nGet the route of your project and open the URL in the browser. (Should be something like vote-ui-pipelines-tutorial.apps.yourclustername)\nFigure 1. Tekton: Example Application OpenShift WebUI With the OpenShift Pipeline operator a new menu item is introduced on the WebUI of OpenShift. All Tekton CLI command which are used above, can actually be replaced with the web interface, in case you prefere this. The big advantage is th graphical presentation of Pipelines and their lifetime.\nI will not create screenshots for every screen, but for example pipelines:\nUnder Pipelines a list of pipelines will be shown.\nFigure 2. OpenShift UI: List of Pipelines Additional Resources Sources [1]: OpenShift Pipelines Tutorial\nTekon\nTekton Task Catalog\n"},{"uri":"https://blog.stderr.at/tags/pipelines/","title":"Pipelines","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/tekton/","title":"Tekton","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/general/2020/04/red-hat-satellite-cheat-sheet/","title":"Red Hat Satellite Cheat Sheet","tags":["Satellite"],"description":"Cheat sheet for Red Hat Satellite","content":" Cheat sheet for various Red Hat Satellite tasks from a newbie to a newbie.\nRequirements up to Satellite 6.7 RHEL 7.X\n4 CPU Cores\n20 GB of RAM\n300 GB disk space\nfor more info see the prerequistes guide\nInstallation Satellite up to version 6.7 uses puppet for installation. You can use\npuppet filebucket to restore files modified by puppet.\nSatellite requires the Red Hat Satellite Infrastructure Subscription, check if it’s available with\nsubscription-manager list --all --available --matches \u0026#39;Red Hat Satellite Infrastructure Subscription\u0026#39; If not attach it with\nsubscription-manager attach --pool=pool_id Next disable all repos and enable only supported repostories via\nsubscription-manager repos --disable \u0026#34;*\u0026#34; and enable required repositories\nsubscription-manager repos --enable=rhel-7-server-rpms \\ --enable=rhel-7-server-satellite-6.6-rpms \\ --enable=rhel-7-server-satellite-maintenance-6-rpms \\ --enable=rhel-server-rhscl-7-rpms \\ --enable=rhel-7-server-ansible-2.8-rpms then clean cached all repo data via\nyum clean all and install satellite packages via\nyum install satellite Install satellite with\nsatellite-installer --scenario satellite \\ --foreman-initial-organization \u0026#34;initial_organization_name\u0026#34; \\ --foreman-initial-location \u0026#34;initial_location_name\u0026#34; \\ --foreman-initial-admin-username admin_user_name \\ --foreman-initial-admin-password admin_password Backup / Restore / Cloning Use satellite-maintain for doing offline and online backups\nsatellite-maintain backup offline /backup/ when using the online option make sure that no new content view or content view versions should be created while the backup is running. basically satellite should be idle.\nCloning Satellite The online/offline options also backup /var/lib/pulp, which contains all downloaded packages. This could be huge. There’s an option to skip this so\nsatellite-maintain backup offline --skip-pulp-tar /backup/ For a restore you always need the content of /var/lib/pulp. This is mainly usefull for cloning satellite. You backup everything except /var/lib/pulp, copy the backup to a second system and rsync /var/lib/pulp to the new system. Then restore the backup and satellite should work as normal on the clone.\nSnaphot backups Satellite also supports backups via LVM snapshots. For more information see Snapshot backup\nUpgrades Read the Satellite release notes\nDo a offline backup see [Backup / Restore]\nYou could clone satellite to a other system\nIf there are local changes to dhcpd or dns configurations use\nsatellite-installer --foreman-proxy-dns-managed=false --foreman-proxy-dhcp-managed=false to stop satellite-install from overwriting those files.\ninstall the latest version of satellite-maintain via\nyum install rubygem-foreman_maintain check for available satellite versions with\nsatellite-maintain upgrade list-versions test the possible upgrade with\nsatellite-maintain upgrade check --target-version 6.7 and finally run the upgrade and PRAY!\nsatellite-maintain upgrade run --target-version 6.7 Various tips and tricks Installing packages via yum Satellite installs a yum plugin called foreman-protector. If you try to install a package via yum you get the following message\nWARNING: Excluding 12190 packages due to foreman-protector. Use foreman-maintain packages install/update \u0026lt;package\u0026gt; to safely install packages without restrictions. Use foreman-maintain upgrade run for full upgrade. so use\nsatellite-maintain install \u0026lt;package name\u0026gt; OS package upgrade This should be done via satellite-maintain because all packages are locked by default (see Installing packages via yum).\nThis basically comes down to running\noreman-maintain upgrade run --target-version 6.6.z for upgrading OS packages if you have satellite 6.6 installed.\n"},{"uri":"https://blog.stderr.at/tags/satellite/","title":"Satellite","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/service-mesh-1.1-released/","title":"Service Mesh 1.1 released","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"OpenShift 4.x Service Mesh Update.","content":" April 10th 2020 Red Hat released Service Mesh version 1.1 which supports the following versions:\nIstio - 1.4.6\nKiali - 1.12.7\nJaeger - 1.17.1\nUpdate To update an operator like Service Mesh, the Operator Life Cycle Manager takes care and automatically updates everything (unless it was configured differently).\nFor the Service Mesh 1.1 update consult Upgrading Red Hat OpenShift Service Mesh It is important to add the version number to the ServiceMeshControlPlane object. The easiest way to do so is:\nLog into OpenShift\nSelect the Namespace istio-system\nGoto _\u0026#34;Installed Operators \u0026gt; Red Hat OpenShift Service Mesh \u0026gt; ServiceMeshControlPlanes \u0026gt; basic-install \u0026gt; YAML\u0026#34;\nUnder spec add the following:\nspec: version: v1.1 Notable Changes ServiceMeshMember Object With the ServiceMeshMember object it is now possible that a project administrator can add a service to the service mesh, instead relying on the cluster administrator to configure the ServiceMeshMemberRoll. To do so create the following object (i.e. under the namespace tutorial)\napiVersion: maistra.io/v1 kind: ServiceMeshMember metadata: name: default namespace: tutorial spec: controlPlaneRef: name: basic-install (1) namespace: istio-system (2) 1 Name of the ServiceMeshControlPlane object 2 name of the service mesh namespace "},{"uri":"https://blog.stderr.at/service-mesh/2020/04/authentication-jwt/","title":"Authentication JWT","tags":["Istio","Service Mesh","OpenShift","OCP","JWT"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 10 - Authentication with JWT","content":" Welcome to tutorial 10 of OpenShift 4 and Service Mesh, where we will discuss authentication with JWT. JSON Web Token (JWT) is an open standard that allows to transmit information between two parties securely as a JSON object. It is an authentication token, which is verified and signed and therefore trusted. The signing can be achieved by using a secret or a public/private key pair.\nService Mesh can be used to configure a policy which enables JWT for your services.\nPreparation Be sure that you have at least the Gateway and VirtualService configured:\noc get istio-io -n tutorial Which should return the following:\nNAME AGE gateway.networking.istio.io/ingress-gateway-exampleapp 45h NAME HOST AGE destinationrule.networking.istio.io/recommendation recommendation 29h NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/ingress-gateway-exampleapp [ingress-gateway-exampleapp] [*] 45h Run some texample traffic, to be sure that our application is still working as expected\nexport GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) sh ~/run.sh 1000 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 31622 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 33056 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 31623 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 33057 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 31624 # 5: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 33058 Enabling End-User Authentication To test this feature we will need a valid token (JWT). More details can be found at the Istio example\nAll we need to create a Policy object\napiVersion: \u0026#34;authentication.istio.io/v1alpha1\u0026#34; kind: \u0026#34;Policy\u0026#34; metadata: name: \u0026#34;jwt-example\u0026#34; spec: targets: - name: customer origins: - jwt: issuer: \u0026#34;testing@secure.istio.io\u0026#34; jwksUri: \u0026#34;https://raw.githubusercontent.com/istio/istio/release-1.2/security/tools/jwt/samples/jwks.json\u0026#34; (1) principalBinding: USE_ORIGIN 1 Path to test a public key After a few seconds the requests will fail with an \u0026#34;authentication failed\u0026#34; error:\nsh ~/run.sh 1000 $GATEWAY_URL # 0: Origin authentication failed. # 1: Origin authentication failed. # 2: Origin authentication failed. # 3: Origin authentication failed. # 4: Origin authentication failed. # 5: Origin authentication failed. In Kiali we see a 100% failure rate.\nFigure 1. Kiali: failing because of authentication error. To be able to connect to our application we first need to fetch a valid token and put this into the header while sending curl.\nexport GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) export TOKEN=$(curl https://raw.githubusercontent.com/istio/istio/release-1.1/security/tools/jwt/samples/demo.jwt -s) for x in $(seq 1 1000); do curl --header \u0026#34;Authorization: Bearer $TOKEN\u0026#34; $GATEWAY_URL -s; done In Kiali the traffic is now working again and authenticated.\nFigure 2. Kiali: Traffic authenticated. Clean Up Remove the policy again, to be ready for the next tutorial.\noc delete policy jwt-example -n tutorial "},{"uri":"https://blog.stderr.at/tags/jwt/","title":"JWT","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/mtls/","title":"MTLS","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/mutual-tls-authentication/","title":"Mutual TLS Authentication","tags":["Istio","Service Mesh","OpenShift","OCP","mTLS"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 9 - Mutual TLS/mTLS Authentication","content":" When more and more microservices are involved in an application, more and more traffic is sent on the network. It should be considered to secure this traffic, to prevent the possibility to inject malicious packets. Mutual TLS/mTLS authentication or two-way authentication offers a way to encrypt service traffic with certificates.\nWith Red Hat OpenShift Service Mesh, Mutual TLS can be used without the microservice knowing that it is happening. The TLS is managed completely by the Service Mesh Operator between two Envoy proxies using a defined mTLS policy.\nIssue 9 of OpenShift 4 and Service Mesh will explain how to enable Mutual TLS inside the Service Mesh to secure the traffic between the different microservices.\nHow does it work? If a microservice sends a request to a server, it must pass the local sidecar Envoy proxy first.\nThe proxy will intercept the outbound request and starts a mutual TLS handshake with the proxy at the server side. During this handshake the certificates are exchanged and loaded into the proxy containers by Service Mesh.\nThe client side Envoy starts a mutual TLS handshake with the server side Envoy.\nThe client proxy does a secure naming check on the server’s certificate to verify that the identity in the certificate is authorized.\nA mutual TLS connection is established between the client and the server.\nThe Envoy proxy at the server sides decrypts the traffic and forwards it to the application through a local TCP connection.\nPreparations Before we can start be sure that the services are setup like in Issue #3. In addition, be sure that the following DestinationRule already exists:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation subsets: - labels: version: v1 name: version-v1 - labels: version: v2 name: version-v2 Now we will create a pod, which is running outside of the Service Mesh. It will not have a sidecar proxy and will simply curl our application.\nStore the following yaml and create the object in our cluster.\napiVersion: extensions/v1beta1 kind: Deployment metadata: labels: app: curl version: v1 name: curl spec: replicas: 1 selector: matchLabels: app: curl version: v1 template: metadata: labels: app: curl version: v1 annotations: (1) sidecar.istio.io/proxyCPU: \u0026#34;500m\u0026#34; sidecar.istio.io/proxyMemory: 400Mi spec: containers: - image: quay.io/maistra_demos/curl:latest command: [\u0026#34;/bin/sleep\u0026#34;, \u0026#34;3650d\u0026#34;] imagePullPolicy: Always name: curl 1 since no sidecar is injected (sidecar.istio.io/inject: \u0026#34;true\u0026#34;), only 1 container will be started. The traffic coming from the microservice customer AND from the external client curl must be simulated. To achieve this the following shell script can be used:\n#!/bin/sh export CURL_POD=$(oc get pods -n tutorial -l app=curl | grep curl | awk \u0026#39;{ print $1}\u0026#39; ) export CUSTOMER_POD=$(oc get pods -n tutorial -l app=customer | grep customer | awk \u0026#39;{ print $1}\u0026#39; ) echo \u0026#34;A load generating script is running in the next step. Ctrl+C to stop\u0026#34; while :; do echo \u0026#34;Executing curl in curl pod\u0026#34; oc exec -n tutorial $CURL_POD -- curl -s http://preference:8080 \u0026gt; /dev/null sleep 0.5 echo \u0026#34;Executing curl in customer pod\u0026#34; oc exec -n tutorial $CUSTOMER_POD -c customer -- curl -s http://preference:8080 \u0026gt; /dev/null sleep 0.5 done By executing this, it will first execute a curl command out of the curl pod and then the same curl command out of the customer container. Kepp this script running\nEnabling Mutual TLS Lets execute the shell script above and verify Kiali. As you notice there are requests coming from the customer microservice and from the source called unknown, which is the curl-service running outside the Service Mesh.\nFigure 1. Kiali: traffic coming from customer microserver and external pod Enable the policy by creating the following object:\napiVersion: \u0026#34;authentication.istio.io/v1alpha1\u0026#34; kind: \u0026#34;Policy\u0026#34; metadata: name: \u0026#34;preference-mutualtls\u0026#34; spec: targets: - name: preference peers: - mtls: mode: STRICT (1) 1 We are enforcing mtls for the target preference After a few seconds the curl pod cannot reach the application anymore:\nExecuting curl in curl pod command terminated with exit code 56 Executing curl in customer pod Executing curl in curl pod command terminated with exit code 56 Executing curl in customer pod Executing curl in curl pod command terminated with exit code 5 This is expected, since the preference service allows traffic over mutual TLS only. This was enforced by the Policy object (STRICT mode). The customer service, which is running inside the Service Mesh receives the error \u0026#34;5053 Service Unavalable\u0026#34; since it tries to send traffic, but it does not know yet to use mTLS.\nIn Kiali you will see the following:\nFigure 2. Kiali: traffic is blocked The curl pod is greyed out, since the traffic it tries to send, never reaches the preference service and is therefor not counted in the metric. To make customer aware that mutual TLS shall be used, a DestinationRule must be configured:\napiVersion: \u0026#34;networking.istio.io/v1alpha3\u0026#34; kind: \u0026#34;DestinationRule\u0026#34; metadata: name: \u0026#34;preference-destination-rule\u0026#34; spec: host: \u0026#34;preference\u0026#34; trafficPolicy: tls: mode: ISTIO_MUTUAL (1) 1 Let’s use mTLS This defines that ISTIO_MUTUAL shall be used for the service preference. The customer service recognizes this and automatically enables mTLS. After a few minutes the traffic graph in Kiali will show \u0026#34;green\u0026#34; traffic from customer through preference to _recommendation:\nFigure 3. Kiali: traffic for Service Mesh components is fine again. Mutual TLS Migration As you can see in the previous section, the curl pod cannot reach the application inside the Service Mesh. This happens because prefernce is strictly enforcing encrypted traffic, but curl only sends plain text. Luckily, Istio provides a method to gradually monitor the traffic and migrate to mTLS. Instead of STRICT mode PERMISSIVE can be used. Enabling permissive mode, preference will accept both, encrypted and plain-text traffic.\nReplace the Policy object with the following configuration:\napiVersion: \u0026#34;authentication.istio.io/v1alpha1\u0026#34; kind: \u0026#34;Policy\u0026#34; metadata: name: \u0026#34;preference-mutualtls\u0026#34; spec: targets: - name: preference peers: - mtls: mode: PERMISSIVE oc replace -f Policy-permissive.yaml Now let’s wait a few minutes and observe Kiali, which should end up with:\nFigure 4. Kiali: Encrypted and Plain-Text traffic As you can see with the lock icon, the traffic between cunstomer and preference is encrypted, while the traffic from unknown (which is our curl pod), is plain-text.\nThe errors you may see in Kiali happen due a known issue: https://issues.jboss.org/browse/MAISTRA-1000 Cleanup Clean up your environment:\noc delete policy -n tutorial preference-mutualtls oc delete destinationrule -n tutorial preference-destination-rule "},{"uri":"https://blog.stderr.at/tags/fault-injection/","title":"Fault Injection","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/fault-injection/","title":"Fault Injection","tags":["Istio","Service Mesh","OpenShift","OCP","Fault Injection"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 8 - Faul Injection/Chaos Testing","content":" Tutorial 8 of OpenShift 4 and Service Mesh tries to cover Fault Injection by using Chaos testing method to verify if your application is running. This is done by adding the property HTTPFaultInjection to the VirtualService. The settings for this property can be for example: delay, to delay the access or abort, to completely abort the connection.\n\u0026#34;Adopting microservices often means more dependencies, and more services you might not control. It also means more requests on the network, increasing the possibility for errors. For these reasons, it’s important to test your services’ behavior when upstream dependencies fail.\u0026#34; [1]\nPreparation Before we start this tutorial, we need to clean up our cluster. This is especially important when you did the previous training Limit Egress/External Traffic.\noc delete deployment recommendation-v3 oc scale deployment recommendation-v2 --replicas=1 oc delete serviceentry worldclockapi-egress-rule oc delete virtualservice worldclockapi-timeout Verify that 2 pods for the recommendation services are running (with 2 containers)\noc get pods -l app=recommendation -n tutorial NAME READY STATUS RESTARTS AGE recommendation-v1-69db8d6c48-h8brv 2/2 Running 0 4d20h recommendation-v2-6c5b86bbd8-jnk8b 2/2 Running 0 4d19h Abort Connection with HTTP Error 503 For the first example, we will need to modify the VirtualService and the DestinationRule. The VirtualService must be extended with a http fault section, which will abort the traffic 50% of the time.\nCreate the VirtualService\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - fault: abort: httpStatus: 503 percent: 50 route: - destination: host: recommendation subset: app-recommendation Apply the change\noc replace -f VirtualService-abort.yaml Existing VirtualService with the name recommendation will be overwritten. Create the DestinationRule\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation subsets: - labels: app: recommendation name: app-recommendation Apply the change\noc replace -f destinationrule-faultinj.yaml Existing Destination with the name recommendation will be overwritten. Check the traffic and verify that 50% of the connections will end with a 503 error:\nexport INGRESS_GATEWAY=$(oc get route customer -n tutorial -o \u0026#39;jsonpath={.spec.host}\u0026#39;) sh ~/run.sh 1000 $GATEWAY_URL Clean Up oc delete virtualservice recommendation Test slow connection with Delay More interesting, in my opinion, to test is a slow connection. This can be tested by adding the fixedDelay property into the VirtualService. Like in the example below, we will use a VirtualService. This time delay instead of abort is used. The fixDelay defines a delay of 7 seconds for 50% of the traffic.\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - fault: delay: fixedDelay: 7.000s percent: 50 route: - destination: host: recommendation subset: app-recommendation If you now send traffic into the application, you will see that some answers will have a delay of 7 seconds. Keep sending traffic in a loop.\nEven more visible it will be, when you goto \u0026#34;Distributed Tracing\u0026#34; at the Kiali UI, select the service recommendation and a small lookback of maybe 5min. You will find that some requests are very fast, while other will tage about 7 seconds.\nFigure 1. Jaeger with delayed traffic. Retry on errors If a microservice is answering with an error, Service Mesh/Istio will automatically try to reach another pod providing the service. These retries can be modified. In order to make everything visible, we will use Kiali to monitor the traffic.\nWe start by sending traffic into the application. This should be split evenly between v1 and v2 of the recommendation microservice\nsh ~/run.sh 1000 $GATEWAY_URL # 8329: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 11145 # 8330: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 9712 # 8331: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 11146 # 8332: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 9713 # 8333: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 11147 In Kiali this ia visible in the Graphs, using the settings: \u0026#34;Versioned app graph\u0026#34; and \u0026#34;Requests percentage\u0026#34;\nFigure 2. Traffic is split by 50% between recommendation v1 nd v2 As second step we need to enable the nasty mode for the microservice v2. This will simulate an outage, respoding with error 503 all the time. This change must be done inside the container:\noc exec -it $(oc get pods|grep recommendation-v2|awk \u0026#39;{ print $1 }\u0026#39;|head -1) -c recommendation /bin/bash Inside the container use the following command and exit the container again\ncurl localhost:8080/misbehave Kiali will now show that v1 will get 100% of the traffic, while v2 is shown as red. When you select the red square of v2 and then move the mouse over the red cross for the failing application, you will see that the pd itself is ready, but that 100% of the traffic is currently failing.\nFigure 3. Traffic for v2 is failing revert the change and fix v2 service\noc exec -it $(oc get pods|grep recommendation-v2|awk \u0026#39;{ print $1 }\u0026#39;|head -1) -c recommendation /bin/bash curl localhost:8080/behave Verify in Kiali that everything is \u0026#34;green\u0026#34; again and that the traffic is split by 50% between v1 and v2.\nSources [1]: Istio By Example - Fault Injection\n"},{"uri":"https://blog.stderr.at/tags/do410/","title":"DO410","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/2020/04/do410-ansible-and-ansible-tower-training-notes/","title":"DO410 Ansible and Ansible Tower training notes","tags":["Ansible","Ansible Tower","DO410"],"description":"Notes taken during the DO410 online training course","content":" Notes taken during Red Hat course D410 Ansible and Ansible Tower.\nAnsible installation make sure that libselinux-python is installed\nAnsible 2.7 requires python 2.6 or 3.5\nyum list installed python windows modules implemented in powershell\nansible requires at least .net 4.0\nConfiguration files Ansible searches for ansible.cfg in the following order:\n$ANSIBLE_CFG\nansible.cfg in the current directory\n$HOME/ansible.cfg\n/etc/ansible/ansible.cfg\nwhichever it finds first will be used.\nuse\nansible --version to see which config file is currently used. you can view/dump/see what changed with\nansible-config [list|dump|view] Default modules List all available modules via\nansible-doc -l For getting help on a specific module use\nansible-doc ping Ad-hoc commmands To display ansible output on a single line per host for easier readablility use the -o option\nansible all -m command -a /bin/hostname -o Use the raw module for directly executing commands on remote systems that do not have python installed.\nansible -m raw Custom Facts Ansible uses custom facts from /etc/ansible/facts.d/. Facts can be stored in .ini style or you can place executable scripts in this directory. The script needs to output JSON. Custom facts are available via ansible_facts.ansible_local.\nMagic variables available hostvars: variables defined for this host\ngroup_names: list of groups this host is a member of\ngroups: list of all groups and hosts in the inventory\ninventory_hostname: host name of the current host as configured in the inventory\nMatching hosts in the inventory Some examples on how to match hosts defined in the inventory\n\u0026#39;*.lab.com\u0026#39;: match all hosts starting with lab.com\n\u0026#39;lab,datacenter\u0026#39;: match all hosts either in lab or datacenter\n\u0026#39;datacenter*\u0026#39;: match all host and host groups starting with datacenter\n\u0026#39;lab,\u0026amp;datacenter\u0026#39;: match hosts in the lab and datacenter group\n\u0026#39;datacenter,!test.lab.com\u0026#39;: match all hosts in datacenter, except test.lab.com\nDynamic inventory Example scripts for dynamic inventories can be found at https://github.com/ansible/ansible/tree/devel/contrib/inventory.\nYou can use ansible-inventory to take a look a the current inventory as json. This also works for static inventories.\nInventories can be combined. Just create a directory containing a static inventory and script to create a dynamic inventory, ansible will happily execute the scripts and merge everything together.\nDebugging The following might be useful when debugging ansible roles and playbooks\nansible-playbook play.yml --syntax-check ansible-playbook play.yml --step ansible-playbook play.yml --start-at-task=\u0026#34;start httpd service\u0026#34; ansible-playbook --check play.yml ansible-playbook --check --diff play.yml Ansible Tower Notes on deploying and working with ansible tower.\nInstallation System requirements:\nat least 4GB of RAM\nactual requirement depends on forks variable\nrecommendation is 100MB memory for each for + 2GB of memory for tower services\n20GB of disk storage, at least 10GB in /var\nSteps for installing:\ndownload setup tar.gz from http://releases.ansible.com/ansible-tower/setup/\nset passwords in inventory\nrun ./setup.sh\nAuthentication Authentication settings can be changed under Settings / Authentication. E.g for configuring Azure AD authentication we are going to need\nan Azure AD oauth2 key and\na Azure AD oauth2 secret\nRBAC separate roles for organizations and inventories\nyou need to assign roles to organizations and inventories\nThe Tower Flow These are the steps to run playbooks against managed nodes in Tower:\nCreate an organization if required\nCreate users\nCreate teams and assign users\nCreate credentials for accessing managed nodes\nAssign credential to organization\nCreate credentials for accessing SCM repositories (e.g. git)\nAssign credentials to users or teams\nCreate a project\nAssign Teams to project\nCreate a job template for executing playbooks\nAnsible Roles support If the project includes a requirements.txt file in the roles/ folder, tower will automatically run\nansible-galaxy install -r roles/requirements.yml -p ./roles/ --force at the end of an update. So this could be used to include external dependencies (like SAP ansible roles).\nJob Templates Ansible playbooks are stored in GIT repositories. A job template defines\nthe inventory used for this job template\nthe project for executing this job\nthis connects the GIT repository used in this project with the template\nthe playbook to execute\nthe credentials for executing jobs\npermissions for users / teams (e.g. admin, execute)\nTower creates jobs from those templates, which are ansible runs executed against managed nodes.\nFact Caching It might be a good idea to use the tower facts cache. To speed up playbook runs set gather_facts: no in the play. Then enable the facts cache in tower.\nIn tower settings set a timeout for the cache\nIn job templates enable Use facts cache\nCreate a playbook that runs on a regular basis to gather facts, e.g.\n- name: Refresh fact cache hosts: all gather_facts: yes Inventory options These are the options for creating inventories in Ansible Tower\nstatic inventory defined in tower\nimporting static inventories via awx-manage\nstatic inventory defined in git repository\ndynamic inventory via a custom script\ndynamic inventory provides by tower (e.g. satellite)\nA special feature in Tower are so called smart inventories. A smart inventory combines all static and dynamic inventories and allows filtering based on facts. Filtering requires a valid fact cache.\nTroubleshooting Tower uses the following components:\npostgresql\nnginx\nmemcached\nrabbitmq\nsupervisord\nUseful tools\nansible-tower-service (e.g. status / restart)\nsupervisorctl (e.g. status)\nawx-manage\nTower stores log files in\n/var/log/tower/ (e.g. tower.log).\n/var/log/supervisor/\n/var/log/nginx/\nOther important directories\n/var/lib/awx/public/static static files served by django\n/var/lib/awx/projects stores all project related files e.g. git checkouts)\n/var/lib/awx/jobs_status job status output\nby default playbook runs are confined to /tmp this might lead to problems with tasks running on the local system. In case of a lost admin password you can use awx-manage to reset the password or create a new superuser:\nawx-manage changepassword admin awx-manage createsuperuser Replacing the default TLS certificates Ansible tower uses nginx to service it’s web interface over TLS. Nginx uses the configuration file /etc/nginx/nginx.conf.\nTo deploy custom TLS certificates used by tower replace the certificate and private key in /etc/tower. You have to replace\n/etc/tower/tower.crt and\n/etc/tower/tower.key\nIt might be a good idea to create a backup copy before overwriting those files.\nBackup and restore Of course backup and restore are done via ansible. The ansible tower setup script setup.sh provides a wrapper around these playbooks. Execute\nsetup.sh -b to perform a backup. This creates a backup .tar.gz file in the current directory.\nTo restore a backup use\nsetup.sh -r this restores the latest backup per default.\nThings to remember Workflow job templates\nadd autocmd FileType yaml setlocal ai ts=2 sw=2 et to .vimrc\nuse sudo yum install python-cryptography if there are many vault files to speed up ansible\n"},{"uri":"https://blog.stderr.at/tags/egress/","title":"Egress","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/limit-egress/external-traffic/","title":"Limit Egress/External Traffic","tags":["Istio","Service Mesh","OpenShift","OCP","Egress"],"description":"OpenShift 4.x and Service Mesh/istio Tutorial 7 - Test and control your egress/external traffic.","content":" Sometimes services are only available from outside the OpenShift cluster (like external API) which must be reached. Part 7 of OpenShift 4 and Service Mesh takes care and explains how to control the egress or external traffic. All operations have been successdully tested on OpenShift 4.3.\nPreparation Before this tutorial can be started, ensure that 3 microservices are deployed (recommendation may have 2 versions) and that the objects Gateway and VirtualService are configured. The status should be like in Issue #4..6\nYou can verify this the following way:\nexport GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) curl $GATEWAY_URL which should simply print:\ncustomer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 7123 Setup recommendation-v3 We need to deploy version 3 of our recommendation microservice. This will perform an external API call to http://worldclockapi.com to retrieve the current time.\nTo deploy the Deployment v3:\ncd ~/istio-tutorial/recommendation oc apply -f kubernetes/Deployment-v3.yml -n tutorial If you list the pods at this moment, you will see that only one container (Ready 1/1) is started. This happens because the Deployment yaml file is missing an annotation. Fixing missing proxy sidecar container After you applied the Deployment-v3.yml, only 1 container is started. The proxy sidecar is not injected, because an annotation is missing in the configuration for the Deployment.\nTo fix this use the following command:\noc patch deployment recommendation-v3 -n tutorial -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;sidecar.istio.io/inject\u0026#34;:\u0026#34;true\u0026#34;}}}}}\u0026#39; This will automatically restart the pod with 2 containers.\nCreate DestinationRule and VirtualService Use the following definition to create (overwrite) the DestinationRule for recommendation-v3.\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation subsets: - labels: version: v3 name: version-v3 Only version 3 is used for now. The other versions are still there, but ignored for our tests. Apply the change\noc apply -f DestinationRule_v3.yaml Define the VirtualService and send 100% of the traffic to v3 of the recommendation microservice.\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - route: - destination: host: recommendation subset: version-v3 weight: 100 As an alternative, you can also edit the existing VirtualService and add the section for version-v3 with a weight of 100, while changing the weight of v1 and v2 to 0. Test egress traffic As usual we test our application by sending traffic to it. The following command should print successful connection requests:\nsh ~/run.sh 1000 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 1 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 2 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 3 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 4 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 5 As you can see 100% of the traffic is sent to v3 AND a new field enters the output. The current time is now shown as well. The information for this field is fetched with an external API call to http://worldclockapi.com.\nThe traffic is simply sent to an external destination. There is not limit yet. Readers of the Istio documentation will miss the object ServiceEntry which somebody should think is required. However, Openshift is currently(?) configured in a way to simply allow ANY traffic. This is defined in a ConfigMap which might be changed to modify the default behavior. However, as soon as ServiceEntry and the appropriate VirtualService is configured, the traffic will be limited as well. Limit/Control external access As you can see above you can simply send egress traffic without any control about what is allowed or not. In order to limit your outgoing traffic a new object called ServiceEntry must be defined as well as a change in your VirtualService will be required.\nDefine the ServiceEntry and apply it to your cluster:\napiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: worldclockapi-egress-rule spec: hosts: - worldclockapi.com ports: - name: http-80 number: 81 (1) protocol: http 1 Wrong port 81 is set on purpose for demonstration The port number: 81 is set on purpose, to prove that the traffic will not work with a wrong ServiceEntry. oc create -f ServiceEntry.yaml To actually limit the traffic a link between the ServiceEntry and a VirtualService, which defines the external destination, must be created. Moreover, a timeout is set for possible connection errors, to keep the application responding even when the external API is down.\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: worldclockapi-timeout (1) spec: hosts: - worldclockapi.com (2) http: - timeout: 3s (3) route: - destination: host: worldclockapi.com weight: 100 (4) 1 The name of the object 2 The external hostname we want to reach 3 The timeout setting in seconds 4 The destination route, which is sending 100% of the external traffic to the host above oc apply -f VirtualService-worldclockapi.yaml If you now run a connection test you will still get an error.\nsh ~/run.sh 1 $GATEWAY_URL # customer =\u0026gt; Error: 503 - preference =\u0026gt; Error: 500 ... Fix ServiceEntry This happens, because we misconfigured the ServiceEntry on purpose to demonstrate that the traffic is sent to worldclockapi.com:80.\nFix the ServiceEntry object and apply to your cluster:\napiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: worldclockapi-egress-rule spec: hosts: - worldclockapi.com ports: - name: http-80 number: 80 (1) protocol: http 1 Changed from 81 to 80 oc apply -f ServiceEntry.yaml Now the traffic should work and gives you back a connection to microservice and a current time:\nsh ~/run.sh 10 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 138 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 139 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 140 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 141 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 142 # 5: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 143 # 6: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 144 Verify Kiali Figure 1. Kiali shows traffic to the external service OPTIONAL: Disallow ANY connections This is a change in the default ConfigMap of the ServiceMesh. Do this on your own risk and always consult the latest documentation of OCP. As explained above, we are able to connect to an external service without any limitation. The ServiceEntry object together with the VirtualService define the actual destination and would disallow traffic if they are wrongly configured, but if you forget these entries, it would still be possible to establish an egress connection.\nIn OpenShift a ConfigMap in the istio-system namespace defines the default behavior. There are two possibilities:\nALLOW_ANY - outbound traffic to unknown destinations will be allowed, in case there are no services or ServiceEntries for the destination port\nREGISTRY_ONLY - restrict outbound traffic to services defined in the service registry as well\nLet’s Cleanup the ServiceEntry and the VirtualService which have been created above\noc delete serviceentry worldclockapi-egress-rule serviceentry.networking.istio.io \u0026#34;worldclockapi-egress-rule\u0026#34; deleted oc delete virtualservice worldclockapi-timeout virtualservice.networking.istio.io \u0026#34;worldclockapi-timeout\u0026#34; deleted Now traffic to the external service will be allowed again Modify the ConfigMap istio in the namespace istio-system\noc get configmap istio -n istio-system -o yaml | sed \u0026#39;s/mode: ALLOW_ANY/mode: REGISTRY_ONLY/g\u0026#39; | oc replace -n istio-system -f - Wait a few seconds and try to connect. You will see that the connection is not possible anymore.\nIf you now re-create the ServiceEntry the connection will be possible again, since the service is registered to the Service Mesh. "},{"uri":"https://blog.stderr.at/service-mesh/2020/04/advanced-routing-example/","title":"Advanced Routing Example","tags":["Istio","Service Mesh","OpenShift","OCP","Grayscale","Canary","DestinationRule","Mirror","Loadbalancer"],"description":"OpenShift 4.x and Service Mesh/istio Tutorial 6 - Advanced Routing. Try routing traffic based on canary, mirroring or loadbalancing traffic.","content":" Welcome to part 6 of OpenShift 4 and Service Mesh Advanced routing, like Canary Deployments, traffic mirroring and loadbalancing are discussed and tested. All operations have been successdully tested on OpenShift 4.3.\nAdvanced Routing During Issue #5 some simple routing was implemented. The traffic was split by 100% to a new version (v2) of the recommendation microservice. This section shall give a brief overview of advanced routing possibilities.\nCanary Deployments A canary deployment is a strategy to roll out a new version of your service by using traffic splitting. A small amount of traffic (10%) will be sent to the new version, while most of the traffic will be sent to the old version still. The traffic to the new version can be analysed and if everything works as expected more and more traffic can be sent to the new version.\nTo enable split traffic, the VirtualService must be update:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - route: - destination: host: recommendation subset: version-v1 weight: 90 - destination: host: recommendation subset: version-v2 weight: 10 Apply the change\noc apply -f VitualService_split_v1_and_v1.yaml Test the traffic and verify that 10% will be sent to v2\nsh ~/run.sh 100 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1060 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1061 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 2060 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1062 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1063 ... If an error is shown, then you most probably forget to configure the DestinationRule as described here. Figure 1. Kiali split traffic 90/10 Routing based on user-agent header It is possible to send traffic to different versions based on the browser type which is calling the application. In our test application the service customer is setting the header baggage-user-agent and propagates it to the other services.\n\u0026gt;\u0026gt; headers.putSingle(\u0026#34;baggage-user-agent\u0026#34;, userAgent); Create the following file\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - match: - headers: baggage-user-agent: regex: .*Safari.* route: - destination: host: recommendation subset: version-v2 - route: - destination: host: recommendation subset: version-v1 and apply the change\noc apply -f VitualService_safari.yaml In order to test the result, either use the appropriate browser or use curl to set the user-agent. As expected, request from Safari are sent to v2, other are sent to v1.\nSafari\ncurl -v -A Safari $GATEWAY_URL [...] \u0026gt; User-Agent: Safari [...] customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 2365 Firefox\ncurl -v -A Firefox $GATEWAY_URL [...] \u0026gt; User-Agent: Firefox [...] customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3762 Mirroring Traffic Mirroring Traffic, aka Dark Launch, will duplicate the traffic to another service, allowing you to analyse it before sending production data to it. Responses of the mirrored requests are ignored.\nRun the following command and be sure that recommendation-v1 and recommendation-v2 are both running:\noc get pod -n tutorial| grep recommendation recommendation-v1-69db8d6c48-h8brv 2/2 Running 0 24h recommendation-v2-6c5b86bbd8-jnk8b 2/2 Running 0 23h Update the VirtualService, so that version v2 will receive mirrored traffic, while the actual request will be sent to v1:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - route: - destination: host: recommendation subset: version-v1 mirror: (1) host: recommendation subset: version-v2 1 This must be set to \u0026#39;mirror\u0026#39; Apply the change\noc apply -f VitualService_mirrored-traffic.yaml Now lets open and follow the logs of recommandation-v2 in order to see that traffic will reach this service, but responses are ignored:\noc logs -f $(oc get pods|grep recommendation-v2|awk \u0026#39;{ print $1 }\u0026#39;) -c recommendation In a second terminal window send some traffic to our service.\nsh ~/run.sh 100 $GATEWAY_URL You will see that only v1 answers, while in the 2nd window, v2 gets the same traffic.\nLoad Balancing In the default OpenShift environment the kube-proxy forwards all requests to pods randomly. With Red Hat ServiceMesh it is possible to add more complexity and let the Envoy proxy handle load balancing for your services.\nThree methods are supported:\nrandom\nround-robin\nleast connection\nThe round robin function is used by default, when there is no DestinationRule configured. We can use the DestinationRule to use the least connection option to see how the traffic is sent.\nBefore we start we need to delete the VirtualService for the recommendation microservice\noc delete virtualservice recommendation The we scale version v2 to 3:\noc scale deployment recommendation-v2 --replicas=3 After a few seconds the folling pods should run now:\nNAME READY STATUS RESTARTS AGE customer-6948b8b959-jdjlg 2/2 Running 1 25h preference-v1-7fdb89c86b-nktqn 2/2 Running 0 25h recommendation-v1-69db8d6c48-h8brv 2/2 Running 0 25h recommendation-v2-6c5b86bbd8-6lgz6 2/2 Running 0 91s recommendation-v2-6c5b86bbd8-dnc8b 2/2 Running 0 91s recommendation-v2-6c5b86bbd8-jnk8b 2/2 Running 0 24h If you send traffic to the application, you would see that 3 quarter are sent to v1 and one is sent to v1.\nWith the following DestinationRule the traffic will be sent randomly to the application\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation trafficPolicy: loadBalancer: simple: RANDOM If you now sent traffic to the service, you will see that the traffic is sent randomly to the versions. (verify the serial number)\nsh ~/run.sh 100 $GATEWAY_URL # 140: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 5729 # 141: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 7119 # 142: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 361 # 143: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 362 # 144: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 5730 # 145: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 362 # 146: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 7120 # 147: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 7121 # 148: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 363 ... "},{"uri":"https://blog.stderr.at/tags/canary/","title":"Canary","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/destinationrule/","title":"DestinationRule","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/grayscale/","title":"Grayscale","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/loadbalancer/","title":"Loadbalancer","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/mirror/","title":"Mirror","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2020/04/helpful-oc-/-kubectl-commands/","title":"Helpful oc / kubectl commands","tags":["oc","kubectl","OpenShift","OCP"],"description":"Openshift 4.x - Collection of usefull oc/kubectl commands.","content":" This is a list of useful oc and/or kubectl commands so they won’t be forgotton. No this is not a joke…​\nList all pods in state Running oc get pods --field-selector=status.phase=Running List all pods in state Running and show there resource usage oc get pods --field-selector=status.phase=Running -o json|jq \u0026#34;.items[] | {name: .metada ta.name, res: .spec.containers[].resources}\u0026#34; List events sort by time created oc get events --sort-by=\u0026#39;.lastTimestamp\u0026#39; Explain objects while specifing the api version Sometimes when you run oc explain you get a message in DESCRIPTION that this particular version is deprecated, e.g. you are running oc explain deployment and get\nDESCRIPTION: DEPRECATED - This group version of Deployment is deprecated by apps/v1/Deployment. See the release notes for more information. Deployment enables declarative updates for Pods and ReplicaSets. Note the DEPRECTATED message above if you want to see the documentation for the object that has not been deprecated you can use\noc explain deployment --api-version=apps/v1 Magic with oc set oc set is actually a very versatile command. Studying oc set -h is a good idea, here are some examples\nSet route weights when alternateBackends in a route are defined oc set route-backends bluegreen blue=1 green=9 Set resources on the command line oc set resources dc cakephp-mysql-example --limits=memory=1Gi,cpu=200m "},{"uri":"https://blog.stderr.at/service-mesh/2020/04/routing-example/","title":"Routing Example","tags":["Istio","Service Mesh","OpenShift","OCP","Grayscale","Canary","DestinationRule"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 5 - Routing Examples. Learn about routing and create your first definitions for VirtualService and DesitnationRule","content":" In part 5 of the OpenShift 4 and Service Mesh tutorials, basic routing, using the objects VirtualService and DesitnationRule, are described. All operations have been successfully tested on OpenShift 4.3.\nSome Theory In this section another version of the recommendation microservice will be deployed. The traffic to the new version will be controlled with different settings of the VirtualService. Multiple scenarios can be realized with ServiceMesh. In general these are defined as follows ([1]):\nBlue-Green Deployments In a Blue-Green deployment the old version (green) is kept running, while a new version (blue) is deployed and tested. When testing is successful, the 100% of the traffic is switched to the new version. If there is any error, the traffic could be switched back to the green version.\nA/B Deployments A/B deployments, in difference to Blue-Green deployments, will enable you to try a new version of the application in a limited way in the production environment. It is possible to specify that the production version gets most of the user requests, while a limited number of requests is sent to the new version. This could be specified by location of the user for example, so that all users from Vienna are sent to the new version, while all others are still using the old version.\nCanary Deployments Canary releases can be used to allow a small, minimum amount of traffic to the new version of your application. This traffic can be increased gradually until all traffic is sent to the new version. If any issues are found, you can roll back and send the traffic to the old version.\nPrerequisites It is assumed that an OpenShift environment is up and running and that Issues #1 - #3 are done at least:\nOpenshift 4 and ServiceMesh 1 - Installation\nOpenshift 4 and ServiceMesh 2 - Deploy Microservices\nOpenshift 4 and ServiceMesh 3 - Ingress Traffic\nPrepare Simple Routing OPTIONAL: Build the recommendation microservice If you want to locally build the microservice, you must change the source code from version v1 to v2 the following way:\nOpen the file:\nistio-tutorial/recommendation/java/vertx/src/main/java/com/redhat/developer/demos/recommendation/RecommendationVerticle.java and change the following line from v1 to v2\nprivate static final String RESPONSE_STRING_FORMAT = \u0026#34;recommendation v2 from \u0026#39;%s\u0026#39;: %d\\n\u0026#34;; Now you can build the image:\ncd istio-tutorial/recommendation/java/vertx mvn package podman build -t example/recommendation:v2 . (1) 1 Note the v2 tag Create second deployment with version2 A deployment with our recommendation:v2 microservice must be created. A service object must not be created this time, as it already exists.\ncd ~/istio-tutorial/recommendation/ oc apply -f kubernetes/Deployment-v2.yml -n tutorial oc get pods -w If you want to diff v1 and v2 deployment, you will notice that the main change is the image which gets pulled.\ndiff recommendation/kubernetes/Deployment-v2.yml recommendation/kubernetes/Deployment.yml 6,7c6,7 \u0026lt; version: v2 \u0026lt; name: recommendation-v2 --- \u0026gt; version: v1 \u0026gt; name: recommendation-v1 13c13 \u0026lt; version: v2 --- \u0026gt; version: v1 18c18 \u0026lt; version: v2 --- \u0026gt; version: v1 27c27 \u0026lt; image: quay.io/rhdevelopers/istio-tutorial-recommendation:v2.1 --- \u0026gt; image: quay.io/rhdevelopers/istio-tutorial-recommendation:v1.1 Call application Execute the test command to access the application. Since no rules are defined yet, the traffic is split by 50% to version 1 and version 2 (round robin):\nsh ~/run.sh 10 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 27 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 27 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 28 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 28 ... In Kiali presents this as well:\nFigure 1. Kiali sends 50% to v1 and v2 Send all traffic to recommendation:v2 To route the traffic accordingly a DestinationRule and a VirtualService must be created for recommendation. While the DesinationRule will add a name to each version, VirtualService specifies the actual destination of the traffic.\nDefine DestinationRule for recommendation The object DestinationRule will define the versions in subsets.\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation subsets: - labels: version: v1 name: version-v1 - labels: version: v2 name: version-v2 Create the object with the command: oc create -f \u0026lt;filename\u0026gt;\nDefine VirtualService for recommendation The VirtualService defines that 100% (weight) of the traffic for recomendation (host) will be sent to the subset (version-v2), which is defined in the DefinationRule\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - route: - destination: host: recommendation subset: version-v2 weight: 100 Create the object with the command: oc create -f \u0026lt;filename\u0026gt;\nCall application If you now call the application, only traffic to v2 should be shown:\nsh ~/run.sh 1000 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 27 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 27 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 28 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 28 ... In Kiali presents this as well and send 100% of the traffic to recommendation:v2:\nFigure 2. Kiali sends 100% to v2 Sources [1]: DZone: Traffic Management With Istio\n"},{"uri":"https://blog.stderr.at/service-mesh/2020/03/ingress-with-custom-domain/","title":"Ingress with custom domain","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 4 - Use a custom domain to get your traffic into your Service Mesh","content":" Since Service Mesh 1.1, there is a better way to achieve the following. Especially the manual creation of the route is not required anymore. Check the following article to Enable Automatic Route Creation. Often the question is how to get traffic into the Service Mesh when using a custom domains. Part 4 our our tutorials series OpenShift 4 and Service Mesh will use a dummy domain \u0026#34;hello-world.com\u0026#34; and explains the required settings which must be done.\nModify Gateway and VirtualService Issue #3 explains how to get ingress traffic into the Service Mesh, by defining the Gateway and the VirtualService. We are currently using the default ingress route defined in the istio_system project. But what if a custom domain shall be used? In such case another route must be defined in the istio-system project and small configuration changes must be applied.\nFirst lets create a slightly modified Gateway.yaml:\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: ingress-gateway-exampleapp spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;hello-world.com\u0026#34; (1) 1 add you custom domain here The only difference is at the hosts which was changed from \u0026#39;*\u0026#39; to \u0026#39;hello-world.com\u0026#39;\nAs second change, the VirtualService must be modified as well with the custom domain:\nVirtualService.yaml:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: ingress-gateway-exampleapp spec: hosts: - \u0026#34;hello-world.com\u0026#34; (1) gateways: - ingress-gateway-exampleapp http: - match: - uri: exact: / route: - destination: host: customer port: number: 8080 1 add you custom domain here Replace current objects in OpenShift:\noc replace -f Gateway.yaml -n tutorial oc replace -f VirtualService.yaml -n tutorial Create a new route under the project istio-system:\napiVersion: route.openshift.io/v1 kind: Route metadata: name: hello-world.com (1) namespace: istio-system (2) spec: host: hello-world.com (3) to: kind: Service name: istio-ingressgateway (4) port: targetPort: 8080 1 add you custom domain here 2 the route must be created at istio-system 3 add you custom domain here 4 this is the service as it was created by the operator OPTIONAL: Add custom domain to local hosts file The custom domain hello-world.com must be resolvable somehow, pointing to the ingress router of OpenShift. This can be done, by adding the domain into the local hosts file (with all limitations this brings with it)\n# Get IP address of: oc -n istio-system get route istio-ingressgateway echo \u0026#34;x.x.x.x hello-world.com\u0026#34; \u0026gt;\u0026gt; /etc/hosts Create some example traffic We will reuse the script of Issue #3 to simulate traffic. Since we changed the domain, the connection will go to hello-world.com\nsh run-check.sh 1000 hello-world.com This will send 1000 requests to our application:\n# 0: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6626 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6627 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6628 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6629 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6630 # 5: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6631 ... "},{"uri":"https://blog.stderr.at/service-mesh/2020/03/ingress-traffic/","title":"Ingress Traffic","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 3 - Get traffic into your Service Mesh and use Kiali to make your traffic visible.","content":" Part 3 of tutorial series OpenShift 4 and Service Mesh will show you how to create a Gateway and a VirtualService, so external traffic actually reaches your Mesh. It also provides an example script to run some curl in a loop.\nConfigure Gateway and VirtualService Example With the microservices deployed during Issue #2, it makes sense to test the access somehow. In order to bring traffic into the application a Gateway object and a VirtualService object must be created.\nThe Gateway will be the entry point which forward the traffic to the istio ingressgateway\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: ingress-gateway-exampleapp spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; As 2nd object a VirtualService must be created:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: ingress-gateway-exampleapp spec: hosts: - \u0026#34;*\u0026#34; gateways: - ingress-gateway-exampleapp http: - match: - uri: exact: / route: - destination: host: customer port: number: 8080 Get all istio-io related objects of your project. These objects represent the network objects of Service Mesh, like Gateway, VirtualService and DestinationRule (explained later)\noc get istio-io -n tutorial NAME HOST AGE destinationrule.networking.istio.io/recommendation recommendation 3d21h NAME AGE gateway.networking.istio.io/ingress-gateway 4d15h NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/ingress-gateway [ingress-gateway] [*] 4d15h Create some example traffic Before we start, lets fetch the default route of our Service Mesh:\nexport GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) This should return: istio-ingressgateway-istio-system.apps.\u0026lt;clustername\u0026gt;\nNow, let’s create a shell script to run some curl commands in a loop and can be easily reused for other scenarios:\n#!/bin/bash numberOfRequests=$1 host2check=$2 if [ $# -eq 0 ]; then echo \u0026#34;better define: \u0026lt;script\u0026gt; #ofrequests hostname2check\u0026#34; echo \u0026#34;Example: run.sh 100 hello.com\u0026#34; let \u0026#34;numberOfRequests=100\u0026#34; else let \u0026#34;i = 0\u0026#34; while [ $i -lt $numberOfRequests ]; do echo -n \u0026#34;# $i: \u0026#34;; curl $2 let \u0026#34;i=$((i + 1))\u0026#34; done fi Run the script and check the output:\nsh run-check.sh 1000 $GATEWAY_URL This will send 1000 requests to our application:\n# 0: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3622 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3623 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3624 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3625 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3626 # 5: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3627 ... Verify in Kiali To verify in Kiali our application, open the URL in your browser and login using your OpenShift credentials.\nIf you do not know the URL for Kiali, execute the following command oc get route kiali -n istio-system Switch the the Graph view and you should see the following picture:\nFigure 1. Kiali Graph "},{"uri":"https://blog.stderr.at/service-mesh/2020/03/deploy-microservices/","title":"Deploy Microservices","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 2 - Deploy some example microservices, which are used in future tutorials.","content":" The second tutorials explains how to install an example application containing thee microservices. All operations have been successfully tested on OpenShift 4.3.\nIntroduction As quickly explained at Issue #1, OpenShift 4.3 and the Service Mesh shall be installed already. At this point you should have all 4 operators installed and ready. The test application used in this scenario is based on Java and contains 3 microservices in the following traffic flow:\nCustomer ⇒ Preference ⇒ Recomandation\nThe microservices are the same as used at the Interactive Learning Portal.\nDeploy microservices First lets create a new project for our tests\noc new-project tutorial EITHER: Add tutorial to ServiceMeshMemberRoll\nService Mesh 1.1 now supports the object ServiceMember which can created under the application namespace and which automatically configures the ServiceMemberRoll. However, below description still works. + OpenShift will auto-inject the sidecar proxy to pods of a namespace, if the namespace is configured in the ServiceMeshMemberRoll object which was created for the operator.\nCreate the file memberroll.yaml:\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; memberroll.yaml apiVersion: maistra.io/v1 kind: ServiceMeshMemberRoll metadata: name: default spec: members: - tutorial EOF Add the namespace to the member roll:\noc apply -f memberroll.yaml -n istio-system If you already have namespaces configured for ServiceMeshMemberRoll, better modify the object manually. Custom Resource Definitions (CRD) do not like to be modified on the fly (currently?) OR: Create ServiceMeshMember object: Available since ServiceMesh 1.1\napiVersion: maistra.io/v1 kind: ServiceMeshMember metadata: name: default namespace: tutorial spec: controlPlaneRef: name: basic-install namespace: istio-system Download the tutorial application locally\ngit clone https://github.com/redhat-developer-demos/istio-tutorial/ Deploy microservice: customer To deploy the first microservice 2 objects (Deployment and Service) must be created. Moreover, the service will be exposed, since the service customer is our entry point. Verify ~/istio-tutorial/customer/kubernetes/Deployment.yml to check where the actual image is coming from: quay.io/rhdevelopers/istio-tutorial-customer:v1.1\ncd ~/istio-tutorial/customer oc apply -f kubernetes/Deployment.yml -n tutorial oc apply -f kubernetes/Service.yml -n tutorial oc expose service customer Check if pods are running with two containers:\noc get pods -w The result should look somehow like this:\nNAME READY STATUS RESTARTS AGE customer-6948b8b959-g77bs 2/2 Running 0 52m If there is only 1 container running, indicated by READY = 1/1, then most likely the ServiceMeshMemberRoll was not updated with the name tutorial or contains a wrong project name. Deploy microservice: preference The deployment of the microservice preference is exactly like it is done for customer, except that no service must be exposed:\ncd ~/istio-tutorial/preference/ oc apply -f kubernetes/Deployment.yml -n tutorial oc apply -f kubernetes/Service.yml -n tutorial Check if pods are running with two containers:\noc get pods -w The result should look somehow like this:\nNAME READY STATUS RESTARTS AGE customer-6948b8b959-g77bs 2/2 Running 0 4d15h preference-v1-7fdb89c86b-gkk5g 2/2 Running 0 4d14h Deploy microservice: recommendation The deployment of the microservice recommendation is exactly like it is done for preference:\ncd ~/istio-tutorial/recommendation/ oc apply -f kubernetes/Deployment.yml -n tutorial oc apply -f kubernetes/Service.yml -n tutorial Check if pods are running with two containers:\noc get pods -w The result should look somehow like this:\nNAME READY STATUS RESTARTS AGE customer-6948b8b959-g77bs 2/2 Running 0 4d15h preference-v1-7fdb89c86b-gkk5g 2/2 Running 0 4d14h recommendation-v1-69db8d6c48-p9w2b 2/2 Running 0 4d14h Optional: build the images It is possible (and probably a good training) to build the microservices locally to understand how this works. In order to achieve this the packages maven and podman must be installed.\nBuild customer Go to the source folder of customer application and build it:\ncd ~/projects/istio-tutorial/customer/java/springboot mvn package It will take a few seconds, but it should give \u0026#34;BUILD SUCCESS\u0026#34; as output, if everything worked.\nNow the image will be built using podman\npodman build -t example/customer . Build preference The image build process of the second microservice follows the same flow as customer:\ncd ~/istio-tutorial/preference/java/springboot mvn package podman build -t example/preference:v1 . the \u0026#34;v1\u0026#34; tag at the image name is important and must be used. Build recommendation The image build process of the third microservice follows the same flow as preference:\ncd ~/projects/istio-tutorial/recommendation/java/vertx mvn package podman build -t example/recommendation:v1 . the \u0026#34;v1\u0026#34; tag at the image name is important and must be used. Later other versions will be deployed. "},{"uri":"https://blog.stderr.at/service-mesh/2020/03/installation/","title":"Installation","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 1 - Installation and First Steps","content":" Everything has a start, this blog as well as the following tutorials. This series of tutorials shall provide a brief and working overview about OpenShift Service Mesh. It is starting with the installation and the first steps, and will continue with advanced settings and configuration options.\nOpenShift 4.x and ServiceMesh UPDATE: At 10th April 2020 Red Hat released Service Mesh version 1.1 which supports: Istio - 1.4.6\nKiali - 1.12.7\nJaeger - 1.17.1\nThe following tutorials for OpenShift Service Mesh are based on the official documentation: OpenShift 4.3 Service Mesh and on the Interactive Learning Portal. All operations have been successfully tested on OpenShift 4.3. Currently OpenShift supports Istio 1.4.6, which shall be updated in one of the future releases.\nTo learn the basics of Service Mesh, please consult the documentation as they are not repeated here.\nIt is assumed that OpenShift has access to external registries, like quay.io.\nOther resource I can recommend are:\nIstio By Example: A very good and brief overview of different topics by Megan O’Keefe.\nIstio: The Istio documentation\nPrerequisites At the very beginning OpenShift must be installed. This tutorial is based on OpenShift 4.3 and a Lab installation on Hetzner was used. Moreover, it is assumed that the OpenShift Client and Git are installed on the local system.\nDuring the tutorials an example application in the namespace tutorial will be deployed. This application will contain 3 microservices:\ncustomer (the entry point)\npreference\nrecommendation\nThis application is also used at the Interactive Learning Portal which can be tested there interactively. However, the training is still based on OpenShift version 3.\nInstall Red Hat Service Mesh To deploy Service Mesh on Openshift 4 follow the guide at Installing Red Hat OpenShift Service Mesh.\nIn short, multiple operators must be installed:\nElasticsearch\nJaeger\nKiali\nService Mesh\nElasticsearch is a very memory intensive application. Per default it will request 16GB of memory which can be reduced on Lab environments. Link Jaeger and Grafana to Kiali In the lab environment it happened that Kiali was not able to auto detect Grafana or Jaeger. This is visible when the link Distributed Tracing is missing in the left menu.\nTo fix this the ServiceMeshControlPlane object in the istio-system namespace must be updated with 3 lines:\noc edit ServiceMeshControlPlane -n istio-system kiali: # ADD THE FOLLOWING LINES dashboard: grafanaURL: https://grafana-istio-system.apps.\u0026lt;your clustername\u0026gt; jaegerURL: https://jaeger-istio-system.apps.\u0026lt;your clustername\u0026gt; This change will take a few minutes to be effective.\n"},{"uri":"https://blog.stderr.at/compliance/1/01/","title":"","tags":[],"description":"","content":"\u003c!DOCTYPE html\u003exccdf_org.open-scap_testresult_xccdf_org.ssgproject.content_profile_cis | OpenSCAP Evaluation ReportOpenSCAP Evaluation ReportGuide to the Secure Configuration of Red Hat OpenShift Container Platform 4with profile CIS Red Hat OpenShift Container Platform 4 BenchmarkThis profile defines a baseline that aligns to the Center for Internet Security® Red Hat OpenShift Container Platform 4 Benchmark™, V0.3, currently unreleased. This profile includes Center for Internet Security® Red Hat OpenShift Container Platform 4 CIS Benchmarks™ content. Note that this part of the profile is meant to run on the Platform that Red Hat OpenShift Container Platform 4 runs on top of. This profile is applicable to OpenShift versions 4.6 and greater.The ComplianceAsCode Project\nhttps://www.open-scap.org/security-policies/scap-security-guide This guide presents a catalog of security-relevant configuration settings for Red Hat OpenShift Container Platform 4. It is a rendering of content structured in the eXtensible Configuration Checklist Description Format (XCCDF) in order to support security automation. The SCAP content is is available in the scap-security-guide package which is developed at https://www.open-scap.org/security-policies/scap-security-guide. Providing system administrators with such guidance informs them how to securely configure systems under their control in a variety of network roles. Policy makers and baseline creators can use this catalog of settings, with its associated references to higher-level security control catalogs, in order to assist them in security baseline creation. This guide is a catalog, not a checklist, and satisfaction of every item is not likely to be possible or sensible in many operational scenarios. However, the XCCDF format enables granular selection and adjustment of settings, and their association with OVAL and OCIL content provides an automated checking capability. Transformations of this document, and its associated automated checking content, are capable of providing baselines that meet a diverse set of policy objectives. Some example XCCDF Profiles, which are selections of items that form checklists and can be used as baselines, are available with this guide. They can be processed, in an automated fashion, with tools that support the Security Content Automation Protocol (SCAP). The NIST National Checklist Program (NCP), which provides required settings for the United States Government, is one example of a baseline created from this guidance. Do not attempt to implement any of the settings in this guide without first testing them in a non-operational environment. The creators of this guidance assume no responsibility whatsoever for its use by other parties, and makes no guarantees, expressed or implied, about its quality, reliability, or any other characteristic. Evaluation CharacteristicsEvaluation targetocp4-cis-api-checks-podBenchmark URL/content/ssg-ocp4-ds.xmlBenchmark IDxccdf_org.ssgproject.content_benchmark_OCP-4Benchmark version0.1.57Profile IDxccdf_org.ssgproject.content_profile_cisStarted at2021-07-20T05:20:59+00:00Finished at2021-07-20T05:21:00+00:00Performed byTest systemcpe:/a:redhat:openscap:1.3.4CPE Platformscpe:/a:redhat:openshift_container_platform:4.1cpe:/a:redhat:openshift_container_platform:4.7cpe:/o:redhat:openshift_container_platform_node:4cpe:/a:redhat:openshift_container_platform:4.6cpe:/a:redhat:openshift_container_platform:4.8cpe:/a:redhat:openshift_container_platform:4.9cpe:/a:redhat:openshift_container_platform:4.10AddressesIPv4 127.0.0.1IPv4 10.130.0.19IPv6 0:0:0:0:0:0:0:1IPv6 fe80:0:0:0:858:aff:fe82:13MAC 00:00:00:00:00:00MAC 0A:58:0A:82:00:13Compliance and ScoringThe target system did not satisfy the conditions of 4 rules! Please review rule results and consider applying remediation. Rule results61 passed 4 failed 28 other Severity of failed rules0 other 0 low 4 medium 0 high ScoreScoring systemScoreMaximumPercenturn:xccdf:scoring:default89.285721100.00000089.29%Rule OverviewpassfixedinformationalfailerrorunknownnotcheckednotapplicableSearch\nGroup rules by: DefaultSeverityResult──────────NIST SP 800-53https://www.cisecurity.org/benchmark/kubernetes/TitleSeverityResultGuide to the Secure Configuration of Red Hat OpenShift Container Platform 4 4x fail 28x notcheckedOpenShift Settings 4x fail 28x notcheckedOpenShift - Account and Access Control 2x notcheckedRestrict Automounting of Service Account TokensmediumnotcheckedEnsure Usage of Unique Service Accounts mediumnotcheckedOpenShift - Kubernetes - Scheduler SettingsEnsure that the bind-address parameter is not usedmediumpassOpenShift Secrets Management 2x notcheckedConsider external secret storagemediumnotcheckedDo Not Use Environment Variables with SecretsmediumnotcheckedOpenShift Kube API Server 3x fail 2x notcheckedConfigure the Client Certificate Authority for the API ServermediumpassConfigure the API Server Minimum Request TimeoutmediumpassEnsure that the Admission Control Plugin AlwaysPullImages is not sethighpassEnsure that the bindAddress is set to a relevant secure portlowpassConfigure the Service Account Public Key for the API ServermediumpassEnsure all admission control plugins are enabledmediumpassEnsure that Audit Log Forwarding Is EnabledmediumfailUse Strong Cryptographic Ciphers on the API ServermediumpassEnsure authorization-mode RBAC is configuredmediumpassConfigure the Certificate for the API ServermediumpassEnsure that the --kubelet-https argument is set to truemediumpassConfigure Kubernetes API Server Maximum Audit Log SizemediumpassConfigure the kubelet Certificate File for the API ServerhighpassDisable basic-auth-file for the API ServermediumpassEnable the NodeRestriction Admission Control PluginmediumpassThe authorization-mode cannot be AlwaysAllowmediumpassEnsure that the service-account-lookup argument is set to truemediumpassEnsure authorization-mode Node is configuredmediumpassDisable Use of the Insecure Bind AddressmediumpassEnable the APIPriorityAndFairness feature gatemediumpassConfigure the kubelet Certificate Authority for the API ServerhighpassEnsure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not usedmediumpassConfigure the Encryption Provider CiphermediumfailProfiling is protected by RBACmediumpassEnsure that anonymous requests to the API Server are authorizedmediumpassEnsure the openshift-oauth-apiserver service uses TLSmediumnotcheckedConfigure the Certificate Key for the API ServermediumpassConfigure the Audit Log PathhighpassEnsure catch-all FlowSchema object for API Priority and Fairness Exists (v1alpha1)mediumpassConfigure the Encryption ProvidermediumfailConfigure the Kubernetes API Server Maximum Retained Audit LogslowpassPrevent Insecure Port AccessmediumpassConfigure the etcd Certificate for the API ServermediumpassConfigure the kubelet Certificate Key for the API ServerhighpassDisable Token-based AuthenticationhighpassEnable the ServiceAccount Admission Control PluginmediumpassConfigure OpenShift API Server Maximum Audit Log SizemediumpassDisable the AlwaysAdmit Admission Control PluginmediumpassEnable the NamespaceLifecycle Admission Control PluginmediumpassConfigure the etcd Certificate Key for the API ServermediumpassEnsure the openshift-oauth-apiserver service uses TLSmediumnotcheckedConfigure the OpenShift API Server Maximum Retained Audit LogslowpassConfigure the etcd Certificate Authority for the API ServermediumpassEnable the SecurityContextConstraint Admission Control PluginmediumpassEnsure catch-all FlowSchema object for API Priority and Fairness ExistsmediumnotapplicableSecurity Context Constraints (SCC) 9x notcheckedLimit Access to the Host IPC NamespacemediumnotcheckedLimit Container Running As Root UsermediumnotcheckedDrop Container CapabilitiesmediumnotcheckedLimit Access to the Host Process ID NamespacemediumnotcheckedLimit Access to the Host Network NamespacemediumnotcheckedLimit Containers Ability to Escalate PrivilegesmediumnotcheckedLimit Container CapabilitiesmediumnotcheckedLimit Privileged Container UsemediumnotcheckedLimit Use of the CAP_NET_RAWmediumnotcheckedAuthenticationConfigure An Identity ProvidermediumpassOpenShift - Master Node Settings 1x fail 2x notcheckedVerify Group Who Owns The Worker Proxy Kubeconfig FilemediumnotcheckedVerify Permissions on the Worker Proxy Kubeconfig FilemediumfailVerify User Who Owns The Worker Proxy Kubeconfig FilemediumnotcheckedOpenShift etcd SettingsEnsure That The etcd Key File Is Correctly SetmediumpassEnsure That The etcd Peer Client Certificate Is Correctly SetmediumpassEnable The Peer Client Certificate AuthenticationmediumpassEnsure That The etcd Peer Key File Is Correctly SetmediumpassEnsure That The etcd Client Certificate Is Correctly SetmediumpassEnable The Client Certificate AuthenticationmediumpassDisable etcd Peer Self-Signed CertificatesmediumpassDisable etcd Self-Signed CertificatesmediumpassOpenShift - General Security Practices 5x notcheckedCreate administrative boundaries between resources using namespacesmediumnotcheckedManage Image Provenance Using ImagePolicyWebhookmediumnotcheckedThe default namespace should not be usedmediumnotcheckedApply Security Context to Your Pods and ContainersmediumnotcheckedEnsure Seccomp Profile Pod DefinitionsmediumnotcheckedOpenShift API ServerConfigure the Audit Log PathhighpassOpenShift Controller SettingsEnsure Controller secure-port argument is setlowpassEnsure that use-service-account-credentials is enabledmediumpassEnsure Controller insecure port argument is unsetlowpassConfigure the Service Account Certificate Authority Key for the Controller ManagermediumpassConfigure the Service Account Private Key for the Controller ManagermediumpassEnsure that the RotateKubeletServerCertificate argument is setmediumpassKubernetes Kubelet SettingsEnsure That The kubelet Client Certificate Is Correctly Setmediumpasskubelet - Disable the Read-Only PortmediumpassEnsure That The kubelet Server Key Is Correctly SetmediumpassOpenShift - Logging SettingsEnsure that the cluster's audit profile is properly setmediumpassRole-based Acess Control 4x notcheckedMinimize Wildcard Usage in Cluster and Local RolesmediumnotcheckedLimit Access to Kubernetes SecretsmediumnotcheckedMinimize Access to Pod CreationmediumnotcheckedProfiling is protected by RBACmediumpassEnsure that the cluster-admin role is only used where requiredmediumnotcheckedNetwork Configuration and Firewalls 2x notcheckedEnsure that application Namespaces have Network Policies defined.highnotcheckedEnsure that the CNI in use supports Network PolicieshighnotcheckedShow all result detailsResult DetailsRestrict Automounting of Service Account Tokensxccdf_org.ssgproject.content_rule_accounts_restrict_service_account_tokens mediumRestrict Automounting of Service Account TokensRule IDxccdf_org.ssgproject.content_rule_accounts_restrict_service_account_tokensResultnotcheckedMulti-check rulenoTime2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.1.6, CM-6, CM-6(1)\nDescriptionService accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server. To ensure pods do not automatically mount tokens, set automountServiceAccountToken to false.RationaleMounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster.Evaluation messagesinfo No candidate or applicable check found.Ensure Usage of Unique Service Accounts xccdf_org.ssgproject.content_rule_accounts_unique_service_account mediumEnsure Usage of Unique Service Accounts Rule IDxccdf_org.ssgproject.content_rule_accounts_unique_service_accountResultnotcheckedMulti-check rulenoTime2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.1.5, CM-6, CM-6(1)\nDescriptionUsing the default service account prevents accurate application rights review and audit tracing. Instead of default, create a new and unique service account with the following command: $ oc create sa service_account_name where service_account_name is the name of a service account that is needed in the project namespace.RationaleKubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod. Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account. This increases auditability of service account rights and access making it easier and more accurate to trace potential malicious behaviors to a specific service account and project.Evaluation messagesinfo No candidate or applicable check found.Ensure that the bind-address parameter is not usedxccdf_org.ssgproject.content_rule_scheduler_no_bind_address mediumCCE-83674-2 Ensure that the bind-address parameter is not usedRule IDxccdf_org.ssgproject.content_rule_scheduler_no_bind_addressResultpassMulti-check rulenoOVAL Definition IDoval:ssg-scheduler_no_bind_address:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83674-2\nReferences: 1.4.2, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionThe Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface.RationaleIn OpenShift 4, The Kubernetes Scheduler operator manages and updates the Kubernetes Scheduler deployed on top of OpenShift. By default, the operator exposes metrics via metrics service. The metrics are collected from the Kubernetes Scheduler operator. Profiling data is sent to healthzPort, the port of the localhost healthz endpoint. Changing this value may disrupt components that monitor the kubelet health.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-scheduler/configmaps/kube-scheduler-pod API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-scheduler/configmaps/kube-scheduler-pod file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-scheduler/configmaps/kube-scheduler-pod' find only one object at path '.data[\"pod.yaml\"]'. oval:ssg-test_scheduler_no_bind_address:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-scheduler/configmaps/kube-scheduler-pod/kubernetes-api-resources/api/v1/namespaces/openshift-kube-scheduler/configmapskube-scheduler-pod.data[\"pod.yaml\"] {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"openshift-kube-scheduler\",\"namespace\":\"openshift-kube-scheduler\",\"creationTimestamp\":null,\"labels\":{\"app\":\"openshift-kube-scheduler\",\"revision\":\"REVISION\",\"scheduler\":\"true\"},\"annotations\":{\"kubectl.kubernetes.io/default-logs-container\":\"kube-scheduler\"}},\"spec\":{\"volumes\":[{\"name\":\"resource-dir\",\"hostPath\":{\"path\":\"/etc/kubernetes/static-pod-resources/kube-scheduler-pod-REVISION\"}},{\"name\":\"cert-dir\",\"hostPath\":{\"path\":\"/etc/kubernetes/static-pod-resources/kube-scheduler-certs\"}}],\"initContainers\":[{\"name\":\"wait-for-host-port\",\"image\":\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0286a17f0671310c1610508127f4730aab5c30048f5fd5af88c0e439eaf5ac30\",\"command\":[\"/usr/bin/timeout\",\"30\",\"/bin/bash\",\"-c\"],\"args\":[\"echo -n \\\"Waiting for port :10259 and :10251 to be released.\\\"\\nwhile [ -n \\\"$(ss -Htan '( sport = 10251 or sport = 10259 )')\\\" ]; do\\n echo -n \\\".\\\"\\n sleep 1\\ndone\\n\"],\"resources\":{\"requests\":{\"cpu\":\"15m\",\"memory\":\"50Mi\"}},\"terminationMessagePolicy\":\"FallbackToLogsOnError\",\"imagePullPolicy\":\"IfNotPresent\"}],\"containers\":[{\"name\":\"kube-scheduler\",\"image\":\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:0286a17f0671310c1610508127f4730aab5c30048f5fd5af88c0e439eaf5ac30\",\"command\":[\"hyperkube\",\"kube-scheduler\"],\"args\":[\"--config=/etc/kubernetes/static-pod-resources/configmaps/config/config.yaml\",\"--cert-dir=/var/run/kubernetes\",\"--port=0\",\"--authentication-kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/scheduler-kubeconfig/kubeconfig\",\"--authorization-kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/scheduler-kubeconfig/kubeconfig\",\"--feature-gates=APIPriorityAndFairness=true,LegacyNodeRoleBehavior=false,NodeDisruptionExclusion=true,RemoveSelfLink=false,RotateKubeletServerCertificate=true,SCTPSupport=true,ServiceNodeExclusion=true,SupportPodPidsLimit=true\",\"-v=2\",\"--tls-cert-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt\",\"--tls-private-key-file=/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key\"],\"ports\":[{\"containerPort\":10259}],\"resources\":{\"requests\":{\"cpu\":\"15m\",\"memory\":\"50Mi\"}},\"volumeMounts\":[{\"name\":\"resource-dir\",\"mountPath\":\"/etc/kubernetes/static-pod-resources\"},{\"name\":\"cert-dir\",\"mountPath\":\"/etc/kubernetes/static-pod-certs\"}],\"livenessProbe\":{\"httpGet\":{\"path\":\"healthz\",\"port\":10259,\"scheme\":\"HTTPS\"},\"initialDelaySeconds\":45},\"readinessProbe\":{\"httpGet\":{\"path\":\"healthz\",\"port\":10259,\"scheme\":\"HTTPS\"},\"initialDelaySeconds\":45},\"terminationMessagePolicy\":\"FallbackToLogsOnError\",\"imagePullPolicy\":\"IfNotPresent\"},{\"name\":\"kube-scheduler-cert-syncer\",\"image\":\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68ed6ae84719afd9c0d17f918a0c09212700f039b39e9b60d96a0624da7cc7ff\",\"command\":[\"cluster-kube-scheduler-operator\",\"cert-syncer\"],\"args\":[\"--kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-scheduler-cert-syncer-kubeconfig/kubeconfig\",\"--namespace=$(POD_NAMESPACE)\",\"--destination-dir=/etc/kubernetes/static-pod-certs\"],\"env\":[{\"name\":\"POD_NAME\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.name\"}}},{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"resources\":{\"requests\":{\"cpu\":\"5m\",\"memory\":\"50Mi\"}},\"volumeMounts\":[{\"name\":\"resource-dir\",\"mountPath\":\"/etc/kubernetes/static-pod-resources\"},{\"name\":\"cert-dir\",\"mountPath\":\"/etc/kubernetes/static-pod-certs\"}],\"terminationMessagePolicy\":\"FallbackToLogsOnError\",\"imagePullPolicy\":\"IfNotPresent\"},{\"name\":\"kube-scheduler-recovery-controller\",\"image\":\"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:68ed6ae84719afd9c0d17f918a0c09212700f039b39e9b60d96a0624da7cc7ff\",\"command\":[\"/bin/bash\",\"-euxo\",\"pipefail\",\"-c\"],\"args\":[\"timeout 3m /bin/bash -exuo pipefail -c 'while [ -n \\\"$(ss -Htanop \\\\( sport = 11443 \\\\))\\\" ]; do sleep 1; done'\\n\\nexec cluster-kube-scheduler-operator cert-recovery-controller --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-scheduler-cert-syncer-kubeconfig/kubeconfig --namespace=${POD_NAMESPACE} --listen=0.0.0.0:11443 -v=2\\n\"],\"env\":[{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"fieldPath\":\"metadata.namespace\"}}}],\"resources\":{\"requests\":{\"cpu\":\"5m\",\"memory\":\"50Mi\"}},\"volumeMounts\":[{\"name\":\"resource-dir\",\"mountPath\":\"/etc/kubernetes/static-pod-resources\"},{\"name\":\"cert-dir\",\"mountPath\":\"/etc/kubernetes/static-pod-certs\"}],\"terminationMessagePolicy\":\"FallbackToLogsOnError\",\"imagePullPolicy\":\"IfNotPresent\"}],\"hostNetwork\":true,\"tolerations\":[{\"operator\":\"Exists\"}],\"priorityClassName\":\"system-node-critical\"},\"status\":{}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-scheduler/configmaps/kube-scheduler-pod'). oval:ssg-test_file_for_scheduler_no_bind_address:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-scheduler/configmaps/kube-scheduler-podregular100065000010006500005262rw------- Consider external secret storagexccdf_org.ssgproject.content_rule_secrets_consider_external_storage mediumConsider external secret storageRule IDxccdf_org.ssgproject.content_rule_secrets_consider_external_storageResultnotcheckedMulti-check rulenoTime2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.4.2, CM-6, CM-6(1)\nDescriptionConsider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.RationaleKubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrets are used across both Kubernetes and non-Kubernetes environments.Evaluation messagesinfo No candidate or applicable check found.Do Not Use Environment Variables with Secretsxccdf_org.ssgproject.content_rule_secrets_no_environment_variables mediumDo Not Use Environment Variables with SecretsRule IDxccdf_org.ssgproject.content_rule_secrets_no_environment_variablesResultnotcheckedMulti-check rulenoTime2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.4.1, CM-6, CM-6(1)\nDescriptionSecrets should be mounted as data volumes instead of environment variables.RationaleEnvironment variables are subject and very susceptible to malicious hijacking methods by an adversary, as such, environment variables should never be used for secrets.Evaluation messagesinfo No candidate or applicable check found.Configure the Client Certificate Authority for the API Serverxccdf_org.ssgproject.content_rule_api_server_client_ca mediumCCE-84284-9 Configure the Client Certificate Authority for the API ServerRule IDxccdf_org.ssgproject.content_rule_api_server_client_caResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_client_ca:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84284-9\nReferences: 1.2.31, SC-8, SC-8(1), SC-8(2)\nDescriptionCertificates must be provided to fully setup TLS client certificate authentication. To ensure the API Server utilizes its own TLS certificates, the clientCA must be configured. Verify that servingInfo has the clientCA configured in the openshift-kube-apiserver config configmap to something similar to: \"apiServerArguments\": { ... \"client-ca-file\": [ \"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\" ], ... RationaleAPI Server communication contains sensitive parameters that should remain encrypted in transit. Configure the API Server to serve only HTTPS traffic. If -clientCA is set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_client_ca:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_client_ca:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Configure the API Server Minimum Request Timeoutxccdf_org.ssgproject.content_rule_api_server_request_timeout mediumConfigure the API Server Minimum Request TimeoutRule IDxccdf_org.ssgproject.content_rule_api_server_request_timeoutResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_request_timeout:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesReferences: 1.2.26, CM-6, CM-6(1)\nDescriptionThe API server minimum request timeout defines the minimum number of seconds a handler must keep a request open before timing it out. To set this, edit the openshift-kube-apiserver configmap and set min-request-timeout under the apiServerArguments field: \"apiServerArguments\":{ ... \"min-request-timeout\":[ 3600 ], ... RationaleSetting global request timout allows extending the API Server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 1800 seconds which might not be suitable for some environments. Setting the limit too low may result in excessive timeouts, and a limit that is too large may exhaust the API Server resources making it prone to Denial-of-Service attack. It is recommended to set this limit as appropriate and change the default limit of 1800 seconds only if needed.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-apiserver/configmaps/config file.OVAL test results details Variable test to check XCCDF variable oval:ssg-test_api_server_request_timeout:tst:1 trueFollowing items have been found on the system:Var refValueoval:ssg-local_variable_api_server_request_timeout:var:13600 Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_request_timeout:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Ensure that the Admission Control Plugin AlwaysPullImages is not setxccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_AlwaysPullImages highEnsure that the Admission Control Plugin AlwaysPullImages is not setRule IDxccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_AlwaysPullImagesResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_admission_control_plugin_AlwaysPullImages:def:1Time2021-07-20T05:20:59+00:00SeverityhighIdentifiers and ReferencesReferences: 1.2.12, CM-6, CM-6(1)\nDescriptionThe AlwaysPullImages admission control plugin should be disabled, since it can introduce new failure modes for control plane components if an image registry is unreachable.RationaleSetting admission control policy to AlwaysPullImages forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image’s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required. However, turning on this admission plugin can introduce new kinds of cluster failure modes. OpenShift 4 master and infrastructure components are deployed as pods. Enabling this feature can result in cases where loss of contact to an image registry can cause a redeployed infrastructure pod (oauth-server for example) to fail on an image pull for an image that is currently present on the node. We use PullIfNotPresent so that a loss of image registry access does not prevent the pod from starting. If it becomes PullAlways, then an image registry access outage can cause key infrastructure components to fail. The pull policy can be managed per container, using imagePullPolicy.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_admission_control_plugin_AlwaysPullImages:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_admission_control_plugin_AlwaysPullImages:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Ensure that the bindAddress is set to a relevant secure portxccdf_org.ssgproject.content_rule_api_server_bind_address lowCCE-83646-0 Ensure that the bindAddress is set to a relevant secure portRule IDxccdf_org.ssgproject.content_rule_api_server_bind_addressResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_bind_address:def:1Time2021-07-20T05:20:59+00:00SeveritylowIdentifiers and ReferencesIdentifiers: CCE-83646-0\nReferences: 1.2.20, CM-6, CM-6(1)\nDescriptionThe bindAddress is set by default to 0.0.0.0:6443, and listening with TLS enabled.RationaleThe OpenShift API server is served over HTTPS with authentication and authorization; the secure API endpoint is bound to 0.0.0.0:6443 by default. In OpenShift, the only supported way to access the API server pod is through the load balancer and then through the internal service. The value is set by the bindAddress argument under the servingInfo parameter.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_bind_address:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_bind_address:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Configure the Service Account Public Key for the API Serverxccdf_org.ssgproject.content_rule_api_server_service_account_public_key mediumCCE-83350-9 Configure the Service Account Public Key for the API ServerRule IDxccdf_org.ssgproject.content_rule_api_server_service_account_public_keyResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_service_account_public_key:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83350-9\nReferences: 1.2.28, CM-6, CM-6(1)\nDescriptionTo ensure the API Server utilizes its own key pair, edit the openshift-kube-apiserver configmap and set the serviceAccountPublicKeyFiles parameter to the public key file for service accounts: ... \"serviceAccountPublicKeyFiles\":[ \"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\" ], ... RationaleBy default if no service-account-key-file is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens are rotated as needed, a separate public/private key pair should be used for signing service account tokens.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_service_account_public_key:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_service_account_public_key:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Ensure all admission control plugins are enabledxccdf_org.ssgproject.content_rule_api_server_no_adm_ctrl_plugins_disabled mediumCCE-83799-7 Ensure all admission control plugins are enabledRule IDxccdf_org.ssgproject.content_rule_api_server_no_adm_ctrl_plugins_disabledResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_no_adm_ctrl_plugins_disabled:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83799-7\nReferences: 1.2.13, 1.2.14, 1.2.14, 1.2.15, 1.2.16, 1.2.17, CM-6, CM-6(1)\nDescriptionTo make sure none of them is explicitly disabled, run the following command: $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data.\"config.yaml\"' | jq '.apiServerArguments.\"disable-admission-plugins\"' and make sure the output is empty.RationaleSeveral hardening controls depend on certain API server admission plugins being enabled. Checking that no admission control plugins are disabled helps assert that all the critical admission control plugins are indeed enabled and providing the security benefits required.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_no_adm_ctrl_plugins_disabled:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_no_adm_ctrl_plugins_disabled:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Ensure that Audit Log Forwarding Is Enabledxccdf_org.ssgproject.content_rule_audit_log_forwarding_enabled mediumCCE-84076-9 Ensure that Audit Log Forwarding Is EnabledRule IDxccdf_org.ssgproject.content_rule_audit_log_forwarding_enabledResultfailMulti-check rulenoOVAL Definition IDoval:ssg-audit_log_forwarding_enabled:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84076-9\nReferences: 1.2.23, AC-2(12), AU-6, AU-6(1), AU-6(3), AU-9(2), SI-4(16), AU-4(1), AU-11, AU-7, AU-7(1)\nDescriptionOpenShift audit works at the API server level, logging all requests coming to the server. Audit is on by default and the best practice is to ship audit logs off the cluster for retention. The cluster-logging-operator is able to do this with the ClusterLogForwarders resource. The forementioned resource can be configured to logs to different third party systems. For more information on this, please reference the official documentation: https://docs.openshift.com/container-platform/4.6/logging/cluster-logging-external.htmlRationaleRetaining logs ensures the ability to go back in time to investigate or correlate any events. Offloading audit logs from the cluster ensures that an attacker that has access to the cluster will not be able to tamper with the logs because of the logs being stored off-site.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /apis/logging.openshift.io/v1/namespaces/openshift-logging/clusterlogforwarders/instance API endpoint to the local /kubernetes-api-resources/apis/logging.openshift.io/v1/namespaces/openshift-logging/clusterlogforwarders/instance file.OVAL test results details In the file '/apis/logging.openshift.io/v1/namespaces/openshift-logging/clusterlogforwarders/instance' find only one object at path 'spec.pipelines[:].inputRefs[:]'. oval:ssg-test_audit_log_forwarding_enabled:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_audit_log_forwarding_enabled:obj:1 of type yamlfilecontent_objectFilepathYamlpath/kubernetes-api-resources/kubernetes-api-resources/apis/logging.openshift.io/v1/namespaces/openshift-logging/clusterlogforwarders/instancespec.pipelines[:].inputRefs[:] Find the file to be checked ('/apis/logging.openshift.io/v1/namespaces/openshift-logging/clusterlogforwarders/instance'). oval:ssg-test_file_for_audit_log_forwarding_enabled:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_for_audit_log_forwarding_enabled:obj:1 of type file_objectFilepath/kubernetes-api-resources/kubernetes-api-resources/apis/logging.openshift.io/v1/namespaces/openshift-logging/clusterlogforwarders/instance Use Strong Cryptographic Ciphers on the API Serverxccdf_org.ssgproject.content_rule_api_server_tls_cipher_suites mediumUse Strong Cryptographic Ciphers on the API ServerRule IDxccdf_org.ssgproject.content_rule_api_server_tls_cipher_suitesResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_tls_cipher_suites:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesReferences: 1.2.35\nDescriptionTo ensure that the API Server is configured to only use strong cryptographic ciphers, verify the openshift-kube-apiserver configmap contains the following set of ciphers, with no additions: \"servingInfo\":{ ... \"cipherSuites\": [ \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\", \"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\", ], ... RationaleTLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided. By default, OpenShift supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.warning Once configured, API Server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_tls_cipher_suites:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_tls_cipher_suites:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Ensure authorization-mode RBAC is configuredxccdf_org.ssgproject.content_rule_api_server_auth_mode_rbac mediumCCE-84102-3 Ensure authorization-mode RBAC is configuredRule IDxccdf_org.ssgproject.content_rule_api_server_auth_mode_rbacResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_auth_mode_rbac:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84102-3\nReferences: 1.2.9, CM-6, CM-6(1)\nDescriptionTo ensure OpenShift restricts different identities to a defined set of operations they are allowed to perform, check that the API server's authorization-mode configuration option list containst RBAC.RationaleRole Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. Enabling RBAC is critical in regulating access to an OpenShift cluster as the RBAC rules specify, given a user, which operations can be executed over a set of namespaced or cluster-wide resources.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_auth_mode_rbac:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_auth_mode_rbac:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Configure the Certificate for the API Serverxccdf_org.ssgproject.content_rule_api_server_tls_cert mediumCCE-83779-9 Configure the Certificate for the API ServerRule IDxccdf_org.ssgproject.content_rule_api_server_tls_certResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_tls_cert:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83779-9\nReferences: 1.2.30, SC-8, SC-8(1), SC-8(2)\nDescriptionTo ensure the API Server utilizes its own TLS certificates, the tls-cert-file must be configured. Verify that the apiServerArguments section has the tls-cert-file configured in the config configmap in the openshift-kube-apiserver namespace similar to: \"apiServerArguments\":{ ... \"tls-cert-file\": [ \"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\" ], ... } RationaleAPI Server communication contains sensitive parameters that should remain encrypted in transit. Configure the API Server to serve only HTTPS traffic.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_tls_cert:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_tls_cert:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Ensure that the --kubelet-https argument is set to truexccdf_org.ssgproject.content_rule_api_server_https_for_kubelet_conn mediumEnsure that the --kubelet-https argument is set to trueRule IDxccdf_org.ssgproject.content_rule_api_server_https_for_kubelet_connResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_https_for_kubelet_conn:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesReferences: 1.2.4, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionThe kube-apiserver ensures https to the kubelet by default. The apiserver flag \"--kubelet-https\" is deprecated and should be either set to \"true\" or omitted from the argument list.RationaleConnections from the kube-apiserver to kubelets could potentially carry sensitive data such as secrets and keys. It is thus important to use in-transit encryption for any communication between the apiserver and kubelets.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_https_for_kubelet_conn:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_https_for_kubelet_conn:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Configure Kubernetes API Server Maximum Audit Log Sizexccdf_org.ssgproject.content_rule_api_server_audit_log_maxsize mediumCCE-83607-2 Configure Kubernetes API Server Maximum Audit Log SizeRule IDxccdf_org.ssgproject.content_rule_api_server_audit_log_maxsizeResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_audit_log_maxsize:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83607-2\nReferences: 1.2.25, CM-6, CM-6(1)\nDescriptionTo rotate audit logs upon reaching a maximum size, edit the openshift-kube-apiserver configmap and set the audit-log-maxsize parameter to an appropriate size in MB. For example, to set it to 100 MB: \"apiServerArguments\":{ ... \"audit-log-maxsize\": [\"100\"], ... RationaleOpenShift automatically rotates log files. Retaining old log files ensures that OpenShift Operators have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, there would be approximately 1 GB of log data available for use in analysis.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_audit_log_maxsize:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_audit_log_maxsize:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Configure the kubelet Certificate File for the API Serverxccdf_org.ssgproject.content_rule_api_server_kubelet_client_cert highCCE-84080-1 Configure the kubelet Certificate File for the API ServerRule IDxccdf_org.ssgproject.content_rule_api_server_kubelet_client_certResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_kubelet_client_cert:def:1Time2021-07-20T05:21:00+00:00SeverityhighIdentifiers and ReferencesIdentifiers: CCE-84080-1\nReferences: 1.2.5, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo enable certificate based kubelet authentication, edit the config configmap in the openshift-kube-apiserver namespace and set the below parameter in the config.yaml key if it is not already configured: \"apiServerArguments\":{ ... \"kubelet-client-certificate\":\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\", ... } RationaleBy default the API Server does not authenticate itself to the kublet's HTTPS endpoints. Requests from the API Server are treated anonymously. Configuring certificate-based kubelet authentication ensures that the API Server authenticates itself to kubelets when submitting requests.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_kubelet_client_cert:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_kubelet_client_cert:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Disable basic-auth-file for the API Serverxccdf_org.ssgproject.content_rule_api_server_basic_auth mediumCCE-83936-5 Disable basic-auth-file for the API ServerRule IDxccdf_org.ssgproject.content_rule_api_server_basic_authResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_basic_auth:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83936-5\nReferences: 1.2.2, CM-6, CM-6(1)\nDescriptionBasic Authentication should not be used for any reason. If needed, edit API Edit the openshift-kube-apiserver configmap and remove the basic-auth-file parameter: \"apiServerArguments\":{ ... \"basic-auth-file\":[ \"/path/to/any/file\" ], ... Alternate authentication mechanisms such as tokens and certificates will need to be used. Username and password for basic authentication will be disabled.RationaleBasic authentication uses plaintext credentials for authentication. Currently the basic authentication credentials last indefinitely, and the password cannot be changed without restarting the API Server. The Basic Authentication is currently supported for convenience and is not intended for production workloads.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_basic_auth:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_basic_auth:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Enable the NodeRestriction Admission Control Pluginxccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_NodeRestriction mediumCCE-83753-4 Enable the NodeRestriction Admission Control PluginRule IDxccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_NodeRestrictionResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_admission_control_plugin_NodeRestriction:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83753-4\nReferences: 1.2.17, CM-6, CM-6(1)\nDescriptionTo limit the Node and Pod objects that a kubelet could modify, ensure that the NodeRestriction plugin on kubelets is enabled in the api-server configuration by running the following command: $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data.\"config.yaml\"' | jq '.apiServerArguments.\"enable-admission-plugins\"'RationaleUsing the NodeRestriction plugin ensures that the kubelet is restricted to the Node and Pod objects that it could modify as defined. Such kubelets will only be allowed to modify their own Node API object, and only modify Pod API objects that are bound to their node.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_admission_control_plugin_NodeRestriction:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_admission_control_plugin_NodeRestriction:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- The authorization-mode cannot be AlwaysAllowxccdf_org.ssgproject.content_rule_api_server_auth_mode_no_aa mediumCCE-84207-0 The authorization-mode cannot be AlwaysAllowRule IDxccdf_org.ssgproject.content_rule_api_server_auth_mode_no_aaResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_auth_mode_no_aa:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84207-0\nReferences: 1.2.7, CM-6, CM-6(1)\nDescriptionDo not always authorize all requests.RationaleThe API Server, can be configured to allow all requests. This mode should not be used on any production cluster.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_auth_mode_no_aa:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_auth_mode_no_aa:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Ensure that the service-account-lookup argument is set to truexccdf_org.ssgproject.content_rule_api_server_service_account_lookup mediumCCE-83370-7 Ensure that the service-account-lookup argument is set to trueRule IDxccdf_org.ssgproject.content_rule_api_server_service_account_lookupResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_service_account_lookup:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83370-7\nReferences: 1.2.27, CM-6, CM-6(1)\nDescriptionValidate service account before validating token.RationaleIf service-account-lookup is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file. OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_service_account_lookup:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_service_account_lookup:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Ensure authorization-mode Node is configuredxccdf_org.ssgproject.content_rule_api_server_auth_mode_node mediumCCE-83889-6 Ensure authorization-mode Node is configuredRule IDxccdf_org.ssgproject.content_rule_api_server_auth_mode_nodeResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_auth_mode_node:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83889-6\nReferences: 1.2.8, CM-6, CM-6(1)\nDescriptionRestrict kubelet nodes to reading only objects associated with them.RationaleThe Node authorization mode only allows kubelets to read Secret, ConfigMap, PersistentVolume, and PersistentVolumeClaim objects associated with their nodes.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_auth_mode_node:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_auth_mode_node:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Disable Use of the Insecure Bind Addressxccdf_org.ssgproject.content_rule_api_server_insecure_bind_address mediumCCE-83955-5 Disable Use of the Insecure Bind AddressRule IDxccdf_org.ssgproject.content_rule_api_server_insecure_bind_addressResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_insecure_bind_address:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83955-5\nReferences: 1.2.18, CM-6, CM-6(1)\nDescriptionOpenShift should not bind to non-loopback insecure addresses. Edit the openshift-kube-apiserver configmap and remove the insecure-bind-address if it exists: \"apiServerArguments\":{ ... \"insecure-bind-address\":[ \"127.0.0.1\" ], ... RationaleIf the API Server is bound to an insecure address the installation would be susceptible to unauthented and unencrypted access to the master node(s). The API Server does not perform authentication checking for insecure binds and the traffic is generally not encrypted.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_insecure_bind_address:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_insecure_bind_address:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Enable the APIPriorityAndFairness feature gatexccdf_org.ssgproject.content_rule_api_server_api_priority_gate_enabled mediumCCE-83656-9 Enable the APIPriorityAndFairness feature gateRule IDxccdf_org.ssgproject.content_rule_api_server_api_priority_gate_enabledResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_api_priority_gate_enabled:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83656-9\nReferences: 1.2.10, CM-6, CM-6(1)\nDescriptionTo limit the rate at which the API Server accepts requests, make sure that the API Priority and Fairness feature is enabled. Using APIPriorityAndFairness feature provides a fine-grained way to control the behaviour of the Kubernetes API server in an overload situation. To enable the APIPriorityAndFairness feature gate, make sure that the feature-gates API server argument, typically set in the config configMap in the openshift-kube-apiserver namespace contains APIPriorityAndFairness=true.RationaleThe APIPriorityAndFairness feature gate enables the use of the FlowSchema API objects which enforce a limit on the number of events that the API Server will accept in a given time slice In a large multi-tenant cluster, there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. It is recommended to limit the rate of events that the API Server will accept.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /apis/operator.openshift.io/v1/kubeapiservers/cluster API endpoint to the local /kubernetes-api-resources/apis/operator.openshift.io/v1/kubeapiservers/cluster file.OVAL test results details In the file '/apis/operator.openshift.io/v1/kubeapiservers/cluster' find only one object at path '.spec.observedConfig.apiServerArguments[\"feature-gates\"][:]'. oval:ssg-test_api_server_api_priority_gate_enabled:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/apis/operator.openshift.io/v1/kubeapiservers/cluster/kubernetes-api-resources/apis/operator.openshift.io/v1/kubeapiserverscluster.spec.observedConfig.apiServerArguments[\"feature-gates\"][:] APIPriorityAndFairness=true RotateKubeletServerCertificate=true SupportPodPidsLimit=true NodeDisruptionExclusion=true ServiceNodeExclusion=true SCTPSupport=true LegacyNodeRoleBehavior=false RemoveSelfLink=false Find the file to be checked ('/apis/operator.openshift.io/v1/kubeapiservers/cluster'). oval:ssg-test_file_for_api_server_api_priority_gate_enabled:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/apis/operator.openshift.io/v1/kubeapiservers/clusterregular100065000010006500009727rw------- Configure the kubelet Certificate Authority for the API Serverxccdf_org.ssgproject.content_rule_api_server_kubelet_certificate_authority highCCE-84196-5 Configure the kubelet Certificate Authority for the API ServerRule IDxccdf_org.ssgproject.content_rule_api_server_kubelet_certificate_authorityResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_kubelet_certificate_authority:def:1Time2021-07-20T05:21:00+00:00SeverityhighIdentifiers and ReferencesIdentifiers: CCE-84196-5\nReferences: 1.2.6, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo ensure OpenShift verifies kubelet certificates before establishing connections, follow the OpenShift documentation and setup the TLS connection between the API Server and kubelets. Edit the openshift-kube-apiserver configmap and set the below parameter if it is not already configured: \"apiServerArguments\":{ ... \"kubelet-certificate-authority\":\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\", ... RationaleConnections from the API Server to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet port-forwarding functionality. These connections terminate at the kubelet HTTPS endpoint. By default, the API Server does not verify the kubelet serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_kubelet_certificate_authority:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_kubelet_certificate_authority:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not usedxccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_SecurityContextDeny mediumCCE-83586-8 Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not usedRule IDxccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_SecurityContextDenyResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_admission_control_plugin_SecurityContextDeny:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83586-8\nReferences: 1.2.13, CM-6, CM-6(1)\nDescriptionInstead of using a customized SecurityContext for pods, a Pod Security Policy (PSP) or a SecurityContextConstraint should be used. These are cluster-level resources that control the actions that a pod can perform and what resource the pod may access. The SecurityContextDeny disallows folks from setting a pod's securityContext fields. Ensure that the list of admission controllers does not include SecurityContextDeny: $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data.\"config.yaml\"' | jq '.apiServerArguments.\"enable-admission-plugins\"' RationaleThe SecurityContextDeny admission control plugin disallows setting any security options for your pods. SecurityContextConstraints allow you to enforce RBAC rules on who can set these options on the pods, and what they're allowed to set. Thus, using the SecurityContextDeny will deter you from enforcing granular permissions on your pods.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_admission_control_plugin_SecurityContextDeny:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_admission_control_plugin_SecurityContextDeny:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Configure the Encryption Provider Cipherxccdf_org.ssgproject.content_rule_api_server_encryption_provider_cipher mediumConfigure the Encryption Provider CipherRule IDxccdf_org.ssgproject.content_rule_api_server_encryption_provider_cipherResultfailMulti-check rulenoOVAL Definition IDoval:ssg-api_server_encryption_provider_cipher:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 1.2.34, SC-28, SC-28(1)\nDescriptionTo ensure the correct cipher, set the encryption type aescbc in the apiserver object which configures the API server itself. spec: encryption: type: aescbc For more information, follow the relevant documentation.Rationaleaescbc is currently the strongest encryption provider, it should be preferred over other providers.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /apis/config.openshift.io/v1/apiservers/cluster API endpoint to the local /kubernetes-api-resources/apis/config.openshift.io/v1/apiservers/cluster file.Remediation script ⇲\n--- apiVersion: config.openshift.io/v1 kind: APIServer metadata: name: cluster spec: encryption: type: aescbc OVAL test results details In the file '/apis/config.openshift.io/v1/apiservers/cluster' find only one object at path '.spec.encryption.type'. oval:ssg-test_api_server_encryption_provider_cipher:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_api_server_encryption_provider_cipher:obj:1 of type yamlfilecontent_objectFilepathYamlpath/kubernetes-api-resources/kubernetes-api-resources/apis/config.openshift.io/v1/apiservers/cluster.spec.encryption.type Find the file to be checked ('/apis/config.openshift.io/v1/apiservers/cluster'). oval:ssg-test_file_for_api_server_encryption_provider_cipher:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/apis/config.openshift.io/v1/apiservers/clusterregular10006500001000650000869rw------- Profiling is protected by RBACxccdf_org.ssgproject.content_rule_api_server_profiling_protected_by_rbac mediumCCE-84212-0 Profiling is protected by RBACRule IDxccdf_org.ssgproject.content_rule_api_server_profiling_protected_by_rbacResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_profiling_protected_by_rbac:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84212-0\nReferences: 1.2.21, CM-6, CM-6(1)\nDescriptionEnsure that the cluster-debugger cluster role includes the /metrics resource URL. This demonstrates that profiling is protected by RBAC, with a specific cluster role to allow access.RationaleProfiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. To ensure the collected data is not exploited, profiling endpoints are secured via RBAC (see cluster-debugger role). By default, the profiling endpoints are accessible only by users bound to cluster-admin or cluster-debugger role. Profiling can not be disabled.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-debugger API endpoint to the local /kubernetes-api-resources/apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-debugger file.OVAL test results details In the file '/apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-debugger' find only one object at path '.rules[0].nonResourceURLs[:]'. oval:ssg-test_api_server_profiling_protected_by_rbac:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-debugger/kubernetes-api-resources/apis/rbac.authorization.k8s.io/v1/clusterrolescluster-debugger.rules[0].nonResourceURLs[:] /debug/pprof /debug/pprof/* /metrics Find the file to be checked ('/apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-debugger'). oval:ssg-test_file_for_api_server_profiling_protected_by_rbac:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-debuggerregular10006500001000650000773rw------- Ensure that anonymous requests to the API Server are authorizedxccdf_org.ssgproject.content_rule_api_server_anonymous_auth mediumEnsure that anonymous requests to the API Server are authorizedRule IDxccdf_org.ssgproject.content_rule_api_server_anonymous_authResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_anonymous_auth:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 1.2.1, CM-6, CM-6(1)\nDescriptionBy default, anonymous access to the OpenShift API is enabled, but at the same time, all requests must be authorized. If no authentication mechanism is used, the request is assigned the system:anonymous virtual user and the system:unauthenticated virtual group. This allows the authorization layer to determin which requests, if any, is an anonymous user authorized to make. To verify the authorization rules for anonymous requests run the following: $ oc describe clusterrolebindings and inspect the bidnings of the system:anonymous virtual user and the system:unauthenticated virtual group. To test that an anonymous request is authorized to access the readyz endpoint, run: $ oc get --as=\"system:anonymous\" --raw='/readyz?verbose' In contrast, a request to list all projects should not be authorized: $ oc get --as=\"system:anonymous\" projectsRationaleWhen enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is not scored. However, you should consider whether anonymous discovery is an acceptable risk for your purposes.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /apis/rbac.authorization.k8s.io/v1/clusterrolebindings API endpoint to the local /kubernetes-api-resources/apis/rbac.authorization.k8s.io/v1/clusterrolebindings file.OVAL test results details In the file '/apis/rbac.authorization.k8s.io/v1/clusterrolebindings' find only one object at path '.items[:]['subjects'][:].name'. oval:ssg-test_api_server_anonymous_auth:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValueValue/kubernetes-api-resources/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kubernetes-api-resources/apis/rbac.authorization.k8s.io/v1clusterrolebindings.items[:]['subjects'][:].name alertmanager-main strimzi-cluster-operator strimzi-cluster-operator system:authenticated cloud-credential-operator system:masters admin system:cluster-admins system:admin cluster-autoscaler cluster-autoscaler-operator cluster-baremetal-operator cluster-monitoring-operator cluster-node-tuning-operator tuned system:cluster-readers cluster-samples-operator cluster-samples-operator system:authenticated cluster-storage-operator default compliance-operator api-resource-collector console system:authenticated console-operator console-operator csi-snapshot-controller-operator cluster-image-registry-operator default default prometheus-k8s grafana system:authenticated operator operator gather gather kube-apiserver system:kube-apiserver kube-state-metrics machine-api-controllers machine-api-operator machine-config-controller machine-config-daemon machine-config-server marketplace-operator metrics-daemon-sa multus multus multus network-diagnostics node-exporter olm-operator-serviceaccount csi-snapshot-controller dns dns-operator pruner ingress-operator router ovn-kubernetes-controller ovn-kubernetes-node openshift-state-metrics olm-operator-serviceaccount postgres-operator prometheus-adapter prometheus-adapter prometheus-k8s prometheus-k8s prometheus-operator prometheus-k8s registry prometheus-adapter prometheus-k8s nfs-client-provisioner system:authenticated system:unauthenticated system:authenticated:oauth apicurio-registry-operator apicurio-registry-operator kube-storage-version-migrator-sa node-bootstrapper system:nodes system:authenticated system:authenticated system:authenticated system:authenticated attachdetach-controller certificate-controller clusterrole-aggregation-controller cronjob-controller daemon-set-controller deployment-controller disruption-controller endpoint-controller endpointslice-controller endpointslicemirroring-controller expand-controller generic-garbage-collector horizontal-pod-autoscaler horizontal-pod-autoscaler job-controller namespace-controller node-controller persistent-volume-binder pod-garbage-collector pv-protection-controller pvc-protection-controller replicaset-controller replication-controller resourcequota-controller root-ca-cert-publisher route-controller service-account-controller service-controller statefulset-controller ttl-controller default-rolebindings-controller system:authenticated default-rolebindings-controller default-rolebindings-controller system:kube-controller-manager kube-dns system:kube-scheduler system:masters system:monitoring system:master system:kube-apiserver system:node-admins system:master system:node-admins node-bootstrapper system:kube-proxy system:nodes system:authenticated system:unauthenticated build-config-change-controller build-controller cluster-quota-reconciliation-controller default-rolebindings-controller deployer-controller deploymentconfig-controller horizontal-pod-autoscaler image-import-controller image-trigger-controller system:serviceaccount:openshift-kube-apiserver:check-endpoints system:serviceaccount:openshift-kube-apiserver:check-endpoints system:serviceaccount:openshift-kube-apiserver:check-endpoints machine-approver-sa namespace-security-allocation-controller origin-namespace-controller pv-recycler-controller resourcequota-controller service-ca service-ingress-ip-controller service-serving-cert-controller serviceaccount-controller serviceaccount-pull-secrets-controller template-instance-controller template-instance-controller template-instance-finalizer-controller template-instance-finalizer-controller template-service-broker unidling-controller system:authenticated cloud-provider oauth-apiserver-sa openshift-apiserver-sa oauth-openshift openshift-controller-manager-sa ingress-to-route-controller authentication-operator openshift-kube-scheduler-operator etcd-operator kube-apiserver-operator localhost-recovery-client kube-controller-manager-operator localhost-recovery-client localhost-recovery-client openshift-kube-scheduler-sa kube-storage-version-migrator-operator openshift-apiserver-operator openshift-config-operator openshift-controller-manager-operator installer-sa installer-sa installer-sa installer-sa service-ca-operator system:authenticated system:unauthenticated openshift-controller-manager-sa system:authenticated:oauth system:authenticated system:unauthenticated system:authenticated system:unauthenticated system:nodes system:serviceaccounts system:kube-scheduler system:authenticated system:unauthenticated telemeter-client telemeter-client thanos-querier Find the file to be checked ('/apis/rbac.authorization.k8s.io/v1/clusterrolebindings'). oval:ssg-test_file_for_api_server_anonymous_auth:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/apis/rbac.authorization.k8s.io/v1/clusterrolebindingsregular10006500001000650000214095rw------- Ensure the openshift-oauth-apiserver service uses TLSxccdf_org.ssgproject.content_rule_api_server_openshift_https_serving_cert mediumEnsure the openshift-oauth-apiserver service uses TLSRule IDxccdf_org.ssgproject.content_rule_api_server_openshift_https_serving_certResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 1.2.4, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionBy default, the OpenShift API Server uses TLS. HTTPS should be used for connections between openshift-apiserver and kube-apiserver. OpenShift API server enables TLS automatically if a TLS key and a certificate are provided via the serving-cert secret in the openshift-apiserver namespace.RationaleConnections between the kube-apiserver and the extension openshift-apiserver could potentially carry sensitive data such as secrets and keys. It is important to use in-transit encryption for any communication between the kube-apiserver and the extension openshift-apiserver.Evaluation messagesinfo No candidate or applicable check found.Configure the Certificate Key for the API Serverxccdf_org.ssgproject.content_rule_api_server_tls_private_key mediumCCE-84282-3 Configure the Certificate Key for the API ServerRule IDxccdf_org.ssgproject.content_rule_api_server_tls_private_keyResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_tls_private_key:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84282-3\nReferences: 1.2.30, SC-8, SC-8(1), SC-8(2)\nDescriptionTo ensure the API Server utilizes its own TLS certificates, the tls-private-key-file must be configured. Verify that the apiServerArguments section has the tls-private-key-file configured in the config configmap in the openshift-kube-apiserver namespace similar to: \"apiServerArguments\":{ ... \"tls-private-key-file\": [ \"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\" ], ... } RationaleAPI Server communication contains sensitive parameters that should remain encrypted in transit. Configure the API Server to serve only HTTPS traffic.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_tls_private_key:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_tls_private_key:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Configure the Audit Log Pathxccdf_org.ssgproject.content_rule_api_server_audit_log_path highCCE-84020-7 Configure the Audit Log PathRule IDxccdf_org.ssgproject.content_rule_api_server_audit_log_pathResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_audit_log_path:def:1Time2021-07-20T05:21:00+00:00SeverityhighIdentifiers and ReferencesIdentifiers: CCE-84020-7\nReferences: 1.2.22, CM-6, CM-6(1)\nDescriptionTo enable auditing on the Kubernetes API Server, the audit log path must be set. Edit the openshift-kube-apiserver configmap and set the audit-log-path to a suitable path and file where audit logs should be written. For example: \"apiServerArguments\":{ ... \"audit-log-path\":\"/var/log/kube-apiserver/audit.log\", ... RationaleAuditing of the Kubernetes API Server is not enabled by default. Auditing the API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected the system by users, administrators, or other system components.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_audit_log_path:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_audit_log_path:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Ensure catch-all FlowSchema object for API Priority and Fairness Exists (v1alpha1)xccdf_org.ssgproject.content_rule_api_server_api_priority_v1alpha1_flowschema_catch_all mediumCCE-84002-5 Ensure catch-all FlowSchema object for API Priority and Fairness Exists (v1alpha1)Rule IDxccdf_org.ssgproject.content_rule_api_server_api_priority_v1alpha1_flowschema_catch_allResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_api_priority_v1alpha1_flowschema_catch_all:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84002-5\nReferences: 1.2.10, CM-6, CM-6(1)\nDescriptionUsing APIPriorityAndFairness feature provides a fine-grained way to control the behaviour of the Kubernetes API server in an overload situation. The well-known FlowSchema catch-all should be available to make sure that every request gets some kind of classification. By default, the catch-all priority level only allows one concurrency share and does not queue requests. To inspect all the FlowSchema objects, run: oc get flowschema To inspect the well-known catch-all object, run the following: oc describe flowschema catch-allRationaleThe FlowSchema API objects enforce a limit on the number of events that the API Server will accept in a given time slice In a large multi-tenant cluster, there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. It is recommended to limit the rate of events that the API Server will accept.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /apis/flowcontrol.apiserver.k8s.io/v1alpha1/flowschemas/catch-all API endpoint to the local /kubernetes-api-resources/apis/flowcontrol.apiserver.k8s.io/v1alpha1/flowschemas/catch-all file.warning Note that this rule is only applicable in OpenShift Container Platform versions 4.7 and below.OVAL test results details In the file '/apis/flowcontrol.apiserver.k8s.io/v1alpha1/flowschemas/catch-all' find only one object at path '.spec.rules[0].subjects[:].group[\"name\"]'. oval:ssg-test_api_server_api_priority_v1alpha1_flowschema_catch_all:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/apis/flowcontrol.apiserver.k8s.io/v1alpha1/flowschemas/catch-all/kubernetes-api-resources/apis/flowcontrol.apiserver.k8s.io/v1alpha1/flowschemascatch-all.spec.rules[0].subjects[:].group[\"name\"] system:unauthenticated system:authenticated Find the file to be checked ('/apis/flowcontrol.apiserver.k8s.io/v1alpha1/flowschemas/catch-all'). oval:ssg-test_file_for_api_server_api_priority_v1alpha1_flowschema_catch_all:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/apis/flowcontrol.apiserver.k8s.io/v1alpha1/flowschemas/catch-allregular100065000010006500001927rw------- Configure the Encryption Providerxccdf_org.ssgproject.content_rule_api_server_encryption_provider_config mediumCCE-83585-0 Configure the Encryption ProviderRule IDxccdf_org.ssgproject.content_rule_api_server_encryption_provider_configResultfailMulti-check rulenoOVAL Definition IDoval:ssg-api_server_encryption_provider_config:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83585-0\nReferences: 1.2.33, SC-28, SC-28(1)\nDescriptionTo encrypt the etcd key-value store, set the encryption type aescbc in the apiserver object which configures the API server itself. spec: encryption: type: aescbc For more information, follow the relevant documentation.Rationaleetcd is a highly available key-value store used by OpenShift deployments for persistent storage of all REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /apis/config.openshift.io/v1/apiservers/cluster API endpoint to the local /kubernetes-api-resources/apis/config.openshift.io/v1/apiservers/cluster file.Remediation script ⇲\n--- apiVersion: config.openshift.io/v1 kind: APIServer metadata: name: cluster spec: encryption: type: aescbc OVAL test results details In the file '/apis/config.openshift.io/v1/apiservers/cluster' find only one object at path '.spec.encryption.type'. oval:ssg-test_api_server_encryption_provider_config:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_api_server_encryption_provider_config:obj:1 of type yamlfilecontent_objectFilepathYamlpath/kubernetes-api-resources/kubernetes-api-resources/apis/config.openshift.io/v1/apiservers/cluster.spec.encryption.type Find the file to be checked ('/apis/config.openshift.io/v1/apiservers/cluster'). oval:ssg-test_file_for_api_server_encryption_provider_config:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/apis/config.openshift.io/v1/apiservers/clusterregular10006500001000650000869rw------- Configure the Kubernetes API Server Maximum Retained Audit Logsxccdf_org.ssgproject.content_rule_api_server_audit_log_maxbackup lowCCE-83739-3 Configure the Kubernetes API Server Maximum Retained Audit LogsRule IDxccdf_org.ssgproject.content_rule_api_server_audit_log_maxbackupResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_audit_log_maxbackup:def:1Time2021-07-20T05:21:00+00:00SeveritylowIdentifiers and ReferencesIdentifiers: CCE-83739-3\nReferences: 1.2.24, CM-6, CM-6(1)\nDescriptionTo configure how many rotations of audit logs are retained, edit the openshift-kube-apiserver configmap and set the audit-log-maxbackup parameter to 10 or to an organizationally appropriate value: \"apiServerArguments\":{ ... \"audit-log-maxbackup\": [10], ... RationaleOpenShift automatically rotates the log files. Retaining old log files ensures OpenShift Operators will have sufficient log data available for carrying out any investigation or correlation. For example, if the audit log size is set to 100 MB and the number of retained log files is set to 10, OpenShift Operators would have approximately 1 GB of log data to use during analysis.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_audit_log_maxbackup:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_audit_log_maxbackup:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Prevent Insecure Port Accessxccdf_org.ssgproject.content_rule_api_server_insecure_port mediumCCE-83813-6 Prevent Insecure Port AccessRule IDxccdf_org.ssgproject.content_rule_api_server_insecure_portResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_insecure_port:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83813-6\nReferences: 1.2.19, CM-6, CM-6(1)\nDescriptionBy default, traffic for the OpenShift API server is served over HTTPS with authentication and authorization, and the secure API endpoint is bound to 0.0.0.0:8443. To ensure that the insecure port configuration has not been enabled, the insecure-port parameter should be set to 0. Edit the openshift-kube-apiserver configmap and change the insecure-port value to 0: \"apiServerArguments\":{ ... \"insecure-port\":[ \"1234\" ], ... RationaleConfiguring the API Server on an insecure port would allow unauthenticated and unencrypted access to your master node(s). It is assumed firewall rules will be configured to ensure this port is not reachable from outside the cluster, however as a defense in depth measure, OpenShift should not be configured to use insecure ports.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_insecure_port:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_insecure_port:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Configure the etcd Certificate for the API Serverxccdf_org.ssgproject.content_rule_api_server_etcd_cert mediumCCE-83876-3 Configure the etcd Certificate for the API ServerRule IDxccdf_org.ssgproject.content_rule_api_server_etcd_certResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_etcd_cert:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83876-3\nReferences: 1.2.29, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo ensure etcd is configured to make use of TLS encryption for client communications, follow the OpenShift documentation and setup the TLS connection between the API Server and etcd. Then, verify that apiServerArguments has the etcd-certfile configured in the openshift-kube-apiserver configmap to something similar to: ... \"etcd-certfile\": [ \"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\" ], ... Rationaleetcd is a highly-available key-value store used by OpenShift deployments for persistent storage of all REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API Server to identify itself to the etcd server using a client certificate and key.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_etcd_cert:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_etcd_cert:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Configure the kubelet Certificate Key for the API Serverxccdf_org.ssgproject.content_rule_api_server_kubelet_client_key highCCE-83591-8 Configure the kubelet Certificate Key for the API ServerRule IDxccdf_org.ssgproject.content_rule_api_server_kubelet_client_keyResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_kubelet_client_key:def:1Time2021-07-20T05:21:00+00:00SeverityhighIdentifiers and ReferencesIdentifiers: CCE-83591-8\nReferences: 1.2.5, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo enable certificate based kubelet authentication, edit the config configmap in the openshift-kube-apiserver namespace and set the below parameter in the config.yaml key if it is not already configured: \"apiServerArguments\":{ ... \"kubelet-client-key\":\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\", ... } RationaleBy default the API Server does not authenticate itself to the kubelet's HTTPS endpoints. Requests from the API Server are treated anonymously. Configuring certificate-based kubelet authentication ensures that the API Server authenticates itself to kubelets when submitting requests.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_kubelet_client_key:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_kubelet_client_key:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Disable Token-based Authenticationxccdf_org.ssgproject.content_rule_api_server_token_auth highCCE-83481-2 Disable Token-based AuthenticationRule IDxccdf_org.ssgproject.content_rule_api_server_token_authResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_token_auth:def:1Time2021-07-20T05:21:00+00:00SeverityhighIdentifiers and ReferencesIdentifiers: CCE-83481-2\nReferences: 1.2.3, CM-6, CM-6(1)\nDescriptionTo ensure OpenShift does not accept token-based authentication, follow the OpenShift documentation and configure alternate mechanisms for authentication. Then, edit the API Server pod specification file Edit the openshift-kube-apiserver configmap and remove the token-auth-file parameter: \"apiServerArguments\":{ ... \"token-auth-file\":[ \"/path/to/any/file\" ], ... RationaleThe token-based authentication utilizes static tokens to authenticate requests to the API Server. The tokens are stored in clear-text in a file on the API Server, and cannot be revoked or rotated without restarting the API Server.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_token_auth:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_token_auth:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Enable the ServiceAccount Admission Control Pluginxccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_ServiceAccount mediumCCE-83791-4 Enable the ServiceAccount Admission Control PluginRule IDxccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_ServiceAccountResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_admission_control_plugin_ServiceAccount:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83791-4\nReferences: 1.2.14, CM-6, CM-6(1)\nDescriptionTo ensure ServiceAccount objects must be created and granted before pod creation is allowed, follow the documentation and create ServiceAccount objects as per your environment. Ensure that the plugin is enabled in the api-server configuration: $ oc -n openshift-kube-apiserver get configmap config -o json | jq -r '.data.\"config.yaml\"' | jq '.apiServerArguments.\"enable-admission-plugins\"'RationaleWhen a pod is created, if a service account is not specified, the pod is automatically assigned the default service account in the same namespace. OpenShift operators should create unique service accounts and let the API Server manage its security tokens.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_admission_control_plugin_ServiceAccount:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_admission_control_plugin_ServiceAccount:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Configure OpenShift API Server Maximum Audit Log Sizexccdf_org.ssgproject.content_rule_ocp_api_server_audit_log_maxsize mediumCCE-83687-4 Configure OpenShift API Server Maximum Audit Log SizeRule IDxccdf_org.ssgproject.content_rule_ocp_api_server_audit_log_maxsizeResultpassMulti-check rulenoOVAL Definition IDoval:ssg-ocp_api_server_audit_log_maxsize:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83687-4\nReferences: 1.2.25, CM-6, CM-6(1)\nDescriptionTo rotate audit logs upon reaching a maximum size, edit the openshift-apiserver configmap and set the audit-log-maxsize parameter to an appropriate size in MB. For example, to set it to 100 MB: \"apiServerArguments\":{ ... \"audit-log-maxsize\": [\"100\"], ... RationaleOpenShift automatically rotates log files. Retaining old log files ensures that OpenShift Operators have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, there would be approximately 1 GB of log data available for use in analysis.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_ocp_api_server_audit_log_maxsize:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-apiserver/configmapsconfig.data[\"config.yaml\"] {\"apiServerArguments\":{\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/openshift-apiserver/audit.log\"],\"audit-policy-file\":[\"/var/run/configmaps/audit/secure-oauth-storage-default.yaml\"],\"shutdown-delay-duration\":[\"3s\"]},\"apiVersion\":\"openshiftcontrolplane.config.openshift.io/v1\",\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"OpenShiftAPIServerConfig\",\"projectConfig\":{\"projectRequestMessage\":\"\"},\"routingConfig\":{\"subdomain\":\"apps.ocp.ispworld.at\"},\"servingInfo\":{\"bindNetwork\":\"tcp\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\"},\"storageConfig\":{\"urls\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\"]}} Find the file to be checked ('/api/v1/namespaces/openshift-apiserver/configmaps/config'). oval:ssg-test_file_for_ocp_api_server_audit_log_maxsize:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-apiserver/configmaps/configregular100065000010006500001565rw------- Disable the AlwaysAdmit Admission Control Pluginxccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_AlwaysAdmit mediumCCE-84148-6 Disable the AlwaysAdmit Admission Control PluginRule IDxccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_AlwaysAdmitResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_admission_control_plugin_AlwaysAdmit:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84148-6\nReferences: 1.2.11, CM-6, CM-6(1)\nDescriptionTo ensure OpenShift only responses to requests explicitly allowed by the admission control plugin. Check that the config ConfigMap object does not contain the AlwaysAdmit plugin.RationaleEnabling the admission control plugin AlwaysAdmit allows all requests and does not provide any filtering.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_admission_control_plugin_AlwaysAdmit:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_admission_control_plugin_AlwaysAdmit:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Enable the NamespaceLifecycle Admission Control Pluginxccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_NamespaceLifecycle mediumCCE-83854-0 Enable the NamespaceLifecycle Admission Control PluginRule IDxccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_NamespaceLifecycleResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_admission_control_plugin_NamespaceLifecycle:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83854-0\nReferences: 1.2.15, CM-6, CM-6(1)\nDescriptionOpenShift enables the NamespaceLifecycle plugin by default.RationaleSetting admission control policy to NamespaceLifecycle ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of new objects.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_admission_control_plugin_NamespaceLifecycle:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_admission_control_plugin_NamespaceLifecycle:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Configure the etcd Certificate Key for the API Serverxccdf_org.ssgproject.content_rule_api_server_etcd_key mediumCCE-83546-2 Configure the etcd Certificate Key for the API ServerRule IDxccdf_org.ssgproject.content_rule_api_server_etcd_keyResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_etcd_key:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83546-2\nReferences: 1.2.29, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo ensure etcd is configured to make use of TLS encryption for client communications, follow the OpenShift documentation and setup the TLS connection between the API Server and etcd. Then, verify that apiServerArguments has the etcd-keyfile configured in the openshift-kube-apiserver configmap to something similar to: ... \"etcd-keyfile\": [ \"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\" ], ... Rationaleetcd is a highly-available key-value store used by OpenShift deployments for persistent storage of all REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API Server to identify itself to the etcd server using a client certificate and key.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_etcd_key:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_etcd_key:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Ensure the openshift-oauth-apiserver service uses TLSxccdf_org.ssgproject.content_rule_api_server_oauth_https_serving_cert mediumEnsure the openshift-oauth-apiserver service uses TLSRule IDxccdf_org.ssgproject.content_rule_api_server_oauth_https_serving_certResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 1.2.4, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionBy default, the OpenShift OAuth API Server uses TLS. HTTPS should be used for connections between openshift-oauth-apiserver and kube-apiserver. OpenShift OAuth API server enables TLS automatically if a TLS key and a certificate are provided via the serving-cert secret in the openshift-oauth-apiserver namespace.RationaleConnections between the kube-apiserver and the extension openshift-oauth-apiserver could potentially carry sensitive data such as secrets and keys. It is important to use in-transit encryption for any communication between the kube-apiserver and the extension openshift-apiserver.Evaluation messagesinfo No candidate or applicable check found.Configure the OpenShift API Server Maximum Retained Audit Logsxccdf_org.ssgproject.content_rule_ocp_api_server_audit_log_maxbackup lowCCE-83977-9 Configure the OpenShift API Server Maximum Retained Audit LogsRule IDxccdf_org.ssgproject.content_rule_ocp_api_server_audit_log_maxbackupResultpassMulti-check rulenoOVAL Definition IDoval:ssg-ocp_api_server_audit_log_maxbackup:def:1Time2021-07-20T05:21:00+00:00SeveritylowIdentifiers and ReferencesIdentifiers: CCE-83977-9\nReferences: 1.2.24, CM-6, CM-6(1)\nDescriptionTo configure how many rotations of audit logs are retained, edit the openshift-apiserver configmap and set the audit-log-maxbackup parameter to 10 or to an organizationally appropriate value: \"apiServerArguments\":{ ... \"audit-log-maxbackup\": [10], ... RationaleOpenShift automatically rotates the log files. Retaining old log files ensures OpenShift Operators will have sufficient log data available for carrying out any investigation or correlation. For example, if the audit log size is set to 100 MB and the number of retained log files is set to 10, OpenShift Operators would have approximately 1 GB of log data to use during analysis.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_ocp_api_server_audit_log_maxbackup:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-apiserver/configmapsconfig.data[\"config.yaml\"] {\"apiServerArguments\":{\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/openshift-apiserver/audit.log\"],\"audit-policy-file\":[\"/var/run/configmaps/audit/secure-oauth-storage-default.yaml\"],\"shutdown-delay-duration\":[\"3s\"]},\"apiVersion\":\"openshiftcontrolplane.config.openshift.io/v1\",\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"OpenShiftAPIServerConfig\",\"projectConfig\":{\"projectRequestMessage\":\"\"},\"routingConfig\":{\"subdomain\":\"apps.ocp.ispworld.at\"},\"servingInfo\":{\"bindNetwork\":\"tcp\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\"},\"storageConfig\":{\"urls\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\"]}} Find the file to be checked ('/api/v1/namespaces/openshift-apiserver/configmaps/config'). oval:ssg-test_file_for_ocp_api_server_audit_log_maxbackup:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-apiserver/configmaps/configregular100065000010006500001565rw------- Configure the etcd Certificate Authority for the API Serverxccdf_org.ssgproject.content_rule_api_server_etcd_ca mediumCCE-84216-1 Configure the etcd Certificate Authority for the API ServerRule IDxccdf_org.ssgproject.content_rule_api_server_etcd_caResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_etcd_ca:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84216-1\nReferences: 1.2.32, SC-8, SC-8(1), SC-8(2)\nDescriptionTo ensure etcd is configured to make use of TLS encryption for client connections, follow the OpenShift documentation and setup the TLS connection between the API Server and etcd. Then, verify that apiServerArguments has the etcd-cafile configured in the openshift-kube-apiserver config configmap to something similar to: \"apiServerArguments\": { ... \"etcd-cafile\": [ \"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\" ], ... Rationaleetcd is a highly-available key-value store used by OpenShift deployments for persistent storage of all REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API Server to identify itself to the etcd server using a SSL Certificate Authority file.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_etcd_ca:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_etcd_ca:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Enable the SecurityContextConstraint Admission Control Pluginxccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_Scc mediumCCE-83602-3 Enable the SecurityContextConstraint Admission Control PluginRule IDxccdf_org.ssgproject.content_rule_api_server_admission_control_plugin_SccResultpassMulti-check rulenoOVAL Definition IDoval:ssg-api_server_admission_control_plugin_Scc:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83602-3\nReferences: 1.2.16, CM-6, CM-6(1)\nDescriptionTo ensure pod permissions are managed, make sure that the SecurityContextConstraint admission control plugin is used.RationaleA Security Context Constraint is a cluster-level resource that controls the actions which a pod can perform and what the pod may access. The SecurityContextConstraint objects define a set of conditions that a pod must run with in order to be accepted into the system. Security Context Constraints are comprised of settings and strategies that control the security features a pod has access to and hence this must be used to control pod access permissions.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_api_server_admission_control_plugin_Scc:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data[\"config.yaml\"] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_api_server_admission_control_plugin_Scc:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Ensure catch-all FlowSchema object for API Priority and Fairness Existsxccdf_org.ssgproject.content_rule_api_server_api_priority_flowschema_catch_all mediumCCE-83522-3 Ensure catch-all FlowSchema object for API Priority and Fairness ExistsRule IDxccdf_org.ssgproject.content_rule_api_server_api_priority_flowschema_catch_allResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83522-3\nReferences: 1.2.10, CM-6, CM-6(1)\nDescriptionUsing APIPriorityAndFairness feature provides a fine-grained way to control the behaviour of the Kubernetes API server in an overload situation. The well-known FlowSchema catch-all should be available to make sure that every request gets some kind of classification. By default, the catch-all priority level only allows one concurrency share and does not queue requests. To inspect all the FlowSchema objects, run: oc get flowschema To inspect the well-known catch-all object, run the following: oc describe flowschema catch-allRationaleThe FlowSchema API objects enforce a limit on the number of events that the API Server will accept in a given time slice In a large multi-tenant cluster, there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. It is recommended to limit the rate of events that the API Server will accept.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /apis/flowcontrol.apiserver.k8s.io/v1beta1/flowschemas/catch-all API endpoint to the local /kubernetes-api-resources/apis/flowcontrol.apiserver.k8s.io/v1beta1/flowschemas/catch-all file.warning Note that this is only applicable in OpenShift Container Platform version 4.8 and higherLimit Access to the Host IPC Namespacexccdf_org.ssgproject.content_rule_scc_limit_ipc_namespace mediumCCE-84042-1 Limit Access to the Host IPC NamespaceRule IDxccdf_org.ssgproject.content_rule_scc_limit_ipc_namespaceResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84042-1\nReferences: 5.2.3, CM-6, CM-6(1)\nDescriptionContainers should not be allowed access to the host's Interprocess Commication (IPC) namespace. To prevent containers from getting access to a host's IPC namespace, the appropriate Security Context Constraints (SCCs) should set allowHostIPC to false.RationaleA container running in the host's IPC namespace can use IPC to interact with processes outside the container potentially allowing an attacker to exploit a host process thereby enabling an attacker to exploit other services.Evaluation messagesinfo No candidate or applicable check found.Limit Container Running As Root Userxccdf_org.ssgproject.content_rule_scc_limit_root_containers mediumLimit Container Running As Root UserRule IDxccdf_org.ssgproject.content_rule_scc_limit_root_containersResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.2.6, CM-6, CM-6(1)\nDescriptionContainers should be limited to only the privileges required to run and should very rarely be run as root user. To prevent containers from running as root user, the appropriate Security Context Constraints (SCCs) should set allowPrivilegedContainer to false.RationalePrivileged containers have access to all Linux Kernel capabilities and devices. If a privileged container were compromised, an attacker would have full access to the container and host.Evaluation messagesinfo No candidate or applicable check found.Drop Container Capabilitiesxccdf_org.ssgproject.content_rule_scc_drop_container_capabilities mediumDrop Container CapabilitiesRule IDxccdf_org.ssgproject.content_rule_scc_drop_container_capabilitiesResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.2.9, CM-6, CM-6(1)\nDescriptionContainers should not enable more capabilites than needed as this opens the door for malicious use. To disable the capabilities, the appropriate Security Context Constraints (SCCs) should set all capabilities as * or a list of capabilities in requiredDropCapabilities.RationaleBy default, containers run with a default set of capabilities as assigned by the Container Runtime which can include dangerous or highly privileged capabilities. Capabilities should be dropped unless absolutely critical for the container to run software as added capabilities that are not required allow for malicious containers or attackers.Evaluation messagesinfo No candidate or applicable check found.Limit Access to the Host Process ID Namespacexccdf_org.ssgproject.content_rule_scc_limit_process_id_namespace mediumLimit Access to the Host Process ID NamespaceRule IDxccdf_org.ssgproject.content_rule_scc_limit_process_id_namespaceResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.2.2, CM-6, CM-6(1)\nDescriptionContainers should not be allowed access to the host's process ID namespace. To prevent containers from getting access to a host's process ID namespace, the appropriate Security Context Constraints (SCCs) should set allowHostPID to false.RationaleA container running in the host's PID namespace can inspect processes running outside the container which can be used to escalate privileges outside of the container.Evaluation messagesinfo No candidate or applicable check found.Limit Access to the Host Network Namespacexccdf_org.ssgproject.content_rule_scc_limit_network_namespace mediumCCE-83492-9 Limit Access to the Host Network NamespaceRule IDxccdf_org.ssgproject.content_rule_scc_limit_network_namespaceResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83492-9\nReferences: 5.2.4, CM-6, CM-6(1)\nDescriptionContainers should not be allowed access to the host's network namespace. To prevent containers from getting access to a host's network namespace, the appropriate Security Context Constraints (SCCs) should set allowHostNetwork to false.RationaleA container running in the host's network namespace could access the host network traffic to and from other pods potentially allowing an attacker to exploit pods and network traffic.Evaluation messagesinfo No candidate or applicable check found.Limit Containers Ability to Escalate Privilegesxccdf_org.ssgproject.content_rule_scc_limit_privilege_escalation mediumCCE-83447-3 Limit Containers Ability to Escalate PrivilegesRule IDxccdf_org.ssgproject.content_rule_scc_limit_privilege_escalationResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83447-3\nReferences: 5.2.5, CM-6, CM-6(1)\nDescriptionContainers should be limited to only the privileges required to run and should not be allowed to escalate their privileges. To prevent containers from escalating privileges, the appropriate Security Context Constraints (SCCs) should set allowPrivilegeEscalation to false.RationalePrivileged containers have access to more of the Linux Kernel capabilities and devices. If a privileged container were compromised, an attacker would have full access to the container and host.Evaluation messagesinfo No candidate or applicable check found.Limit Container Capabilitiesxccdf_org.ssgproject.content_rule_scc_limit_container_allowed_capabilities mediumLimit Container CapabilitiesRule IDxccdf_org.ssgproject.content_rule_scc_limit_container_allowed_capabilitiesResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.2.8, CM-6, CM-6(1)\nDescriptionContainers should not enable more capabilites than needed as this opens the door for malicious use. To enable only the required capabilities, the appropriate Security Context Constraints (SCCs) should set capabilities as a list in allowedCapabilities.RationaleBy default, containers run with a default set of capabilities as assigned by the Container Runtime which can include dangerous or highly privileged capabilities. Capabilities should be dropped unless absolutely critical for the container to run software as added capabilities that are not required allow for malicious containers or attackers.Evaluation messagesinfo No candidate or applicable check found.Limit Privileged Container Usexccdf_org.ssgproject.content_rule_scc_limit_privileged_containers mediumLimit Privileged Container UseRule IDxccdf_org.ssgproject.content_rule_scc_limit_privileged_containersResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.2.1, CM-6, CM-6(1)\nDescriptionContainers should be limited to only the privileges required to run. To prevent containers from running as privileged containers, the appropriate Security Context Constraints (SCCs) should set allowPrivilegedContainer to false.RationalePrivileged containers have access to all Linux Kernel capabilities and devices. If a privileged container were compromised, an attacker would have full access to the container and host.Evaluation messagesinfo No candidate or applicable check found.Limit Use of the CAP_NET_RAWxccdf_org.ssgproject.content_rule_scc_limit_net_raw_capability mediumLimit Use of the CAP_NET_RAWRule IDxccdf_org.ssgproject.content_rule_scc_limit_net_raw_capabilityResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.2.7, CM-6, CM-6(1)\nDescriptionContainers should not enable more capabilites than needed as this opens the door for malicious use. CAP_NET_RAW enables a container to launch a network attack on another container or cluster. To disable the CAP_NET_RAW capability, the appropriate Security Context Constraints (SCCs) should set NET_RAW in requiredDropCapabilities.RationaleBy default, containers run with a default set of capabilities as assigned by the Container Runtime which can include dangerous or highly privileged capabilities. If the CAP_NET_RAW is enabled, it may be misused by malicious containers or attackers.Evaluation messagesinfo No candidate or applicable check found.Configure An Identity Providerxccdf_org.ssgproject.content_rule_idp_is_configured mediumCCE-84088-4 Configure An Identity ProviderRule IDxccdf_org.ssgproject.content_rule_idp_is_configuredResultpassMulti-check rulenoOVAL Definition IDoval:ssg-idp_is_configured:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84088-4\nReferences: 3.1.1, AC-2, AC-2(1), AC-2(2), AC-2(3), AC-2(4), AC-2(5), AC-2(6), AC-2(7), AC-2(8), AC-7\nDescription For users to interact with OpenShift Container Platform, they must first authenticate to the cluster. The authentication layer identifies the user associated with requests to the OpenShift Container Platform API. The authorization layer then uses information about the requesting user to determine if the request is allowed. Understanding authentication | Authentication | OpenShift Container Platform The OpenShift Container Platform includes a built-in OAuth server for token-based authentication. Developers and administrators obtain OAuth access tokens to authenticate themselves to the API. It is recommended for an administrator to configure OAuth to specify an identity provider after the cluster is installed. User access to the cluster is managed through the identity provider. Understanding identity provider configuration | Authentication | OpenShift Container Platform OpenShift includes built-in role based access control (RBAC) to determine whether a user is allowed to perform a given action within the cluster. Roles can have cluster scope or local (i.e. project) scope. Using RBAC to define and apply permissions | Authentication | OpenShift Container Platform Rationale With any authentication mechanism the ability to revoke credentials if they are compromised or no longer required, is a key control. Kubernetes client certificate authentication does not allow for this due to a lack of support for certificate revocation. OpenShift's built-in OAuth server allows credential revocation by relying on the Identity provider, as well as giving the administrators the ability to revoke any tokens given to a specific user. Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /apis/config.openshift.io/v1/oauths/cluster API endpoint to the local /kubernetes-api-resources/apis/config.openshift.io/v1/oauths/cluster file.OVAL test results details In the file '/apis/config.openshift.io/v1/oauths/cluster' find only one object at path '.spec.identityProviders[:].type'. oval:ssg-test_idp_is_configured:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/apis/config.openshift.io/v1/oauths/cluster/kubernetes-api-resources/apis/config.openshift.io/v1/oauthscluster.spec.identityProviders[:].type HTPasswd Find the file to be checked ('/apis/config.openshift.io/v1/oauths/cluster'). oval:ssg-test_file_for_idp_is_configured:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/apis/config.openshift.io/v1/oauths/clusterregular10006500001000650000981rw------- Verify Group Who Owns The Worker Proxy Kubeconfig Filexccdf_org.ssgproject.content_rule_file_groupowner_proxy_kubeconfig mediumVerify Group Who Owns The Worker Proxy Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_proxy_kubeconfigResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 4.1.4, CM-6, CM-6(1)\nDescriptionTo ensure the Kubernetes ConfigMap is mounted into the sdn daemonset pods with the correct ownership, make sure that the sdn-config ConfigMap is mounted using a ConfigMap at the /config mount point and that the sdn container points to that configuration using the --proxy-config command line option. Run: oc get -nopenshift-sdn ds sdn -ojson | jq -r '.spec.template.spec.containers[] | select(.name == \"sdn\")' and ensure the --proxy-config parameter points to /config/kube-proxy-config.yaml and that the config mount point is mounted from the sdn-config ConfigMap.RationaleThe kubeconfig file for kube-proxy provides permissions to the kube-proxy service. The proxy kubeconfig file contains information about the administrative configuration of the OpenShift cluster that is configured on the system. Protection of this file is critical for OpenShift security. The file is provided via a ConfigMap mount, so the kubelet itself makes sure that the file permissions are appropriate for the container taking it into use.Evaluation messagesinfo No candidate or applicable check found.Verify Permissions on the Worker Proxy Kubeconfig Filexccdf_org.ssgproject.content_rule_file_permissions_proxy_kubeconfig mediumCCE-84047-0 Verify Permissions on the Worker Proxy Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_proxy_kubeconfigResultfailMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_proxy_kubeconfig:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84047-0\nReferences: 4.1.3, CM-6, CM-6(1)\nDescriptionTo ensure the Kubernetes ConfigMap is mounted into the sdn daemonset pods with the correct permissions, make sure that the sdn-config ConfigMap is mounted using restrictive permissions. Check that the config VolumeMount mounts the sdn-config configMap with permissions set to 420: { \"configMap\": { \"defaultMode\": 420, \"name\": \"sdn-config\" }, \"name\": \"config\" } RationaleThe kube-proxy kubeconfig file controls various parameters of the kube-proxy service in the worker node. If used, you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system. The kube-proxy runs with the kubeconfig parameters configured as a Kubernetes ConfigMap instead of a file. In this case, there is no proxy kubeconfig file. But appropriate permissions still need to be set in the ConfigMap mount.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /apis/apps/v1/namespaces/openshift-sdn/daemonsets/sdn API endpoint to the local /kubernetes-api-resources/apis/apps/v1/namespaces/openshift-sdn/daemonsets/sdn file.OVAL test results details In the file '/apis/apps/v1/namespaces/openshift-sdn/daemonsets/sdn' find only one object at path 'spec.template.spec.volumes[:].configMap['defaultMode','name']'. oval:ssg-test_file_permissions_proxy_kubeconfig:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_proxy_kubeconfig:obj:1 of type yamlfilecontent_objectFilepathYamlpath/kubernetes-api-resources/kubernetes-api-resources/apis/apps/v1/namespaces/openshift-sdn/daemonsets/sdnspec.template.spec.volumes[:].configMap['defaultMode','name'] Find the file to be checked ('/apis/apps/v1/namespaces/openshift-sdn/daemonsets/sdn'). oval:ssg-test_file_for_file_permissions_proxy_kubeconfig:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_for_file_permissions_proxy_kubeconfig:obj:1 of type file_objectFilepath/kubernetes-api-resources/kubernetes-api-resources/apis/apps/v1/namespaces/openshift-sdn/daemonsets/sdn Verify User Who Owns The Worker Proxy Kubeconfig Filexccdf_org.ssgproject.content_rule_file_owner_proxy_kubeconfig mediumVerify User Who Owns The Worker Proxy Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_owner_proxy_kubeconfigResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 4.1.4, CM-6, CM-6(1)\nDescriptionTo ensure the Kubernetes ConfigMap is mounted into the sdn daemonset pods with the correct ownership, make sure that the sdn-config ConfigMap is mounted using a ConfigMap at the /config mount point and that the sdn container points to that configuration using the --proxy-config command line option. Run: oc get -nopenshift-sdn ds sdn -ojson | jq -r '.spec.template.spec.containers[] | select(.name == \"sdn\")' and ensure the --proxy-config parameter points to /config/kube-proxy-config.yaml and that the config mount point is mounted from the sdn-config ConfigMap.RationaleThe kubeconfig file for kube-proxy provides permissions to the kube-proxy service. The proxy kubeconfig file contains information about the administrative configuration of the OpenShift cluster that is configured on the system. Protection of this file is critical for OpenShift security. The file is provided via a ConfigMap mount, so the kubelet itself makes sure that the file permissions are appropriate for the container taking it into use.Evaluation messagesinfo No candidate or applicable check found.Ensure That The etcd Key File Is Correctly Setxccdf_org.ssgproject.content_rule_etcd_key_file mediumCCE-83745-0 Ensure That The etcd Key File Is Correctly SetRule IDxccdf_org.ssgproject.content_rule_etcd_key_fileResultpassMulti-check rulenoOVAL Definition IDoval:ssg-etcd_key_file:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83745-0\nReferences: 2.1, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo ensure the etcd service is serving TLS to clients, make sure the etcd-pod* ConfigMaps in the openshift-etcd namespace contain the following argument for the etcd binary in the etcd pod: oc get -nopenshift-etcd cm etcd-pod -oyaml | grep \"\\-\\-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-serving-NODE_NAME.key\". Note that the [a-z]+ is being used since the directory might change between OpenShift versions.RationaleWithout cryptographic integrity protections, information can be altered by unauthorized users without detection.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-etcd/configmaps/etcd-pod API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod file.OVAL test results details In the file '/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod' find only one object at path '.data['pod.yaml']'. oval:ssg-test_etcd_key_file:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmapsetcd-pod.data['pod.yaml'] apiVersion: v1 kind: Pod metadata: name: etcd namespace: openshift-etcd labels: app: etcd k8s-app: etcd etcd: \"true\" revision: \"REVISION\" spec: initContainers: - name: etcd-ensure-env-vars image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail : \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST?not set}\" : \"${NODE_NODE_ENVVAR_NAME_ETCD_NAME?not set}\" : \"${NODE_NODE_ENVVAR_NAME_IP?not set}\" # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected node IP to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_IP}\" \u0026gt;\u0026amp;2 exit 1 fi # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected etcd url host to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" \u0026gt;\u0026amp;2 exit 1 fi resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: etcd-resources-copy image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail rm -f $(grep -l '^### Created by cluster-etcd-operator' /usr/local/bin/*) cp -p /etc/kubernetes/static-pod-certs/configmaps/etcd-scripts/*.sh /usr/local/bin resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /usr/local/bin name: usr-local-bin containers: # The etcdctl container should always be first. It is intended to be used # to open a remote shell via `oc rsh` that is ready to run `etcdctl`. - name: etcdctl image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - \"/bin/bash\" - \"-c\" - \"trap TERM INT; sleep infinity \u0026amp; wait\" resources: requests: memory: 60Mi cpu: 10m volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: etcd image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail etcdctl member list || true # this has a non-zero return code if the command is non-zero. If you use an export first, it doesn't and you # will succeed when you should fail. ETCD_INITIAL_CLUSTER=$(discover-etcd-initial-cluster \\ --cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --endpoints=${ALL_ETCD_ENDPOINTS} \\ --data-dir=/var/lib/etcd \\ --target-peer-url-host=${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST} \\ --target-name=NODE_NAME) export ETCD_INITIAL_CLUSTER # we cannot use the \"normal\" port conflict initcontainer because when we upgrade, the existing static pod will never yield, # so we do the detection in etcd container itsefl. echo -n \"Waiting for ports 2379, 2380 and 9978 to be released.\" while [ -n \"$(ss -Htan '( sport = 2379 or sport = 2380 or sport = 9978 )')\" ]; do echo -n \".\" sleep 1 done export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} env | grep ETCD | grep -v NODE set -x # See https://etcd.io/docs/v3.4.0/tuning/ for why we use ionice exec ionice -c2 -n0 etcd \\ --log-level=info \\ --initial-advertise-peer-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2380 \\ --cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.crt \\ --key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.key \\ --trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --client-cert-auth=true \\ --peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --peer-trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --peer-client-cert-auth=true \\ --advertise-client-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2379 \\ --listen-client-urls=https://0.0.0.0:2379 \\ --listen-peer-urls=https://0.0.0.0:2380 \\ --listen-metrics-urls=https://0.0.0.0:9978 || mv /etc/kubernetes/etcd-backup-dir/etcd-member.yaml /etc/kubernetes/manifests env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 600Mi cpu: 300m readinessProbe: tcpSocket: port: 2380 failureThreshold: 3 initialDelaySeconds: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 5 securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir - name: etcd-metrics image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} exec etcd grpc-proxy start \\ --endpoints https://${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}:9978 \\ --metrics-addr https://0.0.0.0:9979 \\ --listen-addr 127.0.0.1:9977 \\ --key /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --key-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.key \\ --cert /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --cert-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.crt \\ --cacert /etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --trusted-ca-file /etc/kubernetes/static-pod-certs/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crt env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 200Mi cpu: 40m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: \"Exists\" volumes: - hostPath: path: /etc/kubernetes/manifests name: static-pod-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-pod-REVISION name: resource-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-certs name: cert-dir - hostPath: path: /var/lib/etcd type: \"\" name: data-dir - hostPath: path: /usr/local/bin name: usr-local-bin Find the file to be checked ('/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod'). oval:ssg-test_file_for_etcd_key_file:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-podregular1000650000100065000019673rw------- Ensure That The etcd Peer Client Certificate Is Correctly Setxccdf_org.ssgproject.content_rule_etcd_peer_cert_file mediumCCE-83847-4 Ensure That The etcd Peer Client Certificate Is Correctly SetRule IDxccdf_org.ssgproject.content_rule_etcd_peer_cert_fileResultpassMulti-check rulenoOVAL Definition IDoval:ssg-etcd_peer_cert_file:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83847-4\nReferences: 2.4, SC-8, SC-8(1), SC-8(2)\nDescriptionTo ensure the etcd service is serving TLS to peers, make sure the etcd-pod* ConfigMaps in the openshift-etcd namespace contain the following argument for the etcd binary in the etcd pod: --peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-peer-NODE_NAME.crt Note that the [a-z]+ is being used since the directory might change between OpenShift versions.RationaleWithout cryptographic integrity protections, information can be altered by unauthorized users without detection.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-etcd/configmaps/etcd-pod API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod file.OVAL test results details In the file '/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod' find only one object at path '.data['pod.yaml']'. oval:ssg-test_etcd_peer_cert_file:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmapsetcd-pod.data['pod.yaml'] apiVersion: v1 kind: Pod metadata: name: etcd namespace: openshift-etcd labels: app: etcd k8s-app: etcd etcd: \"true\" revision: \"REVISION\" spec: initContainers: - name: etcd-ensure-env-vars image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail : \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST?not set}\" : \"${NODE_NODE_ENVVAR_NAME_ETCD_NAME?not set}\" : \"${NODE_NODE_ENVVAR_NAME_IP?not set}\" # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected node IP to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_IP}\" \u0026gt;\u0026amp;2 exit 1 fi # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected etcd url host to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" \u0026gt;\u0026amp;2 exit 1 fi resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: etcd-resources-copy image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail rm -f $(grep -l '^### Created by cluster-etcd-operator' /usr/local/bin/*) cp -p /etc/kubernetes/static-pod-certs/configmaps/etcd-scripts/*.sh /usr/local/bin resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /usr/local/bin name: usr-local-bin containers: # The etcdctl container should always be first. It is intended to be used # to open a remote shell via `oc rsh` that is ready to run `etcdctl`. - name: etcdctl image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - \"/bin/bash\" - \"-c\" - \"trap TERM INT; sleep infinity \u0026amp; wait\" resources: requests: memory: 60Mi cpu: 10m volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: etcd image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail etcdctl member list || true # this has a non-zero return code if the command is non-zero. If you use an export first, it doesn't and you # will succeed when you should fail. ETCD_INITIAL_CLUSTER=$(discover-etcd-initial-cluster \\ --cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --endpoints=${ALL_ETCD_ENDPOINTS} \\ --data-dir=/var/lib/etcd \\ --target-peer-url-host=${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST} \\ --target-name=NODE_NAME) export ETCD_INITIAL_CLUSTER # we cannot use the \"normal\" port conflict initcontainer because when we upgrade, the existing static pod will never yield, # so we do the detection in etcd container itsefl. echo -n \"Waiting for ports 2379, 2380 and 9978 to be released.\" while [ -n \"$(ss -Htan '( sport = 2379 or sport = 2380 or sport = 9978 )')\" ]; do echo -n \".\" sleep 1 done export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} env | grep ETCD | grep -v NODE set -x # See https://etcd.io/docs/v3.4.0/tuning/ for why we use ionice exec ionice -c2 -n0 etcd \\ --log-level=info \\ --initial-advertise-peer-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2380 \\ --cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.crt \\ --key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.key \\ --trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --client-cert-auth=true \\ --peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --peer-trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --peer-client-cert-auth=true \\ --advertise-client-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2379 \\ --listen-client-urls=https://0.0.0.0:2379 \\ --listen-peer-urls=https://0.0.0.0:2380 \\ --listen-metrics-urls=https://0.0.0.0:9978 || mv /etc/kubernetes/etcd-backup-dir/etcd-member.yaml /etc/kubernetes/manifests env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 600Mi cpu: 300m readinessProbe: tcpSocket: port: 2380 failureThreshold: 3 initialDelaySeconds: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 5 securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir - name: etcd-metrics image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} exec etcd grpc-proxy start \\ --endpoints https://${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}:9978 \\ --metrics-addr https://0.0.0.0:9979 \\ --listen-addr 127.0.0.1:9977 \\ --key /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --key-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.key \\ --cert /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --cert-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.crt \\ --cacert /etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --trusted-ca-file /etc/kubernetes/static-pod-certs/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crt env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 200Mi cpu: 40m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: \"Exists\" volumes: - hostPath: path: /etc/kubernetes/manifests name: static-pod-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-pod-REVISION name: resource-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-certs name: cert-dir - hostPath: path: /var/lib/etcd type: \"\" name: data-dir - hostPath: path: /usr/local/bin name: usr-local-bin Find the file to be checked ('/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod'). oval:ssg-test_file_for_etcd_peer_cert_file:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-podregular1000650000100065000019673rw------- Enable The Peer Client Certificate Authenticationxccdf_org.ssgproject.content_rule_etcd_peer_client_cert_auth mediumCCE-83465-5 Enable The Peer Client Certificate AuthenticationRule IDxccdf_org.ssgproject.content_rule_etcd_peer_client_cert_authResultpassMulti-check rulenoOVAL Definition IDoval:ssg-etcd_peer_client_cert_auth:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83465-5\nReferences: 2.5, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo ensure the etcd service is serving TLS to clients, make sure the etcd-pod* ConfigMaps in the openshift-etcd namespace contain the following argument for the etcd binary in the etcd pod: oc get -nopenshift-etcd cm etcd-pod -oyaml | grep \"\\-\\-peer-client-cert-auth=\" the parameter should be set to true.RationaleWithout cryptographic integrity protections, information can be altered by unauthorized users without detection.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-etcd/configmaps/etcd-pod API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod file.OVAL test results details In the file '/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod' find only one object at path '.data['pod.yaml']'. oval:ssg-test_etcd_peer_client_cert_auth:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmapsetcd-pod.data['pod.yaml'] apiVersion: v1 kind: Pod metadata: name: etcd namespace: openshift-etcd labels: app: etcd k8s-app: etcd etcd: \"true\" revision: \"REVISION\" spec: initContainers: - name: etcd-ensure-env-vars image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail : \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST?not set}\" : \"${NODE_NODE_ENVVAR_NAME_ETCD_NAME?not set}\" : \"${NODE_NODE_ENVVAR_NAME_IP?not set}\" # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected node IP to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_IP}\" \u0026gt;\u0026amp;2 exit 1 fi # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected etcd url host to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" \u0026gt;\u0026amp;2 exit 1 fi resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: etcd-resources-copy image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail rm -f $(grep -l '^### Created by cluster-etcd-operator' /usr/local/bin/*) cp -p /etc/kubernetes/static-pod-certs/configmaps/etcd-scripts/*.sh /usr/local/bin resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /usr/local/bin name: usr-local-bin containers: # The etcdctl container should always be first. It is intended to be used # to open a remote shell via `oc rsh` that is ready to run `etcdctl`. - name: etcdctl image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - \"/bin/bash\" - \"-c\" - \"trap TERM INT; sleep infinity \u0026amp; wait\" resources: requests: memory: 60Mi cpu: 10m volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: etcd image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail etcdctl member list || true # this has a non-zero return code if the command is non-zero. If you use an export first, it doesn't and you # will succeed when you should fail. ETCD_INITIAL_CLUSTER=$(discover-etcd-initial-cluster \\ --cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --endpoints=${ALL_ETCD_ENDPOINTS} \\ --data-dir=/var/lib/etcd \\ --target-peer-url-host=${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST} \\ --target-name=NODE_NAME) export ETCD_INITIAL_CLUSTER # we cannot use the \"normal\" port conflict initcontainer because when we upgrade, the existing static pod will never yield, # so we do the detection in etcd container itsefl. echo -n \"Waiting for ports 2379, 2380 and 9978 to be released.\" while [ -n \"$(ss -Htan '( sport = 2379 or sport = 2380 or sport = 9978 )')\" ]; do echo -n \".\" sleep 1 done export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} env | grep ETCD | grep -v NODE set -x # See https://etcd.io/docs/v3.4.0/tuning/ for why we use ionice exec ionice -c2 -n0 etcd \\ --log-level=info \\ --initial-advertise-peer-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2380 \\ --cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.crt \\ --key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.key \\ --trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --client-cert-auth=true \\ --peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --peer-trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --peer-client-cert-auth=true \\ --advertise-client-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2379 \\ --listen-client-urls=https://0.0.0.0:2379 \\ --listen-peer-urls=https://0.0.0.0:2380 \\ --listen-metrics-urls=https://0.0.0.0:9978 || mv /etc/kubernetes/etcd-backup-dir/etcd-member.yaml /etc/kubernetes/manifests env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 600Mi cpu: 300m readinessProbe: tcpSocket: port: 2380 failureThreshold: 3 initialDelaySeconds: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 5 securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir - name: etcd-metrics image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} exec etcd grpc-proxy start \\ --endpoints https://${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}:9978 \\ --metrics-addr https://0.0.0.0:9979 \\ --listen-addr 127.0.0.1:9977 \\ --key /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --key-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.key \\ --cert /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --cert-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.crt \\ --cacert /etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --trusted-ca-file /etc/kubernetes/static-pod-certs/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crt env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 200Mi cpu: 40m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: \"Exists\" volumes: - hostPath: path: /etc/kubernetes/manifests name: static-pod-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-pod-REVISION name: resource-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-certs name: cert-dir - hostPath: path: /var/lib/etcd type: \"\" name: data-dir - hostPath: path: /usr/local/bin name: usr-local-bin Find the file to be checked ('/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod'). oval:ssg-test_file_for_etcd_peer_client_cert_auth:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-podregular1000650000100065000019673rw------- Ensure That The etcd Peer Key File Is Correctly Setxccdf_org.ssgproject.content_rule_etcd_peer_key_file mediumCCE-83711-2 Ensure That The etcd Peer Key File Is Correctly SetRule IDxccdf_org.ssgproject.content_rule_etcd_peer_key_fileResultpassMulti-check rulenoOVAL Definition IDoval:ssg-etcd_peer_key_file:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83711-2\nReferences: 2.4, SC-8, SC-8(1), SC-8(2)\nDescriptionTo ensure the etcd service is serving TLS to peers, make sure the etcd-pod* ConfigMaps in the openshift-etcd namespace contain the following argument for the etcd binary in the etcd pod: oc get -nopenshift-etcd cm etcd-pod -oyaml | grep \"\\-\\-peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-peer-NODE_NAME.key\" Note that the [a-z]+ is being used since the directory might change between OpenShift versions.RationaleWithout cryptographic integrity protections, information can be altered by unauthorized users without detection.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-etcd/configmaps/etcd-pod API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod file.OVAL test results details In the file '/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod' find only one object at path '.data['pod.yaml']'. oval:ssg-test_etcd_peer_key_file:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmapsetcd-pod.data['pod.yaml'] apiVersion: v1 kind: Pod metadata: name: etcd namespace: openshift-etcd labels: app: etcd k8s-app: etcd etcd: \"true\" revision: \"REVISION\" spec: initContainers: - name: etcd-ensure-env-vars image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail : \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST?not set}\" : \"${NODE_NODE_ENVVAR_NAME_ETCD_NAME?not set}\" : \"${NODE_NODE_ENVVAR_NAME_IP?not set}\" # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected node IP to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_IP}\" \u0026gt;\u0026amp;2 exit 1 fi # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected etcd url host to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" \u0026gt;\u0026amp;2 exit 1 fi resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: etcd-resources-copy image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail rm -f $(grep -l '^### Created by cluster-etcd-operator' /usr/local/bin/*) cp -p /etc/kubernetes/static-pod-certs/configmaps/etcd-scripts/*.sh /usr/local/bin resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /usr/local/bin name: usr-local-bin containers: # The etcdctl container should always be first. It is intended to be used # to open a remote shell via `oc rsh` that is ready to run `etcdctl`. - name: etcdctl image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - \"/bin/bash\" - \"-c\" - \"trap TERM INT; sleep infinity \u0026amp; wait\" resources: requests: memory: 60Mi cpu: 10m volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: etcd image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail etcdctl member list || true # this has a non-zero return code if the command is non-zero. If you use an export first, it doesn't and you # will succeed when you should fail. ETCD_INITIAL_CLUSTER=$(discover-etcd-initial-cluster \\ --cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --endpoints=${ALL_ETCD_ENDPOINTS} \\ --data-dir=/var/lib/etcd \\ --target-peer-url-host=${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST} \\ --target-name=NODE_NAME) export ETCD_INITIAL_CLUSTER # we cannot use the \"normal\" port conflict initcontainer because when we upgrade, the existing static pod will never yield, # so we do the detection in etcd container itsefl. echo -n \"Waiting for ports 2379, 2380 and 9978 to be released.\" while [ -n \"$(ss -Htan '( sport = 2379 or sport = 2380 or sport = 9978 )')\" ]; do echo -n \".\" sleep 1 done export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} env | grep ETCD | grep -v NODE set -x # See https://etcd.io/docs/v3.4.0/tuning/ for why we use ionice exec ionice -c2 -n0 etcd \\ --log-level=info \\ --initial-advertise-peer-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2380 \\ --cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.crt \\ --key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.key \\ --trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --client-cert-auth=true \\ --peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --peer-trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --peer-client-cert-auth=true \\ --advertise-client-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2379 \\ --listen-client-urls=https://0.0.0.0:2379 \\ --listen-peer-urls=https://0.0.0.0:2380 \\ --listen-metrics-urls=https://0.0.0.0:9978 || mv /etc/kubernetes/etcd-backup-dir/etcd-member.yaml /etc/kubernetes/manifests env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 600Mi cpu: 300m readinessProbe: tcpSocket: port: 2380 failureThreshold: 3 initialDelaySeconds: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 5 securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir - name: etcd-metrics image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} exec etcd grpc-proxy start \\ --endpoints https://${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}:9978 \\ --metrics-addr https://0.0.0.0:9979 \\ --listen-addr 127.0.0.1:9977 \\ --key /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --key-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.key \\ --cert /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --cert-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.crt \\ --cacert /etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --trusted-ca-file /etc/kubernetes/static-pod-certs/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crt env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 200Mi cpu: 40m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: \"Exists\" volumes: - hostPath: path: /etc/kubernetes/manifests name: static-pod-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-pod-REVISION name: resource-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-certs name: cert-dir - hostPath: path: /var/lib/etcd type: \"\" name: data-dir - hostPath: path: /usr/local/bin name: usr-local-bin Find the file to be checked ('/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod'). oval:ssg-test_file_for_etcd_peer_key_file:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-podregular1000650000100065000019673rw------- Ensure That The etcd Client Certificate Is Correctly Setxccdf_org.ssgproject.content_rule_etcd_cert_file mediumCCE-83553-8 Ensure That The etcd Client Certificate Is Correctly SetRule IDxccdf_org.ssgproject.content_rule_etcd_cert_fileResultpassMulti-check rulenoOVAL Definition IDoval:ssg-etcd_cert_file:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83553-8\nReferences: 2.1, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo ensure the etcd service is serving TLS to clients, make sure the etcd-pod* ConfigMaps in the openshift-etcd namespace contain the following argument for the etcd binary in the etcd pod: --cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-[a-z]+/etcd-serving-NODE_NAME.crt. Note that the [a-z]+ is being used since the directory might change between OpenShift versions.RationaleWithout cryptographic integrity protections, information can be altered by unauthorized users without detection.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-etcd/configmaps/etcd-pod API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod file.OVAL test results details In the file '/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod' find only one object at path '.data['pod.yaml']'. oval:ssg-test_etcd_cert_file:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmapsetcd-pod.data['pod.yaml'] apiVersion: v1 kind: Pod metadata: name: etcd namespace: openshift-etcd labels: app: etcd k8s-app: etcd etcd: \"true\" revision: \"REVISION\" spec: initContainers: - name: etcd-ensure-env-vars image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail : \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST?not set}\" : \"${NODE_NODE_ENVVAR_NAME_ETCD_NAME?not set}\" : \"${NODE_NODE_ENVVAR_NAME_IP?not set}\" # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected node IP to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_IP}\" \u0026gt;\u0026amp;2 exit 1 fi # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected etcd url host to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" \u0026gt;\u0026amp;2 exit 1 fi resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: etcd-resources-copy image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail rm -f $(grep -l '^### Created by cluster-etcd-operator' /usr/local/bin/*) cp -p /etc/kubernetes/static-pod-certs/configmaps/etcd-scripts/*.sh /usr/local/bin resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /usr/local/bin name: usr-local-bin containers: # The etcdctl container should always be first. It is intended to be used # to open a remote shell via `oc rsh` that is ready to run `etcdctl`. - name: etcdctl image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - \"/bin/bash\" - \"-c\" - \"trap TERM INT; sleep infinity \u0026amp; wait\" resources: requests: memory: 60Mi cpu: 10m volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: etcd image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail etcdctl member list || true # this has a non-zero return code if the command is non-zero. If you use an export first, it doesn't and you # will succeed when you should fail. ETCD_INITIAL_CLUSTER=$(discover-etcd-initial-cluster \\ --cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --endpoints=${ALL_ETCD_ENDPOINTS} \\ --data-dir=/var/lib/etcd \\ --target-peer-url-host=${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST} \\ --target-name=NODE_NAME) export ETCD_INITIAL_CLUSTER # we cannot use the \"normal\" port conflict initcontainer because when we upgrade, the existing static pod will never yield, # so we do the detection in etcd container itsefl. echo -n \"Waiting for ports 2379, 2380 and 9978 to be released.\" while [ -n \"$(ss -Htan '( sport = 2379 or sport = 2380 or sport = 9978 )')\" ]; do echo -n \".\" sleep 1 done export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} env | grep ETCD | grep -v NODE set -x # See https://etcd.io/docs/v3.4.0/tuning/ for why we use ionice exec ionice -c2 -n0 etcd \\ --log-level=info \\ --initial-advertise-peer-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2380 \\ --cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.crt \\ --key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.key \\ --trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --client-cert-auth=true \\ --peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --peer-trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --peer-client-cert-auth=true \\ --advertise-client-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2379 \\ --listen-client-urls=https://0.0.0.0:2379 \\ --listen-peer-urls=https://0.0.0.0:2380 \\ --listen-metrics-urls=https://0.0.0.0:9978 || mv /etc/kubernetes/etcd-backup-dir/etcd-member.yaml /etc/kubernetes/manifests env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 600Mi cpu: 300m readinessProbe: tcpSocket: port: 2380 failureThreshold: 3 initialDelaySeconds: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 5 securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir - name: etcd-metrics image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} exec etcd grpc-proxy start \\ --endpoints https://${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}:9978 \\ --metrics-addr https://0.0.0.0:9979 \\ --listen-addr 127.0.0.1:9977 \\ --key /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --key-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.key \\ --cert /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --cert-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.crt \\ --cacert /etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --trusted-ca-file /etc/kubernetes/static-pod-certs/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crt env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 200Mi cpu: 40m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: \"Exists\" volumes: - hostPath: path: /etc/kubernetes/manifests name: static-pod-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-pod-REVISION name: resource-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-certs name: cert-dir - hostPath: path: /var/lib/etcd type: \"\" name: data-dir - hostPath: path: /usr/local/bin name: usr-local-bin Find the file to be checked ('/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod'). oval:ssg-test_file_for_etcd_cert_file:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-podregular1000650000100065000019673rw------- Enable The Client Certificate Authenticationxccdf_org.ssgproject.content_rule_etcd_client_cert_auth mediumCCE-84077-7 Enable The Client Certificate AuthenticationRule IDxccdf_org.ssgproject.content_rule_etcd_client_cert_authResultpassMulti-check rulenoOVAL Definition IDoval:ssg-etcd_client_cert_auth:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84077-7\nReferences: 2.2, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo ensure the etcd service is serving TLS to clients, make sure the etcd-pod* ConfigMaps in the openshift-etcd namespace contain the following argument for the etcd binary in the etcd pod: oc get -nopenshift-etcd cm etcd-pod -oyaml | grep \"\\-\\-client-cert-auth=\" the parameter should be set to true.RationaleWithout cryptographic integrity protections, information can be altered by unauthorized users without detection.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-etcd/configmaps/etcd-pod API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod file.OVAL test results details In the file '/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod' find only one object at path '.data['pod.yaml']'. oval:ssg-test_etcd_client_cert_auth:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmapsetcd-pod.data['pod.yaml'] apiVersion: v1 kind: Pod metadata: name: etcd namespace: openshift-etcd labels: app: etcd k8s-app: etcd etcd: \"true\" revision: \"REVISION\" spec: initContainers: - name: etcd-ensure-env-vars image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail : \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST?not set}\" : \"${NODE_NODE_ENVVAR_NAME_ETCD_NAME?not set}\" : \"${NODE_NODE_ENVVAR_NAME_IP?not set}\" # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected node IP to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_IP}\" \u0026gt;\u0026amp;2 exit 1 fi # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected etcd url host to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" \u0026gt;\u0026amp;2 exit 1 fi resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: etcd-resources-copy image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail rm -f $(grep -l '^### Created by cluster-etcd-operator' /usr/local/bin/*) cp -p /etc/kubernetes/static-pod-certs/configmaps/etcd-scripts/*.sh /usr/local/bin resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /usr/local/bin name: usr-local-bin containers: # The etcdctl container should always be first. It is intended to be used # to open a remote shell via `oc rsh` that is ready to run `etcdctl`. - name: etcdctl image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - \"/bin/bash\" - \"-c\" - \"trap TERM INT; sleep infinity \u0026amp; wait\" resources: requests: memory: 60Mi cpu: 10m volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: etcd image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail etcdctl member list || true # this has a non-zero return code if the command is non-zero. If you use an export first, it doesn't and you # will succeed when you should fail. ETCD_INITIAL_CLUSTER=$(discover-etcd-initial-cluster \\ --cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --endpoints=${ALL_ETCD_ENDPOINTS} \\ --data-dir=/var/lib/etcd \\ --target-peer-url-host=${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST} \\ --target-name=NODE_NAME) export ETCD_INITIAL_CLUSTER # we cannot use the \"normal\" port conflict initcontainer because when we upgrade, the existing static pod will never yield, # so we do the detection in etcd container itsefl. echo -n \"Waiting for ports 2379, 2380 and 9978 to be released.\" while [ -n \"$(ss -Htan '( sport = 2379 or sport = 2380 or sport = 9978 )')\" ]; do echo -n \".\" sleep 1 done export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} env | grep ETCD | grep -v NODE set -x # See https://etcd.io/docs/v3.4.0/tuning/ for why we use ionice exec ionice -c2 -n0 etcd \\ --log-level=info \\ --initial-advertise-peer-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2380 \\ --cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.crt \\ --key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.key \\ --trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --client-cert-auth=true \\ --peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --peer-trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --peer-client-cert-auth=true \\ --advertise-client-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2379 \\ --listen-client-urls=https://0.0.0.0:2379 \\ --listen-peer-urls=https://0.0.0.0:2380 \\ --listen-metrics-urls=https://0.0.0.0:9978 || mv /etc/kubernetes/etcd-backup-dir/etcd-member.yaml /etc/kubernetes/manifests env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 600Mi cpu: 300m readinessProbe: tcpSocket: port: 2380 failureThreshold: 3 initialDelaySeconds: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 5 securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir - name: etcd-metrics image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} exec etcd grpc-proxy start \\ --endpoints https://${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}:9978 \\ --metrics-addr https://0.0.0.0:9979 \\ --listen-addr 127.0.0.1:9977 \\ --key /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --key-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.key \\ --cert /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --cert-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.crt \\ --cacert /etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --trusted-ca-file /etc/kubernetes/static-pod-certs/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crt env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 200Mi cpu: 40m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: \"Exists\" volumes: - hostPath: path: /etc/kubernetes/manifests name: static-pod-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-pod-REVISION name: resource-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-certs name: cert-dir - hostPath: path: /var/lib/etcd type: \"\" name: data-dir - hostPath: path: /usr/local/bin name: usr-local-bin Find the file to be checked ('/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod'). oval:ssg-test_file_for_etcd_client_cert_auth:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-podregular1000650000100065000019673rw------- Disable etcd Peer Self-Signed Certificatesxccdf_org.ssgproject.content_rule_etcd_peer_auto_tls mediumCCE-84184-1 Disable etcd Peer Self-Signed CertificatesRule IDxccdf_org.ssgproject.content_rule_etcd_peer_auto_tlsResultpassMulti-check rulenoOVAL Definition IDoval:ssg-etcd_peer_auto_tls:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84184-1\nReferences: 2.6, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo ensure the etcd service is not using self-signed certificates, run the following command: $ oc get cm/etcd-pod -n openshift-etcd -o yaml The etcd pod configuration contained in the configmap should not contain the --peer-auto-tls=true flag.RationaleWithout cryptographic integrity protections, information can be altered by unauthorized users without detection. Using self-signed certificates ensures that the certificates are never validated against a certificate authority and could lead to compromised and invalidated data.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-etcd/configmaps/etcd-pod API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod file.OVAL test results details In the file '/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod' find only one object at path '.data[\"pod.yaml\"]'. oval:ssg-test_etcd_peer_auto_tls:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmapsetcd-pod.data[\"pod.yaml\"] apiVersion: v1 kind: Pod metadata: name: etcd namespace: openshift-etcd labels: app: etcd k8s-app: etcd etcd: \"true\" revision: \"REVISION\" spec: initContainers: - name: etcd-ensure-env-vars image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail : \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST?not set}\" : \"${NODE_NODE_ENVVAR_NAME_ETCD_NAME?not set}\" : \"${NODE_NODE_ENVVAR_NAME_IP?not set}\" # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected node IP to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_IP}\" \u0026gt;\u0026amp;2 exit 1 fi # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected etcd url host to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" \u0026gt;\u0026amp;2 exit 1 fi resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: etcd-resources-copy image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail rm -f $(grep -l '^### Created by cluster-etcd-operator' /usr/local/bin/*) cp -p /etc/kubernetes/static-pod-certs/configmaps/etcd-scripts/*.sh /usr/local/bin resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /usr/local/bin name: usr-local-bin containers: # The etcdctl container should always be first. It is intended to be used # to open a remote shell via `oc rsh` that is ready to run `etcdctl`. - name: etcdctl image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - \"/bin/bash\" - \"-c\" - \"trap TERM INT; sleep infinity \u0026amp; wait\" resources: requests: memory: 60Mi cpu: 10m volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: etcd image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail etcdctl member list || true # this has a non-zero return code if the command is non-zero. If you use an export first, it doesn't and you # will succeed when you should fail. ETCD_INITIAL_CLUSTER=$(discover-etcd-initial-cluster \\ --cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --endpoints=${ALL_ETCD_ENDPOINTS} \\ --data-dir=/var/lib/etcd \\ --target-peer-url-host=${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST} \\ --target-name=NODE_NAME) export ETCD_INITIAL_CLUSTER # we cannot use the \"normal\" port conflict initcontainer because when we upgrade, the existing static pod will never yield, # so we do the detection in etcd container itsefl. echo -n \"Waiting for ports 2379, 2380 and 9978 to be released.\" while [ -n \"$(ss -Htan '( sport = 2379 or sport = 2380 or sport = 9978 )')\" ]; do echo -n \".\" sleep 1 done export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} env | grep ETCD | grep -v NODE set -x # See https://etcd.io/docs/v3.4.0/tuning/ for why we use ionice exec ionice -c2 -n0 etcd \\ --log-level=info \\ --initial-advertise-peer-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2380 \\ --cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.crt \\ --key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.key \\ --trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --client-cert-auth=true \\ --peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --peer-trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --peer-client-cert-auth=true \\ --advertise-client-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2379 \\ --listen-client-urls=https://0.0.0.0:2379 \\ --listen-peer-urls=https://0.0.0.0:2380 \\ --listen-metrics-urls=https://0.0.0.0:9978 || mv /etc/kubernetes/etcd-backup-dir/etcd-member.yaml /etc/kubernetes/manifests env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 600Mi cpu: 300m readinessProbe: tcpSocket: port: 2380 failureThreshold: 3 initialDelaySeconds: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 5 securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir - name: etcd-metrics image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} exec etcd grpc-proxy start \\ --endpoints https://${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}:9978 \\ --metrics-addr https://0.0.0.0:9979 \\ --listen-addr 127.0.0.1:9977 \\ --key /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --key-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.key \\ --cert /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --cert-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.crt \\ --cacert /etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --trusted-ca-file /etc/kubernetes/static-pod-certs/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crt env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 200Mi cpu: 40m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: \"Exists\" volumes: - hostPath: path: /etc/kubernetes/manifests name: static-pod-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-pod-REVISION name: resource-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-certs name: cert-dir - hostPath: path: /var/lib/etcd type: \"\" name: data-dir - hostPath: path: /usr/local/bin name: usr-local-bin Find the file to be checked ('/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod'). oval:ssg-test_file_for_etcd_peer_auto_tls:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-podregular1000650000100065000019673rw------- Disable etcd Self-Signed Certificatesxccdf_org.ssgproject.content_rule_etcd_auto_tls mediumCCE-84199-9 Disable etcd Self-Signed CertificatesRule IDxccdf_org.ssgproject.content_rule_etcd_auto_tlsResultpassMulti-check rulenoOVAL Definition IDoval:ssg-etcd_auto_tls:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84199-9\nReferences: 2.3, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo ensure the etcd service is not using self-signed certificates, run the following command: $ oc get cm/etcd-pod -n openshift-etcd -o yaml The etcd pod configuration contained in the configmap should not contain the --auto-tls=true flag.RationaleWithout cryptographic integrity protections, information can be altered by unauthorized users without detection. Using self-signed certificates ensures that the certificates are never validated against a certificate authority and could lead to compromised and invalidated data.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-etcd/configmaps/etcd-pod API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod file.OVAL test results details In the file '/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod' find only one object at path '.data[\"pod.yaml\"]'. oval:ssg-test_etcd_auto_tls:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmapsetcd-pod.data[\"pod.yaml\"] apiVersion: v1 kind: Pod metadata: name: etcd namespace: openshift-etcd labels: app: etcd k8s-app: etcd etcd: \"true\" revision: \"REVISION\" spec: initContainers: - name: etcd-ensure-env-vars image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail : \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST?not set}\" : \"${NODE_NODE_ENVVAR_NAME_ETCD_NAME?not set}\" : \"${NODE_NODE_ENVVAR_NAME_IP?not set}\" # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_IP}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected node IP to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_IP}\" \u0026gt;\u0026amp;2 exit 1 fi # check for ipv4 addresses as well as ipv6 addresses with extra square brackets if [[ \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"${NODE_IP}\" \u0026amp;\u0026amp; \"${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" != \"[${NODE_IP}]\" ]]; then # echo the error message to stderr echo \"Expected etcd url host to be ${NODE_IP} got ${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}\" \u0026gt;\u0026amp;2 exit 1 fi resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: NODE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: etcd-resources-copy image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail rm -f $(grep -l '^### Created by cluster-etcd-operator' /usr/local/bin/*) cp -p /etc/kubernetes/static-pod-certs/configmaps/etcd-scripts/*.sh /usr/local/bin resources: requests: memory: 60Mi cpu: 10m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /usr/local/bin name: usr-local-bin containers: # The etcdctl container should always be first. It is intended to be used # to open a remote shell via `oc rsh` that is ready to run `etcdctl`. - name: etcdctl image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - \"/bin/bash\" - \"-c\" - \"trap TERM INT; sleep infinity \u0026amp; wait\" resources: requests: memory: 60Mi cpu: 10m volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" - name: etcd image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail etcdctl member list || true # this has a non-zero return code if the command is non-zero. If you use an export first, it doesn't and you # will succeed when you should fail. ETCD_INITIAL_CLUSTER=$(discover-etcd-initial-cluster \\ --cacert=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --cert=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --key=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --endpoints=${ALL_ETCD_ENDPOINTS} \\ --data-dir=/var/lib/etcd \\ --target-peer-url-host=${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST} \\ --target-name=NODE_NAME) export ETCD_INITIAL_CLUSTER # we cannot use the \"normal\" port conflict initcontainer because when we upgrade, the existing static pod will never yield, # so we do the detection in etcd container itsefl. echo -n \"Waiting for ports 2379, 2380 and 9978 to be released.\" while [ -n \"$(ss -Htan '( sport = 2379 or sport = 2380 or sport = 9978 )')\" ]; do echo -n \".\" sleep 1 done export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} env | grep ETCD | grep -v NODE set -x # See https://etcd.io/docs/v3.4.0/tuning/ for why we use ionice exec ionice -c2 -n0 etcd \\ --log-level=info \\ --initial-advertise-peer-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2380 \\ --cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.crt \\ --key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-serving/etcd-serving-NODE_NAME.key \\ --trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt \\ --client-cert-auth=true \\ --peer-cert-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --peer-key-file=/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --peer-trusted-ca-file=/etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --peer-client-cert-auth=true \\ --advertise-client-urls=https://${NODE_NODE_ENVVAR_NAME_IP}:2379 \\ --listen-client-urls=https://0.0.0.0:2379 \\ --listen-peer-urls=https://0.0.0.0:2380 \\ --listen-metrics-urls=https://0.0.0.0:9978 || mv /etc/kubernetes/etcd-backup-dir/etcd-member.yaml /etc/kubernetes/manifests env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 600Mi cpu: 300m readinessProbe: tcpSocket: port: 2380 failureThreshold: 3 initialDelaySeconds: 3 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 5 securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/manifests name: static-pod-dir - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir - name: etcd-metrics image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615 imagePullPolicy: IfNotPresent terminationMessagePolicy: FallbackToLogsOnError command: - /bin/sh - -c - | #!/bin/sh set -euo pipefail export ETCD_NAME=${NODE_NODE_ENVVAR_NAME_ETCD_NAME} exec etcd grpc-proxy start \\ --endpoints https://${NODE_NODE_ENVVAR_NAME_ETCD_URL_HOST}:9978 \\ --metrics-addr https://0.0.0.0:9979 \\ --listen-addr 127.0.0.1:9977 \\ --key /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key \\ --key-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.key \\ --cert /etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt \\ --cert-file /etc/kubernetes/static-pod-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-NODE_NAME.crt \\ --cacert /etc/kubernetes/static-pod-certs/configmaps/etcd-peer-client-ca/ca-bundle.crt \\ --trusted-ca-file /etc/kubernetes/static-pod-certs/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crt env: - name: \"ALL_ETCD_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_API\" value: \"3\" - name: \"ETCDCTL_CACERT\" value: \"/etc/kubernetes/static-pod-certs/configmaps/etcd-serving-ca/ca-bundle.crt\" - name: \"ETCDCTL_CERT\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.crt\" - name: \"ETCDCTL_ENDPOINTS\" value: \"https://192.168.50.10:2379,https://192.168.50.11:2379,https://192.168.50.12:2379\" - name: \"ETCDCTL_KEY\" value: \"/etc/kubernetes/static-pod-certs/secrets/etcd-all-peer/etcd-peer-NODE_NAME.key\" - name: \"ETCD_CIPHER_SUITES\" value: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\" - name: \"ETCD_DATA_DIR\" value: \"/var/lib/etcd\" - name: \"ETCD_ELECTION_TIMEOUT\" value: \"1000\" - name: \"ETCD_ENABLE_PPROF\" value: \"true\" - name: \"ETCD_HEARTBEAT_INTERVAL\" value: \"100\" - name: \"ETCD_IMAGE\" value: \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1d33e906d435b7a195f5876b722e4b17da684d140fe65c1e5e7ab8771a597615\" - name: \"ETCD_INITIAL_CLUSTER_STATE\" value: \"existing\" - name: \"ETCD_QUOTA_BACKEND_BYTES\" value: \"7516192768\" - name: \"NODE_master_0_ETCD_NAME\" value: \"master-0\" - name: \"NODE_master_0_ETCD_URL_HOST\" value: \"192.168.50.10\" - name: \"NODE_master_0_IP\" value: \"192.168.50.10\" - name: \"NODE_master_1_ETCD_NAME\" value: \"master-1\" - name: \"NODE_master_1_ETCD_URL_HOST\" value: \"192.168.50.11\" - name: \"NODE_master_1_IP\" value: \"192.168.50.11\" - name: \"NODE_master_2_ETCD_NAME\" value: \"master-2\" - name: \"NODE_master_2_ETCD_URL_HOST\" value: \"192.168.50.12\" - name: \"NODE_master_2_IP\" value: \"192.168.50.12\" resources: requests: memory: 200Mi cpu: 40m securityContext: privileged: true volumeMounts: - mountPath: /etc/kubernetes/static-pod-resources name: resource-dir - mountPath: /etc/kubernetes/static-pod-certs name: cert-dir - mountPath: /var/lib/etcd/ name: data-dir hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: \"Exists\" volumes: - hostPath: path: /etc/kubernetes/manifests name: static-pod-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-pod-REVISION name: resource-dir - hostPath: path: /etc/kubernetes/static-pod-resources/etcd-certs name: cert-dir - hostPath: path: /var/lib/etcd type: \"\" name: data-dir - hostPath: path: /usr/local/bin name: usr-local-bin Find the file to be checked ('/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod'). oval:ssg-test_file_for_etcd_auto_tls:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-etcd/configmaps/etcd-podregular1000650000100065000019673rw------- Create administrative boundaries between resources using namespacesxccdf_org.ssgproject.content_rule_general_namespaces_in_use mediumCreate administrative boundaries between resources using namespacesRule IDxccdf_org.ssgproject.content_rule_general_namespaces_in_useResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.6.1, CM-6, CM-6(1)\nDescriptionUse namespaces to isolate your Kubernetes objects.RationaleLimiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users.Evaluation messagesinfo No candidate or applicable check found.Manage Image Provenance Using ImagePolicyWebhookxccdf_org.ssgproject.content_rule_general_configure_imagepolicywebhook mediumManage Image Provenance Using ImagePolicyWebhookRule IDxccdf_org.ssgproject.content_rule_general_configure_imagepolicywebhookResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.5.1, CM-6, CM-6(1)\nDescriptionOpenShift administrators can control which images can be imported, tagged, and run in a cluster. There are two facilities for this purpose: (1) Allowed Registries, allowing administrators to restrict image origins to known external registries; and (2) ImagePolicy Admission plug-in which lets administrators specify specific images which are allowed to run on the OpenShift cluster. Configure an Image policy per the Image Policy chapter in the OpenShift documentation: https://docs.openshift.com/container-platform/4.4/openshift_images/image-configuration.htmlRationaleImage Policy ensures that only approved container images are allowed to be ran on the OpenShift platform.Evaluation messagesinfo No candidate or applicable check found.The default namespace should not be usedxccdf_org.ssgproject.content_rule_general_default_namespace_use mediumThe default namespace should not be usedRule IDxccdf_org.ssgproject.content_rule_general_default_namespace_useResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.6.4, CM-6, CM-6(1)\nDescriptionKubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult.RationaleResources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources.Evaluation messagesinfo No candidate or applicable check found.Apply Security Context to Your Pods and Containersxccdf_org.ssgproject.content_rule_general_apply_scc mediumApply Security Context to Your Pods and ContainersRule IDxccdf_org.ssgproject.content_rule_general_apply_sccResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.6.3, CM-6, CM-6(1)\nDescriptionApply Security Context to your Pods and ContainersRationaleA security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context.Evaluation messagesinfo No candidate or applicable check found.Ensure Seccomp Profile Pod Definitionsxccdf_org.ssgproject.content_rule_general_default_seccomp_profile mediumEnsure Seccomp Profile Pod DefinitionsRule IDxccdf_org.ssgproject.content_rule_general_default_seccomp_profileResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.6.2, CM-6, CM-6(1)\nDescriptionEnable default seccomp profiles in your pod definitions.RationaleSeccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container.Evaluation messagesinfo No candidate or applicable check found.Configure the Audit Log Pathxccdf_org.ssgproject.content_rule_openshift_api_server_audit_log_path highCCE-83547-0 Configure the Audit Log PathRule IDxccdf_org.ssgproject.content_rule_openshift_api_server_audit_log_pathResultpassMulti-check rulenoOVAL Definition IDoval:ssg-openshift_api_server_audit_log_path:def:1Time2021-07-20T05:21:00+00:00SeverityhighIdentifiers and ReferencesIdentifiers: CCE-83547-0\nReferences: 1.2.22, CM-6, CM-6(1)\nDescriptionTo enable auditing on the OpenShift API Server, the audit log path must be set. Edit the openshift-apiserver configmap and set the audit-log-path to a suitable path and file where audit logs should be written. For example: \"apiServerArguments\":{ ... \"audit-log-path\":\"/var/log/openshift-apiserver/audit.log\", ... RationaleAuditing of the API Server is not enabled by default. Auditing the API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected the system by users, administrators, or other system components.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-apiserver/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_openshift_api_server_audit_log_path:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-apiserver/configmapsconfig.data[\"config.yaml\"] {\"apiServerArguments\":{\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/openshift-apiserver/audit.log\"],\"audit-policy-file\":[\"/var/run/configmaps/audit/secure-oauth-storage-default.yaml\"],\"shutdown-delay-duration\":[\"3s\"]},\"apiVersion\":\"openshiftcontrolplane.config.openshift.io/v1\",\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"OpenShiftAPIServerConfig\",\"projectConfig\":{\"projectRequestMessage\":\"\"},\"routingConfig\":{\"subdomain\":\"apps.ocp.ispworld.at\"},\"servingInfo\":{\"bindNetwork\":\"tcp\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\"},\"storageConfig\":{\"urls\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\"]}} Find the file to be checked ('/api/v1/namespaces/openshift-apiserver/configmaps/config'). oval:ssg-test_file_for_openshift_api_server_audit_log_path:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-apiserver/configmaps/configregular100065000010006500001565rw------- Ensure Controller secure-port argument is setxccdf_org.ssgproject.content_rule_controller_secure_port lowCCE-83861-5 Ensure Controller secure-port argument is setRule IDxccdf_org.ssgproject.content_rule_controller_secure_portResultpassMulti-check rulenoOVAL Definition IDoval:ssg-controller_secure_port:def:1Time2021-07-20T05:21:00+00:00SeveritylowIdentifiers and ReferencesIdentifiers: CCE-83861-5\nReferences: 1.3.7, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo ensure the Controller Manager service is bound to secure loopback address using a secure port, set the RotateKubeletServerCertificate option to true in the openshift-kube-controller-manager configmap on the master node(s): \"extendedArguments\": { ... \"secure-port\": [\"10257\"], ... RationaleThe Controller Manager API service is used for health and metrics information and is available without authentication or encryption. As such, it should only be bound to a localhost interface to minimize the cluster's attack surface.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-controller-manager/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_controller_secure_port:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmapsconfig.data[\"config.yaml\"] {\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"extendedArguments\":{\"allocate-node-cidrs\":[\"false\"],\"cert-dir\":[\"/var/run/kubernetes\"],\"cluster-cidr\":[\"10.128.0.0/14\"],\"cluster-name\":[\"ocp-d8r5b\"],\"cluster-signing-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.crt\"],\"cluster-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.key\"],\"configure-cloud-routes\":[\"false\"],\"controllers\":[\"*\",\"-ttl\",\"-bootstrapsigner\",\"-tokencleaner\"],\"enable-dynamic-provisioning\":[\"true\"],\"experimental-cluster-signing-duration\":[\"720h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"flex-volume-plugin-dir\":[\"/etc/kubernetes/kubelet-plugins/volume/exec\"],\"kube-api-burst\":[\"300\"],\"kube-api-qps\":[\"150\"],\"leader-elect\":[\"true\"],\"leader-elect-resource-lock\":[\"configmaps\"],\"leader-elect-retry-period\":[\"3s\"],\"port\":[\"0\"],\"pv-recycler-pod-template-filepath-hostpath\":[\"/etc/kubernetes/static-pod-resources/configmaps/recycler-config/recycler-pod.yaml\"],\"pv-recycler-pod-template-filepath-nfs\":[\"/etc/kubernetes/static-pod-resources/configmaps/recycler-config/recycler-pod.yaml\"],\"root-ca-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/serviceaccount-ca/ca-bundle.crt\"],\"secure-port\":[\"10257\"],\"service-account-private-key-file\":[\"/etc/kubernetes/static-pod-resources/secrets/service-account-private-key/service-account.key\"],\"service-cluster-ip-range\":[\"172.30.0.0/16\"],\"use-service-account-credentials\":[\"true\"]},\"kind\":\"KubeControllerManagerConfig\",\"serviceServingCert\":{\"certFile\":\"/etc/kubernetes/static-pod-resources/configmaps/service-ca/ca-bundle.crt\"}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config'). oval:ssg-test_file_for_controller_secure_port:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/configregular100065000010006500002368rw------- Ensure that use-service-account-credentials is enabledxccdf_org.ssgproject.content_rule_controller_use_service_account mediumCCE-84208-8 Ensure that use-service-account-credentials is enabledRule IDxccdf_org.ssgproject.content_rule_controller_use_service_accountResultpassMulti-check rulenoOVAL Definition IDoval:ssg-controller_use_service_account:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84208-8\nReferences: 1.3.3, CM-6, CM-6(1)\nDescriptionTo ensure individual service account credentials are used, set the use-service-account-credentials option to true in the openshift-kube-controller-manager configmap on the master node(s): \"extendedArguments\": { ... \"use-service-account-credentials\": [ \"true\" ], ... RationaleThe controller manager creates a service account per controller in kube-system namespace, generates an API token and credentials for it, then builds a dedicated API client with that service account credential for each controller loop to use. Setting the use-service-account-credentials to true runs each control loop within the contoller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-controller-manager/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_controller_use_service_account:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmapsconfig.data[\"config.yaml\"] {\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"extendedArguments\":{\"allocate-node-cidrs\":[\"false\"],\"cert-dir\":[\"/var/run/kubernetes\"],\"cluster-cidr\":[\"10.128.0.0/14\"],\"cluster-name\":[\"ocp-d8r5b\"],\"cluster-signing-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.crt\"],\"cluster-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.key\"],\"configure-cloud-routes\":[\"false\"],\"controllers\":[\"*\",\"-ttl\",\"-bootstrapsigner\",\"-tokencleaner\"],\"enable-dynamic-provisioning\":[\"true\"],\"experimental-cluster-signing-duration\":[\"720h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"flex-volume-plugin-dir\":[\"/etc/kubernetes/kubelet-plugins/volume/exec\"],\"kube-api-burst\":[\"300\"],\"kube-api-qps\":[\"150\"],\"leader-elect\":[\"true\"],\"leader-elect-resource-lock\":[\"configmaps\"],\"leader-elect-retry-period\":[\"3s\"],\"port\":[\"0\"],\"pv-recycler-pod-template-filepath-hostpath\":[\"/etc/kubernetes/static-pod-resources/configmaps/recycler-config/recycler-pod.yaml\"],\"pv-recycler-pod-template-filepath-nfs\":[\"/etc/kubernetes/static-pod-resources/configmaps/recycler-config/recycler-pod.yaml\"],\"root-ca-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/serviceaccount-ca/ca-bundle.crt\"],\"secure-port\":[\"10257\"],\"service-account-private-key-file\":[\"/etc/kubernetes/static-pod-resources/secrets/service-account-private-key/service-account.key\"],\"service-cluster-ip-range\":[\"172.30.0.0/16\"],\"use-service-account-credentials\":[\"true\"]},\"kind\":\"KubeControllerManagerConfig\",\"serviceServingCert\":{\"certFile\":\"/etc/kubernetes/static-pod-resources/configmaps/service-ca/ca-bundle.crt\"}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config'). oval:ssg-test_file_for_controller_use_service_account:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/configregular100065000010006500002368rw------- Ensure Controller insecure port argument is unsetxccdf_org.ssgproject.content_rule_controller_insecure_port_disabled lowCCE-83578-5 Ensure Controller insecure port argument is unsetRule IDxccdf_org.ssgproject.content_rule_controller_insecure_port_disabledResultpassMulti-check rulenoOVAL Definition IDoval:ssg-controller_insecure_port_disabled:def:1Time2021-07-20T05:21:00+00:00SeveritylowIdentifiers and ReferencesIdentifiers: CCE-83578-5\nReferences: 1.3.7, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo ensure the Controller Manager service is bound to secure loopback address and a secure port, set the RotateKubeletServerCertificate option to true in the openshift-kube-controller-manager configmap on the master node(s): \"extendedArguments\": { ... \"port\": [\"0\"], ... RationaleThe Controller Manager API service is used for health and metrics information and is available without authentication or encryption. As such, it should only be bound to a localhost interface to minimize the cluster's attack surface.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-controller-manager/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_controller_insecure_port_disabled:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmapsconfig.data[\"config.yaml\"] {\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"extendedArguments\":{\"allocate-node-cidrs\":[\"false\"],\"cert-dir\":[\"/var/run/kubernetes\"],\"cluster-cidr\":[\"10.128.0.0/14\"],\"cluster-name\":[\"ocp-d8r5b\"],\"cluster-signing-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.crt\"],\"cluster-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.key\"],\"configure-cloud-routes\":[\"false\"],\"controllers\":[\"*\",\"-ttl\",\"-bootstrapsigner\",\"-tokencleaner\"],\"enable-dynamic-provisioning\":[\"true\"],\"experimental-cluster-signing-duration\":[\"720h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"flex-volume-plugin-dir\":[\"/etc/kubernetes/kubelet-plugins/volume/exec\"],\"kube-api-burst\":[\"300\"],\"kube-api-qps\":[\"150\"],\"leader-elect\":[\"true\"],\"leader-elect-resource-lock\":[\"configmaps\"],\"leader-elect-retry-period\":[\"3s\"],\"port\":[\"0\"],\"pv-recycler-pod-template-filepath-hostpath\":[\"/etc/kubernetes/static-pod-resources/configmaps/recycler-config/recycler-pod.yaml\"],\"pv-recycler-pod-template-filepath-nfs\":[\"/etc/kubernetes/static-pod-resources/configmaps/recycler-config/recycler-pod.yaml\"],\"root-ca-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/serviceaccount-ca/ca-bundle.crt\"],\"secure-port\":[\"10257\"],\"service-account-private-key-file\":[\"/etc/kubernetes/static-pod-resources/secrets/service-account-private-key/service-account.key\"],\"service-cluster-ip-range\":[\"172.30.0.0/16\"],\"use-service-account-credentials\":[\"true\"]},\"kind\":\"KubeControllerManagerConfig\",\"serviceServingCert\":{\"certFile\":\"/etc/kubernetes/static-pod-resources/configmaps/service-ca/ca-bundle.crt\"}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config'). oval:ssg-test_file_for_controller_insecure_port_disabled:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/configregular100065000010006500002368rw------- Configure the Service Account Certificate Authority Key for the Controller Managerxccdf_org.ssgproject.content_rule_controller_service_account_ca mediumCCE-84244-3 Configure the Service Account Certificate Authority Key for the Controller ManagerRule IDxccdf_org.ssgproject.content_rule_controller_service_account_caResultpassMulti-check rulenoOVAL Definition IDoval:ssg-controller_service_account_ca:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84244-3\nReferences: 1.3.5, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo ensure the API Server utilizes its own key pair, set the masterCA parameter to the public key file for service accounts in the openshift-kube-controller-manager configmap on the master node(s): \"extendedArguments\": { ... \"root-ca-file\": [ \"/etc/kubernetes/static-pod-resources/configmaps/serviceaccount-ca/ca-bundle.crt\" ], ... RationaleService accounts authenticate to the API using tokens signed by a private RSA key. The authentication layer verifies the signature using a matching public RSA key. Configuring the certificate authority file ensures that the API server's signing certificates are validated.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-controller-manager/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_controller_service_account_ca:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmapsconfig.data[\"config.yaml\"] {\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"extendedArguments\":{\"allocate-node-cidrs\":[\"false\"],\"cert-dir\":[\"/var/run/kubernetes\"],\"cluster-cidr\":[\"10.128.0.0/14\"],\"cluster-name\":[\"ocp-d8r5b\"],\"cluster-signing-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.crt\"],\"cluster-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.key\"],\"configure-cloud-routes\":[\"false\"],\"controllers\":[\"*\",\"-ttl\",\"-bootstrapsigner\",\"-tokencleaner\"],\"enable-dynamic-provisioning\":[\"true\"],\"experimental-cluster-signing-duration\":[\"720h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"flex-volume-plugin-dir\":[\"/etc/kubernetes/kubelet-plugins/volume/exec\"],\"kube-api-burst\":[\"300\"],\"kube-api-qps\":[\"150\"],\"leader-elect\":[\"true\"],\"leader-elect-resource-lock\":[\"configmaps\"],\"leader-elect-retry-period\":[\"3s\"],\"port\":[\"0\"],\"pv-recycler-pod-template-filepath-hostpath\":[\"/etc/kubernetes/static-pod-resources/configmaps/recycler-config/recycler-pod.yaml\"],\"pv-recycler-pod-template-filepath-nfs\":[\"/etc/kubernetes/static-pod-resources/configmaps/recycler-config/recycler-pod.yaml\"],\"root-ca-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/serviceaccount-ca/ca-bundle.crt\"],\"secure-port\":[\"10257\"],\"service-account-private-key-file\":[\"/etc/kubernetes/static-pod-resources/secrets/service-account-private-key/service-account.key\"],\"service-cluster-ip-range\":[\"172.30.0.0/16\"],\"use-service-account-credentials\":[\"true\"]},\"kind\":\"KubeControllerManagerConfig\",\"serviceServingCert\":{\"certFile\":\"/etc/kubernetes/static-pod-resources/configmaps/service-ca/ca-bundle.crt\"}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config'). oval:ssg-test_file_for_controller_service_account_ca:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/configregular100065000010006500002368rw------- Configure the Service Account Private Key for the Controller Managerxccdf_org.ssgproject.content_rule_controller_service_account_private_key mediumCCE-83526-4 Configure the Service Account Private Key for the Controller ManagerRule IDxccdf_org.ssgproject.content_rule_controller_service_account_private_keyResultpassMulti-check rulenoOVAL Definition IDoval:ssg-controller_service_account_private_key:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83526-4\nReferences: 1.3.4, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo ensure the API Server utilizes its own key pair, set the privateKeyFile parameter to the public key file for service accounts in the openshift-kube-controller-manager configmap on the master node(s): \"extendedArguments\": { ... \"service-account-private-key-file\": [ \"/etc/kubernetes/static-pod-resources/secrets/service-account-private-key/service-account.key\" ], ... RationaleBy default if no private key file is specified to the API Server, the API Server uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-controller-manager/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config' find only one object at path '.data['config.yaml']'. oval:ssg-test_controller_service_account_private_key:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmapsconfig.data['config.yaml'] {\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"extendedArguments\":{\"allocate-node-cidrs\":[\"false\"],\"cert-dir\":[\"/var/run/kubernetes\"],\"cluster-cidr\":[\"10.128.0.0/14\"],\"cluster-name\":[\"ocp-d8r5b\"],\"cluster-signing-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.crt\"],\"cluster-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.key\"],\"configure-cloud-routes\":[\"false\"],\"controllers\":[\"*\",\"-ttl\",\"-bootstrapsigner\",\"-tokencleaner\"],\"enable-dynamic-provisioning\":[\"true\"],\"experimental-cluster-signing-duration\":[\"720h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"flex-volume-plugin-dir\":[\"/etc/kubernetes/kubelet-plugins/volume/exec\"],\"kube-api-burst\":[\"300\"],\"kube-api-qps\":[\"150\"],\"leader-elect\":[\"true\"],\"leader-elect-resource-lock\":[\"configmaps\"],\"leader-elect-retry-period\":[\"3s\"],\"port\":[\"0\"],\"pv-recycler-pod-template-filepath-hostpath\":[\"/etc/kubernetes/static-pod-resources/configmaps/recycler-config/recycler-pod.yaml\"],\"pv-recycler-pod-template-filepath-nfs\":[\"/etc/kubernetes/static-pod-resources/configmaps/recycler-config/recycler-pod.yaml\"],\"root-ca-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/serviceaccount-ca/ca-bundle.crt\"],\"secure-port\":[\"10257\"],\"service-account-private-key-file\":[\"/etc/kubernetes/static-pod-resources/secrets/service-account-private-key/service-account.key\"],\"service-cluster-ip-range\":[\"172.30.0.0/16\"],\"use-service-account-credentials\":[\"true\"]},\"kind\":\"KubeControllerManagerConfig\",\"serviceServingCert\":{\"certFile\":\"/etc/kubernetes/static-pod-resources/configmaps/service-ca/ca-bundle.crt\"}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config'). oval:ssg-test_file_for_controller_service_account_private_key:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/configregular100065000010006500002368rw------- Ensure that the RotateKubeletServerCertificate argument is setxccdf_org.ssgproject.content_rule_controller_rotate_kubelet_server_certs mediumCCE-83730-2 Ensure that the RotateKubeletServerCertificate argument is setRule IDxccdf_org.ssgproject.content_rule_controller_rotate_kubelet_server_certsResultpassMulti-check rulenoOVAL Definition IDoval:ssg-controller_rotate_kubelet_server_certs:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83730-2\nReferences: 1.3.6, CM-6, CM-6(1), SC-8, SC-8(1)\nDescriptionTo enforce kublet server certificate rotation on the Controller Manager, set the RotateKubeletServerCertificate option to true in the openshift-kube-controller-manager configmap on the master node(s): \"extendedArguments\": { ... \"feature-gates\": [ ... \"RotateKubeletServerCertificate=true\", ... ... RationaleEnabling kubelet certificate rotation causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that there are no downtimes due to expired certificates and thus addressing the availability in the C/I/A security triad.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-controller-manager/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config file.warning This recommendation only applies if you let kubelets get their certificates from the API Server. In case your certificates come from an outside Certificate Authority/tool (e.g. Vault) then you need to take care of rotation yourselfOVAL test results details In the file '/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config' find only one object at path '.data[\"config.yaml\"]'. oval:ssg-test_controller_rotate_kubelet_server_certs:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmapsconfig.data[\"config.yaml\"] {\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"extendedArguments\":{\"allocate-node-cidrs\":[\"false\"],\"cert-dir\":[\"/var/run/kubernetes\"],\"cluster-cidr\":[\"10.128.0.0/14\"],\"cluster-name\":[\"ocp-d8r5b\"],\"cluster-signing-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.crt\"],\"cluster-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/csr-signer/tls.key\"],\"configure-cloud-routes\":[\"false\"],\"controllers\":[\"*\",\"-ttl\",\"-bootstrapsigner\",\"-tokencleaner\"],\"enable-dynamic-provisioning\":[\"true\"],\"experimental-cluster-signing-duration\":[\"720h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"flex-volume-plugin-dir\":[\"/etc/kubernetes/kubelet-plugins/volume/exec\"],\"kube-api-burst\":[\"300\"],\"kube-api-qps\":[\"150\"],\"leader-elect\":[\"true\"],\"leader-elect-resource-lock\":[\"configmaps\"],\"leader-elect-retry-period\":[\"3s\"],\"port\":[\"0\"],\"pv-recycler-pod-template-filepath-hostpath\":[\"/etc/kubernetes/static-pod-resources/configmaps/recycler-config/recycler-pod.yaml\"],\"pv-recycler-pod-template-filepath-nfs\":[\"/etc/kubernetes/static-pod-resources/configmaps/recycler-config/recycler-pod.yaml\"],\"root-ca-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/serviceaccount-ca/ca-bundle.crt\"],\"secure-port\":[\"10257\"],\"service-account-private-key-file\":[\"/etc/kubernetes/static-pod-resources/secrets/service-account-private-key/service-account.key\"],\"service-cluster-ip-range\":[\"172.30.0.0/16\"],\"use-service-account-credentials\":[\"true\"]},\"kind\":\"KubeControllerManagerConfig\",\"serviceServingCert\":{\"certFile\":\"/etc/kubernetes/static-pod-resources/configmaps/service-ca/ca-bundle.crt\"}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config'). oval:ssg-test_file_for_controller_rotate_kubelet_server_certs:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-controller-manager/configmaps/configregular100065000010006500002368rw------- Ensure That The kubelet Client Certificate Is Correctly Setxccdf_org.ssgproject.content_rule_kubelet_configure_tls_cert mediumCCE-83396-2 Ensure That The kubelet Client Certificate Is Correctly SetRule IDxccdf_org.ssgproject.content_rule_kubelet_configure_tls_certResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_configure_tls_cert:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83396-2\nReferences: 4.2.10, SC-8, SC-8(1), SC-8(2)\nDescriptionTo ensure the kubelet TLS client certificate is configured, edit the kubelet configuration file /etc/kubernetes/kubelet.conf and configure the kubelet certificate file. tlsCertFile: /path/to/TLS/cert.keyRationaleWithout cryptographic integrity protections, information can be altered by unauthorized users without detection.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data['config.yaml']'. oval:ssg-test_kubelet_configure_tls_cert:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data['config.yaml'] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_kubelet_configure_tls_cert:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- kubelet - Disable the Read-Only Portxccdf_org.ssgproject.content_rule_kubelet_disable_readonly_port mediumCCE-83427-5 kubelet - Disable the Read-Only PortRule IDxccdf_org.ssgproject.content_rule_kubelet_disable_readonly_portResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_disable_readonly_port:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83427-5\nReferences: 4.2.4, CM-6, CM-6(1)\nDescriptionTo disable the read-only port, edit the kubelet configuration Edit the openshift-kube-apiserver configmap and set the kubelet-read-only-port parameter to 0: \"apiServerArguments\":{ ... \"kubelet-read-only-port\":[ \"0\" ], ... RationaleOpenShift disables the read-only port (10255) on all nodes by setting the read-only port kubelet flag to 0. This ensures only authenticated connections are able to receive information about the OpenShift system.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data['config.yaml']'. oval:ssg-test_kubelet_disable_readonly_port:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data['config.yaml'] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_kubelet_disable_readonly_port:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Ensure That The kubelet Server Key Is Correctly Setxccdf_org.ssgproject.content_rule_kubelet_configure_tls_key mediumEnsure That The kubelet Server Key Is Correctly SetRule IDxccdf_org.ssgproject.content_rule_kubelet_configure_tls_keyResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_configure_tls_key:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 4.2.10, SC-8, SC-8(1), SC-8(2)\nDescriptionTo ensure the kubelet TLS private server key certificate is configured, edit the kubelet configuration file /etc/kubernetes/kubelet.conf and configure the kubelet private key file. tlsPrivateKeyFile: /path/to/TLS/private.keyRationaleWithout cryptographic integrity protections, information can be altered by unauthorized users without detection.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /api/v1/namespaces/openshift-kube-apiserver/configmaps/config API endpoint to the local /kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config file.OVAL test results details In the file '/api/v1/namespaces/openshift-kube-apiserver/configmaps/config' find only one object at path '.data['config.yaml']'. oval:ssg-test_kubelet_configure_tls_key:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/config/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmapsconfig.data['config.yaml'] {\"admission\":{\"pluginConfig\":{\"network.openshift.io/ExternalIPRanger\":{\"configuration\":{\"allowIngressIP\":false,\"apiVersion\":\"network.openshift.io/v1\",\"externalIPNetworkCIDRs\":null,\"kind\":\"ExternalIPRangerAdmissionConfig\"},\"location\":\"\"},\"network.openshift.io/RestrictedEndpointsAdmission\":{\"configuration\":{\"apiVersion\":\"network.openshift.io/v1\",\"kind\":\"RestrictedEndpointsAdmissionConfig\",\"restrictedCIDRs\":[\"10.128.0.0/14\",\"172.30.0.0/16\"]}}}},\"apiServerArguments\":{\"allow-privileged\":[\"true\"],\"anonymous-auth\":[\"true\"],\"api-audiences\":[\"https://kubernetes.default.svc\"],\"audit-log-format\":[\"json\"],\"audit-log-maxbackup\":[\"10\"],\"audit-log-maxsize\":[\"100\"],\"audit-log-path\":[\"/var/log/kube-apiserver/audit.log\"],\"audit-policy-file\":[\"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-audit-policies/default.yaml\"],\"authorization-mode\":[\"Scope\",\"SystemMasters\",\"RBAC\",\"Node\"],\"client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/client-ca/ca-bundle.crt\"],\"enable-admission-plugins\":[\"CertificateApproval\",\"CertificateSigning\",\"CertificateSubjectRestriction\",\"DefaultIngressClass\",\"DefaultStorageClass\",\"DefaultTolerationSeconds\",\"LimitRanger\",\"MutatingAdmissionWebhook\",\"NamespaceLifecycle\",\"NodeRestriction\",\"OwnerReferencesPermissionEnforcement\",\"PersistentVolumeClaimResize\",\"PersistentVolumeLabel\",\"PodNodeSelector\",\"PodTolerationRestriction\",\"Priority\",\"ResourceQuota\",\"RuntimeClass\",\"ServiceAccount\",\"StorageObjectInUseProtection\",\"TaintNodesByCondition\",\"ValidatingAdmissionWebhook\",\"authorization.openshift.io/RestrictSubjectBindings\",\"authorization.openshift.io/ValidateRoleBindingRestriction\",\"config.openshift.io/DenyDeleteClusterConfiguration\",\"config.openshift.io/ValidateAPIServer\",\"config.openshift.io/ValidateAuthentication\",\"config.openshift.io/ValidateConsole\",\"config.openshift.io/ValidateFeatureGate\",\"config.openshift.io/ValidateImage\",\"config.openshift.io/ValidateOAuth\",\"config.openshift.io/ValidateProject\",\"config.openshift.io/ValidateScheduler\",\"image.openshift.io/ImagePolicy\",\"network.openshift.io/ExternalIPRanger\",\"network.openshift.io/RestrictedEndpointsAdmission\",\"quota.openshift.io/ClusterResourceQuota\",\"quota.openshift.io/ValidateClusterResourceQuota\",\"route.openshift.io/IngressAdmission\",\"scheduling.openshift.io/OriginPodNodeEnvironment\",\"security.openshift.io/DefaultSecurityContextConstraints\",\"security.openshift.io/SCCExecRestrictions\",\"security.openshift.io/SecurityContextConstraint\",\"security.openshift.io/ValidateSecurityContextConstraints\"],\"enable-aggregator-routing\":[\"true\"],\"enable-logs-handler\":[\"false\"],\"enable-swagger-ui\":[\"true\"],\"endpoint-reconciler-type\":[\"lease\"],\"etcd-cafile\":[\"/etc/kubernetes/static-pod-resources/configmaps/etcd-serving-ca/ca-bundle.crt\"],\"etcd-certfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.crt\"],\"etcd-keyfile\":[\"/etc/kubernetes/static-pod-resources/secrets/etcd-client/tls.key\"],\"etcd-prefix\":[\"kubernetes.io\"],\"etcd-servers\":[\"https://192.168.50.10:2379\",\"https://192.168.50.11:2379\",\"https://192.168.50.12:2379\",\"https://localhost:2379\"],\"event-ttl\":[\"3h\"],\"feature-gates\":[\"APIPriorityAndFairness=true\",\"RotateKubeletServerCertificate=true\",\"SupportPodPidsLimit=true\",\"NodeDisruptionExclusion=true\",\"ServiceNodeExclusion=true\",\"SCTPSupport=true\",\"LegacyNodeRoleBehavior=false\",\"RemoveSelfLink=false\"],\"goaway-chance\":[\"0\"],\"http2-max-streams-per-connection\":[\"2000\"],\"insecure-port\":[\"0\"],\"kubelet-certificate-authority\":[\"/etc/kubernetes/static-pod-resources/configmaps/kubelet-serving-ca/ca-bundle.crt\"],\"kubelet-client-certificate\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.crt\"],\"kubelet-client-key\":[\"/etc/kubernetes/static-pod-resources/secrets/kubelet-client/tls.key\"],\"kubelet-https\":[\"true\"],\"kubelet-preferred-address-types\":[\"InternalIP\"],\"kubelet-read-only-port\":[\"0\"],\"kubernetes-service-node-port\":[\"0\"],\"max-mutating-requests-inflight\":[\"1000\"],\"max-requests-inflight\":[\"3000\"],\"min-request-timeout\":[\"3600\"],\"proxy-client-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.crt\"],\"proxy-client-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/aggregator-client/tls.key\"],\"requestheader-allowed-names\":[\"kube-apiserver-proxy\",\"system:kube-apiserver-proxy\",\"system:openshift-aggregator\"],\"requestheader-client-ca-file\":[\"/etc/kubernetes/static-pod-certs/configmaps/aggregator-client-ca/ca-bundle.crt\"],\"requestheader-extra-headers-prefix\":[\"X-Remote-Extra-\"],\"requestheader-group-headers\":[\"X-Remote-Group\"],\"requestheader-username-headers\":[\"X-Remote-User\"],\"runtime-config\":[\"flowcontrol.apiserver.k8s.io/v1alpha1=true\"],\"service-account-issuer\":[\"https://kubernetes.default.svc\"],\"service-account-jwks-uri\":[\"https://api-int.ocp.ispworld.at:6443/openid/v1/jwks\"],\"service-account-lookup\":[\"true\"],\"service-account-signing-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/bound-service-account-signing-key/service-account.key\"],\"service-node-port-range\":[\"30000-32767\"],\"shutdown-delay-duration\":[\"70s\"],\"storage-backend\":[\"etcd3\"],\"storage-media-type\":[\"application/vnd.kubernetes.protobuf\"],\"tls-cert-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\"],\"tls-private-key-file\":[\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"]},\"apiVersion\":\"kubecontrolplane.config.openshift.io/v1\",\"authConfig\":{\"oauthMetadataFile\":\"/etc/kubernetes/static-pod-resources/configmaps/oauth-metadata/oauthMetadata\"},\"consolePublicURL\":\"\",\"corsAllowedOrigins\":[\"//127\\\\.0\\\\.0\\\\.1(:|$)\",\"//localhost(:|$)\"],\"imagePolicyConfig\":{\"internalRegistryHostname\":\"image-registry.openshift-image-registry.svc:5000\"},\"kind\":\"KubeAPIServerConfig\",\"projectConfig\":{\"defaultNodeSelector\":\"\"},\"serviceAccountPublicKeyFiles\":[\"/etc/kubernetes/static-pod-resources/configmaps/sa-token-signing-certs\",\"/etc/kubernetes/static-pod-resources/configmaps/bound-sa-token-signing-certs\"],\"servicesSubnet\":\"172.30.0.0/16\",\"servingInfo\":{\"bindAddress\":\"0.0.0.0:6443\",\"bindNetwork\":\"tcp4\",\"cipherSuites\":[\"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\"TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\",\"TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\"],\"minTLSVersion\":\"VersionTLS12\",\"namedCertificates\":[{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/localhost-serving-cert-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/service-network-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/external-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-certs/secrets/internal-loadbalancer-serving-certkey/tls.key\"},{\"certFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.crt\",\"keyFile\":\"/etc/kubernetes/static-pod-resources/secrets/localhost-recovery-serving-certkey/tls.key\"}]}} Find the file to be checked ('/api/v1/namespaces/openshift-kube-apiserver/configmaps/config'). oval:ssg-test_file_for_kubelet_configure_tls_key:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/api/v1/namespaces/openshift-kube-apiserver/configmaps/configregular100065000010006500007872rw------- Ensure that the cluster's audit profile is properly setxccdf_org.ssgproject.content_rule_audit_profile_set mediumCCE-83577-7 Ensure that the cluster's audit profile is properly setRule IDxccdf_org.ssgproject.content_rule_audit_profile_setResultpassMulti-check rulenoOVAL Definition IDoval:ssg-audit_profile_set:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83577-7\nReferences: 3.2.1, 3.2.2, AU-2, AU-3, AU-3(1), AU-6, AU-6(1), AU-7, AU-7(1), AU-8, AU-8(1), AU-9, AU-12, CM-5(1), SI-11, SI-12\nDescription OpenShift can audit the details of requests made to the API server through the standard Kubernetes audit capabilities. In OpenShift, auditing of the API Server is on by default. Audit provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators, or other components of the system. Audit works at the API server level, logging all requests coming to the server. Each audit log contains two entries: The request line containing: A Unique ID allowing to match the response line (see #2) The source IP of the request The HTTP method being invoked The original user invoking the operation The impersonated user for the operation (self meaning himself) The impersonated group for the operation (lookup meaning user's group) The namespace of the request or none The URI as requested The response line containing: The aforementioned unique ID The response code For more information on how to configure the audit profile, please visit the documentation RationaleLogging is an important detective control for all systems, to detect potential unauthorised access.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /apis/config.openshift.io/v1/apiservers/cluster API endpoint to the local /kubernetes-api-resources/apis/config.openshift.io/v1/apiservers/cluster file.OVAL test results details In the file '/apis/config.openshift.io/v1/apiservers/cluster' find only one object at path 'spec.audit.profile'. oval:ssg-test_audit_profile_set:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/apis/config.openshift.io/v1/apiservers/cluster/kubernetes-api-resources/apis/config.openshift.io/v1/apiserversclusterspec.audit.profile Default Find the file to be checked ('/apis/config.openshift.io/v1/apiservers/cluster'). oval:ssg-test_file_for_audit_profile_set:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/apis/config.openshift.io/v1/apiservers/clusterregular10006500001000650000869rw------- Minimize Wildcard Usage in Cluster and Local Rolesxccdf_org.ssgproject.content_rule_rbac_wildcard_use mediumMinimize Wildcard Usage in Cluster and Local RolesRule IDxccdf_org.ssgproject.content_rule_rbac_wildcard_useResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.1.3, CM-6, CM-6(1)\nDescriptionKubernetes Cluster and Local Roles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set either of these using a wildcard * which matches all items. This violates the principle of least privilege and leaves a cluster in a more vulnerable state to privilege abuse.RationaleThe principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API.Evaluation messagesinfo No candidate or applicable check found.Limit Access to Kubernetes Secretsxccdf_org.ssgproject.content_rule_rbac_limit_secrets_access mediumLimit Access to Kubernetes SecretsRule IDxccdf_org.ssgproject.content_rule_rbac_limit_secrets_accessResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.1.2, CM-6, CM-6(1)\nDescriptionThe Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation. To restrict users from secrets, remove get, list, and watch access to unauthorized users to secret objects in the cluster.RationaleInappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets.Evaluation messagesinfo No candidate or applicable check found.Minimize Access to Pod Creationxccdf_org.ssgproject.content_rule_rbac_pod_creation_access mediumMinimize Access to Pod CreationRule IDxccdf_org.ssgproject.content_rule_rbac_pod_creation_accessResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.1.4, CM-6, CM-6(1)\nDescriptionThe ability to create pods in a namespace can provide a number of opportunities for privilege escalation. Where applicable, remove create access to pod objects in the cluster.RationaleThe ability to create pods in a cluster opens up the cluster for privilege escalation.Evaluation messagesinfo No candidate or applicable check found.Profiling is protected by RBACxccdf_org.ssgproject.content_rule_rbac_debug_role_protects_pprof mediumCCE-84182-5 Profiling is protected by RBACRule IDxccdf_org.ssgproject.content_rule_rbac_debug_role_protects_pprofResultpassMulti-check rulenoOVAL Definition IDoval:ssg-rbac_debug_role_protects_pprof:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84182-5\nReferences: 1.3.2, 1.4.1, CM-6, CM-6(1)\nDescriptionEnsure that the cluster-debugger cluster role includes the /debug/pprof resource URL. This demonstrates that profiling is protected by RBAC, with a specific cluster role to allow access.RationaleProfiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface. To ensure the collected data is not exploited, profiling endpoints are secured via RBAC (see cluster-debugger role). By default, the profiling endpoints are accessible only by users bound to cluster-admin or cluster-debugger role. Profiling can not be disabled.Warningswarning This rule's check operates on the cluster configuration dump. Therefore, you need to use a tool that can query the OCP API, retrieve the /apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-debugger API endpoint to the local /kubernetes-api-resources/apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-debugger file.OVAL test results details In the file '/apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-debugger' find only one object at path '.rules[0].nonResourceURLs[:]'. oval:ssg-test_rbac_debug_role_protects_pprof:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/kubernetes-api-resources/apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-debugger/kubernetes-api-resources/apis/rbac.authorization.k8s.io/v1/clusterrolescluster-debugger.rules[0].nonResourceURLs[:] /debug/pprof /debug/pprof/* /metrics Find the file to be checked ('/apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-debugger'). oval:ssg-test_file_for_rbac_debug_role_protects_pprof:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/kubernetes-api-resources/apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-debuggerregular10006500001000650000773rw------- Ensure that the cluster-admin role is only used where requiredxccdf_org.ssgproject.content_rule_rbac_limit_cluster_admin mediumEnsure that the cluster-admin role is only used where requiredRule IDxccdf_org.ssgproject.content_rule_rbac_limit_cluster_adminResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesReferences: 5.1.1, CM-6, CM-6(1)\nDescriptionThe RBAC role cluster-admin provides wide-ranging powers over the environment and should be used only where and when needed.RationaleKubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding's namespace, including the namespace itself.Evaluation messagesinfo No candidate or applicable check found.Ensure that application Namespaces have Network Policies defined.xccdf_org.ssgproject.content_rule_configure_network_policies_namespaces highEnsure that application Namespaces have Network Policies defined.Rule IDxccdf_org.ssgproject.content_rule_configure_network_policies_namespacesResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeverityhighIdentifiers and ReferencesReferences: 5.3.2, CM-6, CM-6(1)\nDescriptionUse network policies to isolate traffic in your cluster network.RationaleRunning different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace.Evaluation messagesinfo No candidate or applicable check found.Ensure that the CNI in use supports Network Policiesxccdf_org.ssgproject.content_rule_configure_network_policies highEnsure that the CNI in use supports Network PoliciesRule IDxccdf_org.ssgproject.content_rule_configure_network_policiesResultnotcheckedMulti-check rulenoTime2021-07-20T05:21:00+00:00SeverityhighIdentifiers and ReferencesReferences: 5.3.1, CM-6, CM-6(1)\nDescriptionThere are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may not be possible to effectively restrict traffic in the cluster. OpenShift supports Kubernetes NetworkPolicy using a Kubernetes Container Network Interface (CNI) plug-in.RationaleKubernetes network policies are enforced by the CNI plugin in use. As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies.Evaluation messagesinfo No candidate or applicable check found.Scroll back to the first ruleRed Hat and Red Hat Enterprise Linux are either registered trademarks or trademarks of Red Hat, Inc. in the United States and other countries. All other names are registered trademarks or trademarks of their respective companies. Generated using OpenSCAP 1.3.4\n"},{"uri":"https://blog.stderr.at/compliance/1/01/","title":"","tags":[],"description":"","content":"\u003c!DOCTYPE html\u003exccdf_org.open-scap_testresult_xccdf_org.ssgproject.content_profile_cis-node | OpenSCAP Evaluation ReportOpenSCAP Evaluation ReportGuide to the Secure Configuration of Red Hat OpenShift Container Platform 4with profile CIS Red Hat OpenShift Container Platform 4 BenchmarkThis profile defines a baseline that aligns to the Center for Internet Security® Red Hat OpenShift Container Platform 4 Benchmark™, V0.3, currently unreleased. This profile includes Center for Internet Security® Red Hat OpenShift Container Platform 4 CIS Benchmarks™ content. Note that this part of the profile is meant to run on the Operating System that Red Hat OpenShift Container Platform 4 runs on top of. This profile is applicable to OpenShift versions 4.6 and greater.The ComplianceAsCode Project\nhttps://www.open-scap.org/security-policies/scap-security-guide This guide presents a catalog of security-relevant configuration settings for Red Hat OpenShift Container Platform 4. It is a rendering of content structured in the eXtensible Configuration Checklist Description Format (XCCDF) in order to support security automation. The SCAP content is is available in the scap-security-guide package which is developed at https://www.open-scap.org/security-policies/scap-security-guide. Providing system administrators with such guidance informs them how to securely configure systems under their control in a variety of network roles. Policy makers and baseline creators can use this catalog of settings, with its associated references to higher-level security control catalogs, in order to assist them in security baseline creation. This guide is a catalog, not a checklist, and satisfaction of every item is not likely to be possible or sensible in many operational scenarios. However, the XCCDF format enables granular selection and adjustment of settings, and their association with OVAL and OCIL content provides an automated checking capability. Transformations of this document, and its associated automated checking content, are capable of providing baselines that meet a diverse set of policy objectives. Some example XCCDF Profiles, which are selections of items that form checklists and can be used as baselines, are available with this guide. They can be processed, in an automated fashion, with tools that support the Security Content Automation Protocol (SCAP). The NIST National Checklist Program (NCP), which provides required settings for the United States Government, is one example of a baseline created from this guidance. Do not attempt to implement any of the settings in this guide without first testing them in a non-operational environment. The creators of this guidance assume no responsibility whatsoever for its use by other parties, and makes no guarantees, expressed or implied, about its quality, reliability, or any other characteristic. Evaluation CharacteristicsEvaluation targetUnknownTarget IDchroot:///hostBenchmark URL/content/ssg-ocp4-ds.xmlBenchmark IDxccdf_org.ssgproject.content_benchmark_OCP-4Benchmark version0.1.57Profile IDxccdf_org.ssgproject.content_profile_cis-nodeStarted at2021-07-20T05:20:57+00:00Finished at2021-07-20T05:21:01+00:00Performed by unknown user Test systemcpe:/a:redhat:openscap:1.3.4CPE Platformscpe:/o:redhat:openshift_container_platform_node:4cpe:/a:redhat:openshift_container_platform:4.1cpe:/a:redhat:openshift_container_platform:4.6cpe:/a:redhat:openshift_container_platform:4.7cpe:/a:redhat:openshift_container_platform:4.8cpe:/a:redhat:openshift_container_platform:4.9cpe:/a:redhat:openshift_container_platform:4.10AddressesCompliance and ScoringThe target system did not satisfy the conditions of 17 rules! Please review rule results and consider applying remediation. Rule results84 passed 17 failed 0 other Severity of failed rules0 other 0 low 17 medium 0 high ScoreScoring systemScoreMaximumPercenturn:xccdf:scoring:default83.712120100.00000083.71%Rule OverviewpassfixedinformationalfailerrorunknownnotcheckednotapplicableSearch\nGroup rules by: DefaultSeverityResult──────────NIST SP 800-53https://www.cisecurity.org/benchmark/kubernetes/TitleSeverityResultGuide to the Secure Configuration of Red Hat OpenShift Container Platform 4 17x failOpenShift Settings 17x failOpenShift - Master Node Settings 4x failVerify Group Who Owns The Open vSwitch Daemon PID FilemediumpassVerify User Who Owns The OpenShift SDN Container Network Interface Plugin IP Address AllocationsmediumfailVerify Group Who Owns The Kubernetes API Server Pod Specification FilemediumpassVerify User Who Owns The OpenShift Controller Manager Kubeconfig FilemediumpassVerify Permissions on the OpenShift Admin Kubeconfig FilesmediumpassVerify Group Who Owns The etcd Member Pod Specification FilemediumpassVerify Permissions on the OpenShift Multus Container Network Interface Plugin FilesmediumpassVerify Permissions on the Open vSwitch Process ID FilemediumpassVerify User Who Owns The Etcd PKI Certificate FilesmediumpassVerify User Who Owns The OpenShift Multus Container Network Interface Plugin FilesmediumpassVerify User Who Owns The Open vSwitch Database Server PIDmediumpassVerify User Who Owns The Open vSwitch Configuration Database LockmediumpassVerify Group Who Owns The Open vSwitch Persistent System IDmediumpassVerify Permissions on the OpenShift SDN CNI Server ConfigmediumpassVerify Group Who Owns The OpenShift SDN Container Network Interface Plugin IP Address AllocationsmediumfailVerify Group Who Owns The Open vSwitch Process ID FilemediumpassVerify User Who Owns The Etcd Database DirectorymediumpassVerify Group Who Owns The Etcd Write-Ahead-Log FilesmediumpassVerify Permissions on the Etcd PKI Certificate FilesmediumpassVerify Group Who Owns The Open vSwitch Database Server PIDmediumpassVerify Permissions on the OpenShift Controller Manager Kubeconfig FilemediumpassVerify Group Who Owns The OpenShift PKI Private Key FilesmediumpassVerify Group Who Owns The OpenShift Container Network Interface FilesmediumpassVerify User Who Owns The Etcd Member Pod Specification FilemediumpassVerify Group Who Owns The OpenShift Admin Kubeconfig FilesmediumpassVerify Group Who Owns The OpenShift PKI Certificate FilesmediumpassVerify User Who Owns The OpenShift Admin Kubeconfig FilesmediumpassVerify Permissions on the Etcd Member Pod Specification FilemediumpassVerify Permissions on the Kubernetes Controller Manager Pod Specificiation FilemediumpassVerify Permissions on the OpenShift PKI Certificate FilesmediumpassVerify Group Who Owns The OpenShift Controller Manager Kubeconfig FilemediumpassVerify Permissions on the Open vSwitch Configuration DatabasemediumpassVerify Permissions on the Open vSwitch Daemon PID FilemediumpassVerify Permissions on the Kubernetes Scheduler Pod Specification FilemediumpassVerify Group Who Owns The OpenShift Multus Container Network Interface Plugin FilesmediumpassVerify User Who Owns The Kubernetes Controller Manager Pod Specificiation FilemediumpassVerify User Who Owns The OpenShift PKI Private Key FilesmediumpassVerify Group Who Owns The Etcd Database DirectorymediumpassVerify User Who Owns The Kubernetes Scheduler Kubeconfig FilemediumpassVerify Permissions on the Etcd Database DirectorymediumpassVerify User Who Owns The Kubernetes Scheduler Pod Specification FilemediumpassVerify User Who Owns The OpenShift Container Network Interface FilesmediumpassVerify User Who Owns The Etcd Write-Ahead-Log FilesmediumpassVerify Permissions on the Etcd Write-Ahead-Log FilesmediumpassVerify User Who Owns The Open vSwitch Configuration DatabasemediumpassVerify User Who Owns The Open vSwitch Process ID FilemediumpassVerify User Who Owns The Kubernetes API Server Pod Specification FilemediumpassVerify Permissions on the OpenShift Container Network Interface FilesmediumpassVerify Permissions on the OpenShift PKI Private Key FilesmediumpassVerify Permissions on the Open vSwitch Persistent System IDmediumpassVerify User Who Owns The OpenShift SDN CNI Server ConfigmediumfailVerify Permissions on the Open vSwitch Database Server PIDmediumpassVerify Group Who Owns The Open vSwitch Configuration DatabasemediumpassVerify Group Who Owns The Etcd PKI Certificate FilesmediumpassVerify Group Who Owns The Open vSwitch Configuration Database LockmediumpassVerify User Who Owns The Open vSwitch Persistent System IDmediumpassVerify Group Who Owns The Kubernetes Scheduler Kubeconfig FilemediumpassVerify User Who Owns The Open vSwitch Daemon PID FilemediumpassVerify Permissions on the OpenShift SDN Container Network Interface Plugin IP Address AllocationsmediumpassVerify Group Who Owns The Kubernetes Controller Manager Pod Specification FilemediumpassVerify Permissions on the Open vSwitch Configuration Database LockmediumpassVerify Permissions on the Kubernetes Scheduler Kubeconfig FilemediumpassVerify User Who Owns The OpenShift PKI Certificate FilesmediumpassVerify Group Who Owns The Kubernetes Scheduler Pod Specification FilemediumpassVerify Permissions on the Kubernetes API Server Pod Specification FilemediumpassVerify Group Who Owns The OpenShift SDN CNI Server ConfigmediumfailOpenShift - Master Node SettingsVerify Group Who Owns The Worker Kubeconfig FilemediumpassVerify Group Who Owns The OpenShift Node Service FilemediumpassVerify Permissions on The Kubelet Configuration FilemediumpassVerify Permissions on the Worker Kubeconfig FilemediumpassVerify Group Who Owns The Kubelet Configuration FilemediumpassVerify User Who Owns The Kubelet Configuration FilemediumpassVerify Group Who Owns the Worker Certificate Authority FilemediumpassVerify Permissions on the OpenShift Node Service FilemediumpassVerify User Who Owns The OpenShift Node Service FilemediumpassVerify User Who Owns The Worker Kubeconfig FilemediumpassVerify Permissions on the Worker Certificate Authority FilemediumpassVerify User Who Owns the Worker Certificate Authority FilemediumpassOpenShift etcd SettingsConfigure A Unique CA Certificate for etcdmediumpassKubernetes Kubelet Settings 13x failEnsure Eviction threshold Settings Are Set - evictionHard: nodefs.availablemediumfailkubelet - Enable Client Certificate RotationmediumpassKubelet - Ensure Event Creation Is Configuredmediumfailkubelet - Disable Hostname OverridemediumpassEnsure Eviction threshold Settings Are Set - evictionSoft: imagefs.availablemediumfailkubelet - Do Not Disable Streaming TimeoutsmediumpassEnsure Eviction threshold Settings Are Set - evictionSoft: memory.availablemediumfailEnsure Eviction threshold Settings Are Set - evictionHard: nodefs.inodesFreemediumfailkubelet - Enable Server Certificate RotationmediumpassEnsure Eviction threshold Settings Are Set - evictionHard: imagefs.availablemediumfailkubelet - Configure the Client CA Certificatemediumpasskubelet - Enable Protect Kernel DefaultsmediumfailEnsure that the Kubelet only makes use of Strong Cryptographic Ciphersmediumfailkubelet - Allow Automatic Firewall ConfigurationmediumpassEnsure Eviction threshold Settings Are Set - evictionHard: imagefs.inodesFreemediumfailEnsure Eviction threshold Settings Are Set - evictionSoft: nodefs.availablemediumfailEnsure authorization is set to Webhookmediumpasskubelet - Enable Certificate RotationmediumpassEnsure Eviction threshold Settings Are Set - evictionSoft: nodefs.inodesFreemediumfailDisable Anonymous Authentication to the KubeletmediumpassEnsure Eviction threshold Settings Are Set - evictionSoft: imagefs.inodesFreemediumfailEnsure Eviction threshold Settings Are Set - evictionHard: memory.availablemediumfailShow all result detailsResult DetailsVerify Group Who Owns The Open vSwitch Daemon PID Filexccdf_org.ssgproject.content_rule_file_groupowner_ovs_vswitchd_pid mediumCCE-84129-6 Verify Group Who Owns The Open vSwitch Daemon PID FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_ovs_vswitchd_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_ovs_vswitchd_pid:def:1Time2021-07-20T05:20:57+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84129-6\nReferences: 1.1.9, CM-6, CM-6(1)\nDescriptionEnsure that the file /run/openvswitch/ovs-vswitchd.pid, is owned by the group openvswitch or hugetlbfs, depending on your settings and Open vSwitch version.RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of /var/run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_groupowner_var_run_ovs_vswitchd_pid_800:tst:1 falseFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/run/openvswitch/ovs-vswitchd.pidregular8008015rw-r--r-- Testing group ownership of /var/run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_groupowner_var_run_ovs_vswitchd_pid_801:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/run/openvswitch/ovs-vswitchd.pidregular8008015rw-r--r-- Verify User Who Owns The OpenShift SDN Container Network Interface Plugin IP Address Allocationsxccdf_org.ssgproject.content_rule_file_owner_ip_allocations mediumCCE-84248-4 Verify User Who Owns The OpenShift SDN Container Network Interface Plugin IP Address AllocationsRule IDxccdf_org.ssgproject.content_rule_file_owner_ip_allocationsResultfailMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_ip_allocations:def:1Time2021-07-20T05:20:57+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84248-4\nReferences: 1.1.10, CM-6, CM-6(1)\nDescription To properly set the owner of /var/lib/cni/networks/openshift-sdn/.*, run the command: $ sudo chown root /var/lib/cni/networks/openshift-sdn/.* RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of ^/var/lib/cni/networks/openshift-sdn/.*$ oval:ssg-test_file_owner_ip_allocations:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_owner_ip_allocations:obj:1 of type file_objectFilepath^/var/lib/cni/networks/openshift-sdn/.*$ Verify Group Who Owns The Kubernetes API Server Pod Specification Filexccdf_org.ssgproject.content_rule_file_groupowner_kube_apiserver mediumCCE-83530-6 Verify Group Who Owns The Kubernetes API Server Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_kube_apiserverResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_kube_apiserver:def:1Time2021-07-20T05:20:57+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83530-6\nReferences: 1.1.2, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yamlRationaleThe Kubernetes specification file contains information about the configuration of the Kubernetes API Server that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes API Server service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing group ownership of ^/etc/kubernetes/static-pod-resources/kube-apiserver-pod-.*/kube-apiserver-pod.yaml$ oval:ssg-test_file_groupowner_kube_apiserver:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/kube-apiserver-pod.yamlregular006904rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6/kube-apiserver-pod.yamlregular006949rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/kube-apiserver-pod.yamlregular006949rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-10/kube-apiserver-pod.yamlregular007089rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8/kube-apiserver-pod.yamlregular006949rw-r--r-- Verify User Who Owns The OpenShift Controller Manager Kubeconfig Filexccdf_org.ssgproject.content_rule_file_owner_controller_manager_kubeconfig mediumCCE-83904-3 Verify User Who Owns The OpenShift Controller Manager Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_owner_controller_manager_kubeconfigResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_controller_manager_kubeconfig:def:1Time2021-07-20T05:20:57+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83904-3\nReferences: 1.1.18, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig RationaleThe Controller Manager's kubeconfig contains information about how the component will access the API server. You should set its file ownership to maintain the integrity of the file.Warningswarning This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing user ownership of ^/etc/kubernetes/static-pod-resources/kube-controller-manager-pod-.*/configmaps/controller-manager-kubeconfig/kubeconfig$ oval:ssg-test_file_owner_controller_manager_kubeconfig:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-8/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-9/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-11/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- Verify Permissions on the OpenShift Admin Kubeconfig Filesxccdf_org.ssgproject.content_rule_file_permissions_master_admin_kubeconfigs mediumCCE-84278-1 Verify Permissions on the OpenShift Admin Kubeconfig FilesRule IDxccdf_org.ssgproject.content_rule_file_permissions_master_admin_kubeconfigsResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_master_admin_kubeconfigs:def:1Time2021-07-20T05:20:57+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84278-1\nReferences: 1.1.13, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig, run the command: $ sudo chmod 0600 /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfigRationaleThere are various kubeconfig files that can be used by the administrator, defining various settings for the administration of the cluster. These files contain credentials that can be used to control the cluster and are needed for disaster recovery and each kubeconfig points to a different endpoint in the cluster. You should restrict its file permissions to maintain the integrity of the kubeconfig file as an attacker who gains access to these files can take over the cluster.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing mode of ^/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/.*\\.kubeconfig$ oval:ssg-test_file_permissions_master_admin_kubeconfigs:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_master_admin_kubeconfigs:obj:1 of type file_objectFilepathFilter^/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/.*\\.kubeconfig$oval:ssg-state_file_permissions_master_admin_kubeconfigs_mode_not_0600:ste:1 Verify Group Who Owns The etcd Member Pod Specification Filexccdf_org.ssgproject.content_rule_file_groupowner_etcd_member mediumCCE-83664-3 Verify Group Who Owns The etcd Member Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_etcd_memberResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_etcd_member:def:1Time2021-07-20T05:20:57+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83664-3\nReferences: 1.1.8, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yamlRationaleThe etcd pod specification file controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing group ownership of ^/etc/kubernetes/static-pod-resources/etcd-pod-.*/etcd-pod.yaml$ oval:ssg-test_file_groupowner_etcd_member:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/etcd-pod-2/etcd-pod.yamlregular0017522rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-3/etcd-pod.yamlregular0017418rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-4/etcd-pod.yamlregular0015993rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-5/etcd-pod.yamlregular0015130rw-r--r-- Verify Permissions on the OpenShift Multus Container Network Interface Plugin Filesxccdf_org.ssgproject.content_rule_file_permissions_multus_conf mediumCCE-83467-1 Verify Permissions on the OpenShift Multus Container Network Interface Plugin FilesRule IDxccdf_org.ssgproject.content_rule_file_permissions_multus_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_multus_conf:def:1Time2021-07-20T05:20:57+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83467-1\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /var/run/multus/cni/net.d/*, run the command: $ sudo chmod 0644 /var/run/multus/cni/net.d/*RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of ^/var/run/multus/cni/net.d/.*$ oval:ssg-test_file_permissions_multus_conf:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_multus_conf:obj:1 of type file_objectFilepathFilter^/var/run/multus/cni/net.d/.*$oval:ssg-state_file_permissions_multus_conf_mode_not_0644:ste:1 Verify Permissions on the Open vSwitch Process ID Filexccdf_org.ssgproject.content_rule_file_permissions_ovs_pid mediumCCE-83666-8 Verify Permissions on the Open vSwitch Process ID FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_ovs_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_ovs_pid:def:1Time2021-07-20T05:20:57+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83666-8\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /var/run/openvswitch/ovs-vswitchd.pid, run the command: $ sudo chmod 0644 /var/run/openvswitch/ovs-vswitchd.pidRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of /var/run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_permissions_ovs_pid:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_ovs_pid:obj:1 of type file_objectFilepathFilter/var/run/openvswitch/ovs-vswitchd.pidoval:ssg-state_file_permissions_ovs_pid_mode_not_0644:ste:1 Verify User Who Owns The Etcd PKI Certificate Filesxccdf_org.ssgproject.content_rule_file_owner_etcd_pki_cert_files mediumCCE-83898-7 Verify User Who Owns The Etcd PKI Certificate FilesRule IDxccdf_org.ssgproject.content_rule_file_owner_etcd_pki_cert_filesResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_etcd_pki_cert_files:def:1Time2021-07-20T05:20:58+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83898-7\nReferences: 1.1.19, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/*/*/*/*.crt, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/*/*/*/*.crt RationaleOpenShift makes use of a number of certificates as part of its operation. You should verify the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by the system administrator.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing user ownership of ^/etc/kubernetes/static-pod-resources/.*/.*/.*/.*\\.crt$ oval:ssg-test_file_owner_etcd_pki_cert_files:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-peer/etcd-peer-master-0.crtregular002445rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-2/secrets/localhost-recovery-client-token/ca.crtregular003560rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-2/configmaps/serviceaccount-ca/ca-bundle.crtregular000rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-certs/secrets/kube-scheduler-client-cert-key/tls.crtregular001168rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-peer/etcd-peer-master-1.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-peer/etcd-peer-master-2.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-peer/etcd-peer-master-0.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving/etcd-serving-master-0.crtregular002709rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/secrets/etcd-client/tls.crtregular001188rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-peer/etcd-peer-master-2.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving/etcd-serving-master-1.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving/etcd-serving-master-2.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/configmaps/etcd-metrics-proxy-client-ca/ca-bundle.crtregular001151rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-2/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crtregular001151rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-2/configmaps/etcd-peer-client-ca/ca-bundle.crtregular001135rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-2/configmaps/etcd-serving-ca/ca-bundle.crtregular001135rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-peer/etcd-peer-master-2.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-peer/etcd-peer-master-0.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-peer/etcd-peer-master-1.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving/etcd-serving-master-2.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving/etcd-serving-master-0.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving/etcd-serving-master-1.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-metrics-proxy-client-ca/ca-bundle.crtregular001151rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crtregular001151rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-peer-client-ca/ca-bundle.crtregular001135rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-serving-ca/ca-bundle.crtregular001135rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/secrets/localhost-recovery-client-token/ca.crtregular003560rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/configmaps/service-ca/ca-bundle.crtregular001212rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/configmaps/serviceaccount-ca/ca-bundle.crtregular000rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/secrets/csr-signer/tls.crtregular001229rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/secrets/kube-controller-manager-client-cert-key/tls.crtregular001180rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/configmaps/aggregator-client-ca/ca-bundle.crtregular001281rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/configmaps/client-ca/ca-bundle.crtregular008527rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/configmaps/trusted-ca-bundle/ca-bundle.crtregular00216090rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/aggregator-client/tls.crtregular001208rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/check-endpoints-client-cert-key/tls.crtregular001220rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/control-plane-node-admin-client-cert-key/tls.crtregular001212rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/external-loadbalancer-serving-certkey/tls.crtregular002417rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/internal-loadbalancer-serving-certkey/tls.crtregular002429rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/localhost-serving-cert-certkey/tls.crtregular002445rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/service-network-serving-certkey/tls.crtregular002713rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/configmaps/aggregator-client-ca/ca-bundle.crtregular001281rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/configmaps/client-ca/ca-bundle.crtregular008527rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/configmaps/trusted-ca-bundle/ca-bundle.crtregular00216090rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/secrets/localhost-recovery-client-token/ca.crtregular003560rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/configmaps/service-ca/ca-bundle.crtregular001212rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/configmaps/serviceaccount-ca/ca-bundle.crtregular007222rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-5/secrets/localhost-recovery-client-token/ca.crtregular003560rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-5/secrets/serving-cert/tls.crtregular002664rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-5/configmaps/serviceaccount-ca/ca-bundle.crtregular007222rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-peer/etcd-peer-master-2.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-peer/etcd-peer-master-0.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-peer/etcd-peer-master-1.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving/etcd-serving-master-0.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving/etcd-serving-master-1.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving/etcd-serving-master-2.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/configmaps/etcd-metrics-proxy-client-ca/ca-bundle.crtregular001151rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-3/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crtregular001151rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-3/configmaps/etcd-peer-client-ca/ca-bundle.crtregular001135rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-3/configmaps/etcd-serving-ca/ca-bundle.crtregular001135rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/secrets/localhost-recovery-client-token/service-ca.crtregular008435rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/secrets/localhost-recovery-client-token/ca.crtregular007222rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/configmaps/service-ca/ca-bundle.crtregular001212rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/configmaps/serviceaccount-ca/ca-bundle.crtregular007222rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/secrets/localhost-recovery-client-token/ca.crtregular007222rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/secrets/localhost-recovery-client-token/service-ca.crtregular008435rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/secrets/serving-cert/tls.crtregular002664rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/configmaps/serviceaccount-ca/ca-bundle.crtregular007222rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/secrets/localhost-recovery-client-token/ca.crtregular007222rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/secrets/localhost-recovery-client-token/service-ca.crtregular008435rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/configmaps/service-ca/ca-bundle.crtregular001212rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/configmaps/serviceaccount-ca/ca-bundle.crtregular007222rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/etcd-client/tls.crtregular001188rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/kubelet-client/tls.crtregular001204rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/localhost-recovery-client-token/service-ca.crtregular008435rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/localhost-recovery-client-token/ca.crtregular007222rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/localhost-recovery-serving-certkey/tls.crtregular002600rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/configmaps/etcd-serving-ca/ca-bundle.crtregular001135rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/configmaps/kubelet-serving-ca/ca-bundle.crtregular002319rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/configmaps/kube-apiserver-server-ca/ca-bundle.crtregular004866rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-peer/etcd-peer-master-1.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-peer/etcd-peer-master-2.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving/etcd-serving-master-2.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving/etcd-serving-master-0.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving/etcd-serving-master-1.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.crtregular002733rw------- ... and 81 more items. Verify User Who Owns The OpenShift Multus Container Network Interface Plugin Filesxccdf_org.ssgproject.content_rule_file_owner_multus_conf mediumCCE-83603-1 Verify User Who Owns The OpenShift Multus Container Network Interface Plugin FilesRule IDxccdf_org.ssgproject.content_rule_file_owner_multus_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_multus_conf:def:1Time2021-07-20T05:20:58+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83603-1\nReferences: 1.1.10, CM-6, CM-6(1)\nDescription To properly set the owner of /var/run/multus/cni/net.d/*, run the command: $ sudo chown root /var/run/multus/cni/net.d/* RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of ^/var/run/multus/cni/net.d/.*$ oval:ssg-test_file_owner_multus_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/run/multus/cni/net.d/10-ovn-kubernetes.confregular00233rw------- Verify User Who Owns The Open vSwitch Database Server PIDxccdf_org.ssgproject.content_rule_file_owner_ovsdb_server_pid mediumCCE-83806-0 Verify User Who Owns The Open vSwitch Database Server PIDRule IDxccdf_org.ssgproject.content_rule_file_owner_ovsdb_server_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_ovsdb_server_pid:def:1Time2021-07-20T05:20:58+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83806-0\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the owner of /run/openvswitch/ovsdb-server.pid, run the command: $ sudo chown openvswitch /run/openvswitch/ovsdb-server.pid RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of /run/openvswitch/ovsdb-server.pid oval:ssg-test_file_owner_ovsdb_server_pid:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/run/openvswitch/ovsdb-server.pidregular8008015rw-r--r-- Verify User Who Owns The Open vSwitch Configuration Database Lockxccdf_org.ssgproject.content_rule_file_owner_ovs_conf_db_lock mediumCCE-83462-2 Verify User Who Owns The Open vSwitch Configuration Database LockRule IDxccdf_org.ssgproject.content_rule_file_owner_ovs_conf_db_lockResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_ovs_conf_db_lock:def:1Time2021-07-20T05:20:58+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83462-2\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/openvswitch/.conf.db.~lock~, run the command: $ sudo chown openvswitch /etc/openvswitch/.conf.db.~lock~ RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of /etc/openvswitch/.conf.db.~lock~ oval:ssg-test_file_owner_ovs_conf_db_lock:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/openvswitch/.conf.db.~lock~regular8008010rw------- Verify Group Who Owns The Open vSwitch Persistent System IDxccdf_org.ssgproject.content_rule_file_groupowner_ovs_sys_id_conf mediumCCE-83677-5 Verify Group Who Owns The Open vSwitch Persistent System IDRule IDxccdf_org.ssgproject.content_rule_file_groupowner_ovs_sys_id_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_ovs_sys_id_conf:def:1Time2021-07-20T05:20:58+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83677-5\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/openvswitch/system-id.conf, run the command: $ sudo chgrp hugetlbfs /etc/openvswitch/system-id.confRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of /etc/openvswitch/system-id.conf oval:ssg-test_file_groupowner_ovs_sys_id_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/openvswitch/system-id.confregular80080137rw-r--r-- Verify Permissions on the OpenShift SDN CNI Server Configxccdf_org.ssgproject.content_rule_file_perms_openshift_sdn_cniserver_config mediumCCE-83927-4 Verify Permissions on the OpenShift SDN CNI Server ConfigRule IDxccdf_org.ssgproject.content_rule_file_perms_openshift_sdn_cniserver_configResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_perms_openshift_sdn_cniserver_config:def:1Time2021-07-20T05:20:58+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83927-4\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /var/run/openshift-sdn/cniserver/config.json, run the command: $ sudo chmod 0444 /var/run/openshift-sdn/cniserver/config.jsonRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of /var/run/openshift-sdn/cniserver/config.json oval:ssg-test_file_permissionsfile_perms_openshift_sdn_cniserver_config:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissionsfile_perms_openshift_sdn_cniserver_config:obj:1 of type file_objectFilepathFilter/var/run/openshift-sdn/cniserver/config.jsonoval:ssg-state_file_permissionsfile_perms_openshift_sdn_cniserver_config_mode_not_0444:ste:1 Verify Group Who Owns The OpenShift SDN Container Network Interface Plugin IP Address Allocationsxccdf_org.ssgproject.content_rule_file_groupowner_ip_allocations mediumCCE-84211-2 Verify Group Who Owns The OpenShift SDN Container Network Interface Plugin IP Address AllocationsRule IDxccdf_org.ssgproject.content_rule_file_groupowner_ip_allocationsResultfailMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_ip_allocations:def:1Time2021-07-20T05:20:58+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84211-2\nReferences: 1.1.10, CM-6, CM-6(1)\nDescription To properly set the group owner of /var/lib/cni/networks/openshift-sdn/.*, run the command: $ sudo chgrp root /var/lib/cni/networks/openshift-sdn/.*RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of ^/var/lib/cni/networks/openshift-sdn/.*$ oval:ssg-test_file_groupowner_ip_allocations:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_groupowner_ip_allocations:obj:1 of type file_objectFilepath^/var/lib/cni/networks/openshift-sdn/.*$ Verify Group Who Owns The Open vSwitch Process ID Filexccdf_org.ssgproject.content_rule_file_groupowner_ovs_pid mediumCCE-83630-4 Verify Group Who Owns The Open vSwitch Process ID FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_ovs_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_ovs_pid:def:1Time2021-07-20T05:20:58+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83630-4\nReferences: 1.1.9, CM-6, CM-6(1)\nDescriptionEnsure that the file /var/run/openvswitch/ovs-vswitchd.pid, is owned by the group openvswitch or hugetlbfs, depending on your settings and Open vSwitch version.RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of /run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_groupowner_run_ovs_vswitchd_pid_800:tst:1 falseFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/run/openvswitch/ovs-vswitchd.pidregular8008015rw-r--r-- Testing group ownership of /run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_groupowner_run_ovs_vswitchd_pid_801:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/run/openvswitch/ovs-vswitchd.pidregular8008015rw-r--r-- Verify User Who Owns The Etcd Database Directoryxccdf_org.ssgproject.content_rule_file_owner_etcd_data_dir mediumCCE-83905-0 Verify User Who Owns The Etcd Database DirectoryRule IDxccdf_org.ssgproject.content_rule_file_owner_etcd_data_dirResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_etcd_data_dir:def:1Time2021-07-20T05:20:58+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83905-0\nReferences: 1.1.12, CM-6, CM-6(1)\nDescription To properly set the owner of /var/lib/etcd/member/, run the command: $ sudo chown root /var/lib/etcd/member/ Rationaleetcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing user ownership of /var/lib/etcd/member/ oval:ssg-test_file_owner_etcd_data_dir:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/lib/etcd/member//directory0029rwx------ Verify Group Who Owns The Etcd Write-Ahead-Log Filesxccdf_org.ssgproject.content_rule_file_groupowner_etcd_data_files mediumCCE-83816-9 Verify Group Who Owns The Etcd Write-Ahead-Log FilesRule IDxccdf_org.ssgproject.content_rule_file_groupowner_etcd_data_filesResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_etcd_data_files:def:1Time2021-07-20T05:20:58+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83816-9\nReferences: 1.1.12, CM-6, CM-6(1)\nDescription To properly set the group owner of /var/lib/etcd/member/wal/*, run the command: $ sudo chgrp root /var/lib/etcd/member/wal/*Rationaleetcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing group ownership of ^/var/lib/etcd/member/wal/.*$ oval:ssg-test_file_groupowner_etcd_data_files:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/lib/etcd/member/wal/0000000000000058-0000000000540992.walregular0064000376rw------- /var/lib/etcd/member/wal/0000000000000059-0000000000552b6f.walregular0064000992rw------- /var/lib/etcd/member/wal/000000000000005a-0000000000564c8f.walregular0064000160rw------- /var/lib/etcd/member/wal/000000000000005b-0000000000576c47.walregular0064000400rw------- /var/lib/etcd/member/wal/000000000000005c-0000000000588cce.walregular0064000000rw------- /var/lib/etcd/member/wal/0.tmpregular0064000000rw------- Verify Permissions on the Etcd PKI Certificate Filesxccdf_org.ssgproject.content_rule_file_permissions_etcd_pki_cert_files mediumCCE-83362-4 Verify Permissions on the Etcd PKI Certificate FilesRule IDxccdf_org.ssgproject.content_rule_file_permissions_etcd_pki_cert_filesResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_etcd_pki_cert_files:def:1Time2021-07-20T05:20:58+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83362-4\nReferences: 1.1.20, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/etcd-*/secrets/*/*.crt, run the command: $ sudo chmod 0600 /etc/kubernetes/static-pod-resources/etcd-*/secrets/*/*.crtRationaleOpenShift makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to 600 or more restrictive to protect their integrity.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing mode of ^/etc/kubernetes/static-pod-resources/etcd-.*/secrets/.*/.*\\.crt$ oval:ssg-test_file_permissions_etcd_pki_cert_files:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_etcd_pki_cert_files:obj:1 of type file_objectFilepathFilter^/etc/kubernetes/static-pod-resources/etcd-.*/secrets/.*/.*\\.crt$oval:ssg-state_file_permissions_etcd_pki_cert_files_mode_not_0600:ste:1 Verify Group Who Owns The Open vSwitch Database Server PIDxccdf_org.ssgproject.content_rule_file_groupowner_ovsdb_server_pid mediumCCE-84166-8 Verify Group Who Owns The Open vSwitch Database Server PIDRule IDxccdf_org.ssgproject.content_rule_file_groupowner_ovsdb_server_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_ovsdb_server_pid:def:1Time2021-07-20T05:20:58+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84166-8\nReferences: 1.1.9, CM-6, CM-6(1)\nDescriptionEnsure that the file /run/openvswitch/ovsdb-server.pid, is owned by the group openvswitch or hugetlbfs, depending on your settings and Open vSwitch version.RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of /run/openvswitch/ovsdb-server.pid oval:ssg-test_file_groupowner_ovsdb_server_pid_800:tst:1 falseFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/run/openvswitch/ovsdb-server.pidregular8008015rw-r--r-- Testing group ownership of /run/openvswitch/ovsdb-server.pid oval:ssg-test_file_groupowner_ovsdb_server_pid_801:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/run/openvswitch/ovsdb-server.pidregular8008015rw-r--r-- Verify Permissions on the OpenShift Controller Manager Kubeconfig Filexccdf_org.ssgproject.content_rule_file_permissions_controller_manager_kubeconfig mediumCCE-83604-9 Verify Permissions on the OpenShift Controller Manager Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_controller_manager_kubeconfigResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_controller_manager_kubeconfig:def:1Time2021-07-20T05:20:58+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83604-9\nReferences: 1.1.17, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig, run the command: $ sudo chmod 0644 /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfigRationaleThe Controller Manager's kubeconfig contains information about how the component will access the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.Warningswarning This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing mode of ^/etc/kubernetes/static-pod-resources/kube-controller-manager-pod-.*/configmaps/controller-manager-kubeconfig/kubeconfig$ oval:ssg-test_file_permissions_controller_manager_kubeconfig:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_controller_manager_kubeconfig:obj:1 of type file_objectFilepathFilter^/etc/kubernetes/static-pod-resources/kube-controller-manager-pod-.*/configmaps/controller-manager-kubeconfig/kubeconfig$oval:ssg-state_file_permissions_controller_manager_kubeconfig_mode_not_0644:ste:1 Verify Group Who Owns The OpenShift PKI Private Key Filesxccdf_org.ssgproject.content_rule_file_groupowner_openshift_pki_key_files mediumCCE-84172-6 Verify Group Who Owns The OpenShift PKI Private Key FilesRule IDxccdf_org.ssgproject.content_rule_file_groupowner_openshift_pki_key_filesResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_openshift_pki_key_files:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84172-6\nReferences: 1.1.19, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/*/*/*/*.key, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/*/*/*/*.keyRationaleOpenShift makes use of a number of certificates as part of its operation. You should verify the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by root:root.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing group ownership of ^/etc/kubernetes/static-pod-resources/.*/.*/.*/.*\\.key$ oval:ssg-test_file_groupowner_openshift_pki_key_files:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7/secrets/service-account-private-key/service-account.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/secrets/etcd-client/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-peer/etcd-peer-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-8/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-peer/etcd-peer-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-certs/secrets/kube-scheduler-client-cert-key/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-peer/etcd-peer-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-peer/etcd-peer-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-peer/etcd-peer-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving/etcd-serving-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving/etcd-serving-master-0.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving/etcd-serving-master-1.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-peer/etcd-peer-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-peer/etcd-peer-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-peer/etcd-peer-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving/etcd-serving-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving/etcd-serving-master-0.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving/etcd-serving-master-1.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/secrets/service-account-private-key/service-account.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/secrets/csr-signer/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/secrets/kube-controller-manager-client-cert-key/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/aggregator-client/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/bound-service-account-signing-key/service-account.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/check-endpoints-client-cert-key/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/control-plane-node-admin-client-cert-key/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/external-loadbalancer-serving-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/internal-loadbalancer-serving-certkey/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/localhost-serving-cert-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/service-network-serving-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/secrets/service-account-private-key/service-account.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-5/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-peer/etcd-peer-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-peer/etcd-peer-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-peer/etcd-peer-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving/etcd-serving-master-0.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving/etcd-serving-master-1.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving/etcd-serving-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/secrets/service-account-private-key/service-account.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/secrets/service-account-private-key/service-account.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/etcd-client/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/kubelet-client/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/localhost-recovery-serving-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-peer/etcd-peer-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-peer/etcd-peer-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving/etcd-serving-master-1.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving/etcd-serving-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving/etcd-serving-master-0.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6/secrets/etcd-client/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6/secrets/kubelet-client/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6/secrets/localhost-recovery-serving-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-7/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-8/secrets/service-account-private-key/service-account.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-8/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8/secrets/etcd-client/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8/secrets/kubelet-client/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8/secrets/localhost-recovery-serving-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/secrets/kubelet-client/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/secrets/localhost-recovery-serving-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-peer/etcd-peer-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-peer/etcd-peer-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-serving/etcd-serving-master-0.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-serving/etcd-serving-master-1.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-serving/etcd-serving-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-10/secrets/etcd-client/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-10/secrets/kubelet-client/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-10/secrets/localhost-recovery-serving-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-9/secrets/service-account-private-key/service-account.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-9/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10/secrets/service-account-private-key/service-account.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-11/secrets/service-account-private-key/service-account.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-11/secrets/serving-cert/tls.keyregular001675rw------- Verify Group Who Owns The OpenShift Container Network Interface Filesxccdf_org.ssgproject.content_rule_file_groupowner_cni_conf mediumCCE-84025-6 Verify Group Who Owns The OpenShift Container Network Interface FilesRule IDxccdf_org.ssgproject.content_rule_file_groupowner_cni_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_cni_conf:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84025-6\nReferences: 1.1.10, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/cni/net.d/*, run the command: $ sudo chgrp root /etc/cni/net.d/*RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of ^/etc/cni/net.d/.*$ oval:ssg-test_file_groupowner_cni_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/cni/net.d/100-crio-bridge.confregular00438rw-r--r-- /etc/cni/net.d/200-loopback.confregular0054rw-r--r-- /etc/cni/net.d/87-podman-bridge.conflistregular00639rw-r--r-- Verify User Who Owns The Etcd Member Pod Specification Filexccdf_org.ssgproject.content_rule_file_owner_etcd_member mediumCCE-83988-6 Verify User Who Owns The Etcd Member Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_owner_etcd_memberResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_etcd_member:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83988-6\nReferences: 1.1.8, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml RationaleThe etcd pod specification file controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing user ownership of ^/etc/kubernetes/static-pod-resources/etcd-pod-.*/etcd-pod.yaml$ oval:ssg-test_file_owner_etcd_member:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/etcd-pod-2/etcd-pod.yamlregular0017522rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-3/etcd-pod.yamlregular0017418rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-4/etcd-pod.yamlregular0015993rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-5/etcd-pod.yamlregular0015130rw-r--r-- Verify Group Who Owns The OpenShift Admin Kubeconfig Filesxccdf_org.ssgproject.content_rule_file_groupowner_master_admin_kubeconfigs mediumCCE-84204-7 Verify Group Who Owns The OpenShift Admin Kubeconfig FilesRule IDxccdf_org.ssgproject.content_rule_file_groupowner_master_admin_kubeconfigsResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_master_admin_kubeconfigs:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84204-7\nReferences: 1.1.14, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfigRationaleThere are various kubeconfig files that can be used by the administrator, defining various settings for the administration of the cluster. These files contain credentials that can be used to control the cluster and are needed for disaster recovery and each kubeconfig points to a different endpoint in the cluster. You should restrict its file permissions to maintain the integrity of the kubeconfig file as an attacker who gains access to these files can take over the cluster.Warningswarning This rule is only applicable for nodes that run the Kubernetes API server service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing group ownership of ^/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/.*\\.kubeconfig$ oval:ssg-test_file_groupowner_master_admin_kubeconfigs:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost-recovery.kubeconfigregular0010755rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfigregular0010697rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/lb-ext.kubeconfigregular0010701rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/lb-int.kubeconfigregular0010705rw------- Verify Group Who Owns The OpenShift PKI Certificate Filesxccdf_org.ssgproject.content_rule_file_groupowner_openshift_pki_cert_files mediumCCE-83922-5 Verify Group Who Owns The OpenShift PKI Certificate FilesRule IDxccdf_org.ssgproject.content_rule_file_groupowner_openshift_pki_cert_filesResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_openshift_pki_cert_files:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83922-5\nReferences: 1.1.19, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/*/*/*/tls.crt, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/*/*/*/tls.crtRationaleOpenShift makes use of a number of certificates as part of its operation. You should verify the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by the system administrator.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing group ownership of ^/etc/kubernetes/static-pod-resources/.*/.*/.*/tls\\.crt$ oval:ssg-test_file_groupowner_openshift_pki_cert_files:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-scheduler-certs/secrets/kube-scheduler-client-cert-key/tls.crtregular001168rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/secrets/etcd-client/tls.crtregular001188rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/secrets/csr-signer/tls.crtregular001229rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/secrets/kube-controller-manager-client-cert-key/tls.crtregular001180rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/aggregator-client/tls.crtregular001208rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/check-endpoints-client-cert-key/tls.crtregular001220rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/control-plane-node-admin-client-cert-key/tls.crtregular001212rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/external-loadbalancer-serving-certkey/tls.crtregular002417rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/internal-loadbalancer-serving-certkey/tls.crtregular002429rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/localhost-serving-cert-certkey/tls.crtregular002445rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/service-network-serving-certkey/tls.crtregular002713rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-5/secrets/serving-cert/tls.crtregular002664rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/secrets/serving-cert/tls.crtregular002664rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/etcd-client/tls.crtregular001188rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/kubelet-client/tls.crtregular001204rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/localhost-recovery-serving-certkey/tls.crtregular002600rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6/secrets/etcd-client/tls.crtregular001188rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6/secrets/kubelet-client/tls.crtregular001204rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6/secrets/localhost-recovery-serving-certkey/tls.crtregular002600rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-7/secrets/serving-cert/tls.crtregular002664rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-8/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8/secrets/etcd-client/tls.crtregular001188rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8/secrets/kubelet-client/tls.crtregular001204rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8/secrets/localhost-recovery-serving-certkey/tls.crtregular002600rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/secrets/kubelet-client/tls.crtregular001204rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/secrets/localhost-recovery-serving-certkey/tls.crtregular002600rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-10/secrets/etcd-client/tls.crtregular001188rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-10/secrets/kubelet-client/tls.crtregular001204rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-10/secrets/localhost-recovery-serving-certkey/tls.crtregular002600rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-8/secrets/serving-cert/tls.crtregular002664rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-9/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-11/secrets/serving-cert/tls.crtregular002761rw------- Verify User Who Owns The OpenShift Admin Kubeconfig Filesxccdf_org.ssgproject.content_rule_file_owner_master_admin_kubeconfigs mediumCCE-83719-5 Verify User Who Owns The OpenShift Admin Kubeconfig FilesRule IDxccdf_org.ssgproject.content_rule_file_owner_master_admin_kubeconfigsResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_master_admin_kubeconfigs:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83719-5\nReferences: 1.1.14, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig RationaleThere are various kubeconfig files that can be used by the administrator, defining various settings for the administration of the cluster. These files contain credentials that can be used to control the cluster and are needed for disaster recovery and each kubeconfig points to a different endpoint in the cluster. You should restrict its file permissions to maintain the integrity of the kubeconfig file as an attacker who gains access to these files can take over the cluster.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing user ownership of ^/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/.*\\.kubeconfig$ oval:ssg-test_file_owner_master_admin_kubeconfigs:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost-recovery.kubeconfigregular0010755rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfigregular0010697rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/lb-ext.kubeconfigregular0010701rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/lb-int.kubeconfigregular0010705rw------- Verify Permissions on the Etcd Member Pod Specification Filexccdf_org.ssgproject.content_rule_file_permissions_etcd_member mediumCCE-83973-8 Verify Permissions on the Etcd Member Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_etcd_memberResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_etcd_member:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83973-8\nReferences: 1.1.7, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml, run the command: $ sudo chmod 0644 /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yamlRationaleThe etcd pod specification file controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing mode of ^/etc/kubernetes/static-pod-resources/etcd-pod-.*/etcd-pod.yaml$ oval:ssg-test_file_permissions_etcd_member:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_etcd_member:obj:1 of type file_objectFilepathFilter^/etc/kubernetes/static-pod-resources/etcd-pod-.*/etcd-pod.yaml$oval:ssg-state_file_permissions_etcd_member_mode_not_0644:ste:1 Verify Permissions on the Kubernetes Controller Manager Pod Specificiation Filexccdf_org.ssgproject.content_rule_file_permissions_kube_controller_manager mediumCCE-84161-9 Verify Permissions on the Kubernetes Controller Manager Pod Specificiation FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_kube_controller_managerResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_kube_controller_manager:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84161-9\nReferences: 1.1.3, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml, run the command: $ sudo chmod 0644 /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yamlRationaleIf the Kubernetes specification file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the configuration of an Kubernetes Controller Manager server that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing mode of ^/etc/kubernetes/static-pod-resources/kube-controller-manager-pod-.*/kube-controller-manager-pod.yaml$ oval:ssg-test_file_permissions_kube_controller_manager:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_kube_controller_manager:obj:1 of type file_objectFilepathFilter^/etc/kubernetes/static-pod-resources/kube-controller-manager-pod-.*/kube-controller-manager-pod.yaml$oval:ssg-state_file_permissions_kube_controller_manager_mode_not_0644:ste:1 Verify Permissions on the OpenShift PKI Certificate Filesxccdf_org.ssgproject.content_rule_file_permissions_openshift_pki_cert_files mediumCCE-83552-0 Verify Permissions on the OpenShift PKI Certificate FilesRule IDxccdf_org.ssgproject.content_rule_file_permissions_openshift_pki_cert_filesResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_openshift_pki_cert_files:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83552-0\nReferences: 1.1.20, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/kube-*/secrets/*/tls.crt, run the command: $ sudo chmod 0600 /etc/kubernetes/static-pod-resources/kube-*/secrets/*/tls.crtRationaleOpenShift makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to 600 or more restrictive to protect their integrity.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing mode of ^/etc/kubernetes/static-pod-resources/kube-.*/secrets/.*/tls\\.crt$ oval:ssg-test_file_permissions_openshift_pki_cert_files:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_openshift_pki_cert_files:obj:1 of type file_objectFilepathFilter^/etc/kubernetes/static-pod-resources/kube-.*/secrets/.*/tls\\.crt$oval:ssg-state_file_permissions_openshift_pki_cert_files_mode_not_0600:ste:1 Verify Group Who Owns The OpenShift Controller Manager Kubeconfig Filexccdf_org.ssgproject.content_rule_file_groupowner_controller_manager_kubeconfig mediumCCE-84095-9 Verify Group Who Owns The OpenShift Controller Manager Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_controller_manager_kubeconfigResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_controller_manager_kubeconfig:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84095-9\nReferences: 1.1.18, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfigRationaleThe Controller Manager's kubeconfig contains information about how the component will access the API server. You should set its file ownership to maintain the integrity of the file.Warningswarning This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing group ownership of ^/etc/kubernetes/static-pod-resources/kube-controller-manager-pod-.*/configmaps/controller-manager-kubeconfig/kubeconfig$ oval:ssg-test_file_groupowner_controller_manager_kubeconfig:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-8/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-9/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-11/configmaps/controller-manager-kubeconfig/kubeconfigregular00673rw-r--r-- Verify Permissions on the Open vSwitch Configuration Databasexccdf_org.ssgproject.content_rule_file_permissions_ovs_conf_db mediumCCE-83788-0 Verify Permissions on the Open vSwitch Configuration DatabaseRule IDxccdf_org.ssgproject.content_rule_file_permissions_ovs_conf_dbResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_ovs_conf_db:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83788-0\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/openvswitch/conf.db, run the command: $ sudo chmod 0640 /etc/openvswitch/conf.dbRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of /etc/openvswitch/conf.db oval:ssg-test_file_permissions_ovs_conf_db:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_ovs_conf_db:obj:1 of type file_objectFilepathFilter/etc/openvswitch/conf.dboval:ssg-state_file_permissions_ovs_conf_db_mode_not_0640:ste:1 Verify Permissions on the Open vSwitch Daemon PID Filexccdf_org.ssgproject.content_rule_file_permissions_ovs_vswitchd_pid mediumCCE-83710-4 Verify Permissions on the Open vSwitch Daemon PID FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_ovs_vswitchd_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_ovs_vswitchd_pid:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83710-4\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /run/openvswitch/ovs-vswitchd.pid, run the command: $ sudo chmod 0644 /run/openvswitch/ovs-vswitchd.pidRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of /run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_permissions_ovs_vswitchd_pid:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_ovs_vswitchd_pid:obj:1 of type file_objectFilepathFilter/run/openvswitch/ovs-vswitchd.pidoval:ssg-state_file_permissions_ovs_vswitchd_pid_mode_not_0644:ste:1 Verify Permissions on the Kubernetes Scheduler Pod Specification Filexccdf_org.ssgproject.content_rule_file_permissions_scheduler mediumCCE-84057-9 Verify Permissions on the Kubernetes Scheduler Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_schedulerResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_scheduler:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84057-9\nReferences: 1.1.5, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml, run the command: $ sudo chmod 0644 /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yamlRationaleIf the Kubernetes specification file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the configuration of an Kubernetes Scheduler service that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing mode of ^/etc/kubernetes/static-pod-resources/kube-scheduler-pod-.*/kube-scheduler-pod.yaml$ oval:ssg-test_file_permissions_scheduler:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_scheduler:obj:1 of type file_objectFilepathFilter^/etc/kubernetes/static-pod-resources/kube-scheduler-pod-.*/kube-scheduler-pod.yaml$oval:ssg-state_file_permissions_scheduler_mode_not_0644:ste:1 Verify Group Who Owns The OpenShift Multus Container Network Interface Plugin Filesxccdf_org.ssgproject.content_rule_file_groupowner_multus_conf mediumCCE-83818-5 Verify Group Who Owns The OpenShift Multus Container Network Interface Plugin FilesRule IDxccdf_org.ssgproject.content_rule_file_groupowner_multus_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_multus_conf:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83818-5\nReferences: 1.1.10, CM-6, CM-6(1)\nDescription To properly set the group owner of /var/run/multus/cni/net.d/*, run the command: $ sudo chgrp root /var/run/multus/cni/net.d/*RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of ^/var/run/multus/cni/net.d/.*$ oval:ssg-test_file_groupowner_multus_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/run/multus/cni/net.d/10-ovn-kubernetes.confregular00233rw------- Verify User Who Owns The Kubernetes Controller Manager Pod Specificiation Filexccdf_org.ssgproject.content_rule_file_owner_kube_controller_manager mediumCCE-83795-5 Verify User Who Owns The Kubernetes Controller Manager Pod Specificiation FileRule IDxccdf_org.ssgproject.content_rule_file_owner_kube_controller_managerResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_kube_controller_manager:def:1Time2021-07-20T05:20:59+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83795-5\nReferences: 1.1.4, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml RationaleThe Kubernetes specification file contains information about the configuration of the Kubernetes Controller Manager Server that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing user ownership of ^/etc/kubernetes/static-pod-resources/kube-controller-manager-pod-.*/kube-controller-manager-pod.yaml$ oval:ssg-test_file_owner_kube_controller_manager:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/kube-controller-manager-pod.yamlregular006043rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/kube-controller-manager-pod.yamlregular006043rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7/kube-controller-manager-pod.yamlregular006088rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-9/kube-controller-manager-pod.yamlregular007784rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/kube-controller-manager-pod.yamlregular006043rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10/kube-controller-manager-pod.yamlregular007786rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-11/kube-controller-manager-pod.yamlregular007823rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/kube-controller-manager-pod.yamlregular006043rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-8/kube-controller-manager-pod.yamlregular005950rw-r--r-- Verify User Who Owns The OpenShift PKI Private Key Filesxccdf_org.ssgproject.content_rule_file_owner_openshift_pki_key_files mediumCCE-83435-8 Verify User Who Owns The OpenShift PKI Private Key FilesRule IDxccdf_org.ssgproject.content_rule_file_owner_openshift_pki_key_filesResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_openshift_pki_key_files:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83435-8\nReferences: 1.1.19, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/*/*/*/*.key, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/*/*/*/*.key RationaleOpenShift makes use of a number of certificates as part of its operation. You should verify the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by root:root.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing user ownership of ^/etc/kubernetes/static-pod-resources/.*/.*/.*/.*\\.key$ oval:ssg-test_file_owner_openshift_pki_key_files:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7/secrets/service-account-private-key/service-account.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/secrets/etcd-client/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-peer/etcd-peer-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-8/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-peer/etcd-peer-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-certs/secrets/kube-scheduler-client-cert-key/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-peer/etcd-peer-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-peer/etcd-peer-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-peer/etcd-peer-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving/etcd-serving-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving/etcd-serving-master-0.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving/etcd-serving-master-1.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-peer/etcd-peer-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-peer/etcd-peer-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-peer/etcd-peer-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving/etcd-serving-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving/etcd-serving-master-0.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving/etcd-serving-master-1.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/secrets/service-account-private-key/service-account.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/secrets/csr-signer/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/secrets/kube-controller-manager-client-cert-key/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/aggregator-client/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/bound-service-account-signing-key/service-account.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/check-endpoints-client-cert-key/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/control-plane-node-admin-client-cert-key/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/external-loadbalancer-serving-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/internal-loadbalancer-serving-certkey/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/localhost-serving-cert-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/service-network-serving-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/secrets/service-account-private-key/service-account.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-5/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-peer/etcd-peer-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-peer/etcd-peer-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-peer/etcd-peer-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving/etcd-serving-master-0.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving/etcd-serving-master-1.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving/etcd-serving-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/secrets/service-account-private-key/service-account.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/secrets/service-account-private-key/service-account.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/etcd-client/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/kubelet-client/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/localhost-recovery-serving-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-peer/etcd-peer-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-peer/etcd-peer-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving/etcd-serving-master-1.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving/etcd-serving-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving/etcd-serving-master-0.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6/secrets/etcd-client/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6/secrets/kubelet-client/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6/secrets/localhost-recovery-serving-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-7/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-8/secrets/service-account-private-key/service-account.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-8/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8/secrets/etcd-client/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8/secrets/kubelet-client/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8/secrets/localhost-recovery-serving-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/secrets/kubelet-client/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/secrets/localhost-recovery-serving-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-peer/etcd-peer-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-peer/etcd-peer-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-serving/etcd-serving-master-0.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-serving/etcd-serving-master-1.keyregular001675rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-serving/etcd-serving-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.keyregular001679rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-10/secrets/etcd-client/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-10/secrets/kubelet-client/tls.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-10/secrets/localhost-recovery-serving-certkey/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-9/secrets/service-account-private-key/service-account.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-9/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10/secrets/service-account-private-key/service-account.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10/secrets/serving-cert/tls.keyregular001675rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-11/secrets/service-account-private-key/service-account.keyregular001679rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-11/secrets/serving-cert/tls.keyregular001675rw------- Verify Group Who Owns The Etcd Database Directoryxccdf_org.ssgproject.content_rule_file_groupowner_etcd_data_dir mediumCCE-83354-1 Verify Group Who Owns The Etcd Database DirectoryRule IDxccdf_org.ssgproject.content_rule_file_groupowner_etcd_data_dirResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_etcd_data_dir:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83354-1\nReferences: 1.1.12, CM-6, CM-6(1)\nDescription To properly set the group owner of /var/lib/etcd/member/, run the command: $ sudo chgrp root /var/lib/etcd/member/Rationaleetcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing group ownership of /var/lib/etcd/member/ oval:ssg-test_file_groupowner_etcd_data_dir:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/lib/etcd/member//directory0029rwx------ Verify User Who Owns The Kubernetes Scheduler Kubeconfig Filexccdf_org.ssgproject.content_rule_file_owner_scheduler_kubeconfig mediumCCE-84017-3 Verify User Who Owns The Kubernetes Scheduler Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_owner_scheduler_kubeconfigResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_scheduler_kubeconfig:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84017-3\nReferences: 1.1.16, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig RationaleThe kubeconfig for the Scheduler contains paramters for the scheduler to access the Kube API. You should set its file ownership to maintain the integrity of the file.Warningswarning This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing user ownership of ^/etc/kubernetes/static-pod-resources/kube-scheduler-pod-.*/configmaps/scheduler-kubeconfig/kubeconfig$ oval:ssg-test_file_owner_scheduler_kubeconfig:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-scheduler-pod-5/configmaps/scheduler-kubeconfig/kubeconfigregular00619rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/configmaps/scheduler-kubeconfig/kubeconfigregular00619rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-8/configmaps/scheduler-kubeconfig/kubeconfigregular00619rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-2/configmaps/scheduler-kubeconfig/kubeconfigregular00619rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-7/configmaps/scheduler-kubeconfig/kubeconfigregular00619rw-r--r-- Verify Permissions on the Etcd Database Directoryxccdf_org.ssgproject.content_rule_file_permissions_etcd_data_dir mediumCCE-84013-2 Verify Permissions on the Etcd Database DirectoryRule IDxccdf_org.ssgproject.content_rule_file_permissions_etcd_data_dirResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_etcd_data_dir:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84013-2\nReferences: 1.1.11, CM-6, CM-6(1)\nDescription To properly set the permissions of /var/lib/etcd, run the command: $ sudo chmod 0700 /var/lib/etcdRationaleetcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing mode of /var/lib/etcd/member/ oval:ssg-test_file_permissions_etcd_data_dir:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_etcd_data_dir:obj:1 of type file_objectPathFilenameFilter/var/lib/etcd/member/no valueoval:ssg-state_file_permissions_etcd_data_dir_mode_not_0700:ste:1 Verify User Who Owns The Kubernetes Scheduler Pod Specification Filexccdf_org.ssgproject.content_rule_file_owner_kube_scheduler mediumCCE-83393-9 Verify User Who Owns The Kubernetes Scheduler Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_owner_kube_schedulerResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_kube_scheduler:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83393-9\nReferences: 1.1.6, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml RationaleThe Kubernetes specification file contains information about the configuration of the Kubernetes scheduler that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing user ownership of ^/etc/kubernetes/static-pod-resources/kube-scheduler-pod-.*/kube-scheduler-pod.yaml$ oval:ssg-test_file_owner_kube_scheduler:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-scheduler-pod-2/kube-scheduler-pod.yamlregular003356rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-5/kube-scheduler-pod.yamlregular003531rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/kube-scheduler-pod.yamlregular003531rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-7/kube-scheduler-pod.yamlregular004547rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-8/kube-scheduler-pod.yamlregular004612rw-r--r-- Verify User Who Owns The OpenShift Container Network Interface Filesxccdf_org.ssgproject.content_rule_file_owner_cni_conf mediumCCE-83460-6 Verify User Who Owns The OpenShift Container Network Interface FilesRule IDxccdf_org.ssgproject.content_rule_file_owner_cni_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_cni_conf:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83460-6\nReferences: 1.1.10, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/cni/net.d/*, run the command: $ sudo chown root /etc/cni/net.d/* RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of ^/etc/cni/net.d/.*$ oval:ssg-test_file_owner_cni_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/cni/net.d/100-crio-bridge.confregular00438rw-r--r-- /etc/cni/net.d/200-loopback.confregular0054rw-r--r-- /etc/cni/net.d/87-podman-bridge.conflistregular00639rw-r--r-- Verify User Who Owns The Etcd Write-Ahead-Log Filesxccdf_org.ssgproject.content_rule_file_owner_etcd_data_files mediumCCE-84010-8 Verify User Who Owns The Etcd Write-Ahead-Log FilesRule IDxccdf_org.ssgproject.content_rule_file_owner_etcd_data_filesResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_etcd_data_files:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84010-8\nReferences: 1.1.12, CM-6, CM-6(1)\nDescription To properly set the owner of /var/lib/etcd/member/wal/*, run the command: $ sudo chown root /var/lib/etcd/member/wal/* Rationaleetcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing user ownership of ^/var/lib/etcd/member/wal/.*$ oval:ssg-test_file_owner_etcd_data_files:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/lib/etcd/member/wal/0000000000000058-0000000000540992.walregular0064000376rw------- /var/lib/etcd/member/wal/0000000000000059-0000000000552b6f.walregular0064000992rw------- /var/lib/etcd/member/wal/000000000000005a-0000000000564c8f.walregular0064000160rw------- /var/lib/etcd/member/wal/000000000000005b-0000000000576c47.walregular0064000400rw------- /var/lib/etcd/member/wal/0.tmpregular0064000000rw------- /var/lib/etcd/member/wal/000000000000005c-0000000000588cce.walregular0064000000rw------- Verify Permissions on the Etcd Write-Ahead-Log Filesxccdf_org.ssgproject.content_rule_file_permissions_etcd_data_files mediumCCE-83382-2 Verify Permissions on the Etcd Write-Ahead-Log FilesRule IDxccdf_org.ssgproject.content_rule_file_permissions_etcd_data_filesResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_etcd_data_files:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83382-2\nReferences: 1.1.11, CM-6, CM-6(1)\nDescription To properly set the permissions of /var/lib/etcd/member/wal/*, run the command: $ sudo chmod 0600 /var/lib/etcd/member/wal/*Rationaleetcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing mode of ^/var/lib/etcd/member/wal/.*$ oval:ssg-test_file_permissions_etcd_data_files:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_etcd_data_files:obj:1 of type file_objectFilepathFilter^/var/lib/etcd/member/wal/.*$oval:ssg-state_file_permissions_etcd_data_files_mode_not_0600:ste:1 Verify User Who Owns The Open vSwitch Configuration Databasexccdf_org.ssgproject.content_rule_file_owner_ovs_conf_db mediumCCE-83489-5 Verify User Who Owns The Open vSwitch Configuration DatabaseRule IDxccdf_org.ssgproject.content_rule_file_owner_ovs_conf_dbResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_ovs_conf_db:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83489-5\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/openvswitch/conf.db, run the command: $ sudo chown openvswitch /etc/openvswitch/conf.db RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of /etc/openvswitch/conf.db oval:ssg-test_file_owner_ovs_conf_db:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/openvswitch/conf.dbregular8008011326372rw-r----- Verify User Who Owns The Open vSwitch Process ID Filexccdf_org.ssgproject.content_rule_file_owner_ovs_pid mediumCCE-83937-3 Verify User Who Owns The Open vSwitch Process ID FileRule IDxccdf_org.ssgproject.content_rule_file_owner_ovs_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_ovs_pid:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83937-3\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the owner of /var/run/openvswitch/ovs-vswitchd.pid, run the command: $ sudo chown openvswitch /var/run/openvswitch/ovs-vswitchd.pid RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of /var/run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_owner_ovs_pid:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/run/openvswitch/ovs-vswitchd.pidregular8008015rw-r--r-- Verify User Who Owns The Kubernetes API Server Pod Specification Filexccdf_org.ssgproject.content_rule_file_owner_kube_apiserver mediumCCE-83372-3 Verify User Who Owns The Kubernetes API Server Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_owner_kube_apiserverResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_kube_apiserver:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83372-3\nReferences: 1.1.2, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml RationaleThe Kubernetes specification file contains information about the configuration of the Kubernetes API Server that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes API Server service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing user ownership of ^/etc/kubernetes/static-pod-resources/kube-apiserver-pod-.*/kube-apiserver-pod.yaml$ oval:ssg-test_file_owner_kube_apiserver:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/kube-apiserver-pod.yamlregular006904rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6/kube-apiserver-pod.yamlregular006949rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/kube-apiserver-pod.yamlregular006949rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-10/kube-apiserver-pod.yamlregular007089rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8/kube-apiserver-pod.yamlregular006949rw-r--r-- Verify Permissions on the OpenShift Container Network Interface Filesxccdf_org.ssgproject.content_rule_file_permissions_cni_conf mediumCCE-83379-8 Verify Permissions on the OpenShift Container Network Interface FilesRule IDxccdf_org.ssgproject.content_rule_file_permissions_cni_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_cni_conf:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83379-8\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/cni/net.d/*, run the command: $ sudo chmod 0644 /etc/cni/net.d/*RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of ^/etc/cni/net.d/.*$ oval:ssg-test_file_permissions_cni_conf:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_cni_conf:obj:1 of type file_objectFilepathFilter^/etc/cni/net.d/.*$oval:ssg-state_file_permissions_cni_conf_mode_not_0644:ste:1 Verify Permissions on the OpenShift PKI Private Key Filesxccdf_org.ssgproject.content_rule_file_permissions_openshift_pki_key_files mediumCCE-83580-1 Verify Permissions on the OpenShift PKI Private Key FilesRule IDxccdf_org.ssgproject.content_rule_file_permissions_openshift_pki_key_filesResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_openshift_pki_key_files:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83580-1\nReferences: 1.1.21, IA-5(2)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/*/*/*/*.key, run the command: $ sudo chmod 0600 /etc/kubernetes/static-pod-resources/*/*/*/*.keyRationaleOpenShift makes use of a number of key files as part of the operation of its components. The permissions on these files should be set to 600 to protect their integrity and confidentiality.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing mode of ^/etc/kubernetes/static-pod-resources/.*/.*/.*/.*\\.key$ oval:ssg-test_file_permissions_openshift_pki_key_files:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_openshift_pki_key_files:obj:1 of type file_objectFilepathFilter^/etc/kubernetes/static-pod-resources/.*/.*/.*/.*\\.key$oval:ssg-state_file_permissions_openshift_pki_key_files_mode_not_0600:ste:1 Verify Permissions on the Open vSwitch Persistent System IDxccdf_org.ssgproject.content_rule_file_permissions_ovs_sys_id_conf mediumCCE-83400-2 Verify Permissions on the Open vSwitch Persistent System IDRule IDxccdf_org.ssgproject.content_rule_file_permissions_ovs_sys_id_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_ovs_sys_id_conf:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83400-2\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/openvswitch/system-id.conf, run the command: $ sudo chmod 0644 /etc/openvswitch/system-id.confRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of /etc/openvswitch/system-id.conf oval:ssg-test_file_permissions_ovs_sys_id_conf:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_ovs_sys_id_conf:obj:1 of type file_objectFilepathFilter/etc/openvswitch/system-id.confoval:ssg-state_file_permissions_ovs_sys_id_conf_mode_not_0644:ste:1 Verify User Who Owns The OpenShift SDN CNI Server Configxccdf_org.ssgproject.content_rule_file_owner_openshift_sdn_cniserver_config mediumCCE-83932-4 Verify User Who Owns The OpenShift SDN CNI Server ConfigRule IDxccdf_org.ssgproject.content_rule_file_owner_openshift_sdn_cniserver_configResultfailMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_openshift_sdn_cniserver_config:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83932-4\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the owner of /var/run/openshift-sdn/cniserver/config.json, run the command: $ sudo chown root /var/run/openshift-sdn/cniserver/config.json RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of /var/run/openshift-sdn/cniserver/config.json oval:ssg-test_file_owner_openshift_sdn_cniserver_config:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_owner_openshift_sdn_cniserver_config:obj:1 of type file_objectFilepath/var/run/openshift-sdn/cniserver/config.json Verify Permissions on the Open vSwitch Database Server PIDxccdf_org.ssgproject.content_rule_file_permissions_ovsdb_server_pid mediumCCE-83679-1 Verify Permissions on the Open vSwitch Database Server PIDRule IDxccdf_org.ssgproject.content_rule_file_permissions_ovsdb_server_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_ovsdb_server_pid:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83679-1\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /run/openvswitch/ovsdb-server.pid, run the command: $ sudo chmod 0644 /run/openvswitch/ovsdb-server.pidRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of /run/openvswitch/ovsdb-server.pid oval:ssg-test_file_permissions_ovsdb_server_pid:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_ovsdb_server_pid:obj:1 of type file_objectFilepathFilter/run/openvswitch/ovsdb-server.pidoval:ssg-state_file_permissions_ovsdb_server_pid_mode_not_0644:ste:1 Verify Group Who Owns The Open vSwitch Configuration Databasexccdf_org.ssgproject.content_rule_file_groupowner_ovs_conf_db mediumCCE-84226-0 Verify Group Who Owns The Open vSwitch Configuration DatabaseRule IDxccdf_org.ssgproject.content_rule_file_groupowner_ovs_conf_dbResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_ovs_conf_db:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84226-0\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/openvswitch/conf.db, run the command: $ sudo chgrp hugetlbfs /etc/openvswitch/conf.dbRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of /etc/openvswitch/conf.db oval:ssg-test_file_groupowner_ovs_conf_db:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/openvswitch/conf.dbregular8008011326372rw-r----- Verify Group Who Owns The Etcd PKI Certificate Filesxccdf_org.ssgproject.content_rule_file_groupowner_etcd_pki_cert_files mediumCCE-83890-4 Verify Group Who Owns The Etcd PKI Certificate FilesRule IDxccdf_org.ssgproject.content_rule_file_groupowner_etcd_pki_cert_filesResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_etcd_pki_cert_files:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83890-4\nReferences: 1.1.19, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/*/*/*/*.crt, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/*/*/*/*.crtRationaleOpenShift makes use of a number of certificates as part of its operation. You should verify the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by the system administrator.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing group ownership of ^/etc/kubernetes/static-pod-resources/.*/.*/.*/.*\\.crt$ oval:ssg-test_file_groupowner_etcd_pki_cert_files:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-peer/etcd-peer-master-0.crtregular002445rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-2/secrets/localhost-recovery-client-token/ca.crtregular003560rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-2/configmaps/serviceaccount-ca/ca-bundle.crtregular000rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-certs/secrets/kube-scheduler-client-cert-key/tls.crtregular001168rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-peer/etcd-peer-master-1.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-peer/etcd-peer-master-2.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-peer/etcd-peer-master-0.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving/etcd-serving-master-0.crtregular002709rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/secrets/etcd-client/tls.crtregular001188rw------- /etc/kubernetes/static-pod-resources/etcd-pod-5/secrets/etcd-all-peer/etcd-peer-master-2.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving/etcd-serving-master-1.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving/etcd-serving-master-2.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-pod-2/configmaps/etcd-metrics-proxy-client-ca/ca-bundle.crtregular001151rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-2/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crtregular001151rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-2/configmaps/etcd-peer-client-ca/ca-bundle.crtregular001135rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-2/configmaps/etcd-serving-ca/ca-bundle.crtregular001135rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-peer/etcd-peer-master-2.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-peer/etcd-peer-master-0.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-peer/etcd-peer-master-1.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving/etcd-serving-master-2.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving/etcd-serving-master-0.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving/etcd-serving-master-1.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-certs/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-metrics-proxy-client-ca/ca-bundle.crtregular001151rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crtregular001151rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-peer-client-ca/ca-bundle.crtregular001135rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-certs/configmaps/etcd-serving-ca/ca-bundle.crtregular001135rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/secrets/localhost-recovery-client-token/ca.crtregular003560rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/configmaps/service-ca/ca-bundle.crtregular001212rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/configmaps/serviceaccount-ca/ca-bundle.crtregular000rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/secrets/csr-signer/tls.crtregular001229rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/secrets/kube-controller-manager-client-cert-key/tls.crtregular001180rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/configmaps/aggregator-client-ca/ca-bundle.crtregular001281rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/configmaps/client-ca/ca-bundle.crtregular008527rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/configmaps/trusted-ca-bundle/ca-bundle.crtregular00216090rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/aggregator-client/tls.crtregular001208rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/check-endpoints-client-cert-key/tls.crtregular001220rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/control-plane-node-admin-client-cert-key/tls.crtregular001212rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/external-loadbalancer-serving-certkey/tls.crtregular002417rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/internal-loadbalancer-serving-certkey/tls.crtregular002429rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/localhost-serving-cert-certkey/tls.crtregular002445rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/service-network-serving-certkey/tls.crtregular002713rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/configmaps/aggregator-client-ca/ca-bundle.crtregular001281rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/configmaps/client-ca/ca-bundle.crtregular008527rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/configmaps/trusted-ca-bundle/ca-bundle.crtregular00216090rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/secrets/localhost-recovery-client-token/ca.crtregular003560rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/configmaps/service-ca/ca-bundle.crtregular001212rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/configmaps/serviceaccount-ca/ca-bundle.crtregular007222rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-5/secrets/localhost-recovery-client-token/ca.crtregular003560rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-5/secrets/serving-cert/tls.crtregular002664rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-5/configmaps/serviceaccount-ca/ca-bundle.crtregular007222rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-peer/etcd-peer-master-2.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-peer/etcd-peer-master-0.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-peer/etcd-peer-master-1.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving/etcd-serving-master-0.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving/etcd-serving-master-1.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving/etcd-serving-master-2.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-2.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-pod-3/configmaps/etcd-metrics-proxy-client-ca/ca-bundle.crtregular001151rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-3/configmaps/etcd-metrics-proxy-serving-ca/ca-bundle.crtregular001151rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-3/configmaps/etcd-peer-client-ca/ca-bundle.crtregular001135rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-3/configmaps/etcd-serving-ca/ca-bundle.crtregular001135rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/secrets/localhost-recovery-client-token/service-ca.crtregular008435rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/secrets/localhost-recovery-client-token/ca.crtregular007222rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/configmaps/service-ca/ca-bundle.crtregular001212rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/configmaps/serviceaccount-ca/ca-bundle.crtregular007222rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/secrets/localhost-recovery-client-token/ca.crtregular007222rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/secrets/localhost-recovery-client-token/service-ca.crtregular008435rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/secrets/serving-cert/tls.crtregular002664rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/configmaps/serviceaccount-ca/ca-bundle.crtregular007222rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/secrets/localhost-recovery-client-token/ca.crtregular007222rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/secrets/localhost-recovery-client-token/service-ca.crtregular008435rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/configmaps/service-ca/ca-bundle.crtregular001212rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/configmaps/serviceaccount-ca/ca-bundle.crtregular007222rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/etcd-client/tls.crtregular001188rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/kubelet-client/tls.crtregular001204rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/localhost-recovery-client-token/service-ca.crtregular008435rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/localhost-recovery-client-token/ca.crtregular007222rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/localhost-recovery-serving-certkey/tls.crtregular002600rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/configmaps/etcd-serving-ca/ca-bundle.crtregular001135rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/configmaps/kubelet-serving-ca/ca-bundle.crtregular002319rw-r--r-- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/configmaps/kube-apiserver-server-ca/ca-bundle.crtregular004866rw-r--r-- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-peer/etcd-peer-master-1.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-peer/etcd-peer-master-2.crtregular002445rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving/etcd-serving-master-2.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving/etcd-serving-master-0.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving/etcd-serving-master-1.crtregular002709rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-0.crtregular002733rw------- /etc/kubernetes/static-pod-resources/etcd-pod-4/secrets/etcd-all-serving-metrics/etcd-serving-metrics-master-1.crtregular002733rw------- ... and 81 more items. Verify Group Who Owns The Open vSwitch Configuration Database Lockxccdf_org.ssgproject.content_rule_file_groupowner_ovs_conf_db_lock mediumCCE-84219-5 Verify Group Who Owns The Open vSwitch Configuration Database LockRule IDxccdf_org.ssgproject.content_rule_file_groupowner_ovs_conf_db_lockResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_ovs_conf_db_lock:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84219-5\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/openvswitch/.conf.db.~lock~, run the command: $ sudo chgrp hugetlbfs /etc/openvswitch/.conf.db.~lock~RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of /etc/openvswitch/.conf.db.~lock~ oval:ssg-test_file_groupowner_ovs_conf_db_lock:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/openvswitch/.conf.db.~lock~regular8008010rw------- Verify User Who Owns The Open vSwitch Persistent System IDxccdf_org.ssgproject.content_rule_file_owner_ovs_sys_id_conf mediumCCE-84085-0 Verify User Who Owns The Open vSwitch Persistent System IDRule IDxccdf_org.ssgproject.content_rule_file_owner_ovs_sys_id_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_ovs_sys_id_conf:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84085-0\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/openvswitch/system-id.conf, run the command: $ sudo chown openvswitch /etc/openvswitch/system-id.conf RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of /etc/openvswitch/system-id.conf oval:ssg-test_file_owner_ovs_sys_id_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/openvswitch/system-id.confregular80080137rw-r--r-- Verify Group Who Owns The Kubernetes Scheduler Kubeconfig Filexccdf_org.ssgproject.content_rule_file_groupowner_scheduler_kubeconfig mediumCCE-83471-3 Verify Group Who Owns The Kubernetes Scheduler Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_scheduler_kubeconfigResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_scheduler_kubeconfig:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83471-3\nReferences: 1.1.16, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfigRationaleThe kubeconfig for the Scheduler contains paramters for the scheduler to access the Kube API. You should set its file ownership to maintain the integrity of the file.Warningswarning This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing group ownership of ^/etc/kubernetes/static-pod-resources/kube-scheduler-pod-.*/configmaps/scheduler-kubeconfig/kubeconfig$ oval:ssg-test_file_groupowner_scheduler_kubeconfig:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-scheduler-pod-5/configmaps/scheduler-kubeconfig/kubeconfigregular00619rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/configmaps/scheduler-kubeconfig/kubeconfigregular00619rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-8/configmaps/scheduler-kubeconfig/kubeconfigregular00619rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-2/configmaps/scheduler-kubeconfig/kubeconfigregular00619rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-7/configmaps/scheduler-kubeconfig/kubeconfigregular00619rw-r--r-- Verify User Who Owns The Open vSwitch Daemon PID Filexccdf_org.ssgproject.content_rule_file_owner_ovs_vswitchd_pid mediumCCE-83888-8 Verify User Who Owns The Open vSwitch Daemon PID FileRule IDxccdf_org.ssgproject.content_rule_file_owner_ovs_vswitchd_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_ovs_vswitchd_pid:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83888-8\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the owner of /run/openvswitch/ovs-vswitchd.pid, run the command: $ sudo chown openvswitch /run/openvswitch/ovs-vswitchd.pid RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of /run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_owner_ovs_vswitchd_pid:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/run/openvswitch/ovs-vswitchd.pidregular8008015rw-r--r-- Verify Permissions on the OpenShift SDN Container Network Interface Plugin IP Address Allocationsxccdf_org.ssgproject.content_rule_file_permissions_ip_allocations mediumCCE-83469-7 Verify Permissions on the OpenShift SDN Container Network Interface Plugin IP Address AllocationsRule IDxccdf_org.ssgproject.content_rule_file_permissions_ip_allocationsResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_ip_allocations:def:1Time2021-07-20T05:21:00+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83469-7\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /var/lib/cni/networks/openshift-sdn/*, run the command: $ sudo chmod 0644 /var/lib/cni/networks/openshift-sdn/*RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of ^/var/lib/cni/networks/openshift-sdn/.*$ oval:ssg-test_file_permissions_ip_allocations:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_ip_allocations:obj:1 of type file_objectFilepathFilter^/var/lib/cni/networks/openshift-sdn/.*$oval:ssg-state_file_permissions_ip_allocations_mode_not_0644:ste:1 Verify Group Who Owns The Kubernetes Controller Manager Pod Specification Filexccdf_org.ssgproject.content_rule_file_groupowner_kube_controller_manager mediumCCE-83953-0 Verify Group Who Owns The Kubernetes Controller Manager Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_kube_controller_managerResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_kube_controller_manager:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83953-0\nReferences: 1.1.4, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yamlRationaleThe Kubernetes specification file contains information about the configuration of the Kubernetes Controller Manager Server that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing group ownership of ^/etc/kubernetes/static-pod-resources/kube-controller-manager-pod-.*/kube-controller-manager-pod.yaml$ oval:ssg-test_file_groupowner_kube_controller_manager:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/kube-controller-manager-pod.yamlregular006043rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/kube-controller-manager-pod.yamlregular006043rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7/kube-controller-manager-pod.yamlregular006088rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-9/kube-controller-manager-pod.yamlregular007784rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/kube-controller-manager-pod.yamlregular006043rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10/kube-controller-manager-pod.yamlregular007786rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-11/kube-controller-manager-pod.yamlregular007823rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/kube-controller-manager-pod.yamlregular006043rw-r--r-- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-8/kube-controller-manager-pod.yamlregular005950rw-r--r-- Verify Permissions on the Open vSwitch Configuration Database Lockxccdf_org.ssgproject.content_rule_file_permissions_ovs_conf_db_lock mediumCCE-84202-1 Verify Permissions on the Open vSwitch Configuration Database LockRule IDxccdf_org.ssgproject.content_rule_file_permissions_ovs_conf_db_lockResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_ovs_conf_db_lock:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84202-1\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/openvswitch/.conf.db.~lock~, run the command: $ sudo chmod 0600 /etc/openvswitch/.conf.db.~lock~RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of /etc/openvswitch/.conf.db.~lock~ oval:ssg-test_file_permissions_ovs_conf_db_lock:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_ovs_conf_db_lock:obj:1 of type file_objectFilepathFilter/etc/openvswitch/.conf.db.~lock~oval:ssg-state_file_permissions_ovs_conf_db_lock_mode_not_0600:ste:1 Verify Permissions on the Kubernetes Scheduler Kubeconfig Filexccdf_org.ssgproject.content_rule_file_permissions_scheduler_kubeconfig mediumCCE-83772-4 Verify Permissions on the Kubernetes Scheduler Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_scheduler_kubeconfigResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_scheduler_kubeconfig:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83772-4\nReferences: 1.1.15, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig, run the command: $ sudo chmod 0644 /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfigRationaleThe kubeconfig for the Scheduler contains paramters for the scheduler to access the Kube API. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.Warningswarning This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing mode of ^/etc/kubernetes/static-pod-resources/kube-scheduler-pod-.*/configmaps/scheduler-kubeconfig/kubeconfig$ oval:ssg-test_file_permissions_scheduler_kubeconfig:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_scheduler_kubeconfig:obj:1 of type file_objectFilepathFilter^/etc/kubernetes/static-pod-resources/kube-scheduler-pod-.*/configmaps/scheduler-kubeconfig/kubeconfig$oval:ssg-state_file_permissions_scheduler_kubeconfig_mode_not_0644:ste:1 Verify User Who Owns The OpenShift PKI Certificate Filesxccdf_org.ssgproject.content_rule_file_owner_openshift_pki_cert_files mediumCCE-83558-7 Verify User Who Owns The OpenShift PKI Certificate FilesRule IDxccdf_org.ssgproject.content_rule_file_owner_openshift_pki_cert_filesResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_openshift_pki_cert_files:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83558-7\nReferences: 1.1.19, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/*/*/*/tls.crt, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/*/*/*/tls.crt RationaleOpenShift makes use of a number of certificates as part of its operation. You should verify the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing user ownership of ^/etc/kubernetes/static-pod-resources/.*/.*/.*/tls\\.crt$ oval:ssg-test_file_owner_openshift_pki_cert_files:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-scheduler-certs/secrets/kube-scheduler-client-cert-key/tls.crtregular001168rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/secrets/etcd-client/tls.crtregular001188rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-3/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/secrets/csr-signer/tls.crtregular001229rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-certs/secrets/kube-controller-manager-client-cert-key/tls.crtregular001180rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/aggregator-client/tls.crtregular001208rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/check-endpoints-client-cert-key/tls.crtregular001220rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/control-plane-node-admin-client-cert-key/tls.crtregular001212rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/external-loadbalancer-serving-certkey/tls.crtregular002417rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/internal-loadbalancer-serving-certkey/tls.crtregular002429rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/localhost-serving-cert-certkey/tls.crtregular002445rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/service-network-serving-certkey/tls.crtregular002713rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-4/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-5/secrets/serving-cert/tls.crtregular002664rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-5/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/secrets/serving-cert/tls.crtregular002664rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-6/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/etcd-client/tls.crtregular001188rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/kubelet-client/tls.crtregular001204rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-5/secrets/localhost-recovery-serving-certkey/tls.crtregular002600rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6/secrets/etcd-client/tls.crtregular001188rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6/secrets/kubelet-client/tls.crtregular001204rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6/secrets/localhost-recovery-serving-certkey/tls.crtregular002600rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-7/secrets/serving-cert/tls.crtregular002664rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-8/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8/secrets/etcd-client/tls.crtregular001188rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8/secrets/kubelet-client/tls.crtregular001204rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-8/secrets/localhost-recovery-serving-certkey/tls.crtregular002600rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/secrets/kubelet-client/tls.crtregular001204rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-9/secrets/localhost-recovery-serving-certkey/tls.crtregular002600rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-10/secrets/etcd-client/tls.crtregular001188rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-10/secrets/kubelet-client/tls.crtregular001204rw------- /etc/kubernetes/static-pod-resources/kube-apiserver-pod-10/secrets/localhost-recovery-serving-certkey/tls.crtregular002600rw------- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-8/secrets/serving-cert/tls.crtregular002664rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-9/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10/secrets/serving-cert/tls.crtregular002761rw------- /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-11/secrets/serving-cert/tls.crtregular002761rw------- Verify Group Who Owns The Kubernetes Scheduler Pod Specification Filexccdf_org.ssgproject.content_rule_file_groupowner_kube_scheduler mediumCCE-83614-8 Verify Group Who Owns The Kubernetes Scheduler Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_kube_schedulerResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_kube_scheduler:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83614-8\nReferences: 1.1.6, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yamlRationaleThe Kubernetes Specification file contains information about the configuration of the Kubernetes scheduler that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing group ownership of ^/etc/kubernetes/static-pod-resources/kube-scheduler-pod-.*/kube-scheduler-pod.yaml$ oval:ssg-test_file_groupowner_kube_scheduler:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/static-pod-resources/kube-scheduler-pod-2/kube-scheduler-pod.yamlregular003356rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-5/kube-scheduler-pod.yamlregular003531rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6/kube-scheduler-pod.yamlregular003531rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-7/kube-scheduler-pod.yamlregular004547rw-r--r-- /etc/kubernetes/static-pod-resources/kube-scheduler-pod-8/kube-scheduler-pod.yamlregular004612rw-r--r-- Verify Permissions on the Kubernetes API Server Pod Specification Filexccdf_org.ssgproject.content_rule_file_permissions_kube_apiserver mediumCCE-83983-7 Verify Permissions on the Kubernetes API Server Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_kube_apiserverResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_kube_apiserver:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83983-7\nReferences: 1.1.1, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml, run the command: $ sudo chmod 0644 /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yamlRationaleIf the Kubernetes specification file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the configuration of the Kubernetes API server that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes API Server service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Testing mode of ^/etc/kubernetes/static-pod-resources/kube-apiserver-pod-.*/kube-apiserver-pod.yaml$ oval:ssg-test_file_permissions_kube_apiserver:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_kube_apiserver:obj:1 of type file_objectFilepathFilter^/etc/kubernetes/static-pod-resources/kube-apiserver-pod-.*/kube-apiserver-pod.yaml$oval:ssg-state_file_permissions_kube_apiserver_mode_not_0644:ste:1 Verify Group Who Owns The OpenShift SDN CNI Server Configxccdf_org.ssgproject.content_rule_file_groupowner_openshift_sdn_cniserver_config mediumCCE-83605-6 Verify Group Who Owns The OpenShift SDN CNI Server ConfigRule IDxccdf_org.ssgproject.content_rule_file_groupowner_openshift_sdn_cniserver_configResultfailMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_openshift_sdn_cniserver_config:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83605-6\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the group owner of /var/run/openshift-sdn/cniserver/config.json, run the command: $ sudo chgrp root /var/run/openshift-sdn/cniserver/config.jsonRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of /var/run/openshift-sdn/cniserver/config.json oval:ssg-test_file_groupowner_openshift_sdn_cniserver_config:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_groupowner_openshift_sdn_cniserver_config:obj:1 of type file_objectFilepath/var/run/openshift-sdn/cniserver/config.json Verify Group Who Owns The Worker Kubeconfig Filexccdf_org.ssgproject.content_rule_file_groupowner_worker_kubeconfig mediumCCE-83409-3 Verify Group Who Owns The Worker Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_worker_kubeconfigResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_worker_kubeconfig:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83409-3\nReferences: 4.1.10, CM-6, CM-6(1)\nDescription To properly set the group owner of /var/lib/kubelet/kubeconfig, run the command: $ sudo chgrp root /var/lib/kubelet/kubeconfigRationaleThe worker kubeconfig file contains information about the administrative configuration of the OpenShift cluster that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing group ownership of /var/lib/kubelet/kubeconfig oval:ssg-test_file_groupowner_worker_kubeconfig:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/lib/kubelet/kubeconfigregular005244rw------- Verify Group Who Owns The OpenShift Node Service Filexccdf_org.ssgproject.content_rule_file_groupowner_worker_service mediumCCE-83975-3 Verify Group Who Owns The OpenShift Node Service FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_worker_serviceResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_worker_service:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83975-3\nReferences: 4.1.2, CM-6, CM-6(1)\nDescription' To properly set the group owner of /etc/systemd/system/kubelet.service, run the command: $ sudo chgrp root /etc/systemd/system/kubelet.service'RationaleThe /etc/systemd/system/kubelet.service file contains information about the configuration of the OpenShift node service that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing group ownership of /etc/systemd/system/kubelet.service oval:ssg-test_file_groupowner_worker_service:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/systemd/system/kubelet.serviceregular001395rw-r--r-- Verify Permissions on The Kubelet Configuration Filexccdf_org.ssgproject.content_rule_file_permissions_kubelet_conf mediumCCE-83470-5 Verify Permissions on The Kubelet Configuration FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_kubelet_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_kubelet_conf:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83470-5\nReferences: 4.1.5, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/kubelet.conf, run the command: $ sudo chmod 0644 /etc/kubernetes/kubelet.confRationaleIf the kubelet configuration file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the configuration of an OpenShift node that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing mode of /etc/kubernetes/kubelet.conf oval:ssg-test_file_permissions_kubelet_conf:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_kubelet_conf:obj:1 of type file_objectFilepathFilter/etc/kubernetes/kubelet.confoval:ssg-state_file_permissions_kubelet_conf_mode_not_0644:ste:1 Verify Permissions on the Worker Kubeconfig Filexccdf_org.ssgproject.content_rule_file_permissions_worker_kubeconfig mediumCCE-83509-0 Verify Permissions on the Worker Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_worker_kubeconfigResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_worker_kubeconfig:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83509-0\nReferences: 4.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /var/lib/kubelet/kubeconfig, run the command: $ sudo chmod 0600 /var/lib/kubelet/kubeconfigRationaleIf the worker kubeconfig file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the administration configuration of the OpenShift cluster that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing mode of /var/lib/kubelet/kubeconfig oval:ssg-test_file_permissions_worker_kubeconfig:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_worker_kubeconfig:obj:1 of type file_objectFilepathFilter/var/lib/kubelet/kubeconfigoval:ssg-state_file_permissions_worker_kubeconfig_mode_not_0600:ste:1 Verify Group Who Owns The Kubelet Configuration Filexccdf_org.ssgproject.content_rule_file_groupowner_kubelet_conf mediumCCE-84233-6 Verify Group Who Owns The Kubelet Configuration FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_kubelet_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_kubelet_conf:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84233-6\nReferences: 4.1.6, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/kubelet.conf, run the command: $ sudo chgrp root /etc/kubernetes/kubelet.confRationaleThe kubelet configuration file contains information about the configuration of the OpenShift node that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing group ownership of /etc/kubernetes/kubelet.conf oval:ssg-test_file_groupowner_kubelet_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/kubelet.confregular00934rw-r--r-- Verify User Who Owns The Kubelet Configuration Filexccdf_org.ssgproject.content_rule_file_owner_kubelet_conf mediumCCE-83976-1 Verify User Who Owns The Kubelet Configuration FileRule IDxccdf_org.ssgproject.content_rule_file_owner_kubelet_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_kubelet_conf:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83976-1\nReferences: 4.1.6, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/kubelet.conf, run the command: $ sudo chown root /etc/kubernetes/kubelet.conf RationaleThe kubelet configuration file contains information about the configuration of the OpenShift node that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing user ownership of /etc/kubernetes/kubelet.conf oval:ssg-test_file_owner_kubelet_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/kubelet.confregular00934rw-r--r-- Verify Group Who Owns the Worker Certificate Authority Filexccdf_org.ssgproject.content_rule_file_groupowner_worker_ca mediumCCE-83440-8 Verify Group Who Owns the Worker Certificate Authority FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_worker_caResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_worker_ca:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83440-8\nReferences: 4.1.8, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/kubelet-ca.crt, run the command: $ sudo chgrp root /etc/kubernetes/kubelet-ca.crtRationaleThe worker certificate authority file contains the certificate authority certificate for an OpenShift node that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing group ownership of /etc/kubernetes/kubelet-ca.crt oval:ssg-test_file_groupowner_worker_ca:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/kubelet-ca.crtregular005875rw-r--r-- Verify Permissions on the OpenShift Node Service Filexccdf_org.ssgproject.content_rule_file_permissions_worker_service mediumCCE-83455-6 Verify Permissions on the OpenShift Node Service FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_worker_serviceResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_worker_service:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83455-6\nReferences: 4.1.1, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/systemd/system/kubelet.service, run the command: $ sudo chmod 0644 /etc/systemd/system/kubelet.serviceRationaleIf the /etc/systemd/system/kubelet.service file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the service configuration of the OpenShift node service that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing mode of /etc/systemd/system/kubelet.service oval:ssg-test_file_permissions_worker_service:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_worker_service:obj:1 of type file_objectFilepathFilter/etc/systemd/system/kubelet.serviceoval:ssg-state_file_permissions_worker_service_mode_not_0644:ste:1 Verify User Who Owns The OpenShift Node Service Filexccdf_org.ssgproject.content_rule_file_owner_worker_service mediumCCE-84193-2 Verify User Who Owns The OpenShift Node Service FileRule IDxccdf_org.ssgproject.content_rule_file_owner_worker_serviceResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_worker_service:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84193-2\nReferences: 4.1.2, CM-6, CM-6(1)\nDescription' To properly set the owner of /etc/systemd/system/kubelet.service, run the command: $ sudo chown root /etc/systemd/system/kubelet.service 'RationaleThe /etc/systemd/system/kubelet.service file contains information about the configuration of the OpenShift node service that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing user ownership of /etc/systemd/system/kubelet.service oval:ssg-test_file_owner_worker_service:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/systemd/system/kubelet.serviceregular001395rw-r--r-- Verify User Who Owns The Worker Kubeconfig Filexccdf_org.ssgproject.content_rule_file_owner_worker_kubeconfig mediumCCE-83408-5 Verify User Who Owns The Worker Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_owner_worker_kubeconfigResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_worker_kubeconfig:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83408-5\nReferences: 4.1.10, CM-6, CM-6(1)\nDescription To properly set the owner of /var/lib/kubelet/kubeconfig, run the command: $ sudo chown root /var/lib/kubelet/kubeconfig RationaleThe worker kubeconfig file contains information about the administrative configuration of the OpenShift cluster that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing user ownership of /var/lib/kubelet/kubeconfig oval:ssg-test_file_owner_worker_kubeconfig:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/lib/kubelet/kubeconfigregular005244rw------- Verify Permissions on the Worker Certificate Authority Filexccdf_org.ssgproject.content_rule_file_permissions_worker_ca mediumCCE-83493-7 Verify Permissions on the Worker Certificate Authority FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_worker_caResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_worker_ca:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83493-7\nReferences: 4.1.7, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/kubelet-ca.crt, run the command: $ sudo chmod 0644 /etc/kubernetes/kubelet-ca.crtRationaleIf the worker certificate authority file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the certificate authority certificate for an OpenShift node that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing mode of /etc/kubernetes/kubelet-ca.crt oval:ssg-test_file_permissions_worker_ca:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_worker_ca:obj:1 of type file_objectFilepathFilter/etc/kubernetes/kubelet-ca.crtoval:ssg-state_file_permissions_worker_ca_mode_not_0644:ste:1 Verify User Who Owns the Worker Certificate Authority Filexccdf_org.ssgproject.content_rule_file_owner_worker_ca mediumCCE-83495-2 Verify User Who Owns the Worker Certificate Authority FileRule IDxccdf_org.ssgproject.content_rule_file_owner_worker_caResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_worker_ca:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83495-2\nReferences: 4.1.8, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/kubelet-ca.crt, run the command: $ sudo chown root /etc/kubernetes/kubelet-ca.crt RationaleThe worker certificate authority file contains the certificate authority certificate for an OpenShift node that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing user ownership of /etc/kubernetes/kubelet-ca.crt oval:ssg-test_file_owner_worker_ca:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/kubelet-ca.crtregular005875rw-r--r-- Configure A Unique CA Certificate for etcdxccdf_org.ssgproject.content_rule_etcd_unique_ca mediumConfigure A Unique CA Certificate for etcdRule IDxccdf_org.ssgproject.content_rule_etcd_unique_caResultpassMulti-check rulenoOVAL Definition IDoval:ssg-etcd_unique_ca:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesReferences: 2.7, CM-6, CM-6(1)\nDescriptionA unique CA certificate should be created for etcd. OpenShift by default creates separate PKIs for etcd and the Kubernetes API server. The same is done for other points of communication in the cluster.RationaleThe Kubernetes API server and etcd utilize separate CA certificates in OpenShift. This ensures that the etcd data is still protected in the event that the API server CA is compromised.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.OVAL test results details Verify that the etcd CA is different than the Kubernetes CA oval:ssg-test_etcd_different_ca_than_k8s:tst:1 trueFollowing items have been found on the system:Var refValueoval:ssg-var_etcd_ca_file_hash:var:164b81276f73c66ed4d7af8b9e50e737d9e3d6d9a755fc5272e5039bfda647581 Ensure Eviction threshold Settings Are Set - evictionHard: nodefs.availablexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_nodefs_available mediumCCE-84138-7 Ensure Eviction threshold Settings Are Set - evictionHard: nodefs.availableRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_nodefs_availableResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_hard_nodefs_available:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84138-7\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the nodefs.available setting of the evictionHard section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionHard['nodefs.available']'. oval:ssg-test_kubelet_eviction_thresholds_set_hard_nodefs_available:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_hard_nodefs_available:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionHard['nodefs.available'] kubelet - Enable Client Certificate Rotationxccdf_org.ssgproject.content_rule_kubelet_enable_client_cert_rotation mediumCCE-83352-5 kubelet - Enable Client Certificate RotationRule IDxccdf_org.ssgproject.content_rule_kubelet_enable_client_cert_rotationResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_enable_client_cert_rotation:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83352-5\nReferences: 4.2.11, CM-6, CM-6(1)\nDescriptionTo enable the kubelet to rotate client certificates, edit the kubelet configuration file /etc/kubernetes/kubelet.conf on the kubelet node(s) and set the below parameter: featureGates: ... RotateKubeletClientCertificate: true ... RationaleAllowing the kubelet to auto-update the certificates ensure that there is no downtime in certificate renewal as well as ensures confidentiality and integrity.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.featureGates.RotateKubeletClientCertificate'. oval:ssg-test_kubelet_enable_client_cert_rotation:tst:1 trueNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_enable_client_cert_rotation:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.featureGates.RotateKubeletClientCertificate Kubelet - Ensure Event Creation Is Configuredxccdf_org.ssgproject.content_rule_kubelet_configure_event_creation mediumCCE-83576-9 Kubelet - Ensure Event Creation Is ConfiguredRule IDxccdf_org.ssgproject.content_rule_kubelet_configure_event_creationResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_configure_event_creation:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83576-9\nReferences: 4.2.9, CM-6, CM-6(1)\nDescriptionSecurity relevant information should be captured. The eventRecordQPS Kubelet option can be used to limit the rate at which events are gathered. Setting this too low could result in relevant events not being logged, however the unlimited setting of 0 could result in a denial of service on the kubelet. Processing and storage systems should be scaled to handle the expected event load. To set the eventRecordQPS option for the kubelet, create a KubeletConfig option along these lines: apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: name: kubelet-config-$pool spec: machineConfigPoolSelector: matchLabels: pools.operator.machineconfiguration.openshift.io/$pool_name: \"\" kubeletConfig: eventRecordQPS: 5 RationaleIt is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data.Warningswarning The MachineConfig Operator does not merge KubeletConfig objects, the last object is used instead. In case you need to set multiple options for kubelet, consider putting all the custom options into a single KubeletConfig object.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.eventRecordQPS'. oval:ssg-test_kubelet_configure_event_creation:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_configure_event_creation:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.eventRecordQPS kubelet - Disable Hostname Overridexccdf_org.ssgproject.content_rule_kubelet_disable_hostname_override mediumkubelet - Disable Hostname OverrideRule IDxccdf_org.ssgproject.content_rule_kubelet_disable_hostname_overrideResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_disable_hostname_override:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesReferences: 4.2.8, CM-6, CM-6(1)\nDescriptionTo prevent the hostname from being overrided, edit the kubelet systemd service file /etc/systemd/system/kubelet.service on the kubelet node(s) and remove the --hostname-override flag if it exists.RationaleAllowing hostnames to be overrided creates issues around resolving nodes in addition to TLS configuration, certificate validation, and log correlation and validation.OVAL test results details Test that the --hostname-override flag is not contained in kubelet.service oval:ssg-test_kubelet_hostname_override_disabled:tst:1 trueNo items have been found conforming to the following objects:Object oval:ssg-obj_kubelet_hostname_override:obj:1 of type textfilecontent54_objectFilepathPatternInstance/etc/systemd/system/kubelet.service^.*--hostname-override.*1 Ensure Eviction threshold Settings Are Set - evictionSoft: imagefs.availablexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_imagefs_available mediumCCE-84127-0 Ensure Eviction threshold Settings Are Set - evictionSoft: imagefs.availableRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_imagefs_availableResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_soft_imagefs_available:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84127-0\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the imagefs.available setting of the evictionSoft section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionSoft['imagefs.available']'. oval:ssg-test_kubelet_eviction_thresholds_set_soft_imagefs_available:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_soft_imagefs_available:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionSoft['imagefs.available'] kubelet - Do Not Disable Streaming Timeoutsxccdf_org.ssgproject.content_rule_kubelet_enable_streaming_connections mediumCCE-84097-5 kubelet - Do Not Disable Streaming TimeoutsRule IDxccdf_org.ssgproject.content_rule_kubelet_enable_streaming_connectionsResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_enable_streaming_connections:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84097-5\nReferences: 4.2.5, CM-6, CM-6(1)\nDescriptionTimouts for streaming connections should not be disabled as they help to prevent denial-of-service attacks. To configure streaming connection timeouts, edit the kubelet configuration file /etc/kubernetes/kubelet.conf on the kubelet node(s) and set the below parameter: streamingConnectionIdleTimeout: 5mRationaleEnsuring connections have timeouts helps to protect against denial-of-service attacks as well as disconnect inactive connections. In addition, setting connections timeouts helps to prevent from running out of ephemeral ports.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.streamingConnectionIdleTimeout'. oval:ssg-test_kubelet_enable_streaming_connections:tst:1 trueNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_enable_streaming_connections:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.streamingConnectionIdleTimeout Ensure Eviction threshold Settings Are Set - evictionSoft: memory.availablexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_memory_available mediumCCE-84222-9 Ensure Eviction threshold Settings Are Set - evictionSoft: memory.availableRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_memory_availableResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_soft_memory_available:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84222-9\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the memory.available setting of the evictionSoft section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionSoft['memory.available']'. oval:ssg-test_kubelet_eviction_thresholds_set_soft_memory_available:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_soft_memory_available:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionSoft['memory.available'] Ensure Eviction threshold Settings Are Set - evictionHard: nodefs.inodesFreexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_nodefs_inodesfree mediumCCE-84141-1 Ensure Eviction threshold Settings Are Set - evictionHard: nodefs.inodesFreeRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_nodefs_inodesfreeResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_hard_nodefs_inodesfree:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84141-1\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the nodefs.inodesFree setting of the evictionHard section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionHard['nodefs.inodesFree']'. oval:ssg-test_kubelet_eviction_thresholds_set_hard_nodefs_inodesfree:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_hard_nodefs_inodesfree:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionHard['nodefs.inodesFree'] kubelet - Enable Server Certificate Rotationxccdf_org.ssgproject.content_rule_kubelet_enable_server_cert_rotation mediumCCE-83356-6 kubelet - Enable Server Certificate RotationRule IDxccdf_org.ssgproject.content_rule_kubelet_enable_server_cert_rotationResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_enable_server_cert_rotation:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83356-6\nReferences: 4.2.12, CM-6, CM-6(1)\nDescriptionTo enable the kubelet to rotate server certificates, edit the kubelet configuration file /etc/kubernetes/kubelet.conf on the kubelet node(s) and set the below parameter: featureGates: ... RotateKubeletServerCertificate: true ... RationaleAllowing the kubelet to auto-update the certificates ensure that there is no downtime in certificate renewal as well as ensures confidentiality and integrity.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.featureGates.RotateKubeletServerCertificate'. oval:ssg-test_kubelet_enable_server_cert_rotation:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/etc/kubernetes/kubelet.conf/etc/kuberneteskubelet.conf.featureGates.RotateKubeletServerCertificate true Ensure Eviction threshold Settings Are Set - evictionHard: imagefs.availablexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_imagefs_available mediumCCE-84144-5 Ensure Eviction threshold Settings Are Set - evictionHard: imagefs.availableRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_imagefs_availableResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_hard_imagefs_available:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84144-5\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the imagefs.available setting of the evictionHard section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionHard['imagefs.available']'. oval:ssg-test_kubelet_eviction_thresholds_set_hard_imagefs_available:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_hard_imagefs_available:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionHard['imagefs.available'] kubelet - Configure the Client CA Certificatexccdf_org.ssgproject.content_rule_kubelet_configure_client_ca mediumCCE-83724-5 kubelet - Configure the Client CA CertificateRule IDxccdf_org.ssgproject.content_rule_kubelet_configure_client_caResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_configure_client_ca:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83724-5\nReferences: 4.2.3, CM-6, CM-6(1)\nDescriptionBy default, the kubelet is not configured with a CA certificate which can subject the kubelet to man-in-the-middle attacks. To configure a client CA certificate, edit the kubelet configuration file /etc/kubernetes/kubelet.conf on the kubelet node(s) and set the below parameter: authentication: ... x509: clientCAFile: /etc/kubernetes/kubelet-ca.crt ... RationaleNot having a CA certificate for the kubelet will subject the kubelet to possible man-in-the-middle attacks especially on unsafe or untrusted networks. Certificate validation for the kubelet allows the API server to validate the kubelet's identity.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.authentication.x509.clientCAFile'. oval:ssg-test_kubelet_configure_client_ca:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/etc/kubernetes/kubelet.conf/etc/kuberneteskubelet.conf.authentication.x509.clientCAFile /etc/kubernetes/kubelet-ca.crt kubelet - Enable Protect Kernel Defaultsxccdf_org.ssgproject.content_rule_kubelet_enable_protect_kernel_defaults mediumkubelet - Enable Protect Kernel DefaultsRule IDxccdf_org.ssgproject.content_rule_kubelet_enable_protect_kernel_defaultsResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_enable_protect_kernel_defaults:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesReferences: 4.2.6, CM-6, CM-6(1)\nDescription Protect tuned kernel parameters from being overwritten by the kubelet. Before enabling this kernel parameter, it's important and necessary to first create a MachineConfig object that persist the required sysctl's. The required sysctl's are the following: kernel.keys.root_maxbytes=25000000 kernel.keys.root_maxkeys=1000000 kernel.panic=10 kernel.panic_on_oops=1 vm.overcommit_memory=1 vm.panic_on_oom=0 The these need to be enabled via MachineConfig since they need to be available as soon as the node starts and before the Kubelet does. The manifest may look as follows: --- apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: master name: 75-master-kubelet-sysctls spec: config: ignition: version: 3.1.0 storage: files: - contents: source: data:,vm.overcommit_memory%3D1%0Avm.panic_on_oom%3D0%0Akernel.panic%3D10%0Akernel.panic_on_oops%3D1%0Akernel.keys.root_maxkeys%3D1000000%0Akernel.keys.root_maxbytes%3D25000000%0A mode: 0644 path: /etc/sysctl.d/90-kubelet.conf overwrite: true This will need to be done for each relevant MachineConfigPool in the cluster. After enabling this and after the changes have successfully rolled out to the whole cluster, it will now be possible to set the protectKernelDefaults parameter. To configure, follow the directions in the documentation RationaleKernel parameters are usually tuned and hardened by the system administrators before putting the systems into production. These parameters protect the kernel and the system. Your kubelet kernel defaults that rely on such parameters should be appropriately set to match the desired secured system state. Ignoring this could potentially lead to running pods with undesired kernel behavior.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.protectKernelDefaults'. oval:ssg-test_kubelet_enable_protect_kernel_defaults:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_enable_protect_kernel_defaults:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.protectKernelDefaults Ensure that the Kubelet only makes use of Strong Cryptographic Ciphersxccdf_org.ssgproject.content_rule_kubelet_configure_tls_cipher_suites mediumEnsure that the Kubelet only makes use of Strong Cryptographic CiphersRule IDxccdf_org.ssgproject.content_rule_kubelet_configure_tls_cipher_suitesResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_configure_tls_cipher_suites:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesReferences: 4.2.13, CM-6, CM-6(1)\nDescriptionEnsure that the Kubelet is configured to only use strong cryptographic ciphers. To set the cipher suites for the kubelet, create new or modify existing KubeletConfig object along these lines, one for every MachineConfigPool: apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: name: kubelet-config-$pool spec: machineConfigPoolSelector: matchLabels: pools.operator.machineconfiguration.openshift.io/$pool_name: \"\" kubeletConfig: tlsCipherSuites: - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 RationaleTLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.tlsCipherSuites[:]'. oval:ssg-test_kubelet_configure_tls_cipher_suites:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_configure_tls_cipher_suites:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.tlsCipherSuites[:] kubelet - Allow Automatic Firewall Configurationxccdf_org.ssgproject.content_rule_kubelet_enable_iptables_util_chains mediumCCE-83775-7 kubelet - Allow Automatic Firewall ConfigurationRule IDxccdf_org.ssgproject.content_rule_kubelet_enable_iptables_util_chainsResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_enable_iptables_util_chains:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83775-7\nReferences: 4.2.7, CM-6, CM-6(1)\nDescriptionThe kubelet has the ability to automatically configure the firewall to allow the containers required ports and connections to networking resources and destinations parameters potentially creating a security incident. To allow the kubelet to modify the firewall, edit the kubelet configuration file /etc/kubernetes/kubelet.conf on the kubelet node(s) and set the below parameter: makeIPTablesUtilChains: trueRationaleThe kubelet should automatically configure the firewall settings to allow access and networking traffic through. This ensures that when a pod or container is running that the correct ports are configured as well as removing the ports when a pod or container is no longer in existence.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.makeIPTablesUtilChains'. oval:ssg-test_kubelet_enable_iptables_util_chains:tst:1 trueNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_enable_iptables_util_chains:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.makeIPTablesUtilChains Ensure Eviction threshold Settings Are Set - evictionHard: imagefs.inodesFreexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_imagefs_inodesfree mediumCCE-84147-8 Ensure Eviction threshold Settings Are Set - evictionHard: imagefs.inodesFreeRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_imagefs_inodesfreeResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_hard_imagefs_inodesfree:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84147-8\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the imagefs.inodesFree setting of the evictionHard section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionHard['imagefs.inodesFree']'. oval:ssg-test_kubelet_eviction_thresholds_set_hard_imagefs_inodesfree:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_hard_imagefs_inodesfree:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionHard['imagefs.inodesFree'] Ensure Eviction threshold Settings Are Set - evictionSoft: nodefs.availablexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_nodefs_available mediumCCE-84119-7 Ensure Eviction threshold Settings Are Set - evictionSoft: nodefs.availableRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_nodefs_availableResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_soft_nodefs_available:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84119-7\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the nodefs.available setting of the evictionSoft section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionSoft['nodefs.available']'. oval:ssg-test_kubelet_eviction_thresholds_set_soft_nodefs_available:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_soft_nodefs_available:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionSoft['nodefs.available'] Ensure authorization is set to Webhookxccdf_org.ssgproject.content_rule_kubelet_authorization_mode mediumCCE-83593-4 Ensure authorization is set to WebhookRule IDxccdf_org.ssgproject.content_rule_kubelet_authorization_modeResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_authorization_mode:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83593-4\nReferences: 4.2.2, CM-6, CM-6(1)\nDescriptionUnauthenticated/unauthorized users should have no access to OpenShift nodes. The Kubelet should be set to only allow Webhook authorization. To ensure that the Kubelet requires authorization, validate that authorization is configured to Webhook in /etc/kubernetes/kubelet.conf: authorization: mode: Webhook ... RationaleEnsuring that the authorization is configured correctly helps enforce that unauthenticated/unauthorized users have no access to OpenShift nodes.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.authorization.mode'. oval:ssg-test_kubelet_authorization_mode:tst:1 trueNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_authorization_mode:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.authorization.mode kubelet - Enable Certificate Rotationxccdf_org.ssgproject.content_rule_kubelet_enable_cert_rotation mediumCCE-83838-3 kubelet - Enable Certificate RotationRule IDxccdf_org.ssgproject.content_rule_kubelet_enable_cert_rotationResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_enable_cert_rotation:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83838-3\nReferences: 4.2.11, CM-6, CM-6(1)\nDescriptionTo enable the kubelet to rotate client certificates, edit the kubelet configuration file /etc/kubernetes/kubelet.conf on the kubelet node(s) and set the below parameter: ... rotateCertificates: true ... RationaleAllowing the kubelet to auto-update the certificates ensure that there is no downtime in certificate renewal as well as ensures confidentiality and integrity.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.rotateCertificates'. oval:ssg-test_kubelet_enable_cert_rotation:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/etc/kubernetes/kubelet.conf/etc/kuberneteskubelet.conf.rotateCertificates true Ensure Eviction threshold Settings Are Set - evictionSoft: nodefs.inodesFreexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_nodefs_inodesfree mediumCCE-84123-9 Ensure Eviction threshold Settings Are Set - evictionSoft: nodefs.inodesFreeRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_nodefs_inodesfreeResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_soft_nodefs_inodesfree:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84123-9\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the nodefs.inodesFree setting of the evictionSoft section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionSoft['nodefs.inodesFree']'. oval:ssg-test_kubelet_eviction_thresholds_set_soft_nodefs_inodesfree:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_soft_nodefs_inodesfree:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionSoft['nodefs.inodesFree'] Disable Anonymous Authentication to the Kubeletxccdf_org.ssgproject.content_rule_kubelet_anonymous_auth mediumCCE-83815-1 Disable Anonymous Authentication to the KubeletRule IDxccdf_org.ssgproject.content_rule_kubelet_anonymous_authResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_anonymous_auth:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83815-1\nReferences: 4.2.1, CM-6, CM-6(1)\nDescriptionBy default, anonymous access to the Kubelet server is enabled. This configuration check ensures that anonymous requests to the Kubelet server are disabled. Edit the Kubelet server configuration file /etc/kubernetes/kubelet.conf on the kubelet node(s) and set the below parameter: authentication: ... anonymous: enabled: false ... RationaleWhen enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. OpenShift Operators should rely on authentication to authorize access and disallow anonymous requests.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.authentication.anonymous.enabled'. oval:ssg-test_kubelet_anonymous_auth:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/etc/kubernetes/kubelet.conf/etc/kuberneteskubelet.conf.authentication.anonymous.enabled false Ensure Eviction threshold Settings Are Set - evictionSoft: imagefs.inodesFreexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_imagefs_inodesfree mediumCCE-84132-0 Ensure Eviction threshold Settings Are Set - evictionSoft: imagefs.inodesFreeRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_imagefs_inodesfreeResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_soft_imagefs_inodesfree:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84132-0\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the imagefs.inodesFree setting of the evictionSoft section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionSoft['imagefs.inodesFree']'. oval:ssg-test_kubelet_eviction_thresholds_set_soft_imagefs_inodesfree:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_soft_imagefs_inodesfree:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionSoft['imagefs.inodesFree'] Ensure Eviction threshold Settings Are Set - evictionHard: memory.availablexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_memory_available mediumCCE-84135-3 Ensure Eviction threshold Settings Are Set - evictionHard: memory.availableRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_memory_availableResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_hard_memory_available:def:1Time2021-07-20T05:21:01+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84135-3\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the memory.available setting of the evictionHard section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionHard['memory.available']'. oval:ssg-test_kubelet_eviction_thresholds_set_hard_memory_available:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_hard_memory_available:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionHard['memory.available'] Scroll back to the first ruleRed Hat and Red Hat Enterprise Linux are either registered trademarks or trademarks of Red Hat, Inc. in the United States and other countries. All other names are registered trademarks or trademarks of their respective companies. Generated using OpenSCAP 1.3.4\n"},{"uri":"https://blog.stderr.at/compliance/1/01/","title":"","tags":[],"description":"","content":"\u003c!DOCTYPE html\u003exccdf_org.open-scap_testresult_xccdf_org.ssgproject.content_profile_cis-node | OpenSCAP Evaluation ReportOpenSCAP Evaluation ReportGuide to the Secure Configuration of Red Hat OpenShift Container Platform 4with profile CIS Red Hat OpenShift Container Platform 4 BenchmarkThis profile defines a baseline that aligns to the Center for Internet Security® Red Hat OpenShift Container Platform 4 Benchmark™, V0.3, currently unreleased. This profile includes Center for Internet Security® Red Hat OpenShift Container Platform 4 CIS Benchmarks™ content. Note that this part of the profile is meant to run on the Operating System that Red Hat OpenShift Container Platform 4 runs on top of. This profile is applicable to OpenShift versions 4.6 and greater.The ComplianceAsCode Project\nhttps://www.open-scap.org/security-policies/scap-security-guide This guide presents a catalog of security-relevant configuration settings for Red Hat OpenShift Container Platform 4. It is a rendering of content structured in the eXtensible Configuration Checklist Description Format (XCCDF) in order to support security automation. The SCAP content is is available in the scap-security-guide package which is developed at https://www.open-scap.org/security-policies/scap-security-guide. Providing system administrators with such guidance informs them how to securely configure systems under their control in a variety of network roles. Policy makers and baseline creators can use this catalog of settings, with its associated references to higher-level security control catalogs, in order to assist them in security baseline creation. This guide is a catalog, not a checklist, and satisfaction of every item is not likely to be possible or sensible in many operational scenarios. However, the XCCDF format enables granular selection and adjustment of settings, and their association with OVAL and OCIL content provides an automated checking capability. Transformations of this document, and its associated automated checking content, are capable of providing baselines that meet a diverse set of policy objectives. Some example XCCDF Profiles, which are selections of items that form checklists and can be used as baselines, are available with this guide. They can be processed, in an automated fashion, with tools that support the Security Content Automation Protocol (SCAP). The NIST National Checklist Program (NCP), which provides required settings for the United States Government, is one example of a baseline created from this guidance. Do not attempt to implement any of the settings in this guide without first testing them in a non-operational environment. The creators of this guidance assume no responsibility whatsoever for its use by other parties, and makes no guarantees, expressed or implied, about its quality, reliability, or any other characteristic. Evaluation CharacteristicsEvaluation targetUnknownTarget IDchroot:///hostBenchmark URL/content/ssg-ocp4-ds.xmlBenchmark IDxccdf_org.ssgproject.content_benchmark_OCP-4Benchmark version0.1.57Profile IDxccdf_org.ssgproject.content_profile_cis-nodeStarted at2021-07-20T05:21:05+00:00Finished at2021-07-20T05:21:05+00:00Performed by unknown user Test systemcpe:/a:redhat:openscap:1.3.4CPE Platformscpe:/o:redhat:openshift_container_platform_node:4cpe:/a:redhat:openshift_container_platform:4.1cpe:/a:redhat:openshift_container_platform:4.6cpe:/a:redhat:openshift_container_platform:4.7cpe:/a:redhat:openshift_container_platform:4.8cpe:/a:redhat:openshift_container_platform:4.9cpe:/a:redhat:openshift_container_platform:4.10AddressesCompliance and ScoringThe target system did not satisfy the conditions of 17 rules! Please review rule results and consider applying remediation. Rule results47 passed 17 failed 0 other Severity of failed rules0 other 0 low 17 medium 0 high ScoreScoring systemScoreMaximumPercenturn:xccdf:scoring:default75.858582100.00000075.86%Rule OverviewpassfixedinformationalfailerrorunknownnotcheckednotapplicableSearch\nGroup rules by: DefaultSeverityResult──────────NIST SP 800-53https://www.cisecurity.org/benchmark/kubernetes/TitleSeverityResultGuide to the Secure Configuration of Red Hat OpenShift Container Platform 4 17x failOpenShift Settings 17x failOpenShift - Master Node Settings 4x failVerify Group Who Owns The Open vSwitch Daemon PID FilemediumpassVerify User Who Owns The OpenShift SDN Container Network Interface Plugin IP Address AllocationsmediumfailVerify Group Who Owns The Kubernetes API Server Pod Specification FilemediumnotapplicableVerify User Who Owns The OpenShift Controller Manager Kubeconfig FilemediumnotapplicableVerify Permissions on the OpenShift Admin Kubeconfig FilesmediumnotapplicableVerify Group Who Owns The etcd Member Pod Specification FilemediumnotapplicableVerify Permissions on the OpenShift Multus Container Network Interface Plugin FilesmediumpassVerify Permissions on the Open vSwitch Process ID FilemediumpassVerify User Who Owns The Etcd PKI Certificate FilesmediumnotapplicableVerify User Who Owns The OpenShift Multus Container Network Interface Plugin FilesmediumpassVerify User Who Owns The Open vSwitch Database Server PIDmediumpassVerify User Who Owns The Open vSwitch Configuration Database LockmediumpassVerify Group Who Owns The Open vSwitch Persistent System IDmediumpassVerify Permissions on the OpenShift SDN CNI Server ConfigmediumpassVerify Group Who Owns The OpenShift SDN Container Network Interface Plugin IP Address AllocationsmediumfailVerify Group Who Owns The Open vSwitch Process ID FilemediumpassVerify User Who Owns The Etcd Database DirectorymediumnotapplicableVerify Group Who Owns The Etcd Write-Ahead-Log FilesmediumnotapplicableVerify Permissions on the Etcd PKI Certificate FilesmediumnotapplicableVerify Group Who Owns The Open vSwitch Database Server PIDmediumpassVerify Permissions on the OpenShift Controller Manager Kubeconfig FilemediumnotapplicableVerify Group Who Owns The OpenShift PKI Private Key FilesmediumnotapplicableVerify Group Who Owns The OpenShift Container Network Interface FilesmediumpassVerify User Who Owns The Etcd Member Pod Specification FilemediumnotapplicableVerify Group Who Owns The OpenShift Admin Kubeconfig FilesmediumnotapplicableVerify Group Who Owns The OpenShift PKI Certificate FilesmediumnotapplicableVerify User Who Owns The OpenShift Admin Kubeconfig FilesmediumnotapplicableVerify Permissions on the Etcd Member Pod Specification FilemediumnotapplicableVerify Permissions on the Kubernetes Controller Manager Pod Specificiation FilemediumnotapplicableVerify Permissions on the OpenShift PKI Certificate FilesmediumnotapplicableVerify Group Who Owns The OpenShift Controller Manager Kubeconfig FilemediumnotapplicableVerify Permissions on the Open vSwitch Configuration DatabasemediumpassVerify Permissions on the Open vSwitch Daemon PID FilemediumpassVerify Permissions on the Kubernetes Scheduler Pod Specification FilemediumnotapplicableVerify Group Who Owns The OpenShift Multus Container Network Interface Plugin FilesmediumpassVerify User Who Owns The Kubernetes Controller Manager Pod Specificiation FilemediumnotapplicableVerify User Who Owns The OpenShift PKI Private Key FilesmediumnotapplicableVerify Group Who Owns The Etcd Database DirectorymediumnotapplicableVerify User Who Owns The Kubernetes Scheduler Kubeconfig FilemediumnotapplicableVerify Permissions on the Etcd Database DirectorymediumnotapplicableVerify User Who Owns The Kubernetes Scheduler Pod Specification FilemediumnotapplicableVerify User Who Owns The OpenShift Container Network Interface FilesmediumpassVerify User Who Owns The Etcd Write-Ahead-Log FilesmediumnotapplicableVerify Permissions on the Etcd Write-Ahead-Log FilesmediumnotapplicableVerify User Who Owns The Open vSwitch Configuration DatabasemediumpassVerify User Who Owns The Open vSwitch Process ID FilemediumpassVerify User Who Owns The Kubernetes API Server Pod Specification FilemediumnotapplicableVerify Permissions on the OpenShift Container Network Interface FilesmediumpassVerify Permissions on the OpenShift PKI Private Key FilesmediumnotapplicableVerify Permissions on the Open vSwitch Persistent System IDmediumpassVerify User Who Owns The OpenShift SDN CNI Server ConfigmediumfailVerify Permissions on the Open vSwitch Database Server PIDmediumpassVerify Group Who Owns The Open vSwitch Configuration DatabasemediumpassVerify Group Who Owns The Etcd PKI Certificate FilesmediumnotapplicableVerify Group Who Owns The Open vSwitch Configuration Database LockmediumpassVerify User Who Owns The Open vSwitch Persistent System IDmediumpassVerify Group Who Owns The Kubernetes Scheduler Kubeconfig FilemediumnotapplicableVerify User Who Owns The Open vSwitch Daemon PID FilemediumpassVerify Permissions on the OpenShift SDN Container Network Interface Plugin IP Address AllocationsmediumpassVerify Group Who Owns The Kubernetes Controller Manager Pod Specification FilemediumnotapplicableVerify Permissions on the Open vSwitch Configuration Database LockmediumpassVerify Permissions on the Kubernetes Scheduler Kubeconfig FilemediumnotapplicableVerify User Who Owns The OpenShift PKI Certificate FilesmediumnotapplicableVerify Group Who Owns The Kubernetes Scheduler Pod Specification FilemediumnotapplicableVerify Permissions on the Kubernetes API Server Pod Specification FilemediumnotapplicableVerify Group Who Owns The OpenShift SDN CNI Server ConfigmediumfailOpenShift - Master Node SettingsVerify Group Who Owns The Worker Kubeconfig FilemediumpassVerify Group Who Owns The OpenShift Node Service FilemediumpassVerify Permissions on The Kubelet Configuration FilemediumpassVerify Permissions on the Worker Kubeconfig FilemediumpassVerify Group Who Owns The Kubelet Configuration FilemediumpassVerify User Who Owns The Kubelet Configuration FilemediumpassVerify Group Who Owns the Worker Certificate Authority FilemediumpassVerify Permissions on the OpenShift Node Service FilemediumpassVerify User Who Owns The OpenShift Node Service FilemediumpassVerify User Who Owns The Worker Kubeconfig FilemediumpassVerify Permissions on the Worker Certificate Authority FilemediumpassVerify User Who Owns the Worker Certificate Authority FilemediumpassOpenShift etcd SettingsConfigure A Unique CA Certificate for etcdmediumnotapplicableKubernetes Kubelet Settings 13x failEnsure Eviction threshold Settings Are Set - evictionHard: nodefs.availablemediumfailkubelet - Enable Client Certificate RotationmediumpassKubelet - Ensure Event Creation Is Configuredmediumfailkubelet - Disable Hostname OverridemediumpassEnsure Eviction threshold Settings Are Set - evictionSoft: imagefs.availablemediumfailkubelet - Do Not Disable Streaming TimeoutsmediumpassEnsure Eviction threshold Settings Are Set - evictionSoft: memory.availablemediumfailEnsure Eviction threshold Settings Are Set - evictionHard: nodefs.inodesFreemediumfailkubelet - Enable Server Certificate RotationmediumpassEnsure Eviction threshold Settings Are Set - evictionHard: imagefs.availablemediumfailkubelet - Configure the Client CA Certificatemediumpasskubelet - Enable Protect Kernel DefaultsmediumfailEnsure that the Kubelet only makes use of Strong Cryptographic Ciphersmediumfailkubelet - Allow Automatic Firewall ConfigurationmediumpassEnsure Eviction threshold Settings Are Set - evictionHard: imagefs.inodesFreemediumfailEnsure Eviction threshold Settings Are Set - evictionSoft: nodefs.availablemediumfailEnsure authorization is set to Webhookmediumpasskubelet - Enable Certificate RotationmediumpassEnsure Eviction threshold Settings Are Set - evictionSoft: nodefs.inodesFreemediumfailDisable Anonymous Authentication to the KubeletmediumpassEnsure Eviction threshold Settings Are Set - evictionSoft: imagefs.inodesFreemediumfailEnsure Eviction threshold Settings Are Set - evictionHard: memory.availablemediumfailShow all result detailsResult DetailsVerify Group Who Owns The Open vSwitch Daemon PID Filexccdf_org.ssgproject.content_rule_file_groupowner_ovs_vswitchd_pid mediumCCE-84129-6 Verify Group Who Owns The Open vSwitch Daemon PID FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_ovs_vswitchd_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_ovs_vswitchd_pid:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84129-6\nReferences: 1.1.9, CM-6, CM-6(1)\nDescriptionEnsure that the file /run/openvswitch/ovs-vswitchd.pid, is owned by the group openvswitch or hugetlbfs, depending on your settings and Open vSwitch version.RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of /var/run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_groupowner_var_run_ovs_vswitchd_pid_800:tst:1 falseFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/run/openvswitch/ovs-vswitchd.pidregular8008015rw-r--r-- Testing group ownership of /var/run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_groupowner_var_run_ovs_vswitchd_pid_801:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/run/openvswitch/ovs-vswitchd.pidregular8008015rw-r--r-- Verify User Who Owns The OpenShift SDN Container Network Interface Plugin IP Address Allocationsxccdf_org.ssgproject.content_rule_file_owner_ip_allocations mediumCCE-84248-4 Verify User Who Owns The OpenShift SDN Container Network Interface Plugin IP Address AllocationsRule IDxccdf_org.ssgproject.content_rule_file_owner_ip_allocationsResultfailMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_ip_allocations:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84248-4\nReferences: 1.1.10, CM-6, CM-6(1)\nDescription To properly set the owner of /var/lib/cni/networks/openshift-sdn/.*, run the command: $ sudo chown root /var/lib/cni/networks/openshift-sdn/.* RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of ^/var/lib/cni/networks/openshift-sdn/.*$ oval:ssg-test_file_owner_ip_allocations:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_owner_ip_allocations:obj:1 of type file_objectFilepath^/var/lib/cni/networks/openshift-sdn/.*$ Verify Group Who Owns The Kubernetes API Server Pod Specification Filexccdf_org.ssgproject.content_rule_file_groupowner_kube_apiserver mediumCCE-83530-6 Verify Group Who Owns The Kubernetes API Server Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_kube_apiserverResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83530-6\nReferences: 1.1.2, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yamlRationaleThe Kubernetes specification file contains information about the configuration of the Kubernetes API Server that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes API Server service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify User Who Owns The OpenShift Controller Manager Kubeconfig Filexccdf_org.ssgproject.content_rule_file_owner_controller_manager_kubeconfig mediumCCE-83904-3 Verify User Who Owns The OpenShift Controller Manager Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_owner_controller_manager_kubeconfigResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83904-3\nReferences: 1.1.18, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig RationaleThe Controller Manager's kubeconfig contains information about how the component will access the API server. You should set its file ownership to maintain the integrity of the file.Warningswarning This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Permissions on the OpenShift Admin Kubeconfig Filesxccdf_org.ssgproject.content_rule_file_permissions_master_admin_kubeconfigs mediumCCE-84278-1 Verify Permissions on the OpenShift Admin Kubeconfig FilesRule IDxccdf_org.ssgproject.content_rule_file_permissions_master_admin_kubeconfigsResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84278-1\nReferences: 1.1.13, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig, run the command: $ sudo chmod 0600 /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfigRationaleThere are various kubeconfig files that can be used by the administrator, defining various settings for the administration of the cluster. These files contain credentials that can be used to control the cluster and are needed for disaster recovery and each kubeconfig points to a different endpoint in the cluster. You should restrict its file permissions to maintain the integrity of the kubeconfig file as an attacker who gains access to these files can take over the cluster.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Group Who Owns The etcd Member Pod Specification Filexccdf_org.ssgproject.content_rule_file_groupowner_etcd_member mediumCCE-83664-3 Verify Group Who Owns The etcd Member Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_etcd_memberResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83664-3\nReferences: 1.1.8, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yamlRationaleThe etcd pod specification file controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Permissions on the OpenShift Multus Container Network Interface Plugin Filesxccdf_org.ssgproject.content_rule_file_permissions_multus_conf mediumCCE-83467-1 Verify Permissions on the OpenShift Multus Container Network Interface Plugin FilesRule IDxccdf_org.ssgproject.content_rule_file_permissions_multus_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_multus_conf:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83467-1\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /var/run/multus/cni/net.d/*, run the command: $ sudo chmod 0644 /var/run/multus/cni/net.d/*RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of ^/var/run/multus/cni/net.d/.*$ oval:ssg-test_file_permissions_multus_conf:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_multus_conf:obj:1 of type file_objectFilepathFilter^/var/run/multus/cni/net.d/.*$oval:ssg-state_file_permissions_multus_conf_mode_not_0644:ste:1 Verify Permissions on the Open vSwitch Process ID Filexccdf_org.ssgproject.content_rule_file_permissions_ovs_pid mediumCCE-83666-8 Verify Permissions on the Open vSwitch Process ID FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_ovs_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_ovs_pid:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83666-8\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /var/run/openvswitch/ovs-vswitchd.pid, run the command: $ sudo chmod 0644 /var/run/openvswitch/ovs-vswitchd.pidRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of /var/run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_permissions_ovs_pid:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_ovs_pid:obj:1 of type file_objectFilepathFilter/var/run/openvswitch/ovs-vswitchd.pidoval:ssg-state_file_permissions_ovs_pid_mode_not_0644:ste:1 Verify User Who Owns The Etcd PKI Certificate Filesxccdf_org.ssgproject.content_rule_file_owner_etcd_pki_cert_files mediumCCE-83898-7 Verify User Who Owns The Etcd PKI Certificate FilesRule IDxccdf_org.ssgproject.content_rule_file_owner_etcd_pki_cert_filesResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83898-7\nReferences: 1.1.19, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/*/*/*/*.crt, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/*/*/*/*.crt RationaleOpenShift makes use of a number of certificates as part of its operation. You should verify the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by the system administrator.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify User Who Owns The OpenShift Multus Container Network Interface Plugin Filesxccdf_org.ssgproject.content_rule_file_owner_multus_conf mediumCCE-83603-1 Verify User Who Owns The OpenShift Multus Container Network Interface Plugin FilesRule IDxccdf_org.ssgproject.content_rule_file_owner_multus_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_multus_conf:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83603-1\nReferences: 1.1.10, CM-6, CM-6(1)\nDescription To properly set the owner of /var/run/multus/cni/net.d/*, run the command: $ sudo chown root /var/run/multus/cni/net.d/* RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of ^/var/run/multus/cni/net.d/.*$ oval:ssg-test_file_owner_multus_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/run/multus/cni/net.d/10-ovn-kubernetes.confregular00233rw------- Verify User Who Owns The Open vSwitch Database Server PIDxccdf_org.ssgproject.content_rule_file_owner_ovsdb_server_pid mediumCCE-83806-0 Verify User Who Owns The Open vSwitch Database Server PIDRule IDxccdf_org.ssgproject.content_rule_file_owner_ovsdb_server_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_ovsdb_server_pid:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83806-0\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the owner of /run/openvswitch/ovsdb-server.pid, run the command: $ sudo chown openvswitch /run/openvswitch/ovsdb-server.pid RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of /run/openvswitch/ovsdb-server.pid oval:ssg-test_file_owner_ovsdb_server_pid:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/run/openvswitch/ovsdb-server.pidregular8008015rw-r--r-- Verify User Who Owns The Open vSwitch Configuration Database Lockxccdf_org.ssgproject.content_rule_file_owner_ovs_conf_db_lock mediumCCE-83462-2 Verify User Who Owns The Open vSwitch Configuration Database LockRule IDxccdf_org.ssgproject.content_rule_file_owner_ovs_conf_db_lockResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_ovs_conf_db_lock:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83462-2\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/openvswitch/.conf.db.~lock~, run the command: $ sudo chown openvswitch /etc/openvswitch/.conf.db.~lock~ RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of /etc/openvswitch/.conf.db.~lock~ oval:ssg-test_file_owner_ovs_conf_db_lock:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/openvswitch/.conf.db.~lock~regular8008010rw------- Verify Group Who Owns The Open vSwitch Persistent System IDxccdf_org.ssgproject.content_rule_file_groupowner_ovs_sys_id_conf mediumCCE-83677-5 Verify Group Who Owns The Open vSwitch Persistent System IDRule IDxccdf_org.ssgproject.content_rule_file_groupowner_ovs_sys_id_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_ovs_sys_id_conf:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83677-5\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/openvswitch/system-id.conf, run the command: $ sudo chgrp hugetlbfs /etc/openvswitch/system-id.confRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of /etc/openvswitch/system-id.conf oval:ssg-test_file_groupowner_ovs_sys_id_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/openvswitch/system-id.confregular80080137rw-r--r-- Verify Permissions on the OpenShift SDN CNI Server Configxccdf_org.ssgproject.content_rule_file_perms_openshift_sdn_cniserver_config mediumCCE-83927-4 Verify Permissions on the OpenShift SDN CNI Server ConfigRule IDxccdf_org.ssgproject.content_rule_file_perms_openshift_sdn_cniserver_configResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_perms_openshift_sdn_cniserver_config:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83927-4\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /var/run/openshift-sdn/cniserver/config.json, run the command: $ sudo chmod 0444 /var/run/openshift-sdn/cniserver/config.jsonRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of /var/run/openshift-sdn/cniserver/config.json oval:ssg-test_file_permissionsfile_perms_openshift_sdn_cniserver_config:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissionsfile_perms_openshift_sdn_cniserver_config:obj:1 of type file_objectFilepathFilter/var/run/openshift-sdn/cniserver/config.jsonoval:ssg-state_file_permissionsfile_perms_openshift_sdn_cniserver_config_mode_not_0444:ste:1 Verify Group Who Owns The OpenShift SDN Container Network Interface Plugin IP Address Allocationsxccdf_org.ssgproject.content_rule_file_groupowner_ip_allocations mediumCCE-84211-2 Verify Group Who Owns The OpenShift SDN Container Network Interface Plugin IP Address AllocationsRule IDxccdf_org.ssgproject.content_rule_file_groupowner_ip_allocationsResultfailMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_ip_allocations:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84211-2\nReferences: 1.1.10, CM-6, CM-6(1)\nDescription To properly set the group owner of /var/lib/cni/networks/openshift-sdn/.*, run the command: $ sudo chgrp root /var/lib/cni/networks/openshift-sdn/.*RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of ^/var/lib/cni/networks/openshift-sdn/.*$ oval:ssg-test_file_groupowner_ip_allocations:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_groupowner_ip_allocations:obj:1 of type file_objectFilepath^/var/lib/cni/networks/openshift-sdn/.*$ Verify Group Who Owns The Open vSwitch Process ID Filexccdf_org.ssgproject.content_rule_file_groupowner_ovs_pid mediumCCE-83630-4 Verify Group Who Owns The Open vSwitch Process ID FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_ovs_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_ovs_pid:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83630-4\nReferences: 1.1.9, CM-6, CM-6(1)\nDescriptionEnsure that the file /var/run/openvswitch/ovs-vswitchd.pid, is owned by the group openvswitch or hugetlbfs, depending on your settings and Open vSwitch version.RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of /run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_groupowner_run_ovs_vswitchd_pid_800:tst:1 falseFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/run/openvswitch/ovs-vswitchd.pidregular8008015rw-r--r-- Testing group ownership of /run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_groupowner_run_ovs_vswitchd_pid_801:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/run/openvswitch/ovs-vswitchd.pidregular8008015rw-r--r-- Verify User Who Owns The Etcd Database Directoryxccdf_org.ssgproject.content_rule_file_owner_etcd_data_dir mediumCCE-83905-0 Verify User Who Owns The Etcd Database DirectoryRule IDxccdf_org.ssgproject.content_rule_file_owner_etcd_data_dirResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83905-0\nReferences: 1.1.12, CM-6, CM-6(1)\nDescription To properly set the owner of /var/lib/etcd/member/, run the command: $ sudo chown root /var/lib/etcd/member/ Rationaleetcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Group Who Owns The Etcd Write-Ahead-Log Filesxccdf_org.ssgproject.content_rule_file_groupowner_etcd_data_files mediumCCE-83816-9 Verify Group Who Owns The Etcd Write-Ahead-Log FilesRule IDxccdf_org.ssgproject.content_rule_file_groupowner_etcd_data_filesResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83816-9\nReferences: 1.1.12, CM-6, CM-6(1)\nDescription To properly set the group owner of /var/lib/etcd/member/wal/*, run the command: $ sudo chgrp root /var/lib/etcd/member/wal/*Rationaleetcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Permissions on the Etcd PKI Certificate Filesxccdf_org.ssgproject.content_rule_file_permissions_etcd_pki_cert_files mediumCCE-83362-4 Verify Permissions on the Etcd PKI Certificate FilesRule IDxccdf_org.ssgproject.content_rule_file_permissions_etcd_pki_cert_filesResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83362-4\nReferences: 1.1.20, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/etcd-*/secrets/*/*.crt, run the command: $ sudo chmod 0600 /etc/kubernetes/static-pod-resources/etcd-*/secrets/*/*.crtRationaleOpenShift makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to 600 or more restrictive to protect their integrity.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Group Who Owns The Open vSwitch Database Server PIDxccdf_org.ssgproject.content_rule_file_groupowner_ovsdb_server_pid mediumCCE-84166-8 Verify Group Who Owns The Open vSwitch Database Server PIDRule IDxccdf_org.ssgproject.content_rule_file_groupowner_ovsdb_server_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_ovsdb_server_pid:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84166-8\nReferences: 1.1.9, CM-6, CM-6(1)\nDescriptionEnsure that the file /run/openvswitch/ovsdb-server.pid, is owned by the group openvswitch or hugetlbfs, depending on your settings and Open vSwitch version.RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of /run/openvswitch/ovsdb-server.pid oval:ssg-test_file_groupowner_ovsdb_server_pid_800:tst:1 falseFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/run/openvswitch/ovsdb-server.pidregular8008015rw-r--r-- Testing group ownership of /run/openvswitch/ovsdb-server.pid oval:ssg-test_file_groupowner_ovsdb_server_pid_801:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/run/openvswitch/ovsdb-server.pidregular8008015rw-r--r-- Verify Permissions on the OpenShift Controller Manager Kubeconfig Filexccdf_org.ssgproject.content_rule_file_permissions_controller_manager_kubeconfig mediumCCE-83604-9 Verify Permissions on the OpenShift Controller Manager Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_controller_manager_kubeconfigResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83604-9\nReferences: 1.1.17, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig, run the command: $ sudo chmod 0644 /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfigRationaleThe Controller Manager's kubeconfig contains information about how the component will access the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.Warningswarning This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Group Who Owns The OpenShift PKI Private Key Filesxccdf_org.ssgproject.content_rule_file_groupowner_openshift_pki_key_files mediumCCE-84172-6 Verify Group Who Owns The OpenShift PKI Private Key FilesRule IDxccdf_org.ssgproject.content_rule_file_groupowner_openshift_pki_key_filesResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84172-6\nReferences: 1.1.19, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/*/*/*/*.key, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/*/*/*/*.keyRationaleOpenShift makes use of a number of certificates as part of its operation. You should verify the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by root:root.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Group Who Owns The OpenShift Container Network Interface Filesxccdf_org.ssgproject.content_rule_file_groupowner_cni_conf mediumCCE-84025-6 Verify Group Who Owns The OpenShift Container Network Interface FilesRule IDxccdf_org.ssgproject.content_rule_file_groupowner_cni_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_cni_conf:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84025-6\nReferences: 1.1.10, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/cni/net.d/*, run the command: $ sudo chgrp root /etc/cni/net.d/*RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of ^/etc/cni/net.d/.*$ oval:ssg-test_file_groupowner_cni_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/cni/net.d/100-crio-bridge.confregular00438rw-r--r-- /etc/cni/net.d/200-loopback.confregular0054rw-r--r-- /etc/cni/net.d/87-podman-bridge.conflistregular00639rw-r--r-- Verify User Who Owns The Etcd Member Pod Specification Filexccdf_org.ssgproject.content_rule_file_owner_etcd_member mediumCCE-83988-6 Verify User Who Owns The Etcd Member Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_owner_etcd_memberResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83988-6\nReferences: 1.1.8, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml RationaleThe etcd pod specification file controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Group Who Owns The OpenShift Admin Kubeconfig Filesxccdf_org.ssgproject.content_rule_file_groupowner_master_admin_kubeconfigs mediumCCE-84204-7 Verify Group Who Owns The OpenShift Admin Kubeconfig FilesRule IDxccdf_org.ssgproject.content_rule_file_groupowner_master_admin_kubeconfigsResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84204-7\nReferences: 1.1.14, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfigRationaleThere are various kubeconfig files that can be used by the administrator, defining various settings for the administration of the cluster. These files contain credentials that can be used to control the cluster and are needed for disaster recovery and each kubeconfig points to a different endpoint in the cluster. You should restrict its file permissions to maintain the integrity of the kubeconfig file as an attacker who gains access to these files can take over the cluster.Warningswarning This rule is only applicable for nodes that run the Kubernetes API server service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Group Who Owns The OpenShift PKI Certificate Filesxccdf_org.ssgproject.content_rule_file_groupowner_openshift_pki_cert_files mediumCCE-83922-5 Verify Group Who Owns The OpenShift PKI Certificate FilesRule IDxccdf_org.ssgproject.content_rule_file_groupowner_openshift_pki_cert_filesResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83922-5\nReferences: 1.1.19, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/*/*/*/tls.crt, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/*/*/*/tls.crtRationaleOpenShift makes use of a number of certificates as part of its operation. You should verify the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by the system administrator.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify User Who Owns The OpenShift Admin Kubeconfig Filesxccdf_org.ssgproject.content_rule_file_owner_master_admin_kubeconfigs mediumCCE-83719-5 Verify User Who Owns The OpenShift Admin Kubeconfig FilesRule IDxccdf_org.ssgproject.content_rule_file_owner_master_admin_kubeconfigsResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83719-5\nReferences: 1.1.14, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/*.kubeconfig RationaleThere are various kubeconfig files that can be used by the administrator, defining various settings for the administration of the cluster. These files contain credentials that can be used to control the cluster and are needed for disaster recovery and each kubeconfig points to a different endpoint in the cluster. You should restrict its file permissions to maintain the integrity of the kubeconfig file as an attacker who gains access to these files can take over the cluster.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Permissions on the Etcd Member Pod Specification Filexccdf_org.ssgproject.content_rule_file_permissions_etcd_member mediumCCE-83973-8 Verify Permissions on the Etcd Member Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_etcd_memberResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83973-8\nReferences: 1.1.7, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml, run the command: $ sudo chmod 0644 /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yamlRationaleThe etcd pod specification file controls various parameters that set the behavior of the etcd service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Permissions on the Kubernetes Controller Manager Pod Specificiation Filexccdf_org.ssgproject.content_rule_file_permissions_kube_controller_manager mediumCCE-84161-9 Verify Permissions on the Kubernetes Controller Manager Pod Specificiation FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_kube_controller_managerResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84161-9\nReferences: 1.1.3, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml, run the command: $ sudo chmod 0644 /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yamlRationaleIf the Kubernetes specification file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the configuration of an Kubernetes Controller Manager server that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Permissions on the OpenShift PKI Certificate Filesxccdf_org.ssgproject.content_rule_file_permissions_openshift_pki_cert_files mediumCCE-83552-0 Verify Permissions on the OpenShift PKI Certificate FilesRule IDxccdf_org.ssgproject.content_rule_file_permissions_openshift_pki_cert_filesResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83552-0\nReferences: 1.1.20, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/kube-*/secrets/*/tls.crt, run the command: $ sudo chmod 0600 /etc/kubernetes/static-pod-resources/kube-*/secrets/*/tls.crtRationaleOpenShift makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to 600 or more restrictive to protect their integrity.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Group Who Owns The OpenShift Controller Manager Kubeconfig Filexccdf_org.ssgproject.content_rule_file_groupowner_controller_manager_kubeconfig mediumCCE-84095-9 Verify Group Who Owns The OpenShift Controller Manager Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_controller_manager_kubeconfigResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84095-9\nReferences: 1.1.18, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfig, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/configmaps/controller-manager-kubeconfig/kubeconfigRationaleThe Controller Manager's kubeconfig contains information about how the component will access the API server. You should set its file ownership to maintain the integrity of the file.Warningswarning This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Permissions on the Open vSwitch Configuration Databasexccdf_org.ssgproject.content_rule_file_permissions_ovs_conf_db mediumCCE-83788-0 Verify Permissions on the Open vSwitch Configuration DatabaseRule IDxccdf_org.ssgproject.content_rule_file_permissions_ovs_conf_dbResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_ovs_conf_db:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83788-0\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/openvswitch/conf.db, run the command: $ sudo chmod 0640 /etc/openvswitch/conf.dbRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of /etc/openvswitch/conf.db oval:ssg-test_file_permissions_ovs_conf_db:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_ovs_conf_db:obj:1 of type file_objectFilepathFilter/etc/openvswitch/conf.dboval:ssg-state_file_permissions_ovs_conf_db_mode_not_0640:ste:1 Verify Permissions on the Open vSwitch Daemon PID Filexccdf_org.ssgproject.content_rule_file_permissions_ovs_vswitchd_pid mediumCCE-83710-4 Verify Permissions on the Open vSwitch Daemon PID FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_ovs_vswitchd_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_ovs_vswitchd_pid:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83710-4\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /run/openvswitch/ovs-vswitchd.pid, run the command: $ sudo chmod 0644 /run/openvswitch/ovs-vswitchd.pidRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of /run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_permissions_ovs_vswitchd_pid:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_ovs_vswitchd_pid:obj:1 of type file_objectFilepathFilter/run/openvswitch/ovs-vswitchd.pidoval:ssg-state_file_permissions_ovs_vswitchd_pid_mode_not_0644:ste:1 Verify Permissions on the Kubernetes Scheduler Pod Specification Filexccdf_org.ssgproject.content_rule_file_permissions_scheduler mediumCCE-84057-9 Verify Permissions on the Kubernetes Scheduler Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_schedulerResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84057-9\nReferences: 1.1.5, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml, run the command: $ sudo chmod 0644 /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yamlRationaleIf the Kubernetes specification file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the configuration of an Kubernetes Scheduler service that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Group Who Owns The OpenShift Multus Container Network Interface Plugin Filesxccdf_org.ssgproject.content_rule_file_groupowner_multus_conf mediumCCE-83818-5 Verify Group Who Owns The OpenShift Multus Container Network Interface Plugin FilesRule IDxccdf_org.ssgproject.content_rule_file_groupowner_multus_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_multus_conf:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83818-5\nReferences: 1.1.10, CM-6, CM-6(1)\nDescription To properly set the group owner of /var/run/multus/cni/net.d/*, run the command: $ sudo chgrp root /var/run/multus/cni/net.d/*RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of ^/var/run/multus/cni/net.d/.*$ oval:ssg-test_file_groupowner_multus_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/run/multus/cni/net.d/10-ovn-kubernetes.confregular00233rw------- Verify User Who Owns The Kubernetes Controller Manager Pod Specificiation Filexccdf_org.ssgproject.content_rule_file_owner_kube_controller_manager mediumCCE-83795-5 Verify User Who Owns The Kubernetes Controller Manager Pod Specificiation FileRule IDxccdf_org.ssgproject.content_rule_file_owner_kube_controller_managerResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83795-5\nReferences: 1.1.4, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml RationaleThe Kubernetes specification file contains information about the configuration of the Kubernetes Controller Manager Server that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify User Who Owns The OpenShift PKI Private Key Filesxccdf_org.ssgproject.content_rule_file_owner_openshift_pki_key_files mediumCCE-83435-8 Verify User Who Owns The OpenShift PKI Private Key FilesRule IDxccdf_org.ssgproject.content_rule_file_owner_openshift_pki_key_filesResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83435-8\nReferences: 1.1.19, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/*/*/*/*.key, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/*/*/*/*.key RationaleOpenShift makes use of a number of certificates as part of its operation. You should verify the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by root:root.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Group Who Owns The Etcd Database Directoryxccdf_org.ssgproject.content_rule_file_groupowner_etcd_data_dir mediumCCE-83354-1 Verify Group Who Owns The Etcd Database DirectoryRule IDxccdf_org.ssgproject.content_rule_file_groupowner_etcd_data_dirResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83354-1\nReferences: 1.1.12, CM-6, CM-6(1)\nDescription To properly set the group owner of /var/lib/etcd/member/, run the command: $ sudo chgrp root /var/lib/etcd/member/Rationaleetcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify User Who Owns The Kubernetes Scheduler Kubeconfig Filexccdf_org.ssgproject.content_rule_file_owner_scheduler_kubeconfig mediumCCE-84017-3 Verify User Who Owns The Kubernetes Scheduler Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_owner_scheduler_kubeconfigResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84017-3\nReferences: 1.1.16, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig RationaleThe kubeconfig for the Scheduler contains paramters for the scheduler to access the Kube API. You should set its file ownership to maintain the integrity of the file.Warningswarning This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Permissions on the Etcd Database Directoryxccdf_org.ssgproject.content_rule_file_permissions_etcd_data_dir mediumCCE-84013-2 Verify Permissions on the Etcd Database DirectoryRule IDxccdf_org.ssgproject.content_rule_file_permissions_etcd_data_dirResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84013-2\nReferences: 1.1.11, CM-6, CM-6(1)\nDescription To properly set the permissions of /var/lib/etcd, run the command: $ sudo chmod 0700 /var/lib/etcdRationaleetcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify User Who Owns The Kubernetes Scheduler Pod Specification Filexccdf_org.ssgproject.content_rule_file_owner_kube_scheduler mediumCCE-83393-9 Verify User Who Owns The Kubernetes Scheduler Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_owner_kube_schedulerResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83393-9\nReferences: 1.1.6, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml RationaleThe Kubernetes specification file contains information about the configuration of the Kubernetes scheduler that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify User Who Owns The OpenShift Container Network Interface Filesxccdf_org.ssgproject.content_rule_file_owner_cni_conf mediumCCE-83460-6 Verify User Who Owns The OpenShift Container Network Interface FilesRule IDxccdf_org.ssgproject.content_rule_file_owner_cni_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_cni_conf:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83460-6\nReferences: 1.1.10, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/cni/net.d/*, run the command: $ sudo chown root /etc/cni/net.d/* RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of ^/etc/cni/net.d/.*$ oval:ssg-test_file_owner_cni_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/cni/net.d/100-crio-bridge.confregular00438rw-r--r-- /etc/cni/net.d/200-loopback.confregular0054rw-r--r-- /etc/cni/net.d/87-podman-bridge.conflistregular00639rw-r--r-- Verify User Who Owns The Etcd Write-Ahead-Log Filesxccdf_org.ssgproject.content_rule_file_owner_etcd_data_files mediumCCE-84010-8 Verify User Who Owns The Etcd Write-Ahead-Log FilesRule IDxccdf_org.ssgproject.content_rule_file_owner_etcd_data_filesResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84010-8\nReferences: 1.1.12, CM-6, CM-6(1)\nDescription To properly set the owner of /var/lib/etcd/member/wal/*, run the command: $ sudo chown root /var/lib/etcd/member/wal/* Rationaleetcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Permissions on the Etcd Write-Ahead-Log Filesxccdf_org.ssgproject.content_rule_file_permissions_etcd_data_files mediumCCE-83382-2 Verify Permissions on the Etcd Write-Ahead-Log FilesRule IDxccdf_org.ssgproject.content_rule_file_permissions_etcd_data_filesResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83382-2\nReferences: 1.1.11, CM-6, CM-6(1)\nDescription To properly set the permissions of /var/lib/etcd/member/wal/*, run the command: $ sudo chmod 0600 /var/lib/etcd/member/wal/*Rationaleetcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify User Who Owns The Open vSwitch Configuration Databasexccdf_org.ssgproject.content_rule_file_owner_ovs_conf_db mediumCCE-83489-5 Verify User Who Owns The Open vSwitch Configuration DatabaseRule IDxccdf_org.ssgproject.content_rule_file_owner_ovs_conf_dbResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_ovs_conf_db:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83489-5\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/openvswitch/conf.db, run the command: $ sudo chown openvswitch /etc/openvswitch/conf.db RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of /etc/openvswitch/conf.db oval:ssg-test_file_owner_ovs_conf_db:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/openvswitch/conf.dbregular800801678330rw-r----- Verify User Who Owns The Open vSwitch Process ID Filexccdf_org.ssgproject.content_rule_file_owner_ovs_pid mediumCCE-83937-3 Verify User Who Owns The Open vSwitch Process ID FileRule IDxccdf_org.ssgproject.content_rule_file_owner_ovs_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_ovs_pid:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83937-3\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the owner of /var/run/openvswitch/ovs-vswitchd.pid, run the command: $ sudo chown openvswitch /var/run/openvswitch/ovs-vswitchd.pid RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of /var/run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_owner_ovs_pid:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/run/openvswitch/ovs-vswitchd.pidregular8008015rw-r--r-- Verify User Who Owns The Kubernetes API Server Pod Specification Filexccdf_org.ssgproject.content_rule_file_owner_kube_apiserver mediumCCE-83372-3 Verify User Who Owns The Kubernetes API Server Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_owner_kube_apiserverResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83372-3\nReferences: 1.1.2, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml RationaleThe Kubernetes specification file contains information about the configuration of the Kubernetes API Server that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes API Server service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Permissions on the OpenShift Container Network Interface Filesxccdf_org.ssgproject.content_rule_file_permissions_cni_conf mediumCCE-83379-8 Verify Permissions on the OpenShift Container Network Interface FilesRule IDxccdf_org.ssgproject.content_rule_file_permissions_cni_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_cni_conf:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83379-8\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/cni/net.d/*, run the command: $ sudo chmod 0644 /etc/cni/net.d/*RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of ^/etc/cni/net.d/.*$ oval:ssg-test_file_permissions_cni_conf:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_cni_conf:obj:1 of type file_objectFilepathFilter^/etc/cni/net.d/.*$oval:ssg-state_file_permissions_cni_conf_mode_not_0644:ste:1 Verify Permissions on the OpenShift PKI Private Key Filesxccdf_org.ssgproject.content_rule_file_permissions_openshift_pki_key_files mediumCCE-83580-1 Verify Permissions on the OpenShift PKI Private Key FilesRule IDxccdf_org.ssgproject.content_rule_file_permissions_openshift_pki_key_filesResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83580-1\nReferences: 1.1.21, IA-5(2)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/*/*/*/*.key, run the command: $ sudo chmod 0600 /etc/kubernetes/static-pod-resources/*/*/*/*.keyRationaleOpenShift makes use of a number of key files as part of the operation of its components. The permissions on these files should be set to 600 to protect their integrity and confidentiality.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Permissions on the Open vSwitch Persistent System IDxccdf_org.ssgproject.content_rule_file_permissions_ovs_sys_id_conf mediumCCE-83400-2 Verify Permissions on the Open vSwitch Persistent System IDRule IDxccdf_org.ssgproject.content_rule_file_permissions_ovs_sys_id_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_ovs_sys_id_conf:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83400-2\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/openvswitch/system-id.conf, run the command: $ sudo chmod 0644 /etc/openvswitch/system-id.confRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of /etc/openvswitch/system-id.conf oval:ssg-test_file_permissions_ovs_sys_id_conf:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_ovs_sys_id_conf:obj:1 of type file_objectFilepathFilter/etc/openvswitch/system-id.confoval:ssg-state_file_permissions_ovs_sys_id_conf_mode_not_0644:ste:1 Verify User Who Owns The OpenShift SDN CNI Server Configxccdf_org.ssgproject.content_rule_file_owner_openshift_sdn_cniserver_config mediumCCE-83932-4 Verify User Who Owns The OpenShift SDN CNI Server ConfigRule IDxccdf_org.ssgproject.content_rule_file_owner_openshift_sdn_cniserver_configResultfailMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_openshift_sdn_cniserver_config:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83932-4\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the owner of /var/run/openshift-sdn/cniserver/config.json, run the command: $ sudo chown root /var/run/openshift-sdn/cniserver/config.json RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of /var/run/openshift-sdn/cniserver/config.json oval:ssg-test_file_owner_openshift_sdn_cniserver_config:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_owner_openshift_sdn_cniserver_config:obj:1 of type file_objectFilepath/var/run/openshift-sdn/cniserver/config.json Verify Permissions on the Open vSwitch Database Server PIDxccdf_org.ssgproject.content_rule_file_permissions_ovsdb_server_pid mediumCCE-83679-1 Verify Permissions on the Open vSwitch Database Server PIDRule IDxccdf_org.ssgproject.content_rule_file_permissions_ovsdb_server_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_ovsdb_server_pid:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83679-1\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /run/openvswitch/ovsdb-server.pid, run the command: $ sudo chmod 0644 /run/openvswitch/ovsdb-server.pidRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of /run/openvswitch/ovsdb-server.pid oval:ssg-test_file_permissions_ovsdb_server_pid:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_ovsdb_server_pid:obj:1 of type file_objectFilepathFilter/run/openvswitch/ovsdb-server.pidoval:ssg-state_file_permissions_ovsdb_server_pid_mode_not_0644:ste:1 Verify Group Who Owns The Open vSwitch Configuration Databasexccdf_org.ssgproject.content_rule_file_groupowner_ovs_conf_db mediumCCE-84226-0 Verify Group Who Owns The Open vSwitch Configuration DatabaseRule IDxccdf_org.ssgproject.content_rule_file_groupowner_ovs_conf_dbResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_ovs_conf_db:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84226-0\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/openvswitch/conf.db, run the command: $ sudo chgrp hugetlbfs /etc/openvswitch/conf.dbRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of /etc/openvswitch/conf.db oval:ssg-test_file_groupowner_ovs_conf_db:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/openvswitch/conf.dbregular800801678330rw-r----- Verify Group Who Owns The Etcd PKI Certificate Filesxccdf_org.ssgproject.content_rule_file_groupowner_etcd_pki_cert_files mediumCCE-83890-4 Verify Group Who Owns The Etcd PKI Certificate FilesRule IDxccdf_org.ssgproject.content_rule_file_groupowner_etcd_pki_cert_filesResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83890-4\nReferences: 1.1.19, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/*/*/*/*.crt, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/*/*/*/*.crtRationaleOpenShift makes use of a number of certificates as part of its operation. You should verify the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by the system administrator.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Group Who Owns The Open vSwitch Configuration Database Lockxccdf_org.ssgproject.content_rule_file_groupowner_ovs_conf_db_lock mediumCCE-84219-5 Verify Group Who Owns The Open vSwitch Configuration Database LockRule IDxccdf_org.ssgproject.content_rule_file_groupowner_ovs_conf_db_lockResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_ovs_conf_db_lock:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84219-5\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/openvswitch/.conf.db.~lock~, run the command: $ sudo chgrp hugetlbfs /etc/openvswitch/.conf.db.~lock~RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of /etc/openvswitch/.conf.db.~lock~ oval:ssg-test_file_groupowner_ovs_conf_db_lock:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/openvswitch/.conf.db.~lock~regular8008010rw------- Verify User Who Owns The Open vSwitch Persistent System IDxccdf_org.ssgproject.content_rule_file_owner_ovs_sys_id_conf mediumCCE-84085-0 Verify User Who Owns The Open vSwitch Persistent System IDRule IDxccdf_org.ssgproject.content_rule_file_owner_ovs_sys_id_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_ovs_sys_id_conf:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84085-0\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/openvswitch/system-id.conf, run the command: $ sudo chown openvswitch /etc/openvswitch/system-id.conf RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of /etc/openvswitch/system-id.conf oval:ssg-test_file_owner_ovs_sys_id_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/openvswitch/system-id.confregular80080137rw-r--r-- Verify Group Who Owns The Kubernetes Scheduler Kubeconfig Filexccdf_org.ssgproject.content_rule_file_groupowner_scheduler_kubeconfig mediumCCE-83471-3 Verify Group Who Owns The Kubernetes Scheduler Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_scheduler_kubeconfigResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83471-3\nReferences: 1.1.16, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfigRationaleThe kubeconfig for the Scheduler contains paramters for the scheduler to access the Kube API. You should set its file ownership to maintain the integrity of the file.Warningswarning This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify User Who Owns The Open vSwitch Daemon PID Filexccdf_org.ssgproject.content_rule_file_owner_ovs_vswitchd_pid mediumCCE-83888-8 Verify User Who Owns The Open vSwitch Daemon PID FileRule IDxccdf_org.ssgproject.content_rule_file_owner_ovs_vswitchd_pidResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_ovs_vswitchd_pid:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83888-8\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the owner of /run/openvswitch/ovs-vswitchd.pid, run the command: $ sudo chown openvswitch /run/openvswitch/ovs-vswitchd.pid RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing user ownership of /run/openvswitch/ovs-vswitchd.pid oval:ssg-test_file_owner_ovs_vswitchd_pid:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/run/openvswitch/ovs-vswitchd.pidregular8008015rw-r--r-- Verify Permissions on the OpenShift SDN Container Network Interface Plugin IP Address Allocationsxccdf_org.ssgproject.content_rule_file_permissions_ip_allocations mediumCCE-83469-7 Verify Permissions on the OpenShift SDN Container Network Interface Plugin IP Address AllocationsRule IDxccdf_org.ssgproject.content_rule_file_permissions_ip_allocationsResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_ip_allocations:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83469-7\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /var/lib/cni/networks/openshift-sdn/*, run the command: $ sudo chmod 0644 /var/lib/cni/networks/openshift-sdn/*RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of ^/var/lib/cni/networks/openshift-sdn/.*$ oval:ssg-test_file_permissions_ip_allocations:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_ip_allocations:obj:1 of type file_objectFilepathFilter^/var/lib/cni/networks/openshift-sdn/.*$oval:ssg-state_file_permissions_ip_allocations_mode_not_0644:ste:1 Verify Group Who Owns The Kubernetes Controller Manager Pod Specification Filexccdf_org.ssgproject.content_rule_file_groupowner_kube_controller_manager mediumCCE-83953-0 Verify Group Who Owns The Kubernetes Controller Manager Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_kube_controller_managerResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83953-0\nReferences: 1.1.4, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yaml, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-*/kube-controller-manager-pod.yamlRationaleThe Kubernetes specification file contains information about the configuration of the Kubernetes Controller Manager Server that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes Controller Manager service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Permissions on the Open vSwitch Configuration Database Lockxccdf_org.ssgproject.content_rule_file_permissions_ovs_conf_db_lock mediumCCE-84202-1 Verify Permissions on the Open vSwitch Configuration Database LockRule IDxccdf_org.ssgproject.content_rule_file_permissions_ovs_conf_db_lockResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_ovs_conf_db_lock:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84202-1\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/openvswitch/.conf.db.~lock~, run the command: $ sudo chmod 0600 /etc/openvswitch/.conf.db.~lock~RationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing mode of /etc/openvswitch/.conf.db.~lock~ oval:ssg-test_file_permissions_ovs_conf_db_lock:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_ovs_conf_db_lock:obj:1 of type file_objectFilepathFilter/etc/openvswitch/.conf.db.~lock~oval:ssg-state_file_permissions_ovs_conf_db_lock_mode_not_0600:ste:1 Verify Permissions on the Kubernetes Scheduler Kubeconfig Filexccdf_org.ssgproject.content_rule_file_permissions_scheduler_kubeconfig mediumCCE-83772-4 Verify Permissions on the Kubernetes Scheduler Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_scheduler_kubeconfigResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83772-4\nReferences: 1.1.15, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfig, run the command: $ sudo chmod 0644 /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/configmaps/scheduler-kubeconfig/kubeconfigRationaleThe kubeconfig for the Scheduler contains paramters for the scheduler to access the Kube API. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.Warningswarning This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify User Who Owns The OpenShift PKI Certificate Filesxccdf_org.ssgproject.content_rule_file_owner_openshift_pki_cert_files mediumCCE-83558-7 Verify User Who Owns The OpenShift PKI Certificate FilesRule IDxccdf_org.ssgproject.content_rule_file_owner_openshift_pki_cert_filesResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83558-7\nReferences: 1.1.19, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/static-pod-resources/*/*/*/tls.crt, run the command: $ sudo chown root /etc/kubernetes/static-pod-resources/*/*/*/tls.crt RationaleOpenShift makes use of a number of certificates as part of its operation. You should verify the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity.Warningswarning This rule is only applicable for nodes that run the Kubernetes Control Plane. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Group Who Owns The Kubernetes Scheduler Pod Specification Filexccdf_org.ssgproject.content_rule_file_groupowner_kube_scheduler mediumCCE-83614-8 Verify Group Who Owns The Kubernetes Scheduler Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_kube_schedulerResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83614-8\nReferences: 1.1.6, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yaml, run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/kube-scheduler-pod-*/kube-scheduler-pod.yamlRationaleThe Kubernetes Specification file contains information about the configuration of the Kubernetes scheduler that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes Scheduler service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Permissions on the Kubernetes API Server Pod Specification Filexccdf_org.ssgproject.content_rule_file_permissions_kube_apiserver mediumCCE-83983-7 Verify Permissions on the Kubernetes API Server Pod Specification FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_kube_apiserverResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83983-7\nReferences: 1.1.1, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yaml, run the command: $ sudo chmod 0644 /etc/kubernetes/static-pod-resources/kube-apiserver-pod-*/kube-apiserver-pod.yamlRationaleIf the Kubernetes specification file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the configuration of the Kubernetes API server that is configured on the system. Protection of this file is critical for OpenShift security.Warningswarning This rule is only applicable for nodes that run the Kubernetes API Server service. The aforementioned service is only running on the nodes labeled \"master\" by default.Verify Group Who Owns The OpenShift SDN CNI Server Configxccdf_org.ssgproject.content_rule_file_groupowner_openshift_sdn_cniserver_config mediumCCE-83605-6 Verify Group Who Owns The OpenShift SDN CNI Server ConfigRule IDxccdf_org.ssgproject.content_rule_file_groupowner_openshift_sdn_cniserver_configResultfailMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_openshift_sdn_cniserver_config:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83605-6\nReferences: 1.1.9, CM-6, CM-6(1)\nDescription To properly set the group owner of /var/run/openshift-sdn/cniserver/config.json, run the command: $ sudo chgrp root /var/run/openshift-sdn/cniserver/config.jsonRationaleCNI (Container Network Interface) files consist of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Allowing writeable access to the files could allow an attacker to modify the networking configuration potentially adding a rogue network connection.OVAL test results details Testing group ownership of /var/run/openshift-sdn/cniserver/config.json oval:ssg-test_file_groupowner_openshift_sdn_cniserver_config:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_groupowner_openshift_sdn_cniserver_config:obj:1 of type file_objectFilepath/var/run/openshift-sdn/cniserver/config.json Verify Group Who Owns The Worker Kubeconfig Filexccdf_org.ssgproject.content_rule_file_groupowner_worker_kubeconfig mediumCCE-83409-3 Verify Group Who Owns The Worker Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_worker_kubeconfigResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_worker_kubeconfig:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83409-3\nReferences: 4.1.10, CM-6, CM-6(1)\nDescription To properly set the group owner of /var/lib/kubelet/kubeconfig, run the command: $ sudo chgrp root /var/lib/kubelet/kubeconfigRationaleThe worker kubeconfig file contains information about the administrative configuration of the OpenShift cluster that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing group ownership of /var/lib/kubelet/kubeconfig oval:ssg-test_file_groupowner_worker_kubeconfig:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/lib/kubelet/kubeconfigregular005244rw------- Verify Group Who Owns The OpenShift Node Service Filexccdf_org.ssgproject.content_rule_file_groupowner_worker_service mediumCCE-83975-3 Verify Group Who Owns The OpenShift Node Service FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_worker_serviceResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_worker_service:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83975-3\nReferences: 4.1.2, CM-6, CM-6(1)\nDescription' To properly set the group owner of /etc/systemd/system/kubelet.service, run the command: $ sudo chgrp root /etc/systemd/system/kubelet.service'RationaleThe /etc/systemd/system/kubelet.service file contains information about the configuration of the OpenShift node service that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing group ownership of /etc/systemd/system/kubelet.service oval:ssg-test_file_groupowner_worker_service:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/systemd/system/kubelet.serviceregular001321rw-r--r-- Verify Permissions on The Kubelet Configuration Filexccdf_org.ssgproject.content_rule_file_permissions_kubelet_conf mediumCCE-83470-5 Verify Permissions on The Kubelet Configuration FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_kubelet_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_kubelet_conf:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83470-5\nReferences: 4.1.5, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/kubelet.conf, run the command: $ sudo chmod 0644 /etc/kubernetes/kubelet.confRationaleIf the kubelet configuration file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the configuration of an OpenShift node that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing mode of /etc/kubernetes/kubelet.conf oval:ssg-test_file_permissions_kubelet_conf:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_kubelet_conf:obj:1 of type file_objectFilepathFilter/etc/kubernetes/kubelet.confoval:ssg-state_file_permissions_kubelet_conf_mode_not_0644:ste:1 Verify Permissions on the Worker Kubeconfig Filexccdf_org.ssgproject.content_rule_file_permissions_worker_kubeconfig mediumCCE-83509-0 Verify Permissions on the Worker Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_worker_kubeconfigResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_worker_kubeconfig:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83509-0\nReferences: 4.1.9, CM-6, CM-6(1)\nDescription To properly set the permissions of /var/lib/kubelet/kubeconfig, run the command: $ sudo chmod 0600 /var/lib/kubelet/kubeconfigRationaleIf the worker kubeconfig file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the administration configuration of the OpenShift cluster that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing mode of /var/lib/kubelet/kubeconfig oval:ssg-test_file_permissions_worker_kubeconfig:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_worker_kubeconfig:obj:1 of type file_objectFilepathFilter/var/lib/kubelet/kubeconfigoval:ssg-state_file_permissions_worker_kubeconfig_mode_not_0600:ste:1 Verify Group Who Owns The Kubelet Configuration Filexccdf_org.ssgproject.content_rule_file_groupowner_kubelet_conf mediumCCE-84233-6 Verify Group Who Owns The Kubelet Configuration FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_kubelet_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_kubelet_conf:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84233-6\nReferences: 4.1.6, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/kubelet.conf, run the command: $ sudo chgrp root /etc/kubernetes/kubelet.confRationaleThe kubelet configuration file contains information about the configuration of the OpenShift node that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing group ownership of /etc/kubernetes/kubelet.conf oval:ssg-test_file_groupowner_kubelet_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/kubelet.confregular00934rw-r--r-- Verify User Who Owns The Kubelet Configuration Filexccdf_org.ssgproject.content_rule_file_owner_kubelet_conf mediumCCE-83976-1 Verify User Who Owns The Kubelet Configuration FileRule IDxccdf_org.ssgproject.content_rule_file_owner_kubelet_confResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_kubelet_conf:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83976-1\nReferences: 4.1.6, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/kubelet.conf, run the command: $ sudo chown root /etc/kubernetes/kubelet.conf RationaleThe kubelet configuration file contains information about the configuration of the OpenShift node that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing user ownership of /etc/kubernetes/kubelet.conf oval:ssg-test_file_owner_kubelet_conf:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/kubelet.confregular00934rw-r--r-- Verify Group Who Owns the Worker Certificate Authority Filexccdf_org.ssgproject.content_rule_file_groupowner_worker_ca mediumCCE-83440-8 Verify Group Who Owns the Worker Certificate Authority FileRule IDxccdf_org.ssgproject.content_rule_file_groupowner_worker_caResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_groupowner_worker_ca:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83440-8\nReferences: 4.1.8, CM-6, CM-6(1)\nDescription To properly set the group owner of /etc/kubernetes/kubelet-ca.crt, run the command: $ sudo chgrp root /etc/kubernetes/kubelet-ca.crtRationaleThe worker certificate authority file contains the certificate authority certificate for an OpenShift node that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing group ownership of /etc/kubernetes/kubelet-ca.crt oval:ssg-test_file_groupowner_worker_ca:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/kubelet-ca.crtregular005875rw-r--r-- Verify Permissions on the OpenShift Node Service Filexccdf_org.ssgproject.content_rule_file_permissions_worker_service mediumCCE-83455-6 Verify Permissions on the OpenShift Node Service FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_worker_serviceResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_worker_service:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83455-6\nReferences: 4.1.1, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/systemd/system/kubelet.service, run the command: $ sudo chmod 0644 /etc/systemd/system/kubelet.serviceRationaleIf the /etc/systemd/system/kubelet.service file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the service configuration of the OpenShift node service that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing mode of /etc/systemd/system/kubelet.service oval:ssg-test_file_permissions_worker_service:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_worker_service:obj:1 of type file_objectFilepathFilter/etc/systemd/system/kubelet.serviceoval:ssg-state_file_permissions_worker_service_mode_not_0644:ste:1 Verify User Who Owns The OpenShift Node Service Filexccdf_org.ssgproject.content_rule_file_owner_worker_service mediumCCE-84193-2 Verify User Who Owns The OpenShift Node Service FileRule IDxccdf_org.ssgproject.content_rule_file_owner_worker_serviceResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_worker_service:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84193-2\nReferences: 4.1.2, CM-6, CM-6(1)\nDescription' To properly set the owner of /etc/systemd/system/kubelet.service, run the command: $ sudo chown root /etc/systemd/system/kubelet.service 'RationaleThe /etc/systemd/system/kubelet.service file contains information about the configuration of the OpenShift node service that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing user ownership of /etc/systemd/system/kubelet.service oval:ssg-test_file_owner_worker_service:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/systemd/system/kubelet.serviceregular001321rw-r--r-- Verify User Who Owns The Worker Kubeconfig Filexccdf_org.ssgproject.content_rule_file_owner_worker_kubeconfig mediumCCE-83408-5 Verify User Who Owns The Worker Kubeconfig FileRule IDxccdf_org.ssgproject.content_rule_file_owner_worker_kubeconfigResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_worker_kubeconfig:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83408-5\nReferences: 4.1.10, CM-6, CM-6(1)\nDescription To properly set the owner of /var/lib/kubelet/kubeconfig, run the command: $ sudo chown root /var/lib/kubelet/kubeconfig RationaleThe worker kubeconfig file contains information about the administrative configuration of the OpenShift cluster that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing user ownership of /var/lib/kubelet/kubeconfig oval:ssg-test_file_owner_worker_kubeconfig:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/var/lib/kubelet/kubeconfigregular005244rw------- Verify Permissions on the Worker Certificate Authority Filexccdf_org.ssgproject.content_rule_file_permissions_worker_ca mediumCCE-83493-7 Verify Permissions on the Worker Certificate Authority FileRule IDxccdf_org.ssgproject.content_rule_file_permissions_worker_caResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_permissions_worker_ca:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83493-7\nReferences: 4.1.7, CM-6, CM-6(1)\nDescription To properly set the permissions of /etc/kubernetes/kubelet-ca.crt, run the command: $ sudo chmod 0644 /etc/kubernetes/kubelet-ca.crtRationaleIf the worker certificate authority file is writable by a group-owner or the world the risk of its compromise is increased. The file contains the certificate authority certificate for an OpenShift node that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing mode of /etc/kubernetes/kubelet-ca.crt oval:ssg-test_file_permissions_worker_ca:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_file_permissions_worker_ca:obj:1 of type file_objectFilepathFilter/etc/kubernetes/kubelet-ca.crtoval:ssg-state_file_permissions_worker_ca_mode_not_0644:ste:1 Verify User Who Owns the Worker Certificate Authority Filexccdf_org.ssgproject.content_rule_file_owner_worker_ca mediumCCE-83495-2 Verify User Who Owns the Worker Certificate Authority FileRule IDxccdf_org.ssgproject.content_rule_file_owner_worker_caResultpassMulti-check rulenoOVAL Definition IDoval:ssg-file_owner_worker_ca:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83495-2\nReferences: 4.1.8, CM-6, CM-6(1)\nDescription To properly set the owner of /etc/kubernetes/kubelet-ca.crt, run the command: $ sudo chown root /etc/kubernetes/kubelet-ca.crt RationaleThe worker certificate authority file contains the certificate authority certificate for an OpenShift node that is configured on the system. Protection of this file is critical for OpenShift security.OVAL test results details Testing user ownership of /etc/kubernetes/kubelet-ca.crt oval:ssg-test_file_owner_worker_ca:tst:1 trueFollowing items have been found on the system:PathTypeUIDGIDSize (B)Permissions/etc/kubernetes/kubelet-ca.crtregular005875rw-r--r-- Configure A Unique CA Certificate for etcdxccdf_org.ssgproject.content_rule_etcd_unique_ca mediumConfigure A Unique CA Certificate for etcdRule IDxccdf_org.ssgproject.content_rule_etcd_unique_caResultnotapplicableMulti-check rulenoTime2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesReferences: 2.7, CM-6, CM-6(1)\nDescriptionA unique CA certificate should be created for etcd. OpenShift by default creates separate PKIs for etcd and the Kubernetes API server. The same is done for other points of communication in the cluster.RationaleThe Kubernetes API server and etcd utilize separate CA certificates in OpenShift. This ensures that the etcd data is still protected in the event that the API server CA is compromised.Warningswarning This rule is only applicable for nodes that run the Etcd service. The aforementioned service is only running on the nodes labeled \"master\" by default.Ensure Eviction threshold Settings Are Set - evictionHard: nodefs.availablexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_nodefs_available mediumCCE-84138-7 Ensure Eviction threshold Settings Are Set - evictionHard: nodefs.availableRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_nodefs_availableResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_hard_nodefs_available:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84138-7\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the nodefs.available setting of the evictionHard section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionHard['nodefs.available']'. oval:ssg-test_kubelet_eviction_thresholds_set_hard_nodefs_available:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_hard_nodefs_available:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionHard['nodefs.available'] kubelet - Enable Client Certificate Rotationxccdf_org.ssgproject.content_rule_kubelet_enable_client_cert_rotation mediumCCE-83352-5 kubelet - Enable Client Certificate RotationRule IDxccdf_org.ssgproject.content_rule_kubelet_enable_client_cert_rotationResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_enable_client_cert_rotation:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83352-5\nReferences: 4.2.11, CM-6, CM-6(1)\nDescriptionTo enable the kubelet to rotate client certificates, edit the kubelet configuration file /etc/kubernetes/kubelet.conf on the kubelet node(s) and set the below parameter: featureGates: ... RotateKubeletClientCertificate: true ... RationaleAllowing the kubelet to auto-update the certificates ensure that there is no downtime in certificate renewal as well as ensures confidentiality and integrity.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.featureGates.RotateKubeletClientCertificate'. oval:ssg-test_kubelet_enable_client_cert_rotation:tst:1 trueNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_enable_client_cert_rotation:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.featureGates.RotateKubeletClientCertificate Kubelet - Ensure Event Creation Is Configuredxccdf_org.ssgproject.content_rule_kubelet_configure_event_creation mediumCCE-83576-9 Kubelet - Ensure Event Creation Is ConfiguredRule IDxccdf_org.ssgproject.content_rule_kubelet_configure_event_creationResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_configure_event_creation:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83576-9\nReferences: 4.2.9, CM-6, CM-6(1)\nDescriptionSecurity relevant information should be captured. The eventRecordQPS Kubelet option can be used to limit the rate at which events are gathered. Setting this too low could result in relevant events not being logged, however the unlimited setting of 0 could result in a denial of service on the kubelet. Processing and storage systems should be scaled to handle the expected event load. To set the eventRecordQPS option for the kubelet, create a KubeletConfig option along these lines: apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: name: kubelet-config-$pool spec: machineConfigPoolSelector: matchLabels: pools.operator.machineconfiguration.openshift.io/$pool_name: \"\" kubeletConfig: eventRecordQPS: 5 RationaleIt is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data.Warningswarning The MachineConfig Operator does not merge KubeletConfig objects, the last object is used instead. In case you need to set multiple options for kubelet, consider putting all the custom options into a single KubeletConfig object.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.eventRecordQPS'. oval:ssg-test_kubelet_configure_event_creation:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_configure_event_creation:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.eventRecordQPS kubelet - Disable Hostname Overridexccdf_org.ssgproject.content_rule_kubelet_disable_hostname_override mediumkubelet - Disable Hostname OverrideRule IDxccdf_org.ssgproject.content_rule_kubelet_disable_hostname_overrideResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_disable_hostname_override:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesReferences: 4.2.8, CM-6, CM-6(1)\nDescriptionTo prevent the hostname from being overrided, edit the kubelet systemd service file /etc/systemd/system/kubelet.service on the kubelet node(s) and remove the --hostname-override flag if it exists.RationaleAllowing hostnames to be overrided creates issues around resolving nodes in addition to TLS configuration, certificate validation, and log correlation and validation.OVAL test results details Test that the --hostname-override flag is not contained in kubelet.service oval:ssg-test_kubelet_hostname_override_disabled:tst:1 trueNo items have been found conforming to the following objects:Object oval:ssg-obj_kubelet_hostname_override:obj:1 of type textfilecontent54_objectFilepathPatternInstance/etc/systemd/system/kubelet.service^.*--hostname-override.*1 Ensure Eviction threshold Settings Are Set - evictionSoft: imagefs.availablexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_imagefs_available mediumCCE-84127-0 Ensure Eviction threshold Settings Are Set - evictionSoft: imagefs.availableRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_imagefs_availableResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_soft_imagefs_available:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84127-0\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the imagefs.available setting of the evictionSoft section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionSoft['imagefs.available']'. oval:ssg-test_kubelet_eviction_thresholds_set_soft_imagefs_available:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_soft_imagefs_available:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionSoft['imagefs.available'] kubelet - Do Not Disable Streaming Timeoutsxccdf_org.ssgproject.content_rule_kubelet_enable_streaming_connections mediumCCE-84097-5 kubelet - Do Not Disable Streaming TimeoutsRule IDxccdf_org.ssgproject.content_rule_kubelet_enable_streaming_connectionsResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_enable_streaming_connections:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84097-5\nReferences: 4.2.5, CM-6, CM-6(1)\nDescriptionTimouts for streaming connections should not be disabled as they help to prevent denial-of-service attacks. To configure streaming connection timeouts, edit the kubelet configuration file /etc/kubernetes/kubelet.conf on the kubelet node(s) and set the below parameter: streamingConnectionIdleTimeout: 5mRationaleEnsuring connections have timeouts helps to protect against denial-of-service attacks as well as disconnect inactive connections. In addition, setting connections timeouts helps to prevent from running out of ephemeral ports.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.streamingConnectionIdleTimeout'. oval:ssg-test_kubelet_enable_streaming_connections:tst:1 trueNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_enable_streaming_connections:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.streamingConnectionIdleTimeout Ensure Eviction threshold Settings Are Set - evictionSoft: memory.availablexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_memory_available mediumCCE-84222-9 Ensure Eviction threshold Settings Are Set - evictionSoft: memory.availableRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_memory_availableResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_soft_memory_available:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84222-9\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the memory.available setting of the evictionSoft section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionSoft['memory.available']'. oval:ssg-test_kubelet_eviction_thresholds_set_soft_memory_available:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_soft_memory_available:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionSoft['memory.available'] Ensure Eviction threshold Settings Are Set - evictionHard: nodefs.inodesFreexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_nodefs_inodesfree mediumCCE-84141-1 Ensure Eviction threshold Settings Are Set - evictionHard: nodefs.inodesFreeRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_nodefs_inodesfreeResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_hard_nodefs_inodesfree:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84141-1\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the nodefs.inodesFree setting of the evictionHard section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionHard['nodefs.inodesFree']'. oval:ssg-test_kubelet_eviction_thresholds_set_hard_nodefs_inodesfree:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_hard_nodefs_inodesfree:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionHard['nodefs.inodesFree'] kubelet - Enable Server Certificate Rotationxccdf_org.ssgproject.content_rule_kubelet_enable_server_cert_rotation mediumCCE-83356-6 kubelet - Enable Server Certificate RotationRule IDxccdf_org.ssgproject.content_rule_kubelet_enable_server_cert_rotationResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_enable_server_cert_rotation:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83356-6\nReferences: 4.2.12, CM-6, CM-6(1)\nDescriptionTo enable the kubelet to rotate server certificates, edit the kubelet configuration file /etc/kubernetes/kubelet.conf on the kubelet node(s) and set the below parameter: featureGates: ... RotateKubeletServerCertificate: true ... RationaleAllowing the kubelet to auto-update the certificates ensure that there is no downtime in certificate renewal as well as ensures confidentiality and integrity.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.featureGates.RotateKubeletServerCertificate'. oval:ssg-test_kubelet_enable_server_cert_rotation:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/etc/kubernetes/kubelet.conf/etc/kuberneteskubelet.conf.featureGates.RotateKubeletServerCertificate true Ensure Eviction threshold Settings Are Set - evictionHard: imagefs.availablexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_imagefs_available mediumCCE-84144-5 Ensure Eviction threshold Settings Are Set - evictionHard: imagefs.availableRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_imagefs_availableResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_hard_imagefs_available:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84144-5\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the imagefs.available setting of the evictionHard section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionHard['imagefs.available']'. oval:ssg-test_kubelet_eviction_thresholds_set_hard_imagefs_available:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_hard_imagefs_available:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionHard['imagefs.available'] kubelet - Configure the Client CA Certificatexccdf_org.ssgproject.content_rule_kubelet_configure_client_ca mediumCCE-83724-5 kubelet - Configure the Client CA CertificateRule IDxccdf_org.ssgproject.content_rule_kubelet_configure_client_caResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_configure_client_ca:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83724-5\nReferences: 4.2.3, CM-6, CM-6(1)\nDescriptionBy default, the kubelet is not configured with a CA certificate which can subject the kubelet to man-in-the-middle attacks. To configure a client CA certificate, edit the kubelet configuration file /etc/kubernetes/kubelet.conf on the kubelet node(s) and set the below parameter: authentication: ... x509: clientCAFile: /etc/kubernetes/kubelet-ca.crt ... RationaleNot having a CA certificate for the kubelet will subject the kubelet to possible man-in-the-middle attacks especially on unsafe or untrusted networks. Certificate validation for the kubelet allows the API server to validate the kubelet's identity.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.authentication.x509.clientCAFile'. oval:ssg-test_kubelet_configure_client_ca:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/etc/kubernetes/kubelet.conf/etc/kuberneteskubelet.conf.authentication.x509.clientCAFile /etc/kubernetes/kubelet-ca.crt kubelet - Enable Protect Kernel Defaultsxccdf_org.ssgproject.content_rule_kubelet_enable_protect_kernel_defaults mediumkubelet - Enable Protect Kernel DefaultsRule IDxccdf_org.ssgproject.content_rule_kubelet_enable_protect_kernel_defaultsResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_enable_protect_kernel_defaults:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesReferences: 4.2.6, CM-6, CM-6(1)\nDescription Protect tuned kernel parameters from being overwritten by the kubelet. Before enabling this kernel parameter, it's important and necessary to first create a MachineConfig object that persist the required sysctl's. The required sysctl's are the following: kernel.keys.root_maxbytes=25000000 kernel.keys.root_maxkeys=1000000 kernel.panic=10 kernel.panic_on_oops=1 vm.overcommit_memory=1 vm.panic_on_oom=0 The these need to be enabled via MachineConfig since they need to be available as soon as the node starts and before the Kubelet does. The manifest may look as follows: --- apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: master name: 75-master-kubelet-sysctls spec: config: ignition: version: 3.1.0 storage: files: - contents: source: data:,vm.overcommit_memory%3D1%0Avm.panic_on_oom%3D0%0Akernel.panic%3D10%0Akernel.panic_on_oops%3D1%0Akernel.keys.root_maxkeys%3D1000000%0Akernel.keys.root_maxbytes%3D25000000%0A mode: 0644 path: /etc/sysctl.d/90-kubelet.conf overwrite: true This will need to be done for each relevant MachineConfigPool in the cluster. After enabling this and after the changes have successfully rolled out to the whole cluster, it will now be possible to set the protectKernelDefaults parameter. To configure, follow the directions in the documentation RationaleKernel parameters are usually tuned and hardened by the system administrators before putting the systems into production. These parameters protect the kernel and the system. Your kubelet kernel defaults that rely on such parameters should be appropriately set to match the desired secured system state. Ignoring this could potentially lead to running pods with undesired kernel behavior.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.protectKernelDefaults'. oval:ssg-test_kubelet_enable_protect_kernel_defaults:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_enable_protect_kernel_defaults:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.protectKernelDefaults Ensure that the Kubelet only makes use of Strong Cryptographic Ciphersxccdf_org.ssgproject.content_rule_kubelet_configure_tls_cipher_suites mediumEnsure that the Kubelet only makes use of Strong Cryptographic CiphersRule IDxccdf_org.ssgproject.content_rule_kubelet_configure_tls_cipher_suitesResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_configure_tls_cipher_suites:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesReferences: 4.2.13, CM-6, CM-6(1)\nDescriptionEnsure that the Kubelet is configured to only use strong cryptographic ciphers. To set the cipher suites for the kubelet, create new or modify existing KubeletConfig object along these lines, one for every MachineConfigPool: apiVersion: machineconfiguration.openshift.io/v1 kind: KubeletConfig metadata: name: kubelet-config-$pool spec: machineConfigPoolSelector: matchLabels: pools.operator.machineconfiguration.openshift.io/$pool_name: \"\" kubeletConfig: tlsCipherSuites: - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 RationaleTLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.tlsCipherSuites[:]'. oval:ssg-test_kubelet_configure_tls_cipher_suites:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_configure_tls_cipher_suites:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.tlsCipherSuites[:] kubelet - Allow Automatic Firewall Configurationxccdf_org.ssgproject.content_rule_kubelet_enable_iptables_util_chains mediumCCE-83775-7 kubelet - Allow Automatic Firewall ConfigurationRule IDxccdf_org.ssgproject.content_rule_kubelet_enable_iptables_util_chainsResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_enable_iptables_util_chains:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83775-7\nReferences: 4.2.7, CM-6, CM-6(1)\nDescriptionThe kubelet has the ability to automatically configure the firewall to allow the containers required ports and connections to networking resources and destinations parameters potentially creating a security incident. To allow the kubelet to modify the firewall, edit the kubelet configuration file /etc/kubernetes/kubelet.conf on the kubelet node(s) and set the below parameter: makeIPTablesUtilChains: trueRationaleThe kubelet should automatically configure the firewall settings to allow access and networking traffic through. This ensures that when a pod or container is running that the correct ports are configured as well as removing the ports when a pod or container is no longer in existence.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.makeIPTablesUtilChains'. oval:ssg-test_kubelet_enable_iptables_util_chains:tst:1 trueNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_enable_iptables_util_chains:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.makeIPTablesUtilChains Ensure Eviction threshold Settings Are Set - evictionHard: imagefs.inodesFreexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_imagefs_inodesfree mediumCCE-84147-8 Ensure Eviction threshold Settings Are Set - evictionHard: imagefs.inodesFreeRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_imagefs_inodesfreeResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_hard_imagefs_inodesfree:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84147-8\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the imagefs.inodesFree setting of the evictionHard section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionHard['imagefs.inodesFree']'. oval:ssg-test_kubelet_eviction_thresholds_set_hard_imagefs_inodesfree:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_hard_imagefs_inodesfree:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionHard['imagefs.inodesFree'] Ensure Eviction threshold Settings Are Set - evictionSoft: nodefs.availablexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_nodefs_available mediumCCE-84119-7 Ensure Eviction threshold Settings Are Set - evictionSoft: nodefs.availableRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_nodefs_availableResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_soft_nodefs_available:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84119-7\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the nodefs.available setting of the evictionSoft section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionSoft['nodefs.available']'. oval:ssg-test_kubelet_eviction_thresholds_set_soft_nodefs_available:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_soft_nodefs_available:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionSoft['nodefs.available'] Ensure authorization is set to Webhookxccdf_org.ssgproject.content_rule_kubelet_authorization_mode mediumCCE-83593-4 Ensure authorization is set to WebhookRule IDxccdf_org.ssgproject.content_rule_kubelet_authorization_modeResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_authorization_mode:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83593-4\nReferences: 4.2.2, CM-6, CM-6(1)\nDescriptionUnauthenticated/unauthorized users should have no access to OpenShift nodes. The Kubelet should be set to only allow Webhook authorization. To ensure that the Kubelet requires authorization, validate that authorization is configured to Webhook in /etc/kubernetes/kubelet.conf: authorization: mode: Webhook ... RationaleEnsuring that the authorization is configured correctly helps enforce that unauthenticated/unauthorized users have no access to OpenShift nodes.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.authorization.mode'. oval:ssg-test_kubelet_authorization_mode:tst:1 trueNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_authorization_mode:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.authorization.mode kubelet - Enable Certificate Rotationxccdf_org.ssgproject.content_rule_kubelet_enable_cert_rotation mediumCCE-83838-3 kubelet - Enable Certificate RotationRule IDxccdf_org.ssgproject.content_rule_kubelet_enable_cert_rotationResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_enable_cert_rotation:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83838-3\nReferences: 4.2.11, CM-6, CM-6(1)\nDescriptionTo enable the kubelet to rotate client certificates, edit the kubelet configuration file /etc/kubernetes/kubelet.conf on the kubelet node(s) and set the below parameter: ... rotateCertificates: true ... RationaleAllowing the kubelet to auto-update the certificates ensure that there is no downtime in certificate renewal as well as ensures confidentiality and integrity.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.rotateCertificates'. oval:ssg-test_kubelet_enable_cert_rotation:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/etc/kubernetes/kubelet.conf/etc/kuberneteskubelet.conf.rotateCertificates true Ensure Eviction threshold Settings Are Set - evictionSoft: nodefs.inodesFreexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_nodefs_inodesfree mediumCCE-84123-9 Ensure Eviction threshold Settings Are Set - evictionSoft: nodefs.inodesFreeRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_nodefs_inodesfreeResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_soft_nodefs_inodesfree:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84123-9\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the nodefs.inodesFree setting of the evictionSoft section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionSoft['nodefs.inodesFree']'. oval:ssg-test_kubelet_eviction_thresholds_set_soft_nodefs_inodesfree:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_soft_nodefs_inodesfree:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionSoft['nodefs.inodesFree'] Disable Anonymous Authentication to the Kubeletxccdf_org.ssgproject.content_rule_kubelet_anonymous_auth mediumCCE-83815-1 Disable Anonymous Authentication to the KubeletRule IDxccdf_org.ssgproject.content_rule_kubelet_anonymous_authResultpassMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_anonymous_auth:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-83815-1\nReferences: 4.2.1, CM-6, CM-6(1)\nDescriptionBy default, anonymous access to the Kubelet server is enabled. This configuration check ensures that anonymous requests to the Kubelet server are disabled. Edit the Kubelet server configuration file /etc/kubernetes/kubelet.conf on the kubelet node(s) and set the below parameter: authentication: ... anonymous: enabled: false ... RationaleWhen enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. OpenShift Operators should rely on authentication to authorize access and disallow anonymous requests.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.authentication.anonymous.enabled'. oval:ssg-test_kubelet_anonymous_auth:tst:1 trueFollowing items have been found on the system:FilepathPathFilenameYamlpathValue/etc/kubernetes/kubelet.conf/etc/kuberneteskubelet.conf.authentication.anonymous.enabled false Ensure Eviction threshold Settings Are Set - evictionSoft: imagefs.inodesFreexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_imagefs_inodesfree mediumCCE-84132-0 Ensure Eviction threshold Settings Are Set - evictionSoft: imagefs.inodesFreeRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_soft_imagefs_inodesfreeResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_soft_imagefs_inodesfree:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84132-0\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the imagefs.inodesFree setting of the evictionSoft section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionSoft['imagefs.inodesFree']'. oval:ssg-test_kubelet_eviction_thresholds_set_soft_imagefs_inodesfree:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_soft_imagefs_inodesfree:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionSoft['imagefs.inodesFree'] Ensure Eviction threshold Settings Are Set - evictionHard: memory.availablexccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_memory_available mediumCCE-84135-3 Ensure Eviction threshold Settings Are Set - evictionHard: memory.availableRule IDxccdf_org.ssgproject.content_rule_kubelet_eviction_thresholds_set_hard_memory_availableResultfailMulti-check rulenoOVAL Definition IDoval:ssg-kubelet_eviction_thresholds_set_hard_memory_available:def:1Time2021-07-20T05:21:05+00:00SeveritymediumIdentifiers and ReferencesIdentifiers: CCE-84135-3\nReferences: 1.3.1, CM-6, CM-6(1)\nDescription Two types of garbage collection are performed on an OpenShift Container Platform node:\nContainer garbage collection: Removes terminated containers. Image garbage collection: Removes images not referenced by any running pods. Container garbage collection can be performed using eviction thresholds. Image garbage collection relies on disk usage as reported by cAdvisor on the node to decide which images to remove from the node. The OpenShift administrator can configure how OpenShift Container Platform performs garbage collection by creating a kubeletConfig object for each Machine Config Pool using any combination of the following: soft eviction for containers hard eviction for containers eviction for images To configure, follow the directions in the documentation This rule pertains to the memory.available setting of the evictionHard section. RationaleGarbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.OVAL test results details In the file '/etc/kubernetes/kubelet.conf' find only one object at path '.evictionHard['memory.available']'. oval:ssg-test_kubelet_eviction_thresholds_set_hard_memory_available:tst:1 falseNo items have been found conforming to the following objects:Object oval:ssg-object_kubelet_eviction_thresholds_set_hard_memory_available:obj:1 of type yamlfilecontent_objectFilepathYamlpath/etc/kubernetes/kubelet.conf.evictionHard['memory.available'] Scroll back to the first ruleRed Hat and Red Hat Enterprise Linux are either registered trademarks or trademarks of Red Hat, Inc. in the United States and other countries. All other names are registered trademarks or trademarks of their respective companies. Generated using OpenSCAP 1.3.4\n"},{"uri":"https://blog.stderr.at/tags/aap/","title":"AAP","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/anit-affinity/","title":"Anit-Affinity","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/archive/","title":"Archive","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/aro/","title":"ARO","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/authors/thomas-jungbauer/","title":"Articles by Thomas Jungbauer","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/authors/toni-schmidbauer/","title":"Articles by Toni Schmidbauer","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/automation/","title":"Automation","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/cicd/","title":"CICD","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/rss/","title":"RSS Feeds","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/sealed-secrets/","title":"Sealed Secrets","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/style-guide/","title":"Style Guide","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/tollerations/","title":"Tollerations","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/topology-spread-contstraints/","title":"Topology Spread Contstraints","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/tower/","title":"Tower","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/vpn/","title":"VPN","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/yaml/","title":"YAML","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/","title":"YAUB Yet Another Useless Blog","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector","Taints","Tollerations","Topology Spread Contstraints","Descheduler","Affinity","Anit-Affinity","Pipelines","OCP","Tekton","GitOps","Operator","Grafana","Thanos","Sealed Secrets","Storage","Vault","oc","kubectl","SSL","Cert Manager","Pipleines","CI/CD","Supply Chain","Rekor","cosign","SBOM","ACS","stackrox","SSL","SSO","Ansible","Automation","AAP","istio","Service Mesh","Azure","Compliance","Security"],"description":"","content":"Welcome to Yet Another Useless Blog Well we hope the articles here are not totally useless.\nWho are we, you might ask. We (Thomas Jungbauer and Toni Schmidbauer) are two old IT guys, working in the business since more than 20 years (each or us). At the moment we are architects at Red Hat Austria, mainly responsible helping customers with OpenShift or Ansible architecture.\nThe articles in this blog shall help to easily test and understand specific issues so they can be reproduced and tested. We simply wrote down what we saw in the field and of what we thought it might be helpful, so no frustrating searches in documentations or manual testing is required.\n"}]