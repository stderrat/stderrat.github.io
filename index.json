[{"uri":"https://blog.stderr.at/day-2/etcd/","title":"etcd","tags":["OCP","Day-2","OpenShift","etcd"],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/labels_environmets/","title":"Labels &amp; Environments","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector","Environments","Ingress","IngressController","Labels","Annotations"],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/","title":"OpenShift","tags":["Pipelines","OpenShift","OCP","Tekton","GitOps","Operator","Grafana","Thanos","Sealed Secrets","Storage","Day-2","Vault","oc","kubectl","SSL","Cert Manager"],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/","title":"OpenShift Day-2","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/pod-placement/","title":"Pod Placement","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector","Taints","Tollerations","Topology Spread Contstraints","Descheduler","Affinity","Anit-Affinity"],"description":"","content":""},{"uri":"https://blog.stderr.at/securesupplychain/","title":"Secure Supply Chain","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","Rekor","cosign","SBOM","ACS"],"description":"","content":""},{"uri":"https://blog.stderr.at/compliance/","title":"Compliance","tags":["Quay","Security","OpenShift","Compliance","Compliance Operator"],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/","title":"Service Mesh","tags":["OCP","Istio","OpenShift","Service Mesh"],"description":"","content":""},{"uri":"https://blog.stderr.at/general/","title":"General","tags":["Satellite","github","git"],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/","title":"Ansible","tags":["Ansible","Automation","Tower","Controller","AAP","YAML","CICD","Azure","Style Guide"],"description":"","content":""},{"uri":"https://blog.stderr.at/acs/","title":"Advanced Cluster Security","tags":["ACS","Advanced Cluster Security","OpenShift","Security","Keycloak","Authentication","SSO","Stackrox"],"description":"","content":""},{"uri":"https://blog.stderr.at/azure/","title":"Azure","tags":["Azure","ARO","OpenShift","VPN"],"description":"","content":""},{"uri":"https://blog.stderr.at/java/","title":"Java","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/quay/","title":"Quay","tags":["Quay","Registry","OpenShift","Container Security","Operator"],"description":"","content":""},{"uri":"https://blog.stderr.at/archive/","title":"Archive","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/legaldisclosure/","title":"Legal Disclosure","tags":[],"description":"","content":" Legal Disclosure Information in accordance with Section 5 TMG\nToni Schmidbauer \u0026amp; Thomas Jungbauer 1200 Vienna, Austria\nContact Information E-Mail:\nInternet address: https://blog.stderr.at/\nDisclaimer Accountability for content The contents of our pages have been created with the utmost care. However, we cannot guarantee the contents\u0026#39; accuracy, completeness or topicality. According to statutory provisions, we are furthermore responsible for our own content on these web pages. In this matter, please note that we are not obliged to monitor the transmitted or saved information of third parties, or investigate circumstances pointing to illegal activity. Our obligations to remove or block the use of information under generally applicable laws remain unaffected by this as per §§ 8 to 10 of the Telemedia Act (TMG).\nAccountability for links Responsibility for the content of external links (to web pages of third parties) lies solely with the operators of the linked pages. No violations were evident to us at the time of linking. Should any legal infringement become known to us, we will remove the respective link immediately.\nCopyright Our web pages and their contents are subject to German copyright law. Unless expressly permitted by law, every form of utilizing, reproducing or processing works subject to copyright protection on our web pages requires the prior consent of the respective owner of the rights. Individual reproductions of a work are only allowed for private use. The materials from these pages are copyrighted and any unauthorized use may violate copyright laws.\nQuelle: http://translate-24h.de\n"},{"uri":"https://blog.stderr.at/tags/acs/","title":"ACS","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/advanced-cluster-security/","title":"Advanced Cluster Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/cicd/","title":"CICD","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/cosign/","title":"cosign","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-15-securesupplychain-intro/","title":"Introduction to a Secure Supply Chain","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","Rekor","cosign","SBOM","ACS","Sigstore"],"description":"Introduction to Secure Supply Chain","content":" The goal of the following (\u0026#34;short\u0026#34;) series is to build a secure CI/CD pipeline step by step using OpenShift Pipelines (based on Tekton). The whole build process shall pull and build an image, upload it to a development environment and subsequently update the production environment.\nThe main focus here is security. Several steps and tools shall help to build and deploy a Secure Supply Chain.\nThe whole process is part of a Red Hat workshop which can present to your organization. I did some tweaks and created a step-by-step plan in order to remember it …​ since I am getting old :)\nThe Journey to Secure Supply Chain This series includes the following articles:\nListen to Events\nPipelines\nSonarQube\nVerify Git Commit\nBuild and Sign Image\nScanning with ACS\nGenerating a SBOM\nUpdating Kubernetes Manifests\nLinting Kubernetes Manifests\nThe Example Application\nACS Deployment Check\nVerify TLOG Signature\nBring it to Production\nPrerequisites In order to develop our Secure Supply Chain, we need an OpenShift 4 Cluster. I am currently using OpenShift 4.13. Moreover, the OpenShift Pipelines operator must be deployed. It is based on Tekton and provides a Kubernetes-native way to create CI/CD pipelines.\nThe operator is deployed using the Operator Hub inside your cluster. Simply search for OpenShift Pipelines and install the operator using the default settings.\nFigure 1. Install OpenShift Pipelines Finally, you will need a GitHub account to be able to fork some repositories.\nSome steps in the pipeline are working tightly with GitHub, especially the very last one that is talking GitHub’s API. However, any Git-system should work, and probably just minor changes will be required. Everything else will be installed during the different steps described in the upcoming articles, while we build and tweak our pipeline.\nRemember, the big goal of our pipeline is NOT to simply pull, build and push our code, but to integrate certain security tools like code scanning, image scanning and linting. Otherwise, it would be boring.\nUsed Tools The following list of tools (or specifications) are used for our pipeline. They will be deployed when the appropriate step requires it.\nAdvanced Cluster Security\nroxctl\nSonarQube\nQuay - quay.io as public registry\nCoSign (sigstore)\nRekor (sigstore)\nSoftware Bill of Material (SBOM)\nKubeLinter\nKubeScore\nYamlLint\nkubesplit\n"},{"uri":"https://blog.stderr.at/tags/linting/","title":"Linting","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ocp/","title":"OCP","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/openshift/","title":"OpenShift","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/openshift/","title":"OpenShift","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/pipelines/","title":"Pipelines","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/pipleines/","title":"Pipleines","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/rekor/","title":"Rekor","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/sbom/","title":"SBOM","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/sigstore/","title":"Sigstore","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/sonarqube/","title":"SonarQube","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-16-securesupplychain-step1/","title":"Step 1 - Listen to Events","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain"],"description":"Step 1 Secure Supply Chain","content":" In this first step, we will simply prepare our environment to be able to retrieve calls from Git. In Git we will fork a prepared source code into a repository and any time a developer pushes a new code into our repository a webhook will notify OpenShift Pipelines to start the pipeline. Like most pipelines, the first task to be executed is to fetch the source code so it can be used for the next steps. The application I am going to use is called globex-ui and is an example webUI build with Angular.\nGoals The goals of this step are:\nCreate the EventListener and Trigger-Settings that will take care of notifications by GitHub.\nCreate a secret for GitHub authentication.\nFork a prepared source code and create a webhook inside Git.\nCreate the EventListener The first thing we need to create is a Namespace that will be responsible for all our Pipeline-objects. In this example, it is called ci:\noc new-project ci Now we need to create the so-called EventListener. This requires the creation of several objects:\nCreate a TriggerBinding: A TriggerBinding captures fields from an event and provides them as named parameters to the TriggerTemplate and subsequently to the PipelineRun.\nWe will create two TriggerBindings:\nglobex-ui - For the required settings of our example application\ngithub-push - For the relevant parameters to push into git. These parameters will be provided by Git whenever Git is using the Webhook to inform OpenShift that a new push event happened.\nCopy the following examples into your cluster.\nThe list of parameters in these manifests will be extended throughout this series. apiVersion: triggers.tekton.dev/v1beta1 kind: TriggerBinding metadata: name: globex-ui namespace: ci spec: params: - name: tlsVerify (1) value: \u0026#34;false\u0026#34; - name: gitRepoHost (2) value: github.com 1 Default values for verifying SSL is \u0026#34;false\u0026#34; (Since I do not have certificates in place) 2 The default value for the Git URL is github.com apiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerBinding metadata: name: github-push namespace: ci spec: params: - name: gitrepositoryurl (1) value: $(body.repository.clone_url) - name: fullname value: $(body.repository.full_name) - name: io.openshift.build.commit.ref value: $(extensions.ref) - name: io.openshift.build.commit.id value: $(body.head_commit.id) - name: io.openshift.build.commit.date value: $(body.head_commit.timestamp) - name: io.openshift.build.commit.message value: $(body.head_commit.message) - name: io.openshift.build.commit.author value: $(body.head_commit.author.name) 1 Several parameters, coming from Git via the Webhook, for example the exact URL to the repository, the ID of the commit, the date of the commit etc. Create a TriggerTemplate: A TriggerTemplate acts as a blueprint for PipelineRuns (or TaskRuns). The resources and parameters here will be used when our Pipeline is executed. It also defines the workspaces, that will be used by the pipeline. For now, we are using the space shared-data where we will pull the source code for further checks.\napiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerTemplate metadata: name: app-globex-ui-template namespace: ci spec: params: (1) - description: The git repository URL. name: gitrepositoryurl - description: The repository name for this PullRequest. name: fullname - description: The git branch for this PR. name: io.openshift.build.commit.ref - description: the specific commit SHA. name: io.openshift.build.commit.id - description: The date at which the commit was made name: io.openshift.build.commit.date - description: The commit message name: io.openshift.build.commit.message - description: The name of the github user handle that made the commit name: io.openshift.build.commit.author - description: The host name of the git repo name: gitRepoHost - description: Enable image repository TLS certification verification. name: tlsVerify - description: Extra parameters passed for the push command when pushing images. name: build_extra_args - description: Target image repository name name: imageRepo resourcetemplates: (2) - apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: secure-supply-chain- (3) spec: params: (4) - name: REPO_HOST value: $(tt.params.gitRepoHost) - name: GIT_REPO value: $(tt.params.gitrepositoryurl) - name: TLSVERIFY value: $(tt.params.tlsVerify) - name: BUILD_EXTRA_ARGS value: $(tt.params.build_extra_args) - name: IMAGE_REPO value: $(tt.params.imageRepo) - name: IMAGE_TAG value: \u0026gt;- $(tt.params.io.openshift.build.commit.ref)-$(tt.params.io.openshift.build.commit.id) - name: COMMIT_SHA value: $(tt.params.io.openshift.build.commit.id) - name: GIT_REF value: $(tt.params.io.openshift.build.commit.ref) - name: COMMIT_DATE value: $(tt.params.io.openshift.build.commit.date) - name: COMMIT_AUTHOR value: $(tt.params.io.openshift.build.commit.author) - name: COMMIT_MESSAGE value: $(tt.params.io.openshift.build.commit.message) pipelineRef: (5) name: secure-supply-chain serviceAccountName: pipeline (6) workspaces: (7) - name: shared-data volumeClaimTemplate: metadata: creationTimestamp: null spec: accessModes: - ReadWriteOnce resources: requests: storage: 3Gi status: {} 1 List of parameters for this TriggerTemplate, that should be used further for the pipeline. 2 The resources we are going to use. 3 The name prefix of the generated PipelineRun 4 List of parameters that shall be provided to the pipeline 5 The reference to the pipeline that shall be executed. 6 Name of the ServiceAccount that will execute the Pipeline. Per default, this is pipeline which is managed by the Operator. 7 The workspaces that will be used by the PipelineRun. Currently shared-data only. Create an EventListener that sets up a Service and listens for specific events and exposes a sink that receives incoming events, for example from a GitHub Webhook. It connects TriggerTemplate to a TriggerBinding. In this example, we create a Listener with 1 replica (that’s enough for testing) and connect our two TriggerBindings.\nWe also refer to the secret webhook-secret-globex-ui which will hold the password for GitHub to authenticate. We filter any push event coming from my Git repository tjungbauer/globex-ui\napiVersion: triggers.tekton.dev/v1alpha1 kind: EventListener metadata: name: globex-ui-event-listener namespace: ci spec: namespaceSelector: {} resources: kubernetesResource: replicas: 1 spec: template: metadata: creationTimestamp: null spec: containers: null serviceAccountName: pipeline triggers: (1) - bindings: - kind: TriggerBinding ref: globex-ui - kind: TriggerBinding ref: github-push interceptors: - params: - name: secretRef value: secretKey: webhook-secret-key secretName: webhook-secret-globex-ui (2) ref: kind: ClusterInterceptor name: github - params: - name: filter (3) value: \u0026gt;- (header.match(\u0026#39;X-GitHub-Event\u0026#39;, \u0026#39;push\u0026#39;) \u0026amp;\u0026amp; body.repository.full_name == \u0026#39;tjungbauer/globex-ui\u0026#39;) - name: overlays value: - expression: \u0026#39;body.ref.split(\u0026#39;\u0026#39;/\u0026#39;\u0026#39;)[2]\u0026#39; key: ref ref: kind: ClusterInterceptor name: cel name: build-from-push-globex-ui template: (4) ref: app-globex-ui-template 1 TriggerBindings that are used. 2 Reference to the secret. 3 A filter for push events and our repository name. 4 The TriggerTemplate that will be used. Now let us create a Route object to allow external traffic (from Git) to the EventListener.\napiVersion: route.openshift.io/v1 kind: Route metadata: name: el-event-listener namespace: ci spec: port: targetPort: http-listener to: kind: Service name: el-globex-ui-event-listener (1) weight: 100 tls: termination: edge insecureEdgeTerminationPolicy: Redirect wildcardPolicy: None 1 Service that will be automatically created when the EventListener has been created. And finally, we create a Secret to allow GitHub to authenticate. The name of the Secret is referenced inside the EventListener object.\nkind: Secret apiVersion: v1 metadata: name: webhook-secret-globex-ui (1) namespace: ci stringData: webhook-secret-key: yoursecret (2) type: Opaque 1 Name as referenced in the EventListener 2 Your super secure password Prepare GitHub Now we have everything in place to prepare our source code in Git. All we need to do is to create a repository that holds our source code and a Webhook.\nFork the Source Code: https://github.com/redhat-gpte-devopsautomation/globex-ui\nWhy fork? I want to be able to update the files and trigger the Pipeline whenever I want to. My forked repository can be found at: https://github.com/tjungbauer/globex-ui\nCreate a Webhook in GitHub. Go to Settings \u0026gt; Webhooks and add a new Webhook using:\nFigure 1. Create a new Webhook. The Route URL that was created.\nContent type: application/json.\nYour Password as used in the secret above.\nEnable or disable SSL verification, since I was too lazy to create a certificate at my demo cluster, I disabled it.\nAnd select which events, shall be sent to the Listener. In our case, push events are just fine.\nAfter a few seconds GitHub should have validated the Webhook (reload the page eventually)\nFigure 2. Verify Webhook Summary That’s it, we now have a Git repository, that will send any push-event to the EventListener, which uses the Triggers to fill out any required parameters and starts the pipeline named: secure-supply-chain.\nThis pipeline does not exist yet and will be created in the next step together with its first task to pull from the Git repository.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-17-securesupplychain-step2/","title":"Step 2 - Pipelines","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain"],"description":"Step 2 Secure Supply Chain","content":" We will now create the Pipeline and try to trigger it for the first time to verify if our Webhook works as intended.\nGoals The goals of this step are:\nCreate the Pipeline with a first task\nUpdate the Github repository, to verify if the Webhook works\nVerify if the PipelineRun is successful\nCreate the Pipeline The Pipeline object is responsible to define the Tasks (steps) that should be executed. Whenever a Pipeline is started a PipelineRun is created that performs each defined Task in the defined order and logs the output. Tasks can run subsequently or in parallel.\nCurrently, the Pipeline has one task pull-source-code which is defined as a ClusterTask \u0026#34;git-clone\u0026#34;. The purpose is to simply pull the source code to the workspace \u0026#34;shared-data\u0026#34;.\napiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: secure-supply-chain (1) namespace: ci spec: params: (2) - name: REPO_HOST type: string - name: COMMIT_SHA type: string - name: TLSVERIFY type: string - name: BUILD_EXTRA_ARGS type: string - name: IMAGE_REPO type: string - name: IMAGE_TAG type: string - name: GIT_REF type: string - name: COMMIT_DATE type: string - name: COMMIT_AUTHOR type: string - name: COMMIT_MESSAGE type: string - name: GIT_REPO type: string tasks: (3) - name: pull-source-code (4) params: - name: url (5) value: $(params.GIT_REPO) - name: revision value: $(params.GIT_REF) - name: deleteExisting value: \u0026#39;true\u0026#39; taskRef: (6) kind: ClusterTask name: git-clone workspaces: (7) - name: output workspace: shared-data workspaces: (8) - name: shared-data 1 Name of the Pipeline as referenced in the TriggerTemplate. 2 List of Parameters, hopefully, injected by the EventListener. 3 List of Tasks that will be executed. 4 Name of the Task. 5 Parameters used in this Task. 6 The Reference to the task. Here a ClusterTask named \u0026#34;git-clone\u0026#34; is used. 7 Workspace that shall be used in this Task. 8 Workspaces available in this Pipeline. The initial Pipeline will now look like the following (Go to: Pipelines \u0026gt; Pipelines \u0026gt; secure-supply-chain)\nFigure 1. Initial Pipeline Our first Run Now it is time to update something in our Git Repository and verify if everything can be executed successfully.\nTo update, it is enough to simply add a space in the README.md file and push it to Git.\nIf the Webhook works as expected, Git will notify our EventListener, which will then trigger the Pipeline. A PipelineRun is created, that executes all Tasks that are defined in the Pipeline (currently just 1)\nYou can monitor the progress of the PipelineRun:\nFigure 2. PipelineRun Overview On the Details-page you can see which step is currently executed:\nFigure 3. PipelineRun Details Eventually, the PipelineRun finishes successfully.\nFigure 4. PipelineRun Finished You can analyze the Logs in case of an Error or to get more details of a certain Task:\nFigure 5. Task Logs Summary We have now created our first Pipeline and tested the GitHub Webhook. Whenever we push changes to the code, Git will notify the EventListener which will trigger the Pipeline with all required Parameters.\nA PipelineRun is generated and is executing the defined Tasks. Currently, not much is done, expect cloning the Git repository.\nIn the next steps, we will evolve our Pipeline to perform security checks and sign our image.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-18-securesupplychain-step3/","title":"Step 3 - SonarQube","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","SonarQube"],"description":"Step 3 Secure Supply Chain","content":" After the Pipeline has been created and tested we will add another Task to verify the source code and check for possible security issues, leveraging the tool SonarQube by Sonar.\nGoals The goals of this step are:\nInstall and configure SonarQube\nAdd a Task to scan our source code for vulnerabilities\nVerify the results.\nSonarQube SonarQube by Sonar helps developers to deliver clean code. With the integration into our CICD pipeline it will detect issues and reports them back to the developers. The results will be shown in a dashboard. We will install the Community version of SonarQube, which is enough for our showcase.\nSonarQube Installation To install SonarQube I have prepared a Helm Chart that I use with GitOps when I deploy a new lab environment. Feel free to use it. It simply calls the Chart that is provided by Sonar. In addition, it creates a Job that changes the default administrator password to a different one.\nThe values file is good to go, the only item you must change is the route in the very first line. Also, if you prefer not to deploy any plugins (for example, the German language pack), you can remove the appropriate line.\nBefore you run the Helm, you need to provide a Sealed Secret (or manually create a Secret) like the following:\nkind: Secret apiVersion: v1 metadata: name: credentials namespace: sonarqube data: adminpass: \u0026lt;your base64 password string\u0026gt; type: Opaque This password will be used by the Job \u0026#34;change-admin-password\u0026#34; and will configure a new password for the user \u0026#34;admin\u0026#34;\nOnce everything is installed (this will take several minutes), you can access SonarQube using the URL you defined in the values file.\nFigure 1. SonarQube SonarQube Create Token Our Pipeline will talk to SonarQube and request a scan of the source code. To be able to do this, we need to create a Token in SonarQube and store it as a Secret in our ci namespace.\nClick on \u0026#34;My Account\u0026#34; (Upper right corner) \u0026gt; Security and create a new token:\nFigure 2. SonarQube Token Copy the token and create the following Secret:\nkind: Secret apiVersion: v1 metadata: name: globex-ui-sonarqube-secret namespace: ci stringData: token: \u0026lt;your SonarQube Token\u0026gt; (1) type: Opaque 1 The generated Token NOT base64 encrypted (I am using stringData in this case) Modify the Pipeline Now it is time to bring the SonarQube scan task into our Pipeline. This requires some modifications to existing objects and the creation of some new ones.\nModify the TriggerBinding.\nAdd the following lines to globex-ui TriggerBinding\n- name: sonarqubeHostUrl value: http://sonarqube.apps.ocp.aws.ispworld.at/ (1) 1 The URL of SonarQube So, it will look like this:\napiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerBinding metadata: name: globex-ui namespace: ci spec: params: - name: tlsVerify value: \u0026#39;false\u0026#39; - name: gitRepoHost value: github.com - name: sonarqubeHostUrl value: http://sonarqube.apps.ocp.aws.ispworld.at/ (1) 1 The URL of SonarQube Modify the TriggerTemplate\nAdd the following lines:\nspec: params: (1) - description: Sonarqube host url name: sonarqubeHostUrl ... resourcetemplates: - apiVersion: tekton.dev/v1beta1 spec: params: (2) - name: SONARQUBE_HOST_URL value: $(tt.params.sonarqubeHostUrl) - name: SONARQUBE_PROJECT_KEY value: globex-ui (3) - name: SONARQUBE_PROJECT_SECRET value: globex-ui-sonarqube-secret (4) ... 1 Parameters provided by the TriggerBinding. 2 Parameters provided to the Pipeline. 3 Project that will be created in SonarQube. 4 Secret of the SonarQube token. The result should look like the following:\napiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerTemplate metadata: name: app-globex-ui-template namespace: ci spec: params: - description: The git repository URL. name: gitrepositoryurl - description: The repository name for this PullRequest. name: fullname - description: The git branch for this PR. name: io.openshift.build.commit.ref - description: the specific commit SHA. name: io.openshift.build.commit.id - description: The date at which the commit was made name: io.openshift.build.commit.date - description: The commit message name: io.openshift.build.commit.message - description: The name of the github user handle that made the commit name: io.openshift.build.commit.author - description: The host name of the git repo name: gitRepoHost - description: Enable image repository TLS certification verification. name: tlsVerify - description: Extra parameters passed for the push command when pushing images. name: build_extra_args - description: Target image repository name name: imageRepo - description: Sonarqube host url name: sonarqubeHostUrl resourcetemplates: - apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: secure-supply-chain- spec: params: - name: REPO_HOST value: $(tt.params.gitRepoHost) - name: GIT_REPO value: $(tt.params.gitrepositoryurl) - name: TLSVERIFY value: $(tt.params.tlsVerify) - name: BUILD_EXTRA_ARGS value: $(tt.params.build_extra_args) - name: IMAGE_REPO value: $(tt.params.imageRepo) - name: IMAGE_TAG value: \u0026gt;- $(tt.params.io.openshift.build.commit.ref)-$(tt.params.io.openshift.build.commit.id) - name: COMMIT_SHA value: $(tt.params.io.openshift.build.commit.id) - name: GIT_REF value: $(tt.params.io.openshift.build.commit.ref) - name: COMMIT_DATE value: $(tt.params.io.openshift.build.commit.date) - name: COMMIT_AUTHOR value: $(tt.params.io.openshift.build.commit.author) - name: COMMIT_MESSAGE value: $(tt.params.io.openshift.build.commit.message) - name: SONARQUBE_HOST_URL value: $(tt.params.sonarqubeHostUrl) - name: SONARQUBE_PROJECT_KEY value: globex-ui - name: SONARQUBE_PROJECT_SECRET value: globex-ui-sonarqube-secret pipelineRef: name: secure-supply-chain serviceAccountName: pipeline workspaces: - name: shared-data volumeClaimTemplate: metadata: creationTimestamp: null spec: accessModes: - ReadWriteOnce resources: requests: storage: 3Gi status: {} Create the Task scan-source. This task will use the pulled source code and uses SonarQube to let it scan our code.\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: scan-code namespace: ci spec: description: \u0026gt;- Source code scan using sonar-scanner and SonarQube. params: - default: \u0026#39;docker.io/sonarsource/sonar-scanner-cli:latest\u0026#39; (1) name: scanImage type: string - default: \u0026#39;https://sonarqube-sonarqube.myplaceholder.com/\u0026#39; (2) name: sonarqubeHostUrl type: string - default: object-detection-rest name: sonarqubeProjectKey type: string - default: object-detection-rest-sonarqube-secret name: sonarqubeProjectSecret type: string - default: \u0026#39;true\u0026#39; name: verbose type: string steps: - env: - name: SONAR_TOKEN_WEB_UI (3) valueFrom: secretKeyRef: key: token name: $(params.sonarqubeProjectSecret) (4) image: $(params.scanImage) name: scan-code resources: {} script: \u0026gt; (5) set -x echo $(ls -a) sonar-scanner -X -Dsonar.projectKey=$(params.sonarqubeProjectKey) -Dsonar.sources=./ -Dsonar.host.url=$(params.sonarqubeHostUrl) -Dsonar.login=$SONAR_TOKEN_WEB_UI workingDir: /workspace/repository workspaces: (6) - name: repository 1 Image containing SonarQube command line tool. The cluster must be able to connect to docker.io. 2 Default parameters for this Task that might be overwritten. 3 The Secret with the token. 4 Parameter as set by the PipelineRun which gets the value from the TriggerTemplate. 5 Script that is executed to scan the source code. 6 The workspace where we can find the source code. Update your Pipeline and add the following task:\nspec: params: ... - name: SONARQUBE_HOST_URL type: string - name: SONARQUBE_PROJECT_KEY type: string - name: SONARQUBE_PROJECT_SECRET type: string tasks: ... - name: scan-source params: (1) - name: sonarqubeHostUrl value: $(params.SONARQUBE_HOST_URL) - name: sonarqubeProjectKey value: $(params.SONARQUBE_PROJECT_KEY) - name: sonarqubeProjectSecret value: $(params.SONARQUBE_PROJECT_SECRET) runAfter: (2) - pull-source-code taskRef: (3) kind: Task name: scan-code workspaces: (4) - name: repository workspace: shared-data 1 Parameters that shall be provided for the Task. 2 The task should run AFTER the source has been pulled …​ which makes sense. 3 Reference to the Task we created above. 4 Workspace shared-data where the source code was pulled from the previous Task. The full pipeline objects now look like the following:\nExpand me... apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: secure-supply-chain namespace: ci spec: params: - name: REPO_HOST type: string - name: COMMIT_SHA type: string - name: TLSVERIFY type: string - name: BUILD_EXTRA_ARGS type: string - name: IMAGE_REPO type: string - name: IMAGE_TAG type: string - name: GIT_REF type: string - name: COMMIT_DATE type: string - name: COMMIT_AUTHOR type: string - name: COMMIT_MESSAGE type: string - name: GIT_REPO type: string - name: SONARQUBE_HOST_URL type: string - name: SONARQUBE_PROJECT_KEY type: string - name: SONARQUBE_PROJECT_SECRET type: string tasks: - name: pull-source-code params: - name: url value: $(params.GIT_REPO) - name: revision value: $(params.GIT_REF) - name: deleteExisting value: \u0026#39;true\u0026#39; taskRef: kind: ClusterTask name: git-clone workspaces: - name: output workspace: shared-data - name: scan-source params: - name: sonarqubeHostUrl value: $(params.SONARQUBE_HOST_URL) - name: sonarqubeProjectKey value: $(params.SONARQUBE_PROJECT_KEY) - name: sonarqubeProjectSecret value: $(params.SONARQUBE_PROJECT_SECRET) runAfter: - pull-source-code taskRef: kind: Task name: scan-code workspaces: - name: repository workspace: shared-data workspaces: - name: shared-data The Pipeline now has a second task:\nFigure 3. Pipeline Execute the Pipeline Let’s update the README.md of our source code again to trigger another PipelineRun. After the code has been pulled it should now perform the second task and scan the quality of the source code.\nYou can monitor the progress of the PipelineRun again:\nFigure 4. PipelineRun Details Once the PipelineRun executed both tasks successfully, we can check SonarQube.\nThe project globex-ui has been created which shows the results of our scan:\nFigure 5. SonarQube Results Summary We have now added a Task to our Pipeline that performs a code analysis of our source code. The results are shown in SonarQube and the developers can react accordingly.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-19-securesupplychain-step4/","title":"Step 4 - Verify Git Commit","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain"],"description":"Step 4 Secure Supply Chain","content":" Besides checking the source code quality, we should also verify if the commit into Git was done by someone/something we trust. It is a good practice to sign all commits to Git. You need to prepare your Git account and create trusted certificates.\nI will not describe how exactly you need to configure Git to sign your commit. Verify the following link to learn more about Signing Commits Goals The goals of this step are:\nVerify if the last commit has been signed\nPrerequisites Signing public key\nConfigured Git to verify your gpg signature\nWhen your commit is signed, Git will show that:\nFigure 1. Pipeline Steps Create the following Secret that contains your PUBLIC key.\nkind: Secret apiVersion: v1 metadata: name: gpg-public-key namespace: ci data: public.key: \u0026gt;- \u0026lt;Base64 PUBLIC GPG KEY\u0026gt; (1) type: Opaque 1 Public key, containing BEGIN/END lines base64 encoded. Create the following Task:\napiVersion: tekton.dev/v1 kind: Task metadata: name: verify-source-code-commit-signature namespace: ci spec: description: This task verifies the latest commit and signature against the gpg public key steps: - computeResources: {} image: registry.redhat.io/openshift-pipelines/pipelines-git-init-rhel8:v1.10.4-4 name: git-verify script: | set -x (1) gpg --import /workspace/secrets/public.key git config --global --add safe.directory /workspace/repository git verify-commit HEAD || (echo \u0026#34;Unable to verify commit at HEAD!\u0026#34; \u0026amp;\u0026amp; exit 1) workingDir: /workspace/repository workspaces: - name: repository - name: secrets (2) 1 The script to verify the signature of the commit, 2 The workspace that mounts the Secret containing the gpg key, Modify the TriggerTemplate and add the following 3 lines\nworkspaces: ... - name: secrets secret: secretName: gpg-public-key (1) 1 The name of the Secret where the public key can be found. Update the pipeline to execute the task verify-commit-signature, which is running in parallel to the SonarQube scan.\n- name: verify-commit-signature runAfter: - pull-source-code (1) taskRef: kind: Task name: verify-source-code-commit-signature (2) workspaces: (3) - name: repository workspace: shared-data - name: secrets workspace: secrets workspaces: ... - name: secrets (4) 1 This task runs after pull-source-code but in parallels with the SonarQube task. 2 Task reference 3 Workspaces that are used in this Task 4 Additional workspace for the Pipeline The full pipeline objects now look like the following:\nExpand me... apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: secure-supply-chain namespace: ci spec: params: - name: REPO_HOST type: string - name: COMMIT_SHA type: string - name: TLSVERIFY type: string - name: BUILD_EXTRA_ARGS type: string - name: IMAGE_REPO type: string - name: IMAGE_TAG type: string - name: GIT_REF type: string - name: COMMIT_DATE type: string - name: COMMIT_AUTHOR type: string - name: COMMIT_MESSAGE type: string - name: GIT_REPO type: string - name: SONARQUBE_HOST_URL type: string - name: SONARQUBE_PROJECT_KEY type: string - name: SONARQUBE_PROJECT_SECRET type: string tasks: - name: pull-source-code params: - name: url value: $(params.GIT_REPO) - name: revision value: $(params.GIT_REF) - name: deleteExisting value: \u0026#39;true\u0026#39; taskRef: kind: ClusterTask name: git-clone workspaces: - name: output workspace: shared-data - name: scan-source params: - name: sonarqubeHostUrl value: $(params.SONARQUBE_HOST_URL) - name: sonarqubeProjectKey value: $(params.SONARQUBE_PROJECT_KEY) - name: sonarqubeProjectSecret value: $(params.SONARQUBE_PROJECT_SECRET) runAfter: - pull-source-code taskRef: kind: Task name: scan-code workspaces: - name: repository workspace: shared-data - name: verify-commit-signature runAfter: - pull-source-code taskRef: kind: Task name: verify-source-code-commit-signature workspaces: - name: repository workspace: shared-data - name: secrets workspace: secrets workspaces: - name: shared-data - name: secrets The status of the Pipeline now is:\nFigure 2. Pipeline Execute the Pipeline Let’s update the README.md of our source code again to trigger another PipelineRun.\nNow the 3rd task will verify if the commit was signed.\nFigure 3. PipelineRun Details In the logs of the Task, we can see that the commit was signed and could be verified. See:\n... gpg: Good signature from \u0026#34;Thomas Jungbauer \u0026lt;tjungbau@redhat.com\u0026gt;\u0026#34; ... Figure 4. Signature Verification Summary At this stage we have a Pipeline, that pulls our code, does a code analysis, and verifies if the commit has been signed. The very next step is to build the image and push it into an Image Registry.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-20-securesupplychain-step5/","title":"Step 5 - Build and Sign Image","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","Sigstore"],"description":"Step 5 Secure Supply Chain","content":" Finally, after pulling and checking the code, we are going to create the image. During this process the image will be signed and uploaded to the public registry Quay.io.\nGoals The goals of this step are:\nBuild the image\nSign the image\nUpload it Quay.io\nYou can use any registry you like. Prerequisites You need to have an account in Quay.io and create a repository there.\nI have created the repository https://quay.io/repository/tjungbau/secure-supply-chain-demo which will contain the images and the signatures.\nSteps Create a Robot account in Quay.io and get the authentication json. Go to \u0026#34;Account Settings \u0026gt; Robot Accounts\u0026#34; and create a new account.\nFigure 1. Quay Robot Account Allow this account WRITE permissions to the repository\nFigure 2. Quay Robot Account Permissions Select the just-created account and go to Kubernetes Secret.\nHere you can view or download the Secret. It should look like the following:\napiVersion: v1 kind: Secret metadata: name: tjungbau-secure-supply-chain-demo-pull-secret data: .dockerconfigjson: \u0026lt;SECURE\u0026gt; type: kubernetes.io/dockerconfigjson Copy the content and store it in your ci Namespace.\nThe name will be probably different in other examples. To be able for the ServiceAccount, which is executing the Pipelines, to use this Secret, we need to modify the ServiceAccount object.\nIn our case the ServiceAccount is called pipeline. Modify it and add the following lines:\nsecrets: ... - name: tjungbau-secure-supply-chain-demo-pull-secret (1) imagePullSecrets: - name: tjungbau-secure-supply-chain-demo-pull-secret (2) 1 Use your appropriate name for the secret 2 Use your appropriate name for the secret, required to be able to sign the image Update the Pipeline object and add the following block\nThis time we do not need to add a Task object, because we are using a ClusterTask \u0026#34;buildah\u0026#34; that takes care of the building process. As workspace \u0026#34;shared-data\u0026#34; is used again, which has all the source code stored:\n- name: build-sign-image params: - name: TLSVERIFY value: $(params.TLSVERIFY) - name: BUILD_EXTRA_ARGS value: \u0026gt;- --label=io.openshift.build.commit.author=\u0026#39;$(params.COMMIT_AUTHOR)\u0026#39; --label=io.openshift.build.commit.date=\u0026#39;$(params.COMMIT_DATE)\u0026#39; --label=io.openshift.build.commit.id=\u0026#39;$(params.COMMIT_SHA)\u0026#39; --label=io.openshift.build.commit.message=\u0026#39;$(params.COMMIT_MESSAGE)\u0026#39; --label=io.openshift.build.commit.ref=\u0026#39;$(params.GIT_REF)\u0026#39; --ulimit=nofile=4096:4096 - name: IMAGE value: \u0026#39;$(params.IMAGE_REPO):$(params.IMAGE_TAG)\u0026#39; retries: 1 runAfter: - scan-source - verify-commit-signature taskRef: kind: ClusterTask name: buildah workspaces: - name: source workspace: shared-data Update the TriggerBinding globex-ui and define the URL for your registry respectively for your image repository\nspec: parameters: - name: imageRepo value: quay.io/tjungbau/secure-supply-chain-demo With this configuration, we could already build the image and push it into the registry. However, we also would like to sign it. Prepare Tekton for Image Signing To automatically sign images, we use TektonChains which is a controller that allows you to manage your supply chain security by signing TaskRuns and/or OCI images. Once the build-Task has pushed the images to Quay, Tekton will detect this event (by monitoring specific values of Task results) and then creates and pushes the signature for that image.\nBy default, Tekton Chains observes all task run executions. When the task runs complete, Tekton Chains signs and stores all artefacts.\nTekton Chain is part of OpenShift Pipelines and has the following main features:\nYou can sign task runs, task run results, and OCI registry images with cryptographic keys that are generated by tools such as cosign and skopeo.\nYou can use attestation formats such as in-toto.\nYou can securely store signatures and signed artefacts using OCI repository as a storage backend.\nIn our case, we will use CoSign for signing the images and Rekor for the attestation.\nTo activate image signing add the following to the TektonConfig object by removing the \u0026#34;chain{}\u0026#34; entry.\nchain: artifacts.oci.storage: oci (1) artifacts.taskrun.format: in-toto (2) artifacts.taskrun.storage: oci (3) transparency.enabled: true (4) 1 The storage backend for storing OCI signatures. 2 The format for storing TaskRun payloads. Can be in-toto or slsa/v1. 3 The storage backend for TaskRun signatures. Multiple can be defined. 4 Enable or disable automatic binary transparency uploads. The URL for uploading binary transparency attestations is https://rekor.sigstore.dev by default. More details can be found at https://docs.openshift.com/container-platform/4.13/cicd/pipelines/using-tekton-chains-for-openshift-pipelines-supply-chain-security.html CoSign - Signing the Image We will use CoSign to sign our image. To do so we need to download the cosign binary and generate a key pair:\nBe sure that you are logged into the OpenShift cluster. cosign generate-key-pair k8s://openshift-pipelines/signing-secrets Enter password for private key: Enter password for private key again: Successfully created secret signing-secrets in namespace openshift-pipelines oc get secrets signing-secrets -n openshift-pipelines NAME TYPE DATA AGE signing-secrets Opaque 3 46h Cosign will ask you to enter a password and will then create a Kubernetes secret in the Namespace openshift-pipelines.\nWhen OpenShift Pipelines now execute a Task that is pushing an image, this Secret will be used to sign the image.\nLet’s update our source code. The image will be built and pushed into Quay. The small black shield indicates that this image has been signed.\nFigure 3. Quay Signed Image Besides the actual image the files: *.sig and *.att can be seen. The first one is the signature, the second one shows metadata about the attestation retrieved from Rekor which can be compared with the Rekor URL.\nRekor Rekor’s goals are to provide an immutable tamper resistant ledger of metadata generated within a software projects supply chain. Rekor will enable software maintainers and build systems to record signed metadata to an immutable record. Other parties can then query said metadata to enable them to make informed decisions on trust and non-repudiation of an object’s lifecycle. For more details visit the sigstore website.\nThe Rekor project provides a restful API based server for validation and a transparency log for storage. A CLI application is available to make and verify entries, query the transparency log for inclusion proof, integrity verification of the transparency log or retrieval of entries by either public key or artifact. Rekor Git\nAn important feature of Tekton Chains is that it integrates seamlessly with an application called Rekor. The configuration transparency.enabled: true enables the call to the Rekor API that can be found by default at: https://rekor.sigstore.dev. This will create a transparency log which can be used for verification purposes later.\nOf course, it would be possible to install your own server. Learn in the official Docs how this can be achieved. This means every time our build-Tasks successfully completes a URL to the transparency logs will be attached to the Task.\nAfter our PipelineRuns were successful, we can verify if the image has an entry in the Rekor transparency log. This is important to ensure that the image went through a valid signing process. At a later step (before we create the pull request for the production update) we will verify if the log exists.\nThe following annotations should have been added to the TaskRun secure-supply-chain-XXX-build-sign-image automatically:\nmetadata: annotations: chains.tekton.dev/signed: \u0026#39;true\u0026#39; chains.tekton.dev/transparency: \u0026#39;https://rekor.sigstore.dev/api/v1/log/entries?logIndex=25593495\u0026#39; This created the following transparency logs:\n{ \u0026#34;24296fb24b8ad77a7ae84f4b950336d1872a066aeb9463dfbc231a7d3b73dd040dd5faefb3c013a7\u0026#34;: { \u0026#34;body\u0026#34;: \u0026#34;eyJhcGlWZXJzaW9uIjoiMC4wLjEiLCJraW5kIjoiaGFzaGVkcmVrb3JkIiwic3BlYyI6eyJkYXRhIjp7Imhhc2giOnsiYWxnb3JpdGhtIjoic2hhMjU2IiwidmFsdWUiOiJmYzE2ODM3Mzc1ODUyNjc0MTQ4MjIyMDJhMWU2Y2RkZDkxNWI4NWUzYzhhNDVhZmI0YzEyYmE2YmNhMGNmNDQyIn19LCJzaWduYXR1cmUiOnsiY29udGVudCI6Ik1FWUNJUUNRMnlnU2ZGTllHS2pzbXRIYjVielAwdlRNMFgyNHVlNXlKbjJxdWFWSyt3SWhBSmhoSWpweEFybDJERjFsVmpMT0ZzVnRhMzJmbXJXRmxXWkdRQ015b2xrayIsInB1YmxpY0tleSI6eyJjb250ZW50IjoiTFMwdExTMUNSVWRKVGlCUVZVSk1TVU1nUzBWWkxTMHRMUzBLVFVacmQwVjNXVWhMYjFwSmVtb3dRMEZSV1VsTGIxcEplbW93UkVGUlkwUlJaMEZGVEc0clduaENNMHNyS3pWSk1pOWlNWFJqVFVKcVZXODRjWGx6YVFwNlQzUlpVbGRQUTBwTVRWQlNWVGR4WTFJeVMyWlZZazAzYlVwUlNtbHZjbXR6VW1KUmNHMTBTekV4WjJNM1pIVkZObGg0WmxOdlpWUkJQVDBLTFMwdExTMUZUa1FnVUZWQ1RFbERJRXRGV1MwdExTMHRDZz09In19fX0=\u0026#34;, \u0026#34;integratedTime\u0026#34;: 1688060588, \u0026#34;logID\u0026#34;: \u0026#34;c0d23d6ad406973f9559f3ba2d1ca01f84147d8ffc5b8445c224f98b9591801d\u0026#34;, \u0026#34;logIndex\u0026#34;: 25593495, \u0026#34;verification\u0026#34;: { \u0026#34;inclusionProof\u0026#34;: { \u0026#34;checkpoint\u0026#34;: \u0026#34;rekor.sigstore.dev - 2605736670972794746\\n21430853\\n10sx5mwekx2c8jltWbt0WeJu+tNM9FOBNQoyPiC3CPQ=\\nTimestamp: 1688061275289390813\\n\\n— rekor.sigstore.dev wNI9ajBFAiEAyzFYKKghB+RQsbMIIzjywdA9vFjsusQoMWMUJPRjw3ECIDVeYSGKn7zwXWxGLhQlIGAI1Ca0Gu7ZuLJ64xR3SrY0\\n\u0026#34;, \u0026#34;hashes\u0026#34;: [ \u0026#34;5cc5dba1f5f420acc9ed049c77b04c4fdd66cf6ef1ebd46b66e26039bb9ba06c\u0026#34;, \u0026#34;3041520c9fec6177225063053503f6d20b09e86b72968b737e3211a233dab6ce\u0026#34;, \u0026#34;ddb57132bba122df920d4816af7ac02bef03aec533cc7b45f8552ce19c023d10\u0026#34;, \u0026#34;8a510da356706a0a822daf3c3a935176d3634c6e0cce6830fe90a8771a3bc83d\u0026#34;, \u0026#34;7d254234192620827306cc5024ffb8ae699bfe2725c8e6aa13757f5471d416d4\u0026#34;, \u0026#34;648a4e45da960f4ad286dfa22ae93721c9ba82ee05edd5a7134d9b19e35735e0\u0026#34;, \u0026#34;9dcf0f21690ec420bffbc1d10c383b7d3dc568d26bd8fd9e8771abb01f5976dc\u0026#34;, \u0026#34;86575ffec011d78ad393e7af267693c0c59360e2299b308369951d8362032733\u0026#34;, \u0026#34;0088ef1f6ec1e992fa6c91837287f2bd7eea238e5af7467e21e6df0dc8efe516\u0026#34;, \u0026#34;b149d95903a4e554ac1c381f1afff31bb62b6e12b6b2fc95f36b8e198e1a42cd\u0026#34;, \u0026#34;de73a694862cebd660ef1cecdd1bb66e273ab0651ca939bf7fa6f5f567bafe93\u0026#34;, \u0026#34;a3c84734ddae3102952584443dea70e3dec2cc085af7cf0ba3530751966861ec\u0026#34;, \u0026#34;0be5c7bbcf481d1efcfc63a27fce447cf6345f7bb3155cf5de39de592c16d52d\u0026#34;, \u0026#34;f597f4bae8df3d6fc6eebfe3eabd7d393e08781f6f16b42398eca8512398fff1\u0026#34;, \u0026#34;4e35fcb3c0a59e7f329994002b38db35f5d511f499ba009e10b31f5d27563607\u0026#34;, \u0026#34;47044b7ac3aab820e44f0010538d7de71e17a11f4140cbbe9eeb37f78b77cc7d\u0026#34;, \u0026#34;eee63677e2591eefe06ab537d6dd1b4060770682744c8287879b5dcd3365a5b2\u0026#34;, \u0026#34;ff41aa21106dbe03996b4335dd158c7ffafd144e45022193de19b2b9136c3e42\u0026#34;, \u0026#34;e6ebdeef2e23335d8d7049ba5a0049a90593efdfe9c1b4548946b44a19d7214f\u0026#34;, \u0026#34;dd51e840e892d70093ad7e1db1e2dea3d50334c7345d360e444d22fc49ed9f5e\u0026#34;, \u0026#34;ad712c98424de0f1284d4f144b8a95b5d22c181d4c0a246518e7a9a220bdf643\u0026#34; ], \u0026#34;logIndex\u0026#34;: 21430064, \u0026#34;rootHash\u0026#34;: \u0026#34;d74b31e66c1e931d9cf2396d59bb7459e26efad34cf45381350a323e20b708f4\u0026#34;, \u0026#34;treeSize\u0026#34;: 21430853 }, \u0026#34;signedEntryTimestamp\u0026#34;: \u0026#34;MEYCIQDqmqb4k95FjiBNogOAmjTskkIPaGslgvrSND4pQdUALwIhAMt85yLsa5Ei+3NsmJ906/T9Hx1YZDDHQfHlTEYqqcaE\u0026#34; } } } Summary Finally, we have a signed image uploaded to Quay. In addition, a transparency log has been created. Now let us add some security checks in the next steps.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-21-securesupplychain-step6/","title":"Step 6 - Scanning with ACS","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","ACS","Advanced Cluster Security"],"description":"Step 6 Secure Supply Chain","content":" In this step we will install Advanced Cluster Security (ACS) and create 2 new steps in our Pipeline to scan the image for vulnerabilities and security policy. A custom security policy, configured in ACS, will verify if the image is signed.\nGoals The goals of this step are:\nInstall ACS.\nCreate a custom security policy into ACS.\nConfigure CoSign integration in ACS.\nCreate a step in the Pipeline to perform an image scan (vulnerabilities).\nCreate a step in the Pipeline to perform an image check (security policies).\nInstall Advanced Cluster Security (ACS) We will install ACS on our cluster which will be called local-cluster inside ACS.\nThe deployment will happen via Argo CD (OpenShift GitOps) that will use the following Helm Chart: RHACS\nThe Argo CD Application will trigger the deployment which will take several minutes.\nThe deployment will install a minimal version of ACS and tries to limit the required resources and replicas. For production environments, you probably want to increase the settings. apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: in-cluster-init-rhacs namespace: openshift-gitops spec: destination: namespace: default server: \u0026#39;https://kubernetes.default.svc\u0026#39; (1) info: - name: Description value: \u0026gt;- Initialize Red Hat Advanced Cluster Security and deploy Central and SecuredCluster project: in-cluster source: path: charts/rhacs-full-stack (2) repoURL: \u0026#39;https://github.com/tjungbauer/helm-charts\u0026#39; (3) targetRevision: main 1 Installing on the local cluster. 2 Path to the Helm chart. 3 URL to the Helm chart repository. Figure 1. Syncing ACS Deployment Application This GitOps Application will start a full-stack ACS deployment. It does (in this exact order):\nInstalls the ACS Operator into rhacs-operator\nVerifies if the Operator is ready\nCreates the Namespace stackrox\nAdds a ConsoleLink in the upper right action menu of OpenShift\nFigure 2. ACS ConsoleLink Creates the Central custom resource with minimum resources\nCreates an init-bundle to add the first (local) cluster\nAdds the custom resource SecureCluster to install the rest of the ACS components\nAnd finally, runs a basic configuration that enables authentication via OpenShift and provides the kubeadmin user admin privileges. This can be disabled in the values file if you prefer not to configure this, or if kubeadmin does not exist anymore in your cluster.\nThe whole process will take a while and uses syncwaves and hooks.\nConfigure ACS Integration Generate Authentication Token Create an authentication Token in ACS\nGo to \u0026#34;Platform Configuration \u0026gt; Integrations \u0026gt; API Token\u0026#34;\nFigure 3. ACS Generate Token Generate a new Token with at least Continuous Integration privileges and save the created Token.\nBack in OpenShift, a Secret must be created with the key rox_api_token\nkind: Secret apiVersion: v1 metadata: name: stackrox-secret namespace: ci data: rox_api_token: \u0026lt;base 64 Token\u0026gt; (1) type: Opaque 1 Base64 decoded ACS Token. Create a 2nd Secret that contains the ACS endpoint and its port:\nkind: Secret apiVersion: v1 metadata: name: stackrox-endpoint namespace: ci stringData: rox_central_endpoint: central-stackrox.apps.ocp.aws.ispworld.at:443 (1) type: Opaque 1 The endpoint URL of my example cluster. Note: the port MUST be added here. Integrate CoSign with ACS Configure CoSign Integration\nDuring Step 5 we have created a CoSign key pair to sign our images. To integrate with ACS, we need to retrieve the public key. In OpenShift, open the Secret signing-secrets in the Namespace openshift-pipelines and extract the key cosign.pub.\nIt will look something like tjis:\n-----BEGIN PUBLIC KEY----- key... -----END PUBLIC KEY----- In ACS, go to \u0026#34;Platform Configuration \u0026gt; Integrations \u0026gt; Signature\u0026#34; and create a new CoSign integration.\nFigure 4. ACS CoSign Integration Enter a name and the public key and activate this integration\nFigure 5. ACS Create CoSign Integration This will enable ACS to verify the CoSign signature of the image.\nCreate a Custom Policy that verifies the Signature Create a custom security policy, that verifies if our image has been signed or not.\nTo be sure that every image is correctly signed, we create a custom security policy that verifies this signature using our CoSign integration. This policy can be configured as inform or enforce. Enforce means that the pipeline will fail (during the Task acs-image-check) if the image signature cannot be checked.\nLet’s create our policy by hand for this time. However, it is also possible to export/import rules. The important part here is that link to the CoSign integration is valid, otherwise ACS cannot verify the signature.\nOpen \u0026#34;Platform Configuration \u0026gt; Policy \u0026gt; and click the button Create Policy\u0026#34;\nName: Trusted Signature Policy\nSeverity: High\nCategories: Security Best Practices\nClick Next\nLifecycle stage: check Build and Deploy\nResponse method: Inform and enforce \u0026gt; This will make the Task in the Pipeline fail when the signature cannot be verified.\nConfigure enforcement behavior:\nActivate: Enforce on Build \u0026gt; Only fail for build lifecycles.\nLeave Deployment disabled for now. A good practice would be to enable it, but for other steps in this series, it will be required to keep it disabled for now.\nLeave Runtime disabled.\nClick Next\nAs policy criteria find \u0026#34;Image Registry \u0026gt; Image signature\u0026#34; and drag and drop it to the policy section\nFigure 6. ACS Policy Criteria Click on Select and select the CoSign integration\nFigure 7. ACS Assign CoSign integration to policy Click Next\nLeave the Policy scope for now and click Next\nReview the Policy and Save it.\nPrepare the Pipeline Tasks Now, finally, after all these preparations we are going to integrate ACS into our Pipeline. For this, we will create 2 tasks:\nacs-image-scan: Will scan the image for vulnerabilities\nacs-image-check: Will verify if the image would violate any security policy, especially the custom policy we have created.\nBoth checks will use the roxctl command line tool.\nIt is recommended to add these two Tasks to any Pipeline system you are using when you have ACS installed to leverage the full potential of ACS. It does not matter if you are using Tekton, Jenkins or something else. You can find examples of integrations at the Git repository: https://github.com/stackrox/contributions/. Task acs-image-scan To scan for vulnerabilities, create the following Task. It leverages the command line tool roxctl which can also be used on your local machine to perform such scans manually.\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: acs-image-scan namespace: ci spec: description: \u0026gt;- Scan an image with StackRox/RHACS. This tasks allows you to check an image against vulnerabilities. params: - description: | Secret containing the address:port tuple for StackRox Central) (example - rox.stackrox.io:443) name: rox_central_endpoint type: string - description: Secret containing the StackRox API token with CI permissions name: rox_api_token type: string - description: | Full name of image to scan (example -- gcr.io/rox/sample:5.0-rc1) name: image type: string - default: \u0026#39;false\u0026#39; description: | When set to `\u0026#34;true\u0026#34;`, skip verifying the TLS certs of the Central endpoint. Defaults to `\u0026#34;false\u0026#34;`. name: insecure-skip-tls-verify type: string results: - description: Output of `roxctl image check` name: check_output type: string steps: - env: (1) - name: ROX_API_TOKEN valueFrom: secretKeyRef: key: rox_api_token name: $(params.rox_api_token) - name: ROX_CENTRAL_ENDPOINT valueFrom: secretKeyRef: key: rox_central_endpoint name: $(params.rox_central_endpoint) image: \u0026gt;- registry.access.redhat.com/ubi9@sha256:089bd3b82a78ac45c0eed231bb58bfb43bfcd0560d9bba240fc6355502c92976 name: rox-image-scan resources: {} script: | (2) #!/usr/bin/env bash set +x curl -s -k -L -H \u0026#34;Authorization: Bearer $ROX_API_TOKEN\u0026#34; \\ \u0026#34;https://$ROX_CENTRAL_ENDPOINT/api/cli/download/roxctl-linux\u0026#34; \\ --output ./roxctl \\ \u0026gt; /dev/null chmod +x ./roxctl \u0026gt; /dev/null ./roxctl image scan \\ (3) $( [ \u0026#34;$(params.insecure-skip-tls-verify)\u0026#34; = \u0026#34;true\u0026#34; ] \u0026amp;\u0026amp; \\ echo -n \u0026#34;--insecure-skip-tls-verify\u0026#34;) \\ -e \u0026#34;$ROX_CENTRAL_ENDPOINT\u0026#34; --image \u0026#34;$(params.image)\u0026#34; 1 Token end endpoint which we defined as Secrets 2 Script that downloads and executes the CLI roxctl 3 Calling \u0026#34;roxctl image scan\u0026#34; Task Image Check To verify the policies that are configured in ACS we create the Task acs-image-check. We will use the command line tool roxctl again.\nACS comes with several (80+) build-in policies. Some of them are activated, but none of them configured to enforce a policy. Except, the policy Trusted Signature Policy we have created above. apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: acs-image-check namespace: ci spec: description: \u0026gt;- Policy check an image with StackRox/RHACS This tasks allows you to check an image against build-time policies and apply enforcement to fail builds. It\u0026#39;s a companion to the stackrox-image-scan task, which returns full vulnerability scan results for an image. params: - description: | Secret containing the address:port tuple for StackRox Central) (example - rox.stackrox.io:443) name: rox_central_endpoint type: string - description: Secret containing the StackRox API token with CI permissions name: rox_api_token type: string - description: | Full name of image to scan (example -- gcr.io/rox/sample:5.0-rc1) name: image type: string - default: \u0026#39;false\u0026#39; description: | When set to `\u0026#34;true\u0026#34;`, skip verifying the TLS certs of the Central endpoint. Defaults to `\u0026#34;false\u0026#34;`. name: insecure-skip-tls-verify type: string results: - description: Output of `roxctl image check` name: check_output type: string steps: - env: (1) - name: ROX_API_TOKEN valueFrom: secretKeyRef: key: rox_api_token name: $(params.rox_api_token) - name: ROX_CENTRAL_ENDPOINT valueFrom: secretKeyRef: key: rox_central_endpoint name: $(params.rox_central_endpoint) image: \u0026gt;- registry.access.redhat.com/ubi9@sha256:089bd3b82a78ac45c0eed231bb58bfb43bfcd0560d9bba240fc6355502c92976 name: rox-image-check resources: {} script: | #!/usr/bin/env bash (2) set +x curl -s -k -L -H \u0026#34;Authorization: Bearer $ROX_API_TOKEN\u0026#34; \\ \u0026#34;https://$ROX_CENTRAL_ENDPOINT/api/cli/download/roxctl-linux\u0026#34; \\ --output ./roxctl \\ \u0026gt; /dev/null chmod +x ./roxctl \u0026gt; /dev/null ./roxctl image check \\ (3) $( [ \u0026#34;$(params.insecure-skip-tls-verify)\u0026#34; = \u0026#34;true\u0026#34; ] \u0026amp;\u0026amp; \\ echo -n \u0026#34;--insecure-skip-tls-verify\u0026#34;) \\ -e \u0026#34;$ROX_CENTRAL_ENDPOINT\u0026#34; --image \u0026#34;$(params.image)\u0026#34; 1 Token end endpoint which we defined as Secrets 2 Script that downloads and executes the CLI roxctl 3 Calling \u0026#34;roxctl image check\u0026#34; Extend the Pipeline Now it is time to extend our Pipeline with the two tasks. Add the following blocks to the Pipeline secure-supply-chain:\n- name: acs-image-scan (1) params: (2) - name: rox_central_endpoint value: stackrox-endpoint - name: rox_api_token value: stackrox-secret - name: image value: \u0026#39;$(params.IMAGE_REPO):$(params.IMAGE_TAG)\u0026#39; - name: insecure-skip-tls-verify (3) value: \u0026#39;true\u0026#39; runAfter: (4) - build-sign-image taskRef: kind: Task name: acs-image-scan - name: acs-image-check (5) params: (6) - name: rox_central_endpoint value: stackrox-endpoint - name: rox_api_token value: stackrox-secret - name: image value: \u0026#39;$(params.IMAGE_REPO):$(params.IMAGE_TAG)\u0026#39; - name: insecure-skip-tls-verify (7) value: \u0026#39;true\u0026#39; runAfter: - build-sign-image (8) taskRef: kind: Task name: acs-image-check 1 Scanning for vulnerabilities 2 Provide parameters to the Task 3 Set to \u0026#39;false\u0026#39; if you have valid certificates. 4 Run after the Task \u0026#34;build-sign-image\u0026#34; 5 Scanning security policies 6 Provide parameters to the Task 7 Set to \u0026#39;false\u0026#39; if you have valid certificates. 8 Run after the Task \u0026#34;build-sign-image\u0026#34; The Pipeline will now look like this:\nFigure 8. Pipeline Details Execute the Pipeline Let’s trigger another PipelineRun by updating the README.md of our source code. While the previous Tasks should run successfully, let’s monitor our two ACS tasks. However, now the Pipeline is running quite long already. This means it is time for a coffee.\nStill here? Good, eventually our two ACS tasks have finished successfully.\nThe Task image-scan is looking for vulnerabilities and did not find any high-severity issues.\nFigure 9. ACS Image Scan Result The Task image-check verified the security policies. You can see in the Logs that the *Trusted Signature Policy was not violated. Another might be shown, but since all other policies are configured to notify only (instead of \u0026#34;enforce\u0026#34;), the whole Task will end successfully. This is good enough for our first tests. The important bit is that the image has been signed.\nFigure 10. ACS Image Check Result Summary Now we have integrated ACS and are using its powers to scan images for vulnerabilities and check them against security policies. Two Tasks have been created that are executed after the image has been built and are leveraging ACS’s command line tool roxctl\nWhenever vulnerabilities are found, or security policies are violated (if the policy is set to enforce) the Pipeline will fail.\nSuch tasks should be built in any pipeline you are creating, independently of using ACS or any other security verification tool.\nIn the next step, we will create a Task that creates a Software Bill of Material (SBOM) for us.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-22-securesupplychain-step7/","title":"Step 7 - Generating a SBOM","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","SBOM"],"description":"Step 7 Secure Supply Chain","content":" A software bill of materials (SBOM) offers an insight into the makeup of an application developed using third-party commercial tools and open-source software. It is a machine-readable, formally structured list of all components. This list is important to gain visibility into what risks might exist in a package and their potential impact. It allows to prioritize the risks and define a remediation path. The teams can use the SBOM to monitor all components that are used across an application.\nWe will create a Task that will generate a SBOM and uploads it into an SBOM repository.\nGoals The goals of this step are:\nInstall a SBOM repository.\nGenerate a SBOM and upload it to the repository.\nTool/Standard CycloneDX OWASP CycloneDX is a full-stack Bill of Materials (BOM) standard that provides advanced supply chain capabilities for cyber risk reduction. The specification supports:\nSoftware Bill of Materials (SBOM)\nSoftware-as-a-Service Bill of Materials (SaaSBOM)\nHardware Bill of Materials (HBOM)\nOperations Bill of Materials (OBOM)\nVulnerability Disclosure Reports (VDR)\nVulnerability Exploitability eXchange (VEX)\nStrategic direction of the specification is managed by the CycloneDX Core Working Group, is backed by the OWASP Foundation, and is supported by the global information security community.\nSource: https://cyclonedx.org/ The CycloneDX module for Node.js creates a valid CycloneDX Software Bill-of-Materials (SBOM) containing an aggregate of all project dependencies. CycloneDX is a lightweight SBOM specification that is easily created, human and machine-readable, and simple to parse.\nhttps://cyclonedx.org/capabilities/sbom/ We will use this standard and will deploy a small Pod in our cluster to be able to upload our SBOM locally.\nInstall CycloneDX Repository Server We can install a CycloneDX Repository Server, which is a simple Pod running in our cluster. You can use the following Argo CD application, which will create the Namespace (cyclonedx),Deployment, Route, and Service.\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: in-cluster-install-cyclonedx namespace: openshift-gitops spec: destination: namespace: cyclonedx server: \u0026#39;https://kubernetes.default.svc\u0026#39; info: - name: Description value: Install CycloneDX SBOM Repository project: in-cluster source: path: clusters/management-cluster/cyclonedx (1) repoURL: \u0026#39;https://github.com/tjungbauer/openshift-cluster-bootstrap\u0026#39; targetRevision: main 1 Path and URL to the Git repository. In Argo CD the following Application will be created:\nFigure 1. CycloneDX Installation Update the Tekton Pipeline Update the TriggerBinding globex-ui and add the following:\n- name: cyclonedxHostUrl value: \u0026gt;- https://cyclonedx-bom-repo-server-cyclonedx.apps.ocp.aws.ispworld.at (1) 1 DO NOT add a trailing slash here. Update the TriggerTemplate and add the following to the appropriate blocks:\nspec: parames: ... - description: Cyclonedx host url name: cyclonedxHostUrl (1) resourcetemplates: ... spec: params: ... - name: CYCLONEDX_HOST_URL value: $(tt.params.cyclonedxHostUrl) (2) 1 The parameter defined in the TriggerBinding 2 The parameter as it will be provided to the Pipeline. Install the following Task\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: generate-sbom namespace: ci spec: params: (1) - default: \u0026gt;- https://cyclonedx-bom-repo-server-cyclonedx.apps.placeholder.com/ name: cyclonedxHostUrl type: string results: - description: The url location of the generate SBOM name: sbomUrl type: string steps: - image: cyclonedx/cyclonedx-node name: generate-sbom resources: requests: memory: 1Gi script: \u0026gt; (2) apk add git curl npm install . /usr/src/cyclonedx-bom/bin/make-bom.js -o bom.xml curl -v -k $(params.cyclonedxHostUrl)/v1/bom -H \u0026#34;Content-Type: application/vnd.cyclonedx+xml; version=1.4\u0026#34; -H \u0026#34;Accept: */*\u0026#34; -d @bom.xml -D /tmp/header.txt LOCATION=$(cat /tmp/header.txt | grep location: | awk \u0026#39;{print $2}\u0026#39; | sed \u0026#39;s|http:|https:|g\u0026#39;) printf \u0026#34;%s\u0026#34; \u0026#34;$LOCATION\u0026#34; \u0026gt; \u0026#34;$(results.sbomUrl.path)\u0026#34; echo \u0026#34;SBOM URL accessible on Results of TaskRun $(context.taskRun.name)\u0026#34; workingDir: /workspace/repository workspaces: - name: repository 1 Default parameters for this task. Might be overwritten, by the EventListener 2 Script to be executed. It will download a script and upload the SBOM to our CycloneDX server. Update the Pipeline object\nspec: params: ... - name: CYCLONEDX_HOST_URL (1) type: string tasks: ... - name: generate-sbom (2) params: - name: cyclonedxHostUrl value: $(params.CYCLONEDX_HOST_URL) runAfter: - build-sign-image (3) taskRef: kind: Task name: generate-sbom workspaces: - name: repository workspace: shared-data 1 Add this to the parameter section 2 The new Task to generate a SBOM 3 Run after build-sign-image (and in parallel to the ACS tasks) Figure 2. Pipeline Execute the Pipeline Let’s trigger the pipeline again. If everything works well, the URL to the uploaded SBOM will be visible in the TaskRun log.\nFigure 3. Pipeline When you open this URL, you will get a huge list of components end dependencies.\nFor example, the following snippet shows that the angular component animations (version 13.2.6) is used.\n\u0026lt;components\u0026gt; \u0026lt;component type=\u0026#34;library\u0026#34; bom-ref=\u0026#34;pkg:npm/%40angular/animations@13.2.6\u0026#34;\u0026gt; \u0026lt;author\u0026gt;angular\u0026lt;/author\u0026gt; \u0026lt;group\u0026gt;@angular\u0026lt;/group\u0026gt; \u0026lt;name\u0026gt;animations\u0026lt;/name\u0026gt; \u0026lt;version\u0026gt;13.2.6\u0026lt;/version\u0026gt; \u0026lt;description\u0026gt;Angular - animations integration with web-animations\u0026lt;/description\u0026gt; \u0026lt;licenses\u0026gt; \u0026lt;license\u0026gt; \u0026lt;id\u0026gt;MIT\u0026lt;/id\u0026gt; \u0026lt;/license\u0026gt; \u0026lt;/licenses\u0026gt; \u0026lt;purl\u0026gt;pkg:npm/%40angular/animations@13.2.6\u0026lt;/purl\u0026gt; \u0026lt;externalReferences\u0026gt; \u0026lt;reference type=\u0026#34;website\u0026#34;\u0026gt; \u0026lt;url\u0026gt;https://github.com/angular/angular#readme\u0026lt;/url\u0026gt; \u0026lt;/reference\u0026gt; \u0026lt;reference type=\u0026#34;issue-tracker\u0026#34;\u0026gt; \u0026lt;url\u0026gt;https://github.com/angular/angular/issues\u0026lt;/url\u0026gt; \u0026lt;/reference\u0026gt; \u0026lt;reference type=\u0026#34;vcs\u0026#34;\u0026gt; \u0026lt;url\u0026gt;git+https://github.com/angular/angular.git\u0026lt;/url\u0026gt; \u0026lt;/reference\u0026gt; \u0026lt;/externalReferences\u0026gt; \u0026lt;/component\u0026gt; Summary This concludes our security scans for the whole build process. With the SBOM you have now a complete list of components and dependencies of your images.\nDuring the next steps, we will work with the Kubernetes manifests and update them, perform a linting on these manifests, and try to deploy the updated image to our DEV environment.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-23-securesupplychain-step8/","title":"Step 8 - Updating Kubernetes Manifests","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain"],"description":"Step 8 Secure Supply Chain","content":" With the finalization of the build process and the security checks during this phase, it is now time to update the Kubernetes manifests and provide the new tag for the created image. I have forked (and cleaned up) another repository that will store all yaml specifications that are required for OpenShift. You can find this repository at: Kubernetes Manifests\nGoals The goals of this step are:\nUpdate the manifest in Git with the new image tag\nPreparing the Pipeline Let’s create the Task object that will take care of the update.\nThe Task will use git commands to update the manifests accordingly.\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: update-manifest namespace: ci spec: description: \u0026gt;- This task updates the manifest for the current application to point to the image tag created with the short commit. params: (1) - description: Used to tag the built image. name: image type: string - default: main description: Target branch to push to name: target-branch type: string - default: Tekton Pipeline description: Git user name for performing the push operation. name: git_user_name type: string - default: tekton@tekton.com description: Git user email for performing the push operation. name: git_user_email type: string - description: File in which the image configuration is stored. name: configuration_file type: string - description: Repo in which the image configuration is stored. name: repository type: string steps: (2) - image: registry.redhat.io/openshift-pipelines/pipelines-git-init-rhel8:v1.10.4-4 name: git resources: {} script: \u0026gt; #!/usr/bin/env sh set -eux # Setting up the git config. git config --global user.email \u0026#34;$(params.git_user_email)\u0026#34; git config --global user.name \u0026#34;$(params.git_user_name)\u0026#34; # Checkout target branch to avoid the detached HEAD state TMPDIR=$(mktemp -d) cd $TMPDIR git clone $(params.repository) cd securing-software-supply-chain git checkout $(params.target-branch) # Set to the short commit value passed as parameter. # Notice the enclosing \u0026#34; to keep it as a string in the resulting YAML. IMAGE=\\\u0026#34;$(params.image)\\\u0026#34; sed -i \u0026#34;s#\\(.*value:\\s*\\).*#\\1 ${IMAGE}#\u0026#34; $(params.configuration_file) git add $(params.configuration_file) git commit -m \u0026#34;Automatically updated manifest to point to image tag $IMAGE\u0026#34; git push origin $(params.target-branch) 1 Required default parameters. 2 Script to clone and push the update to Git. To the Pipeline we have to add the following section\nspec: params: (1) - name: MANIFEST_FILE type: string - name: MANIFEST_FILE_PROD type: string - name: MANIFEST_REPO type: string - name: MANIFEST_REPO_NAME type: string - name: MANIFEST_GIT_REF type: string ... tasks: ... - name: update-dev-manifest params: - name: image value: \u0026#39;$(params.IMAGE_REPO):$(params.IMAGE_TAG)\u0026#39; - name: configuration_file value: $(params.MANIFEST_FILE) - name: repository value: $(params.MANIFEST_REPO) - name: git_user_name value: $(params.COMMIT_AUTHOR) runAfter: (2) - acs-image-check - acs-image-scan - generate-sbom taskRef: kind: Task name: update-manifest 1 Parameters that define the settings for the manifest repository 2 This time we are running after these three tasks Update the TriggerBinding globex-ui\n- name: manifestRepo value: git@github.com:tjungbauer/securing-software-supply-chain.git (1) - name: manifestRepoRef value: main - name: manifestFile (2) value: application/globex/overlays/dev/kustomization.yaml - name: manifestFileProd (3) value: application/globex/overlays/prod/kustomization.yaml - name: manifestRepoName value: tjungbauer/securing-software-supply-chain 1 The SSH url to GitHub 2 The overlay that will be used for the DEV environment 3 The overlay that will be used for the PROD environment Update the TriggerTemplate and add:\nspec: params: - description: The file to update to point to newly built image name: manifestFile - description: The file to update to point to newly built image in prod name: manifestFileProd - description: The repo to update to point to newly built image name: manifestRepo - description: The reference to the repo name: manifestRepoRef - description: The full name of the repo name: manifestRepoName ... resourcetemplates: ... spec: params: - name: MANIFEST_FILE value: $(tt.params.manifestFile) - name: MANIFEST_FILE_PROD value: $(tt.params.manifestFileProd) - name: MANIFEST_REPO value: $(tt.params.manifestRepo) - name: MANIFEST_GIT_REF value: $(tt.params.manifestRepoRef) - name: MANIFEST_REPO_NAME value: $(tt.params.manifestRepoName) ... Git SSH Key To be able to do changes in Git, which is using SSH keys to authenticate, we need to specify a Secret that knows the SSH private key and Github’s host keys.\nBe sure that to create an SSH key (for example using ssh-keygen) and that GitHub knows about this key. You can add your key at \u0026#34;Account \u0026gt; Settings \u0026gt; SSH and GPG keys\u0026#34;.\nRun the following command to get the host keys of GitHub:\nssh-keyscan -H github.com # github.com:22 SSH-2.0-babeld-c89ab1f3 # github.com:22 SSH-2.0-babeld-c89ab1f3 |1|cr4dkqIl2SnZqktvRsFnx1lA5Ag=|eie8EHXqQHgCZJq7F/TtRRZcBbc= ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk= # github.com:22 SSH-2.0-babeld-c89ab1f3 |1|r6z4yeEV9Cog/2I4stZp1A34BAE=|EU8VcdD+KHMJJt9uPL4jh5zK0fI= ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBEmKSENjQEezOmxkZMy7opKgwFB9nkt5YRrYMjNuG5N87uRgg6CLrbo5wAdT/y6v0mKV0U2w0WZ2YB/++Tpockg= # github.com:22 SSH-2.0-babeld-c89ab1f3 |1|hVnTyBzb/gSR8jpz+NUziUkHy1A=|ENMDVCVsLfz2CWLa1C+BnzZI8Yg= ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOMqqnkVzrm0SdG6UOoqKLsabgH5C9okWi0dh2l9GKJl # github.com:22 SSH-2.0-babeld-c89ab1f3 Finally, we need to create a Secret with the following content:\napiVersion: v1 kind: Secret metadata: annotations: tekton.dev/git-0: github.com (1) name: git-ssh-key namespace: ci type: kubernetes.io/ssh-auth data: ssh-privatekey: \u0026lt;BASE64 PRIVAT SSH KEY \u0026gt; (2) known_hosts: \u0026lt; BASE64 of Githubs hostkey\u0026gt; (3) 1 This annotation defines for which host Tekton will inject the Secret. 2 A base64 decoded string of your PRIVATE ssh key. 3 A base64 decoded string of the host keys Update the pipeline ServiceAccount and add the new Secret:\nsecrets: ... - name: git-ssh-key Execute the Pipeline Time to trigger the Pipeline, which now looks like:\nFigure 1. Pipeline Details The new Task will automatically push the new image tag to the Kubernetes manifests Git repository. From there it can later be used to roll out the new version.\nIn Git you can track the changes:\nFigure 2. Git Commit Summary We have now updated the Kubernetes manifests with the new image tag. In the next steps, we will verify these manifests (linting) and then try to deploy the new image on the DEV environment.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-24-securesupplychain-step9/","title":"Step 9 - Linting Kubernetes Manifests","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","Linting"],"description":"Step 9 Secure Supply Chain","content":" At this point we have checked our source code, verified that it has been signed and has no vulnerabilities, generated a SBOM and updated the Kubernetes manifests, that are responsible to deploy our application on OpenShift. As everything with OpenShift these Kubernetes objects are simple yaml files. In this step we will perform a linting on these files, to verify if they follow certain rules and best practices.\nGoals The goals of this step are:\nClone the Kubernetes manifest into a workspace\nCreate Tasks that perform the linting\nWARN when the manifests do not follow best practices (we will not let the Task fail here, because I would need to modify all manifests)\nWhat is linting A linter is an analysis tool that verifies if your code has any errors, bugs or stylistic errors. SonarQube could be seen as such tool. We used it to scan our source code. For Kubernetes manifests we can use tools that check the yaml files against best practices or security. For example, it could notify you if you forgot to define resources or probes in your manifests.\nTools In this step, I will use three linting tools. In no way, this means you need to use all three. I only use them for demonstration purposes. However, to lint your yaml files or Helm charts is a common practice you should consider.\nFor this demonstration I am leveraging:\nKubeLinter\nYamllint\nKube-score\nIn the end, you should choose the tool that fits best for you. Manifests The manifest we are going to use can be found at my GitHub repository: Securing Software Supply Chain. It is a fork and the original can be found here.\nPrepare Pipeline Modify the TriggerTemplate and add a parameter LINTING_INFORM_ONLY that either sets the Tasks to inform or enforce (failing when linting does find issues). In addition, we define a new workspace, where we will download and store the Kubernetes manifests.\nspec: params: ... - description: Only inform on linting errors (Log) but do not actually fail name: lintingInformOnly (1) resourcetemplates: ... spec: params: ... - name: LINTING_INFORM_ONLY value: $(tt.params.lintingInformOnly) ... workspaces: ... - name: shared-data-manifests (2) volumeClaimTemplate: metadata: creationTimestamp: null spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi status: {} 1 New parameter to either inform only when linting is unsuccessful or to enforce that the Task will end with an error. 2 Workspace to download and store the yaml files. Update the TriggerBinding to set the values for the PipelineRun.\nspec: params: ... - name: lintingInformOnly (1) value: \u0026#39;true\u0026#39; 1 Set the parameter to \u0026#39;true\u0026#39; Update the Pipeline object. Here we will need to add the parameter and four Tasks (clone the Git Repo, KubeLinter, Yamllint and kube-score) as well as the workspace.\nspec: params: ... - name: LINTING_INFORM_ONLY (1) type: string ... - name: pull-manifests (2) params: - name: url value: $(params.MANIFEST_REPO) - name: revision value: $(params.MANIFEST_GIT_REF) - name: deleteExisting value: \u0026#39;true\u0026#39; runAfter: - update-dev-manifest (3) taskRef: kind: ClusterTask (4) name: git-clone workspaces: (5) - name: output workspace: shared-data-manifests - name: kube-linter (6) params: - name: informLintingOnly (7) value: $(params.LINTING_INFORM_ONLY) runAfter: - pull-manifests (8) taskRef: kind: Task name: kube-linter workspaces: - name: repository workspace: shared-data-manifests - name: kube-score (9) params: - name: informLintingOnly value: $(params.LINTING_INFORM_ONLY) runAfter: - pull-manifests (10) taskRef: kind: Task name: kube-score workspaces: - name: repository (11) workspace: shared-data-manifests - name: yaml-lint (12) params: - name: informLintingOnly value: $(params.LINTING_INFORM_ONLY) runAfter: - pull-manifests (13) taskRef: kind: Task name: yaml-lint workspaces: - name: repository (14) workspace: shared-data-manifests workspaces: ... - name: shared-data-manifests 1 New parameter assigned to the Pipeline. 2 Task to clone the repository to the workspace. 3 Will run after the Pipeline has updated the manifests with the new image. 4 Is a child of the ClusterTask git-clone. 5 The workspace to clone the repository. 6 Task to execute KubeLinter. 7 Parameter to either enforce or inform only. 8 Will run after the repository has been cloned. 9 Task to execute kube-score. 10 Will run after the repository has been cloned. 11 Workspace where the cloned repository can be found. 12 Task to execute Yamllint. 13 Will run after the repository has been cloned. 14 Workspace where the cloned repository can be found. Remember: It is not required to execute three different linter tools. It is only done as a showcase. I personally like KubeLinter. Choose whatever tool is suitable for you. Create the different Task objects for the linter tools. Each Task will execute a linter program and provides its very own Log.\nI have created the image linter-image that contains the three required binaries. It is available at Quay.io and its original Dockerfile can be found here. Use it at your own risk :). KubeLinter\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: kube-linter namespace: ci spec: description: \u0026gt;- Task to run KubeLinter and perform a linting of Kubernetes manifets. params: - default: \u0026#39;false\u0026#39; name: informLintingOnly type: string steps: - image: \u0026#39;quay.io/tjungbau/linter-image:v1.0.2\u0026#39; name: kube-linter resources: {} script: \u0026gt; #!/usr/bin/env bash RC=0 kube-linter lint /workspace/repository/. --config \u0026#34;/workspace/repository/.kube-linter.yaml\u0026#34; (1) if [ $? -gt 0 ]; then RC=1 fi # We actually do not fail but inform only if [ \u0026#34;$(params.informLintingOnly)\u0026#34; = \u0026#34;true\u0026#34; ]; then echo \u0026#34;Informing only, task will not fail. Actual return code was $RC\u0026#34; exit 0; fi (exit $RC) workingDir: /workspace/repository workspaces: - name: repository 1 Execute kube-linter using the configuration stored in the repository. kube-score\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: kube-score namespace: ci spec: description: \u0026gt;- Task to run kube-score and perform a linting of Kubernetes manifets. params: - default: \u0026#39;false\u0026#39; name: informLintingOnly type: string steps: - image: \u0026#39;quay.io/tjungbau/linter-image:v1.0.2\u0026#39; name: kube-linter resources: {} script: \u0026gt; #!/usr/bin/env bash RC=0 KUBESCORE_IGNORE_TESTS=\u0026#34;${KUBESCORE_IGNORE_TESTS:-container-image-pull-policy,pod-networkpolicy}\u0026#34; (1) for i in `find . -name \u0026#39;*.yaml\u0026#39; -type f`; do kube-score score --ignore-test ${KUBESCORE_IGNORE_TESTS} $i; let RC=RC+$?; done if [ $? -gt 0 ]; then RC=1 fi # We actually do not fail but inform only if [ \u0026#34;$(params.informLintingOnly)\u0026#34; = \u0026#34;true\u0026#34; ]; then echo \u0026#34;Informing only, task will not fail. Actual return code was $RC\u0026#34; exit 0; fi (exit $RC) workingDir: /workspace/repository workspaces: - name: repository 1 Disable checks for Network Policies or image Pull policy for kube-score. Yammllint\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: yaml-lint namespace: ci spec: description: \u0026gt;- Task to run yamllint and perform a linting of Kubernetes manifets. params: - default: \u0026#39;false\u0026#39; name: informLintingOnly type: string steps: - image: \u0026#39;quay.io/tjungbau/linter-image:v1.0.2\u0026#39; name: yaml-lint resources: {} script: | #!/usr/bin/env bash for files in `find . -type f -name \u0026#39;*.yaml\u0026#39;`; do (1) yamllint -c /workspace/repository/.yamllint.yaml ${files}; let var=var+$? done # We actually do not fail but inform only if [ \u0026#34;$(params.informLintingOnly)\u0026#34; = \u0026#34;true\u0026#34; ]; then echo \u0026#34;Informing only, task will not fail. Actual return code was $var\u0026#34; exit 0; fi (exit $var) workingDir: /workspace/repository workspaces: - name: repository 1 Execute kube-linter using the configuration stored in the repository. Execute the Pipeline The Pipeline now looks like this:\nFigure 1. Pipeline Details Remember, you typically need only one linter tool, not three different ones. Since we inform only you will see some errors in the logs. For example, for kube-linter:\nFigure 2. Kube-Linter Results Summary Now, all our yaml manifests have been linted, with three different tools. And because we do not fail at this stage, we can continue. The next steps will be some deployment checks.\nSince everything is done using Argo CD and the manifests have been updated during the step \u0026#34;update-manifest\u0026#34;, the changes will be most likely already deployed. Even if the linting-step comes later and might even fail. This is fine because we first deploy on a DEV environment. So, if linting fails, it will prohibit the rollout to production, while some application testing can still be done on DEV. "},{"uri":"https://blog.stderr.at/tags/supply-chain/","title":"Supply Chain","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/tekton/","title":"Tekton","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/tekton/","title":"Tekton","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-25-securesupplychain-step10/","title":"Step 10 - The Example Application","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","Linting"],"description":"Step 10 Secure Supply Chain","content":" If you read all articles up to here (congratulations) you know that we always update the README file of \u0026#34;The Application\u0026#34;. But what is this application exactly and how can we update it like in a real-life example? The Globex UI application is built with Angular and was prepared for this journey. It is a quite complex application and requires Kafka to be installed as well. However, since I am not a developer and this tool was already available, I forked and re-used it. The original can be found at Globex UI\nGoals The goals of this step are:\nDeploy AMQ Streams Operator\nDeploy Globex DEV\nDeploy Globex Prod\nVerify Deployments\nIntroduction The Globex UI, although quite complex, will provide a simple web interface. Since I do not have multiple clusters, I will use the Namespaces globex-dev and globex-prod to distinguish between the two environments. This means, our DEV environment will run inside globex-dev and PROD in globex-prod. Kafka related stuff will be deployed in the Namespace kafka.\nFigure 1. Globex UI The deployment will be done automatically by GitOps. As soon as our Pipeline will be executed and subsequently updates our image in the Kubernetes manifests (see step: Linting Kubernetes Manifests), a new version of Globex UI will be deployed. The production update will follow in later steps in the pipeline.\nGitOps will monitor changes in our Kubernetes manifest repository. Here, we use Kustomize overlays to separate between DEV and PROD.\nAs a general recommendation: Do everything with GitOps. If it is not in Git, it does not exist. Installation To install all required components, we create several GitOps Applications.\nAlthough we create these Argo CD Applications by hand, it is recommended to put these definitions into Git also. Since this is just a demo, I am using the cluster instance of OpenShift GitOps. DO NOT deploy customer workload using the default instance, since it has privileged permissions on the cluster. Instead, create a second instance (or multiple) for the developers. The following Applications will be created:\nFigure 2. GitOps Applications Install AMQ Streams Operator As the very first step, we need to deploy the AMQ Streams Operator. Simply search for the operator in the Operator Hub and deploy it using the default values.\nAlso, this deployment could and should be done using Argo CD. For the sake of speed, I have done this manually here. Figure 3. AMQ Streams Operator Install Kafka for Globex Now we can deploy all Kafka-related stuff using GitOps. Let’s create the following Application object:\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: globex-kafka namespace: openshift-gitops spec: destination: namespace: kafka (1) server: \u0026#39;https://kubernetes.default.svc\u0026#39; (2) project: default source: path: application/kafka/overlays/dev (3) repoURL: \u0026#39;https://github.com/tjungbauer/securing-software-supply-chain\u0026#39; targetRevision: HEAD syncPolicy: automated: (4) selfHeal: true syncOptions: - RespectIgnoreDifferences=true - CreateNamespace=true 1 The deployment must be done in the Namespace kafka 2 We install using the local GitOps server instance. 3 Path/URL to the GitHub Kustomize repository. 4 Auto-Update in case of changes are detected. Install Globex DEV The DEV installation will deploy the test version of our application into the Namespace globex-dev\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: globex-dev (1) namespace: openshift-gitops spec: destination: namespace: globex-dev (2) server: \u0026#39;https://kubernetes.default.svc\u0026#39; (3) ignoreDifferences: - group: \u0026#39;*\u0026#39; jqPathExpressions: - \u0026#39;.imagePullSecrets[] | select(.name|test(\u0026#34;.-dockercfg-.\u0026#34;))\u0026#39; kind: ServiceAccount project: default source: path: application/globex/overlays/dev (4) repoURL: \u0026#39;https://github.com/tjungbauer/securing-software-supply-chain\u0026#39; targetRevision: HEAD syncPolicy: automated: (5) selfHeal: true syncOptions: - RespectIgnoreDifferences=true - CreateNamespace=true (6) 1 Globex DEV instance 2 Install into the Namespace globex-dev 3 We install using the local GitOps server instance. 4 Path/URL to the GitHub Kustomize repository. 5 Auto-Update in case of changes are detected. 6 Create the Namespace if it does not exist. Install Globex PROD And finally, the same for the PROD instance:\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: globex-prod (1) namespace: openshift-gitops spec: destination: namespace: globex-prod (2) server: \u0026#39;https://kubernetes.default.svc\u0026#39; (3) ignoreDifferences: - group: \u0026#39;*\u0026#39; jqPathExpressions: - \u0026#39;.imagePullSecrets[] | select(.name|test(\u0026#34;.-dockercfg-.\u0026#34;))\u0026#39; kind: ServiceAccount project: default source: path: application/globex/overlays/prod (4) repoURL: \u0026#39;https://github.com/tjungbauer/securing-software-supply-chain\u0026#39; targetRevision: HEAD syncPolicy: automated: (5) selfHeal: true syncOptions: - RespectIgnoreDifferences=true - CreateNamespace=true (6) 1 Globex DEV instance 2 Install into the Namespace globex-prod 3 We install using the local GitOps server instance. 4 Path/URL to the GitHub Kustomize repository. 5 Auto-Update in case of changes are detected. 6 Create the Namespace if it does not exist. Summary During this step we have added nothing new to our pipeline, but deployed our example application Globex UI instead, including Kafka. The next steps will now do another verification against ACS and if the transparency logs are available, and then finally prepares everything for production deployment.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-26-securesupplychain-step11/","title":"Step 11 - ACS Deployment Check","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","ACS"],"description":"Step 11 Secure Supply Chain","content":" After the Pipeline prepared the new image for DEV it should be checked against ACS for policy compliance. This ensures that the deployment manifest adheres to policy requirements. The command line tool roxctl will be leveraged to perform this task.\nGoals The goals of this step are:\nCreate a Task that performs a deployment check using roxctl\nCreate the Task Create the following Task object. This Task is a bit more complex, since we want to verify the Deployment, we first need to build the Kustomize files which would create a big yaml file and will store it into the output folder (all.yaml). Then we use kubesplit to split this huge file into different smaller ones. Finally, the roxctl step will search for a file called deployment—​globex-ui.yml and performs the security checks (verify if any security policy is violated in ACS) against this file.\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: acs-deploy-check namespace: ci spec: description: \u0026gt;- Policy check a deployment with StackRox/RHACS This tasks allows you to check a deployment against build-time policies and apply enforcement to fail builds. It\u0026#39;s a companion to the stackrox-image-scan task, which returns full vulnerability scan results for an image. params: - description: | Secret containing the address:port tuple for StackRox Central) (example - rox.stackrox.io:443) name: rox_central_endpoint type: string - description: Secret containing the StackRox API token with CI permissions name: rox_api_token type: string - default: \u0026#39;https://default-git-url.git\u0026#39; name: gitRepositoryUrl type: string - default: main name: gitRepositoryRevision type: string - default: \u0026#39;true\u0026#39; name: verbose type: string - default: \u0026#39;false\u0026#39; description: | When set to `\u0026#34;true\u0026#34;`, skip verifying the TLS certs of the Central endpoint. Defaults to `\u0026#34;false\u0026#34;`. name: insecure-skip-tls-verify type: string results: - description: Output of `roxctl deployment check` name: check_output type: string steps: - image: quay.io/wpernath/kustomize-ubi name: kustomize-build resources: {} script: | (1) #!/usr/bin/env sh set -eu -o pipefail cd /workspace/repository/application/globex/overlays/dev mkdir -p /workspace/repository/input mkdir -p /workspace/repository/output kustomize build . --output /workspace/repository/input/all.yaml workingDir: /workspace/repository - image: looztra/kubesplit (2) name: kustomize-split resources: {} script: \u0026gt; #!/usr/bin/env sh set -eu -o pipefail kubesplit -i /workspace/repository/input/all.yaml -o /workspace/repository/output workingDir: /workspace/repository - env: - name: ROX_API_TOKEN valueFrom: secretKeyRef: key: rox_api_token name: $(params.rox_api_token) - name: ROX_CENTRAL_ENDPOINT valueFrom: secretKeyRef: key: rox_central_endpoint name: $(params.rox_central_endpoint) image: \u0026#39;registry.access.redhat.com/ubi8:8.7-1026\u0026#39; name: rox-deploy-scan (3) resources: {} script: | #!/usr/bin/env bash set +x cd /workspace/repository/output curl -s -k -L -H \u0026#34;Authorization: Bearer $ROX_API_TOKEN\u0026#34; \\ \u0026#34;https://$ROX_CENTRAL_ENDPOINT/api/cli/download/roxctl-linux\u0026#34; \\ --output ./roxctl \\ \u0026gt; /dev/null chmod +x ./roxctl \u0026gt; /dev/null DEPLOYMENT_FILE=$(ls -1a | grep *deployment--globex-ui.yml) ./roxctl deployment check \\ $( [ \u0026#34;$(params.insecure-skip-tls-verify)\u0026#34; = \u0026#34;true\u0026#34; ] \u0026amp;\u0026amp; \\ echo -n \u0026#34;--insecure-skip-tls-verify\u0026#34;) \\ -e \u0026#34;$ROX_CENTRAL_ENDPOINT\u0026#34; --file \u0026#34;$DEPLOYMENT_FILE\u0026#34; workingDir: /workspace/repository workspaces: - name: repository 1 Build Kustomize and store the output into /workspace/repository/input/all.yaml. 2 Split the huge yaml file into separate ones and store them into /workspace/repository/output. 3 Search for the file deployment—​globex-ui.yml and perform a roxctl deployment check. Update the Pipeline The Pipeline object must be extended with another Task:\n- name: acs-deploy-check params: - name: rox_central_endpoint (1) value: stackrox-endpoint - name: rox_api_token value: stackrox-secret - name: insecure-skip-tls-verify value: \u0026#39;true\u0026#39; runAfter: (2) - yaml-lint - kube-score - kube-linter taskRef: kind: Task name: acs-deploy-check workspaces: - name: repository workspace: shared-data-manifests (3) 1 The parameters required for ACS. 2 This task runs after the linting tasks. 3 The workspace, where the manifests have been pulled. Execute the Pipeline Again, we trigger our pipeline by simply updating the README.md of our source code.\nThe ACS check will verify if any security policies, that are valid for Deployment-states, are violated.\nMy test returned the following result.\nFigure 1. Pipeline Details However, since the policies are configured to \u0026#34;inform only\u0026#34;, the PipelineRun will finish successfully.\nSummary All ACS checks have been done now. The deployment does not violate any security policy that is configured in ACS …​ or to be more exact: It does not violate enforced policy, thus the Task will end successfully.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-27-securesupplychain-step12/","title":"Step 12 - Verify TLog Signature","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","Rekor"],"description":"Step 12 Secure Supply Chain","content":" Since the image has been deployed on DEV now, we need to prepare everything for production. Before we start, we need to confirm that the whole signing process has been passed and that our signature has been applied correctly. With CoSign we signed our image and used Rekor to create a transparency log. We will use this log to confirm that the image was signed.\nGoals The goals of this step are:\nVerify if the image has been signed using Rekor transparency log.\nCoSign public key Before we start, we need a secret in the Namespace ci that provides the PUBLIC key of CoSign. This key was generated in a previous Step 5 - CoSign - Signing the Image\nkind: Secret apiVersion: v1 metadata: name: cosign-secret (1) namespace: ci data: cosign.pub: \u0026gt;- \u0026lt;base64 CoSign PUBLIC key\u0026gt; type: Opaque 1 base64 decoded PUBLIC key of CoSign Create the Task The following task will use several steps to fetch and parse the transparency log, that has been created. The different tasks are using images from Redhat-gpte. The command line tool rekor-cli will be used to verify the entries in the log.\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: rekor-verify namespace: ci spec: params: - default: image-registry-secret (1) name: registrySecret type: string - default: cosign-secret (2) name: cosignSecret type: string - default: \u0026gt;- quay.io/placeholder/test:latest name: image type: string steps: - env: - name: REGISTRY_SECRET valueFrom: secretKeyRef: key: .dockerconfigjson name: $(params.registrySecret) - name: COSIGN_PUBLIC_KEY valueFrom: secretKeyRef: key: cosign.pub name: $(params.cosignSecret) image: quay.io/tjungbau/pipeline-tools:v1.0.0 name: cosign-verify-image resources: {} script: \u0026gt; mkdir -p /home/cosign/.docker/ echo \u0026#34;${REGISTRY_SECRET}\u0026#34; \u0026gt; /home/cosign/.docker/config.json echo \u0026#34;${COSIGN_PUBLIC_KEY}\u0026#34; \u0026gt; /workspace/cosign.pub cosign verify --key /workspace/cosign.pub $(params.image) --output-file /workspace/cosign.verify cat /workspace/cosign.verify | jq --raw-output \u0026#39;.[0] | .critical | .image | .[\u0026#34;docker-manifest-digest\u0026#34;]\u0026#39; \u0026gt; /workspace/cosign.sha rekor-cli search --sha $(cat /workspace/cosign.sha) --format json \u0026gt; /workspace/rekor.search cat /workspace/rekor.search | jq \u0026#39;.UUIDs[0]\u0026#39; | sed \u0026#39;s/\\\u0026#34;//g\u0026#39; \u0026gt; /workspace/rekor.uuid rekor-cli get --uuid $(cat /workspace/rekor.uuid) --format json \u0026gt; /workspace/rekor.get cat /workspace/rekor.get | jq -r .Attestation 1 Registry pull secret. 2 CoSign public key. Update the Pipeline The Pipeline object must be extended with another Task:\n- name: verify-tlog-signature params: (1) - name: registrySecret (2) value: tjungbau-secure-supply-chain-demo-pull-secret - name: cosignSecret (3) value: cosign-secret - name: image value: \u0026#39;$(params.IMAGE_REPO):$(params.IMAGE_TAG)\u0026#39; runAfter: (4) - yaml-lint - kube-score - kube-linter taskRef: kind: Task name: rekor-verify 1 The parameters required for this task. 2 Pull secret for quay.io, which was created during step 5. 3 CoSign public key. 4 This task runs after the linting tasks. Execute the Pipeline Triggering the pipeline will now check the signature of an image.\nThe logs should show something like this:\nFigure 1. Rekor Image Signature Verification As you can see: The image has been signed correctly and the transparency logs can be parsed.\nSummary Finally, everything has been verified and we can bring everything into production. The final two steps will create a new branch in the Git manifest repository and a pull request that can be manually merged.\n"},{"uri":"https://blog.stderr.at/securesupplychain/2023-06-28-securesupplychain-step13/","title":"Step 13 - Bring it to Production","tags":["OCP","Tekton","OpenShift","Pipleines","CI/CD","Supply Chain","Rekor"],"description":"Step 13 Secure Supply Chain","content":" If you reached this article, congratulations! You read through tons of pages to build up a Pipeline. The last two steps in our Pipeline are: Creating a new branch and creating a pull request, with the changes of the image tag that must be approved and will be merged then (We will not update the main branch directly!). Finally, we will do a \u0026#34;real\u0026#34; update to the application to see the actual changes.\nGoals The goals of this step are:\nCreate a task that creates a new branch in Git repository.\nCreate a task that creates a pull request in Git repository.\nPerform a full End2End run of the Pipeline and approve the pull request.\nCreate a token at GitHub To create a pull request we need to authenticate against the Git api. For this, we will need a token. In GitHub open your personal settings (by clicking on your avatar) and then go to \u0026#34;Developer settings\u0026#34;.\nSelect \u0026#34;Personal access tokens\u0026#34; and \u0026#34;fine-graining tokens\u0026#34; to create a new token.\nFigure 1. GitHub Personal Access Token Enter the required fields:\nName: Secure Supply Chain\nExpiration: Whatever your like (max is 1 year)\nRepository access: Only select repositories\nSelect your repository, for example: tjungbauer/securing-software-supply-chain\nPermissions: \u0026#34;Pull requests\u0026#34; \u0026gt; Read and write\nFigure 2. GitHub Personal Access Token Created Save the created token and create the following secret:\nkind: Secret apiVersion: v1 metadata: name: github-token namespace: ci stringData: token: \u0026lt;Token Value\u0026gt; (1) type: Opaque 1 Clear-text token. If already base64 encoded, change stringData to data. Task: Create a new branch Since we do not update the main branch directly, we will create a new (feature) branch, that will be used for a pull request and can be deleted after the pull request has been merged.\nCreate the following Task object\nThis Task will use git commands to create a new feature-branch and pushes the changes into that branch.\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: new-branch-manifest-repo namespace: ci spec: description: \u0026gt;- This task creates a branch for a PR to point to the image tag created with the short commit. params: - description: Used to tag the built image. name: image type: string - default: main description: Target branch to push to name: target-branch type: string - default: Tekton Pipeline description: Git user name for performing the push operation. name: git_user_name type: string - default: tekton@tekton.com description: Git user email for performing the push operation. name: git_user_email type: string - description: File in which the image configuration is stored. name: configuration_file type: string - description: Repo in which the image configuration is stored. name: repository type: string steps: - image: registry.redhat.io/openshift-pipelines/pipelines-git-init-rhel8:v1.10.4-4 name: git resources: {} script: \u0026gt;- # Setting up the git config. git config --global user.email \u0026#34;$(params.git_user_email)\u0026#34; git config --global user.name \u0026#34;$(params.git_user_name)\u0026#34; # Checkout target branch to avoid the detached HEAD state TMPDIR=$(mktemp -d) cd $TMPDIR git clone $(params.repository) (1) cd securing-software-supply-chain git checkout -b $(params.target-branch) (2) # Set to the short commit value passed as parameter. # Notice the enclosing \u0026#34; to keep it as a string in the resulting YAML. IMAGE=\\\u0026#34;$(params.image)\\\u0026#34; sed -i \u0026#34;s#\\(.*value:\\s*\\).*#\\1 ${IMAGE}#\u0026#34; $(params.configuration_file) git add $(params.configuration_file) (3) git commit -m \u0026#34;Automatically updated manifest to point to image tag $IMAGE\u0026#34; git push origin $(params.target-branch) 1 Clone the main repository. 2 Create a new feature branch. 3 Add, commit, and push everything to the new branch. Modify the Pipeline object\nThe Task must be added to the Pipeline, it provides several required parameters.\n- name: create-prod-manifest-branch params: - name: image value: \u0026#39;$(params.IMAGE_REPO):$(params.IMAGE_TAG)\u0026#39; - name: configuration_file value: $(params.MANIFEST_FILE_PROD) - name: repository value: $(params.MANIFEST_REPO) - name: git_user_name value: $(params.COMMIT_AUTHOR) - name: target-branch value: feature-for-$(params.COMMIT_SHA) runAfter: - acs-deploy-check - verify-tlog-signature taskRef: kind: Task name: new-branch-manifest-repo Task: Create a Pull request Create the following Task object\nThe following task will take the token and create a new pull request at GitHub:\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: git-open-pull-request namespace: ci spec: description: \u0026gt;- This task will open a PR on Github based on several parameters. This could be useful in GitOps repositories for example. params: - default: api.github.com description: | The GitHub host, adjust this if you run a GitHub enteprise or Gitea name: GITHUB_HOST_URL type: string - default: \u0026#39;\u0026#39; description: | The API path prefix, GitHub Enterprise has a prefix e.g. /api/v3 name: API_PATH_PREFIX type: string - description: | The GitHub repository full name, e.g.: tektoncd/catalog name: REPO_FULL_NAME type: string - default: github description: \u0026gt; The name of the kubernetes secret that contains the GitHub token, default: github name: GITHUB_TOKEN_SECRET_NAME type: string - default: token description: \u0026gt; The key within the kubernetes secret that contains the GitHub token, default: token name: GITHUB_TOKEN_SECRET_KEY type: string - default: Bearer description: \u0026gt; The type of authentication to use. You could use the less secure \u0026#34;Basic\u0026#34; for example name: AUTH_TYPE type: string - description: | The name of the branch where your changes are implemented. name: HEAD type: string - description: | The name of the branch you want the changes pulled into. name: BASE type: string - description: | The body description of the pull request. name: BODY type: string - description: | The title of the pull request. name: TITLE type: string results: - description: Number of the created pull request. name: NUMBER type: string - description: URL of the created pull request. name: URL type: string steps: - env: - name: PULLREQUEST_NUMBER_PATH value: $(results.NUMBER.path) - name: PULLREQUEST_URL_PATH value: $(results.URL.path) image: \u0026#39;registry.access.redhat.com/ubi8/python-38:1\u0026#39; name: open-pr resources: {} script: \u0026gt;- #!/usr/libexec/platform-python (1) \u0026#34;\u0026#34;\u0026#34;This script will open a PR on Github\u0026#34;\u0026#34;\u0026#34; import json import os import sys import http.client github_token = (2) open(\u0026#34;/etc/github-open-pr/$(params.GITHUB_TOKEN_SECRET_KEY)\u0026#34;, \u0026#34;r\u0026#34;).read() open_pr_url = \u0026#34;/repos/$(params.REPO_FULL_NAME)/pulls\u0026#34; data = { (3) \u0026#34;head\u0026#34;: \u0026#34;$(params.HEAD)\u0026#34;, \u0026#34;base\u0026#34;: \u0026#34;$(params.BASE)\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;\u0026#34;\u0026#34;$(params.TITLE)\u0026#34;\u0026#34;\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;\u0026#34;\u0026#34;$(params.BODY)\u0026#34;\u0026#34;\u0026#34; } print(\u0026#34;Sending this data to GitHub: \u0026#34;) print(data) authHeader = \u0026#34;Bearer \u0026#34; + github_token giturl = \u0026#34;api.\u0026#34;+\u0026#34;$(params.GITHUB_HOST_URL)\u0026#34; conn = http.client.HTTPSConnection(giturl) conn.request( \u0026#34;POST\u0026#34;, open_pr_url, body=json.dumps(data), headers={ \u0026#34;User-Agent\u0026#34;: \u0026#34;OpenShift Pipelines\u0026#34;, \u0026#34;Authorization\u0026#34;: authHeader.strip(), \u0026#34;Accept\u0026#34;: \u0026#34;application/vnd.github+json\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-GitHub-Api-Version\u0026#34;: \u0026#34;2022-11-28\u0026#34; }) resp = conn.getresponse() if not str(resp.status).startswith(\u0026#34;2\u0026#34;): print(\u0026#34;Error: %d\u0026#34; % (resp.status)) print(resp.read()) sys.exit(1) else: # https://docs.github.com/en/rest/reference/pulls#create-a-pull-request body = json.loads(resp.read().decode()) open(os.environ.get(\u0026#39;PULLREQUEST_NUMBER_PATH\u0026#39;), \u0026#39;w\u0026#39;).write(f\u0026#39;{body[\u0026#34;number\u0026#34;]}\u0026#39;) open(os.environ.get(\u0026#39;PULLREQUEST_URL_PATH\u0026#39;), \u0026#39;w\u0026#39;).write(body[\u0026#34;html_url\u0026#34;]) print(\u0026#34;GitHub pull request created for $(params.REPO_FULL_NAME): \u0026#34; f\u0026#39;number={body[\u0026#34;number\u0026#34;]} url={body[\u0026#34;html_url\u0026#34;]}\u0026#39;) volumeMounts: - mountPath: /etc/github-open-pr name: githubtoken readOnly: true volumes: - name: githubtoken secret: secretName: $(params.GITHUB_TOKEN_SECRET_NAME) 1 Python script to create the pull request. 2 The token from the secret object. 3 The data we will send to GitHub. Modify the Pipeline object\n- name: issue-prod-pull-request params: - name: GITHUB_HOST_URL value: $(params.REPO_HOST) - name: GITHUB_TOKEN_SECRET_NAME value: github-token - name: REPO_FULL_NAME value: $(params.MANIFEST_REPO_NAME) - name: HEAD value: feature-for-$(params.COMMIT_SHA) - name: BASE value: main - name: BODY value: Update prod image for $(params.COMMIT_MESSAGE) - name: TITLE value: \u0026#39;Production update: $(params.COMMIT_MESSAGE)\u0026#39; runAfter: - create-prod-manifest-branch taskRef: kind: Task name: git-open-pull-request Review the whole Pipeline We did it, we created a Secure Supply Chain using Tekton Tasks. The full Pipeline now looks like this:\nFigure 3. Pipeline Details The last step will create a pull request on Git. When this request is approved and merged, the update will finally happen in the production environment. This is a manual process to have control what comes in production and what does not.\nExecute full Pipeline E2E It is time to execute the whole pipeline now end to end. We will do a real update to the application now, so we can see the differences.\nAs described in step 10, the DEV and PROD environments are running on the same cluster. In the field, this will probably not happen, but for now, it is good enough. GitOps/Argo CD monitors any changes and automatically updates whenever the Git repository (Kubernetes Manifests) is changed. During the PipelineRun we will update the image tag for DEV, which automatically rolls out and create a Pull request which is waiting for approval and will roll out the changes onto production.\nBoth environments have a route to access the application. At the moment both will look the same:\nFigure 4. Globex DEV origin Update application The repository of Globex UI is forked at: https://github.com/tjungbauer/globex-ui. We used it throughout this journey to update the README.md file. The readme file does not really change anything. So, let’s update the UI itself.\nlook for the file src/index.html and add the following line before \u0026lt;/body\u0026gt;\n\u0026lt;center\u0026gt;\u0026lt;strong\u0026gt;My very important update\u0026lt;/strong\u0026gt;\u0026lt;/center\u0026gt; Save this change and push it to GitHub. This will trigger the Pipeline which is running quite long. However, once it is finished, the DEV environment should now show the new line in the UI.\nAfter the pipeline updated the image tag in Git, the GitOps process must fetch this change. This may take a while. You can speed this up by refreshing the \u0026#34;Application\u0026#34; inside the Argo CD interface. It should then automatically synchronize. The update can now be seen in the browser. The \u0026#34;important update\u0026#34; is visible at the bottom of the page.\nFigure 5. Globex DEV updated The production environment was not yet updated. Instead, a pull request has been created:\nFigure 6. Open pull request This request can be reviewed and merged. As you can see there was only one change in the files:\nFigure 7. Open pull request - changed files Merge the pull request and wait until Argo CD fetched the changes and updates the production environment. This is it, the changes are done and promoted to production:\nFigure 8. Globex PROD updated Conclusion This concludes this journey to a Secure Supply Chain using Tekton (OpenShift Pipelines). Is this the best must-have you need to do? No, it is an example, a demonstration. Feel free to use and modify it. You can also use other tools for the tasks or the pipeline as such. It does not matter if you use Tekton, Jenkins, Gitlab Runner etc. What is important is that you secure your whole supply chain as much as possible. Any image you create should be signed to ensure that the source can be trusted. Every source code should be verified against best practices and all images should be scanned for vulnerabilities and policy violations during the build AND the deployment process.\n"},{"uri":"https://blog.stderr.at/tags/argo-cd/","title":"Argo CD","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/day-2/","title":"Day-2","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/gitops/","title":"GitOps","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/oc/","title":"oc","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/operator/","title":"Operator","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2023-03-20-operator-installation-with-argocd/","title":"Operator installation with Argo CD","tags":["oc","operator","OpenShift","OCP","GitOps","Argo CD"],"description":"Openshift 4.x - Using Argo CD to deploy Operators","content":" GitOps for application deployment and cluster configuration is a must-have I am trying to convince every customer to follow from the very beginning when starting the Kubernetes journey. For me, as more on the infrastructure side of things, I am more focused on the configuration of an environment. Meaning, configuring a cluster, installing an operator etc.\nIn this article, I would like to share how I deal with cluster configuration when certain Kubernetes objects are dependent on each other and how to use Kubernetes but also Argo CD features to resolve these dependencies.\nThis article assumes that you have the openshift-gitops Operator, which provides Argo CD, already installed, and configured. If you are new to GitOps check out this article: Argo CD TL;DR If you want to jump directly to the technical fun part, go here: Let’s start.\nThe Idea Everything should be seen as a code. Everything should be possible to be deployed in a repeatable way. With a GitOps approach, everything is stored naturally in Git and from there a GitOps agent validates and synchronizes changes to one or more clusters.\nWhen it comes to OpenShift, Red Hat supports Argo CD using the Operator openshift-gitops. This gives you everything you need to deploy an Argo CD instance. The only thing you need to take care of is a Git repository, no matter if it is GitHub, Gitlab, Bitbucket etc.\nThe Problem Sometimes Kubernetes objects depend on each other. This is especially true when you would like to install and configure Operators, where the configuration, based on a Customer Resource Definition (CRD), can only happen after the Operator has been installed and is ready.\nWhy is that? Well, when you want to deploy an Operator, you will store a “Subscription object” in Git. Argo CD will take this object and applies it to the cluster. However, for an Operator, the creation of the Subscription object is just the first step. A lot of other steps are required until the Operator gets ready. Unfortunately, Argo CD cannot verify if the installation is successful. All it sees is that the Subscription object has been created and then it immediately tries to deploy the CRD. The CRD which is not yet available on the system because the Operator is still installing it.\nEven if you use Argo CD features like Sync waves it would wait until the Operator is successfully installed because for Argo CD the “success” is the creation of the Subscription object.\nSubsequently, the Argo CD synchronisation process will fail. You could now try to automatically “Retry” the sync or use multiple Argo CD applications that you execute one after each other, but I was not fully happy with that and tried a different approach.\nMy Solution Let’s say I would like to deploy and configure the Compliance Operator. The steps would be:\nInstall the Operator.\nWait until the Operator is ready.\nConfigure Operator specific CRDs.\nThis “Wait until the Operator is ready” is the tricky party for Argo CD. What I have done is the following:\nInstall the Operator, this is the first step and is done during Sync Wave 0.\nCreate a Kubernetes Job that verifies the status of the Operator. This Job additionally requires a ServiceAccount and a role with a binding. The configured Sync Wave is 1. Moreover, I use a Hook (another Argo CD feature) with the deletion policy “HookSucceeded”. This makes sure that the Job, ServiceAccount, Role and RoleBinding are removed after the status has been verified. The verification is successful as soon as the Operator status says “Succeeded”. In fact, all the Job does is to execute some oc commands. For example,\noc get clusterserviceversion openshift-gitops-operator.v1.8.0 -n openshift-gitops -o jsonpath={.status.phase} Succeeded Finally, during the next Sync Wave (2+) the CRD can be deployed. In this case, I deploy the object ScanSettingBinding.\nIn Argo CD everything is correctly synchronized, and the Operator and its configuration are in place.\nIf you are new to the compliance operator, I recommend the following article: Compliance Operator I use this approach for every Operator that I would like to install and configure at the same time. For example, I do the same for Advanced Cluster Security or Advanced Cluster Management where I use the Job to verify if everything is ready before I let Argo CD continue.\nMore information about Sync Waves and Hooks can be found in the official Argo CD documentation: Sync Phases and Waves Let’s see this in Action Prerequisites\nOpenShift cluster 4.x\nopenshift-gitops is installed and ready to be used.\nAccess to GitHub (or to your own Repository)\nI will be using my Helm Chart repository at https://charts.stderr.at/ and from there the charts:\ncompliance-operator-full-stack\nhelper-operator (sub chart): Responsible to install the Operators.\nhelper-status-checker (sub chart): Responsible to check the status of the Operator.\nWhy do I use Helm charts? There is no specific reason for that. I started with Helm for the cluster configuration and now it has evolved with a separate Chart repository and sub-charts and so on.\nArgo CD Application In Argo CD I have the following Application:\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: in-cluster-install-compliance-scans namespace: openshift-gitops spec: destination: namespace: default server: \u0026#39;https://kubernetes.default.svc\u0026#39; (1) info: - name: Description value: Deploy and configure the Compliance Scan Operator project: in-cluster source: path: charts/compliance-operator-full-stack (2) repoURL: \u0026#39;https://github.com/tjungbauer/helm-charts\u0026#39; targetRevision: main 1 Installing on the local cluster where Argo CD is installed. 2 Git configuration, including path and revision. Actually, this Application is created out of an ApplicationSet, but I did not want to make it too complex :) The Application would like to synchronize the objects:\nSubscription\nOperatorGroup\nNamespace (openshift-compliance)\nScanSettingBinding\nFigure 1. Argo CD: Installing Compliance Operator Where are the objects we need for the Job? Since they are only available during the Sync-Hook they will not show up here. In fact, they will only show up during the time they are alive and will disappear again after the status of the operator has been verified. Helm Chart Configuration The Helm Chart gets its configuration from a values file. You can verify the whole file on GitHub.\nThe important pieces here are that some variables are handed over to the appropriate Sub Charts.\nOperator Configuration This part is handed over to the Chart “helper-operator”.\nhelper-operator: operators: compliance-operator: enabled: true syncwave: \u0026#39;0\u0026#39; namespace: name: openshift-compliance create: true subscription: channel: release-0.1 approval: Automatic operatorName: compliance-operator source: redhat-operators sourceNamespace: openshift-marketplace operatorgroup: create: true notownnamespace: true It is executed during Sync Wave 0 and defines if a Namespace (openshift-compliance) shall be created (true) and the specification of the Operator which you need to know upfront:\nchannel: Defines which channel shall be used. Some operators offer different channels.\napproval: Either Automatic or Manuel … defines if the Operator shall be updated automatically or requires and approval.\noperatorName: the actual name of the Operator (compliance-operator)\nsource: Where does this Operator come from (redhat-operator)\nsourceNamespace: In this case openshift-marketplace\nYou can fetch these values by looking at the Packagemanifest:\noc get packagemanifest compliance-operator -o yaml Status Checker Configuration This part is handed over to the Sub-Chart \u0026#34;helper-status-checker\u0026#34;\u0026#34;. The main values here are the operatorName and the namespace where the Operator is installed.\nWhat is not visible here is the Sync Wave, which is per default set to 1 inside the Helm Chart. If you need to overwrite it, it can be configured in this section as well.\nhelper-status-checker: enabled: true (1) # use the value of the currentCSV (packagemanifest) but WITHOUT the version !! operatorName: compliance-operator (2) # where operator is installed namespace: name: openshift-compliance (3) serviceAccount: create: true name: \u0026#34;sa-compliance\u0026#34; (4) 1 Is the status checker enabled or is it not. 2 The name of the operator as it is reported by the value currentCSV inside the packageManifest 3 The namespace where the Operator has been installed. 4 The name of the ServiceAccount that is created temporarily. The operatorName is sometimes different than the Operator name required for helper-operator chart. Here it seems the value of the currentCSV must be used but without the version number. (The Job will look up the version itself) Operator CRD configuration The final section of the values file manages the configuration for the Operator itself. This section does not use a Sub Chart. Instead, the variables are used in the Main-Chart. In this example, the ScanSettingBinding will be configured during Sync Wave 3, which is all we need to basic functionality.\ncompliance: scansettingbinding: enabled: true syncwave: \u0026#39;3\u0026#39; (1) profiles: (2) - name: ocp4-cis-node - name: ocp4-cis scansetting: default 1 Define the Sync Wave. This value must be higher than the Sync Wave of the helper-status-checker 2 ScanSettingBinding configuration. Two profiles are used in this example. Synchronizing Argo CD Basic Application in Argo CD before it is synced:\nFigure 2. Argo CD: Application Sync Wave 0: Synchronization has started. Namespace and Subscription are deployed.\nFigure 3. Argo CD: Synchronization is started (Sync Wave 0) Sync Wave 1: Status Checker Job has started and tries to verify the Operator.\nFigure 4. Argo CD: Status Checker Job started (Sync Wave 1) The Log output of the Operator. You can see that the status switches from Pending, to Installing to Succeeded.\nFigure 5. Argo CD: Log of the Status Checker Pod After Sync Wave 3 the whole Application has been synchronized and the Checker Job has been removed.\nFigure 6. Argo CD: Compliance Operator is fully deployed "},{"uri":"https://blog.stderr.at/tags/cert-manager/","title":"Cert Manager","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/certificates/","title":"Certificates","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ssl/","title":"SSL","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2023-16-02-ssl-certificate-management/","title":"SSL Certificate Management for OpenShift on AWS","tags":["oc","OpenShift","OCP","SSL","Certificates","Cert Manager"],"description":"Using cert-manager to automatically issue new certificates","content":" Finally, after a long time on my backlog, I had some time to look into the Cert-Manager Operator and use this Operator to automatically issue new SSL certificates. This article shall show step-by-step how to create a certificate request and use this certificate for a Route and access a service via your Browser. I will focus on the technical part, using a given domain on AWS Route53.\nIntroduction After a new OpenShift Cluster has been deployed, self-signed certificates are used to access the Routes (for example the console) and the API. Typically, an application is exposed to the world using the schema \u0026lt;app-name\u0026gt;-\u0026lt;namespace-name\u0026gt;.apps.\u0026lt;clusterdomain\u0026gt;.\nWe will try to create a certificate for a specific application with a custom domain name and for the cluster domains: *.apps.clusterdomain and api.clusterdomain.\nThe domain is already available and delegated to AWS Route53. As certificate authority, I am using Let’s Encrypt.\nWe will install 2 operators:\nCert Manager: to issue new certificates.\nCert Utils Operator: injects the certificate into a Route object.\nThe Cert Utils Operator can provide additional information for a certificate and monitors the expiration date. Here we mainly use it to automatically inject Route objects by defining specific annotations. Prerequisites OpenShift cluster with a user that has privileges to install Operators.\nA domain hosted for example at Route 53\nCredentials for your Cloud Provider (AWS)\nDeploy an Example Application Let’s use the super complex demo application bookimport.\noc new-project bookimport oc apply -f https://raw.githubusercontent.com/tjungbauer/book-import/master-no-pre-post/book-import/deployment.yaml -n bookimport oc apply -f https://raw.githubusercontent.com/tjungbauer/book-import/master-no-pre-post/book-import/service.yaml -n bookimport oc expose service book-import -n bookimport oc get route -n bookimport The last command will print you an URL which, copied into the browser, will open our application:\nFigure 1. Application Book Import using HTTP As you can see in the address line the connection is not secured (Nicht sicher in German) and my domain is *.apps.ocp.aws.ispworld.at\nConfigure an AWS user for accessing Route 53 On AWS I have currently 2 Zones, the public aws.ispworld.at and a private zone, created by the OpenShift Installer.\nFigure 2. Domain Zones Before you can manage your domains a user with appropriate privileges must be created.\nStore the following in the file policy.json. This will allow a user to perform DNS Upgrades.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;route53:GetChange\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:route53:::change/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;route53:ChangeResourceRecordSets\u0026#34;, \u0026#34;route53:ListResourceRecordSets\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:route53:::hostedzone/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53:ListResourceRecordSets\u0026#34;, \u0026#34;route53:ListHostedZonesByName\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Apply the new policy to AWS and store the ARN into a variable:\naws iam create-policy --policy-name AllowDNSUpdates --policy-document file://policy.json export POLICY_ARN=$(aws iam list-policies --query \u0026#39;Policies[?PolicyName==`AllowDNSUpdates`].Arn\u0026#39; --output text) Create the user route53-openshift and assign the policy to that user:\naws iam create-user --user-name route53-openshift aws iam attach-user-policy --policy-arn $POLICY_ARN --user-name route53-openshift Finally, create the access key and store the AccessKeyId and the SecretAccessKey for later use:\naws iam create-access-key --user-name route53-openshift --output json { \u0026#34;AccessKey\u0026#34;: { \u0026#34;UserName\u0026#34;: \u0026#34;route53-openshift\u0026#34;, \u0026#34;AccessKeyId\u0026#34;: \u0026#34;XXXXXXXXXXXXXX\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;Active\u0026#34;, \u0026#34;SecretAccessKey\u0026#34;: \u0026#34;XXXXXXXXXXXXXXXXXXX\u0026#34;, \u0026#34;CreateDate\u0026#34;: \u0026#34;2023-02-15T12:34:06+00:00\u0026#34; } } Installing Operators to OpenShift We will install 2 Operators to our cluster:\ncert-manager Operator for Red Hat OpenShift\nCert Utils Operator\nSimply search both on OLM and install them keeping the default values.\nThe Cert Utils Operator is a Community Operator. Figure 3. Operators This will install the Cert-Manager into the namespace openshift-cert-manager\nConfigure the Cert-Manager Operator Before we can issue a certificate, we need to create a secret with our AWS SecretAccessKey (see above):\noc create secret generic prod-route53-credentials-secret --from-literal secret-access-key=\u0026#34;XXXXXXXXXXXXXXXXXXX\u0026#34; -n openshift-cert-manager As next step, we create a ClusterIssuer that will be available cluster-wide using Let’s Encrypt as certificate authority:\nThe connection to Let’s Encrypt is using the productive API. If you would like to use the staging environment instead, change the server URL to https://acme-staging-v02.api.letsencrypt.org/directory apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: email: your@email.com (1) preferredChain: \u0026#39;\u0026#39; privateKeySecretRef: name: letsencrypt-account-key server: \u0026#39;https://acme-v02.api.letsencrypt.org/directory\u0026#39; solvers: - dns01: route53: accessKeyID: XXXXXXXXXXXXXX (2) region: eu-central-1 (3) secretAccessKeySecretRef: key: secret-access-key name: prod-route53-credentials-secret (4) selector: dnsZones: - your-domain (5) 1 Change your email address 2 Use the AccessKeyId created above 3 Using AWS you need to define a region 4 The name of the secret created during the step before 5 Your public domain, for example aws.ispworld.at. Once created the ClusterIssuer should switch to the status \u0026#34;Ready\u0026#34;\noc describe clusterissuer letsencrypt-prod Status: ... Conditions: Last Transition Time: 2023-02-16T13:54:49Z Message: The ACME account was registered with the ACME server Observed Generation: 1 Reason: ACMEAccountRegistered Status: True Type: Ready OPTIONAL: When using private Domains or Firewalls As you can see in one of the images above, I have two domains:\naws.ispworld.at\nocp.aws.ispworld.at\nThe first one is marked as public, that means everybody can resolve names. The second one is set to private and only define VPCs (in this case the cluster itself) can resolve hostnames.\nIn case of the following error:\nE0216 15:27:29.513080 1 controller.go:163] cert-manager/challenges \u0026#34;msg\u0026#34;=\u0026#34;re-queuing item due to error processing\u0026#34; \u0026#34;error\u0026#34;=\u0026#34;failed to determine Route 53 hosted zone ID: zone not found in Route 53 for domain _acme-challenge.bookimport.apps.ocp.aws.ispworld.at.\u0026#34; \u0026#34;key\u0026#34;=\u0026#34;bookimport/bookimport-cert-jbmh6-2173685137-2399596362\u0026#34; Add the following into ClusterManager\noc edit CertManager.operator.openshift.io/cluster unsupportedConfigOverrides: controller: args: - --v=2 - --cluster-resource-namespace=$(POD_NAMESPACE) - --leader-election-namespace=kube-system - --dns01-recursive-nameservers-only - --dns01-recursive-nameservers=ns-362.awsdns-45.com:53,ns-930.awsdns-52.net:53 (1) 1 List of nameserver the PUBLIC domain is hosted on. The Operator will then try to resolve the names using the specified nameserver only.\nIssue a new certificate At this step, we can create a Certificate:\napiVersion: cert-manager.io/v1 kind: Certificate metadata: name: bookimport-cert (1) namespace: bookimport (2) spec: dnsNames: - bookimport.apps.ocp.aws.ispworld.at (3) issuerRef: kind: ClusterIssuer name: letsencrypt-prod (4) secretName: bookimport.apps.ocp.aws.ispworld.at-certificate (5) 1 Name of the certificate objects 2 Application namespace 3 List of domain names 4 Issuer that shall be used 5 Name of the Secret that will be created and hold the certificate information Create a Route After a while the certificate will be Ready:\noc get certificate/bookimport-cert -n bookimport NAME READY SECRET AGE bookimport-cert True bookimport.apps.ocp.aws.ispworld.at-certificate 87m Now we can create a Route object to configure the IngressController. The important part here is the annotation, which will tell the Cert Utils Operator to automatically inject the certificate.\nkind: Route apiVersion: route.openshift.io/v1 metadata: name: bookimport-tls namespace: bookimport annotations: cert-utils-operator.redhat-cop.io/certs-from-secret: bookimport.apps.ocp.aws.ispworld.at-certificate (1) spec: host: bookimport.apps.ocp.aws.ispworld.at (2) to: kind: Service name: book-import weight: 100 tls: termination: edge port: targetPort: web wildcardPolicy: None 1 Annotation that points to the Secret which stored the certificate. The values of this Secret will be automatically injected into this Route object. 2 The hostname for our Route As you can see, the Browser will show no warning when opening the URL.\nFigure 4. Book Import using HTTPS Cluster Default Certificates During a cluster deployment, OpenShift will create self-signed certificates for its API and for the default IngressController *.apps.clusterdomain.\nUsually, we want to change them as well. So why not use the Cert-Manager to issue the appropriate certificates?\nDefault IngressController For the default IngressController I create a certificate request with 2 domain names: the wildcard and the base domain (just to be sure, actually the wildcard should be enough)\napiVersion: cert-manager.io/v1 kind: Certificate metadata: name: router-certificate namespace: openshift-ingress (1) spec: dnsNames: - apps.ocp.aws.ispworld.at (2) - \u0026#39;*.apps.ocp.aws.ispworld.at\u0026#39; issuerRef: kind: ClusterIssuer name: letsencrypt-prod secretName: router-certificate (3) 1 The default IngressController runs in the namespace openshift-ingress 2 List of domains 3 Name of the Secret that will be created once the Certificate has been approved. After a while, the certificate request should be Ready again. In the namespace openshift-ingress a Secret will be available with the name router-certificate\nAPI For the API URL we do the same. This time it is stored in the namepsace openshift-config\napiVersion: cert-manager.io/v1 kind: Certificate metadata: name: api-certificate namespace: openshift-config spec: dnsNames: - api.ocp.aws.ispworld.at issuerRef: kind: ClusterIssuer name: letsencrypt-prod secretName: api-certificate It is possible to create one certificate with all required Domainnames. Just be sure that the Secret is available in the appropriate Namespace. Patching API Server and IngressController As a final step we need to patch the IngressController and the API server so they will use the correct Secrets with the officially signed certificates.\n# IngressController oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch=\u0026#39;{\u0026#34;spec\u0026#34;: { \u0026#34;defaultCertificate\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;router-certificate\u0026#34; }}}\u0026#39; # API Server oc patch apiserver cluster --type=merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;servingCerts\u0026#34;: {\u0026#34;namedCertificates\u0026#34;: [{\u0026#34;names\u0026#34;: [\u0026#34;api.ocp.aws.ispworld.at\u0026#34;], \u0026#34;servingCertificate\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;api-certificate\u0026#34;}}]}}}\u0026#39; (1) 1 Be sure to use the correct URL for the API This will restart a bunch of services. Once everything is up and running again (your can watch using the command watch oc get co), the correct certificate will be shown in the browser:\nFigure 5. UI or via curl:\ncurl -v https://api.ocp.aws.ispworld.at:6443 * Connected to api.ocp.aws.ispworld.at (13.52.208.31) port 6443 [...] * Server certificate: * subject: CN=api.ocp.aws.ispworld.at * start date: Feb 16 15:11:36 2023 GMT * expire date: May 17 15:11:35 2023 GMT * subjectAltName: host \u0026#34;api.ocp.aws.ispworld.at\u0026#34; matched cert\u0026#39;s \u0026#34;api.ocp.aws.ispworld.at\u0026#34; * issuer: C=US; O=Let\u0026#39;s Encrypt; CN=R3 * SSL certificate verify ok. Summary Now with these steps, it is possible to issue new Certificates. Of course, there I many more options to configure a certificate. I encourage everybody to read the official documentation of the Cert Manager.\nEspecially, if you are interested in the whole certificate Certificate Lifecycle\n"},{"uri":"https://blog.stderr.at/categories/argocd/","title":"ArgoCD","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/gitops/","title":"GitOps","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/kubectl/","title":"kubectl","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2022-11-04-argocd-and-serversideapply/","title":"Using ServerSideApply with ArgoCD","tags":["oc","kubectl","OpenShift","OCP","GitOps","Argo CD"],"description":"Using ServerSideApply with ArgoCD","content":" „If it is not in GitOps, it does not exist“ - However, managing objects partially only by Gitops was always an issue, since ArgoCD would like to manage the whole object. For example, when you tried to work with node labels and would like to manage them via Gitops, you would need to put the whole node object into ArgoCD. This is impractical since the node object is very complex and typically managed by the cluster. There were 3rd party solutions (like the patch operator), that helped with this issue.\nHowever, with the Kubernetes feature Server-Side Apply this problem is solved. Read further to see a working example of this feature.\nWhat is Server-Side Apply (SSA) Quoting from Kuberneted Documentation:\nServer-Side Apply helps users and controllers manage their resources through declarative configurations. Clients can create and modify their objects declaratively by sending their fully specified intent.\nA fully specified intent is a partial object that only includes the fields and values for which the user has an opinion. That intent either creates a new object or is combined, by the server, with the existing object.\nIn other words: you can send a snippet of an object to the cluster and the cluster will eventually combine everything on the server and not validate on the client side first. All you need is a way to identify the object. Usually, the name and maybe the namespace too.\nSSA and ArgoCD When it comes to GitOps the implementation of SSA is quite new. However, it is important to note, that (managed field) conflicts are currently not handled by ArgoCD. Instead, ArgoCD forces a change and overrides everything, even if the field is managed by somebody else. This might be improved in the future. Nevertheless …​ let’s test the feature.\nPrerequisites The support of the Server-Side Apply feature is currently available in the latest version of ArgoCD. This means, that the channel of the openshift-gitops operator must be changed to \u0026#34;latest\u0026#34;, which will deploy openshift-gitops version 1.6\nA new stable version will arrive soon. :)\nNode Labelling Chart In this example, I would like to use a Helm chart that will try to set two different labels on 2 nodes. This is a very easy example to demonstrate the feature.\nAs a Helm chart, I have prepared the following: https://github.com/tjungbauer/openshift-cluster-bootstrap/tree/main/clusters/management-cluster/node-labels\nThe values for this chart are straightforward: per node, a list of custom labels is defined.\nhelper-server-side-apply: nodes: (1) - name: ip-10-0-233-237.us-west-1.compute.internal (2) enabled: true custom_labels: (3) environment: \u0026#39;Production\u0026#39; gpu: false - name: ip-10-0-193-67.us-west-1.compute.internal enabled: true custom_labels: environment: \u0026#39;Test\u0026#39; gpu: true 1 List of nodes 2 Node name as OpenShift knows the node (oc get nodes) 3 List of labels that should be added to the node: here environment and gpu The Chart is using a sub-chart called helper-server-side-apply. The source can be found at the Helm Repository The output of this Helm Chart will be the following:\n# Source: node-labels/charts/helper-server-side-apply/templates/node.yaml kind: Node apiVersion: v1 metadata: name: \u0026#34;ip-10-0-233-237.us-west-1.compute.internal\u0026#34; (1) labels: gitops.ownedBy: openshift-gitops helm.sh/chart: helper-server-side-apply-1.0.3 app.kubernetes.io/name: helper-server-side-apply app.kubernetes.io/instance: release-name app.kubernetes.io/managed-by: Helm environment: \u0026#34;Production\u0026#34; (2) gpu: \u0026#34;false\u0026#34; (3) --- # Source: node-labels/charts/helper-server-side-apply/templates/node.yaml kind: Node apiVersion: v1 metadata: name: \u0026#34;ip-10-0-193-67.us-west-1.compute.internal\u0026#34; labels: gitops.ownedBy: openshift-gitops helm.sh/chart: helper-server-side-apply-1.0.3 app.kubernetes.io/name: helper-server-side-apply app.kubernetes.io/instance: release-name app.kubernetes.io/managed-by: Helm environment: \u0026#34;Test\u0026#34; gpu: \u0026#34;true\u0026#34; 1 The name of the node and our identifier 2 The first label we set 3 The second label we set This is not a full definition of a Node object. The only things defined are the node name and the labels. (Besides the customer labels we would like to add, some default labels are added automatically.) ArgoCD Application So we have a Helm chart in Git. Perfect, but to automate everything with Gitops we need to create the object Application. For example the following:\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: node-labelling namespace: openshift-gitops spec: destination: namespace: default server: \u0026#39;https://kubernetes.default.svc\u0026#39; info: - name: Description value: Deploy Node Labels project: default source: helm: valueFiles: - values.yaml path: clusters/management-cluster/node-labels (1) repoURL: \u0026#39;https://github.com/tjungbauer/openshift-cluster-bootstrap\u0026#39; targetRevision: main syncPolicy: syncOptions: - ServerSideApply=true (2) - Validate=false (3) 1 Path and URL of the node labelling Helm chart 2 Must be set to true to enable SSA 3 Must be set to false to skip schema validation The two syncOptions are important to set. Since the yaml output might not pass the validation, the schema validation should be disabled. This will create the following application in ArgoCD:\nFigure 1. Argo CD: Application Syncing the Application When you now synchronize the ArgoCD application, ArgoCD will take the yaml and will tell Kubernetes (or OpenShift) to perform a Server-Side Apply. This will result in the following yaml for the node:\nkind: Node apiVersion: v1 metadata: name: ip-10-0-193-67.us-west-1.compute.internal labels: beta.kubernetes.io/os: linux app.kubernetes.io/instance: node-labelling [...] node-role.kubernetes.io/worker: \u0026#39;\u0026#39; gitops.ownedBy: openshift-gitops [...] environment: Test [...] That’s it …​ all the magic is done.\n"},{"uri":"https://blog.stderr.at/tags/day-2/","title":"Day-2","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/secrets/","title":"Secrets","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2022-08-16-hashicorp-vault/","title":"Secrets Management - Vault on OpenShift","tags":["OCP","Day-2","OpenShift","Vault","Secrets"],"description":"Secrets Management with HashiCorp Vault","content":" Sensitive information in OpenShift or Kubernetes is stored as a so-called Secret. The management of these Secrets is one of the most important questions, when it comes to OpenShift. Secrets in Kubernetes are encoded in base64. This is not an encryption format. Even if etcd is encrypted at rest, everybody can decode a given base64 string which is stored in the Secret.\nFor example: The string Thomas encoded as base64 is VGhvbWFzCg==. This is simply a masked plain text and it is not secure to share these values, especially not on Git. To make your CI/CD pipelines or Gitops process secure, you need to think of a secure way to manage your Secrets. Thus, your Secret objects must be encrypted somehow. HashiCorp Vault is one option to achieve this requirement.\nHashiCorp Vault HashiCorp Vault is a solution to solve our Secret management problem. It stores and encrypts our sensitive information at rest and in transit and enables you to create fine grained access controls (ACL). This defines who has access to a specific secret. Application A should only get access to secret A and so on. However, that is just the tip of the iceberg. Vault can do much more. If you are interested in further details, check out the introduction video by Armon, the Co-Founder of Hashicorp Vault at: https://learn.hashicorp.com/tutorials/vault/getting-started-intro?in=vault/getting-started\nFor this article we will keep it simple and cover:\nInstalling HashiCorp Vault to OpenShift\nIntegrating the plugin \u0026#34;Kubernetes Authentication\u0026#34; to access the Secrets\nAccessing a static secret stored in HashiCorp Vault by an example application\nInstalling Vault The easiest way to install Vault is by using the supported Helm chart.\nAdd the Helm repository to your repo:\nhelm repo add hashicorp https://helm.releases.hashicorp.com Update the repository to get the latest updates.\nhelm repo update Before we install Vault, we need to create a values file to set certain variables. Let’s create a file called \u0026#34;overwrite-values.yaml\u0026#34; with the following content\nglobal: openshift: true server: ha: enabled: true replicas: 3 raft: enabled: true This will tell HashiCorp Vault the environment we are going to install is (OpenShift), enables high availability with 3 replicas and enables RAFT (see below for some introduction).\nFinally, let us deploy HashiCorp Vault into the namespace vault using the values file we have created in the previous step:\nhelm upgrade --install vault hashicorp/vault --values bootstrap/vault/overwrite-values.yaml --namespace=vault --create-namespace This will start the agent-injector and 3 vault-pods:\noc get pods -n vault NAME READY STATUS RESTARTS AGE vault-0 0/1 Running 0 36s vault-1 0/1 Running 0 36s vault-2 0/1 Running 0 36s vault-agent-injector-74c848f67b-sq4dq 1/1 Running 0 37s vault agent injector: detecting applications with annotations that require vault agent which will get injected\nvault-0: vault server\nThe vault servers remain in not-ready state until Vault has been unsealed by you.\nWhen you check the logs of one of the pods you will see:\noc logs vault-0 -n vault ... 2022-08-11T12:31:22.497Z [INFO] core: security barrier not initialized 2022-08-11T12:31:22.497Z [INFO] core: seal configuration missing, not initialized Vault always starts uninitialized and sealed, to protect the secrets. You need to give at least three different unseal-keys in order to be able to use Vault.\nVault’s seal mechanism uses Shamir’s secret sharing. This is a manual process to secure the cluster if it restarts. You can use auto-unseal for specific cloud providers to bypass the manual requirement. Raft Storage During the deployment we have enabled Raft which is the integrated HA storage for Vault. Vault can use several different styles of storage backends, but when it comes to HA and Kubernetes, Raft is easiest one since it has no other dependencies, and it is well known inside OpenShift. Etcd is using Raft as well. It is a distributed Consensus Algorithm where multiple members form a cluster and elect one leader. The leader has the responsibility to replicate everything to the followers. Since this leader election requires a majority an odd number of cluster members must be available to ensure there is a minimum number left if a member is failing.\nInitialize and Unseal Vault As mentioned, the Vault Pods are not fully available yet, since Vault it currently sealed and cannot be used. The first thing to do is to initialize and unseal it.\nThe following commands will login into one Pod and execute commands from there. Let’s get the status of our Vault fist:\noc -n vault exec -it vault-0 -- vault operator init -status This will return the message: Vault is not initialized\nThe following command will initialize Vault for further usage:\noc exec -ti -n vault vault-0 -- vault operator init -format=json \u0026gt; unseal.json This will create the file unseal.json locally on your machine. Keep this file secure It contains by default 5 key shards and the root token you will need to unseal Vault and authenticate as root.\nKeep this file secure, it contains keys to unseal Vault and the root_token to authenticate against. Never share this file. It will look like this:\n{ \u0026#34;unseal_keys_b64\u0026#34;: [ \u0026#34;key_1\u0026#34;, \u0026#34;key_2\u0026#34;, \u0026#34;key_3\u0026#34;, \u0026#34;key_4\u0026#34;, \u0026#34;key_5\u0026#34; ], \u0026#34;unseal_keys_hex\u0026#34;: [ \u0026#34;key_hex_1\u0026#34;, \u0026#34;key_hex_2\u0026#34;, \u0026#34;key_hex_3\u0026#34;, \u0026#34;key_hex_4\u0026#34;, \u0026#34;key_hex_5\u0026#34; ], \u0026#34;unseal_shares\u0026#34;: 5, \u0026#34;unseal_threshold\u0026#34;: 3, \u0026#34;recovery_keys_b64\u0026#34;: [], \u0026#34;recovery_keys_hex\u0026#34;: [], \u0026#34;recovery_keys_shares\u0026#34;: 5, \u0026#34;recovery_keys_threshold\u0026#34;: 3, \u0026#34;root_token\u0026#34;: \u0026#34;root.token\u0026#34; } With the initialization in place Vault is put into a sealed mode. This means Vault cannot decrypt secrets at this moment. To unseal Vault you need the unseal key, which is split into multiple shards using Shamir’s secret sharing. A certain number of individual shards (default 3) must be provided to reconstruct the unseal key.\nTo unseal Vault lets login to our Pod \u0026#34;vault-0\u0026#34; and unseal it. Use the following command and provide one of the keys:\noc exec -ti -n vault vault-0 -- vault operator unseal Unseal Key (will be hidden): Key Value --- ----- Seal Type shamir Initialized true (1) Sealed true (2) Total Shares 5 Threshold 3 Unseal Progress 1/3 (3) Unseal Nonce 08b01535-be15-e865-251c-f948ed0661c9 Version 1.11.2 Build Date 2022-07-29T09:48:47Z Storage Type raft HA Enabled true 1 Vault is initialized 2 Vault is still sealed 3 The unseal progress: Currently 1 out of 3 keys have been provided Use the same command another 2 times using different keys to complete the unseal process:\nAt the end the following output should be shown:\nUnseal Key (will be hidden): Key Value --- ----- Seal Type shamir Initialized true Sealed false (1) Total Shares 5 Threshold 3 Version 1.11.2 Build Date 2022-07-29T09:48:47Z Storage Type raft Cluster Name vault-cluster-f7402e5b (2) Cluster ID aff648f0-b3a2-1fdd-12f6-492842b08b2b HA Enabled true HA Cluster https://vault-0.vault-internal:8201 HA Mode active (3) Active Since 2022-08-16T07:20:45.828215961Z Raft Committed Index 36 Raft Applied Index 36 1 Vault is now unsealed 2 Name of our cluster 3 High availability is enabled Vault-0 is now initialized, but there are 2 other members in our HA cluster which must be added. Let vault-1 and vault-2 join the cluster and perform the same unseal process as previously: use 3 different keys:\noc exec -ti vault-1 -n vault -- vault operator raft join http://vault-0.vault-internal:8200 # 3 times.... oc exec -ti vault-1 -n vault -- vault operator unseal oc exec -ti vault-2 -n vault -- vault operator raft join http://vault-0.vault-internal:8200 # 3 times... oc exec -ti vault-2 -n vault -- vault operator unseal Verify Vault Cluster To verify if the Raft cluster has successfully been initialized, run the following.\nFirst, login using the root_token, that was created above, on the vault-0 pod.\noc exec -ti vault-0 -n vault -- vault login Token (will be hidden): \u0026lt;root_token\u0026gt; oc exec -ti vault-0 -n vault -- vault operator raft list-peers This should return:\nNode Address State Voter ---- ------- ----- ----- 16ec7490-f621-42ea-976d-5f054cfaeecc vault-0.vault-internal:8201 leader true 60ba2885-432a-c7d3-d280-a824f0acce42 vault-1.vault-internal:8201 follower true bcc4f551-79bc-47e5-d01b-97fc12d1afa5 vault-2.vault-internal:8201 follower true As you can see Vault-0 is the leader while the other two members are followers.\nConfigure Kubernetes Authentication There are multiple ways how an application can interact with Vault. One example is to use Tokens. This is quite easy but has the disadvantage that it does require additional steps of managing the life cycle of such token, moreover they might be shared, which is not what we want.\nHashiCorp Vault supports different authentication methods. One of which is the Kubernetes Auth Method that must be enabled before we can use. The Kubernetes Auth Method makes use of Jason Web Tokens (JWT)s that are bound to a Service Account. When we tell Vault that a Service Account is fine to authenticate, then a Deployment using this account is able to authenticate and request Secrets.\nVault has a plugin ecosystem, which allows to enable certain plugins. To enable Kubernetes Auth Method use the following process:\nLogin vault-0 pods oc exec -it vault-0 -n vault — /bin/sh\nexecute the command: vault auth enable kubernetes which returns: Success! Enabled kubernetes auth method at: kubernetes/\nSet up the Kubernetes configuration to use Vault’s service account JWT.\nthe address to the OpenShift API (KUBERNETES_PORT_443_TCP_ADDR) is automatically available via an environment variable. vault write auth/kubernetes/config issuer=\u0026#34;\u0026#34; \\ token_reviewer_jwt=\u0026#34;$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\u0026#34; \\ kubernetes_host=\u0026#34;https://$KUBERNETES_PORT_443_TCP_ADDR:443\u0026#34; \\ kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt Success! Data written to: auth/kubernetes/config With this step authentication against OpenShift is enabled.\nConfigure a Secret With the Kubernetes Auth Method in place we can configure a secret to test our setup. We will use an example application called expenses that has a MySQL database. The static password to bootstrap this database shall be stored in Vault. A plugin called key-value secrets engine will be used to achieve this.\nThere are other plugins that are specifically designed to automatically rotate secrets. For example, it is possible to dynamically create user credentials für MySQL.\nYou can list available engines by using the command: oc -n vault exec -it vault-0 — vault secrets list There are currently two versions of this key/value engine:\nKV Version 1: does not versionize the key/values, thus updates will overwrite the old values.\nKV Version 2: does versionize the key/value pairs\nIn our example we will use version 2.\nLike the authentication method, we need to enable the secrets engine:\noc -n vault exec -it vault-0 -- vault secrets enable \\ -path=expense/static \\ (1) -version=2 \\ (2) kv (3) 1 API path where our secrets are stored 2 Version 2 3 name of our engine You can list the enabled engines with the following command:\noc -n vault exec -it vault-0 -- vault secrets list Path Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_f1e955f9 per-token private secret storage expense/static/ kv kv_6db09e5d n/a identity/ identity identity_ca05e6ab identity store (1) sys/ system system_e34a76c3 system endpoints used for control, policy and debugging 1 Enabled KV secrets engine using the path expense/static/ Now lets put a secret into our store. We will store our super-secure MySQL password into expense/static/mysql\nMYSQL_DB_PASSWORD=mysuperpassword$ oc -n vault exec -it vault-0 -- vault kv put expense/static/mysql db_login_password=${MYSQL_DB_PASSWORD} This command will store the key db_login_password with the database as value. We can get the secret by calling:\noc -n vault exec -it vault-0 -- vault kv get expense/static/mysql ====== Secret Path ====== expense/static/data/mysql (1) ======= Metadata ======= Key Value --- ----- created_time 2022-08-17T06:08:05.839663508Z custom_metadata \u0026lt;nil\u0026gt; deletion_time n/a destroyed false version 1 ========== Data ========== Key Value --- ----- db_login_password mysuperpassword$ (2) 1 The data path of our secret 2 our password Configuring policies The Secret is now stored at expense/static/mysql but there is no policy in place. Everybody who is authenticated and is calling this path will get to see the secrets. Luckily, one or more policies can be assigned to the authentication method. A policy defines capabilities that allow you to perform certain actions.\nThe following capabilities are known:\ncreate - to create new data\nread - to read data\ndelete - to delete data\nlist - to list data\nPolicies can be written either in JSON or HCL (HashiCorp Configuration Language). Let’s create a file with the following content:\npath \u0026#34;expense/static/data/mysql\u0026#34; { capabilities = [\u0026#34;read\u0026#34;, \u0026#34;list\u0026#34;] } KV Version2 stores the secrets in a path with the prefix data/ This will limit my access to read and list only.\nWrite the policy:\ncat my-policy.hcl | oc -n vault exec -it vault-0 -- vault policy write expense-db-mysql - Next, we are going to bind the Vault secret to a service account and a namespace. Both objects will be created later, when we deploy the application.\noc -n vault exec -it vault-0 -- vault write auth/kubernetes/role/expense-db-mysql \\ (1) bound_service_account_names=expense-db-mysql \\ (2) bound_service_account_namespaces=expenses \\ (3) policies=expense-db-mysql \\ (4) ttl=1h (5) 1 Path or our new role 2 Name of the service account we will create and that will be used by the application 3 Name of the namespace we will create 4 Name of the policy we created earlier 5 The token is valid for 1 hour, after this period the service account must re-authenticate Let’s start an Application Now we will create our MySQL application into the namespace expenses. Use the following command to create the namespace and the application containing the objects Deployment, ServiceAccount (expense-db-mysql) and Service.\nSee at the Github Page for a full yaml specification of the three objects. oc new-project expenses oc apply -f https://raw.githubusercontent.com/joatmon08/vault-argocd/part-1/database/deployment.yaml The deployment will start a Pod with a sidecar container vault-agent. This sidecar is automatically created and must not be defined inside the Deployment specification. Instead, some annotations in the Deployment define what the container should be automatically injected and also where to find our secret:\nannotations: vault.hashicorp.com/agent-inject: \u0026#34;true\u0026#34; (1) vault.hashicorp.com/role: \u0026#34;expense-db-mysql\u0026#34; (2) vault.hashicorp.com/agent-inject-secret-db: \u0026#34;expense/static/data/mysql\u0026#34; (3) vault.hashicorp.com/agent-inject-template-db: | (4) {{ with secret \u0026#34;expense/static/data/mysql\u0026#34; -}} export MYSQL_ROOT_PASSWORD=\u0026#34;{{ .Data.data.db_login_password }}\u0026#34; {{- end }} ... spec: serviceAccountName: expense-db-mysql (5) 1 Defines that the vault-agent side car container shall be automatically injected. This is the most important annotation. 2 Name of the role that was created created previously 3 The agent will inject the data from expense/static/data/mysql and stores it in a file db The file name is everything that comes after vault.hashicorp.com/agent-inject-secret- 4 Configuration…​ the template that defines how the secret will be rendered 5 The service account name we bound our secret to, using the Consul language. In this case the MySQL password is simply exported The vault-agent is requesting the database password from Vault and provides it to the application where it is stored at /vault/secrets/db\noc -n expenses exec -it $(oc get pods -l=app=expense-db-mysql -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) -c expense-db-mysql -- cat /vault/secrets/db # output export MYSQL_ROOT_PASSWORD=\u0026#34;mysuperpassword$\u0026#34; The Deployment sources this file when it starts and MySQL will take this information to configure itself.\nTIP: Using vault CLI on your local environment All above commands that are dealing with Vault commands, first login to a pod and then execure the commands from there.\nIf you have the Vault CLI installed on your local machine, you can open a port forwarding to your Vault cluster at OpenShift and execute the commands locally:\noc port-forward -n vault svc/vault 8200 export VAULT_ADDR=http://localhost:8200 vault login ... Thanks Thanks to the wonderful Rosemary Wang and her Github repository: https://github.com/joatmon08/vault-argocd/tree/part-1\nAlso check out the Youtube Video: GitOps Guide to the Galaxy (Ep 31) | GitOps With Vault Part 1 in which Rosemary and Christian discuss this setup\n"},{"uri":"https://blog.stderr.at/tags/vault/","title":"Vault","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2022-04-22-multi-cloud-gateway/","title":"Overview of Red Hat&#39;s Multi Cloud Gateway (Noobaa)","tags":[],"description":"","content":" This is my personal summary of experimenting with Red Hat\u0026#39;s Multi Cloud Gateway (MCG) based on the upstream Noobaa project. MCG is part of Red Hat\u0026#39;s OpenShift Data Foundation (ODF). ODF bundles the upstream projects Ceph and Noobaa.\nOverview Noobaa, or the Multicloud Gateway (MCG), is a S3 based data federation tool. It allows you to use S3 backends from various sources and\nsync replicate or simply use existing S3 buckets. Currently the following sources, or backing stores are supported:\nAWS S3 Azure Blob Google Cloud Storage Any other S3 compatible storage, for example\nCeph Minio Noobaa also supports using a local file system as a backing store for S3.\nThe main purpose is to provide a single API endpoint for applications using various S3 backends.\nOne of the main features of Noobaa is the storage pipeline. With a standard Noobaa S3 bucket, when storing a new Object Noobaa executes the following steps:\nChunking of the Object De-duplication Compression and Encryption This means that data stored in public cloud S3 offerings is automatically encrypted. Noobaa also supports using Hashicorp Vault for storing and retrieving encryption keys.\nIf you need to skip the storage pipeline, Noobaa also supports namespace buckets. For example these type of buckets allow you to write directly to AWS S3 and retrieve Objects via Noobaa. Or it could be used to migrate buckets from one cloud provider to another.\nNoobaa also has support for triggering JavaScript based function when\ncreating new objects reading existing objects deleting objects Setup With OpenShift Plus or an OpenShift Data Foundation subscription you can use the OpenShift Data Foundation Operator.\nFor testing Noobaa we used the standalone installation method without setting up Ceph storage (see here). OpenShift was running in AWS for testing.\nIf you would like to use the upstream version you can use the Noobaa operator (https://github.com/noobaa/noobaa-operator). This is what the OpenShift Data Foundation (ODF) is using as well.\nCommand line interface Noobaa also comes with a command line interface noobaa. It\u0026#39;s available via an ODF subscription or can be installed separately. See the noobaa-operator readme for more information.\nResources Before using an S3 object store with Noobaa we need to create so called Resources. This can be done via the Noobaa user interface or via the command line. For example the following commands create a new Resource using an AWS S3 bucket as a backing store\n# create an S3 bucket in eu-north-1 aws s3api create-bucket \\ --region eu-north-1 \\ --bucket tosmi-eu-north-1 \\ --create-bucket-configuration LocationConstraint=eu-north-1 # create an S3 bucket in eu-north-1 aws s3api create-bucket \\ --region eu-west-1 \\ --bucket tosmi-eu-west-1 \\ --create-bucket-configuration LocationConstraint=eu-west-1 # create Noobaa backing store using the tosmi-eu-north-1 bucket above noobaa backingstore create aws-s3 \\ --region eu-north-1 \\ --target-bucket tosmi-eu-north-1 aws-eu-north # create Noobaa backing store using the tosmi-eu-west-1 bucket above noobaa backingstore create aws-s3 \\ --region eu-west-1 \\ --target-bucket tosmi-eu-west-1 aws-eu-west Or if we would like to use Azure blob\n# create two resource groups for storage az group create --location northeurope -g mcg-northeurope # create two storage accounts az storage account create --name mcgnortheurope -g mcg-northeurope --location northeurope --sku Standard_LRS --kind StorageV2 # create containers for storing blobs az storage container create --account-name mcgnortheurope -n mcg-northeurope # list storage account keys for noobaa az storage account list az storage account show -g mcg-northeurope -n mcgnortheurope az storage account keys list -g mcg-westeurope -n mcgwesteurope az storage account keys list -g mcg-northeurope -n mcgnortheurope noobaa backingstore create \\ azure-blob azure-northeurope \\ --account-key=\u0026#34;\u0026lt;the key\u0026gt;\u0026#34; \\ --account-name=mcgnortheurope \\ --target-blob-container=mcg-northeurope Using\nnoobaa backingstore list we are able to confirm that our stores were created successfully.\nBuckets After creating the backend stores we are able to create Buckets and define the layout of backends.\nThere are two ways how to create buckets, either directly via the Noobaa UI, or using Kubernetes (K8s) objects.\nWe will focus on using K8s objects in this post.\nRequired K8s objects The Noobaa operator provides the following Custom Resource Definitions:\nBackingStore: we already created BackingStores in the Resources section BucketClass: a bucket class defines the layout of our bucket (single, mirrored or tiered) StorageClass: a standard K8s StorageClass referencing the BucketClass ObjectBucketClaim: A OBC or ObjectBucketClaim creates the bucket for us in Noobaa. Additionally the Noobaa operator creates a ConfigMap and a Secret with the same name as the Bucket, storing access details (ConfigMap) and credentials (Secret) for accessing the bucket. BucketClass Let\u0026#39;s create a example BucketClass which mirrors objects between the AWS S3 buckets eu-west-1 and eu-north-1.\napiVersion: noobaa.io/v1alpha1 kind: BucketClass metadata: labels: app: noobaa name: aws-mirrored-bucket-class namespace: openshift-storage spec: placementPolicy: tiers: - backingStores: - aws-eu-north - aws-eu-west placement: Mirror So we are defining a BucketClass aws-mirrored-bucket-class that has the following placement policy:\nA single tier with one backing store The backing store uses two AWS buckets\naws-eu-north aws-eu-west The placement policy is mirror, so all objects uploaded to buckets using this BucketClass will be mirrored between aws-eu-north and aws-eu-west. A BucketClass could have multiple tiers, moving cold data transparently to a lower tier, but let\u0026#39;s keep this simple.\nStorageClass After creating our BucketClass we are now able to define a standard K8s StorageClass:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: description: Provides Mirrored Object Bucket Claims (OBCs) in AWS name: aws-mirrored-openshift-storage.noobaa.io parameters: bucketclass: aws-mirrored-bucket-class provisioner: openshift-storage.noobaa.io/obc reclaimPolicy: Delete volumeBindingMode: Immediate This StorageClass uses our BucketClass aws-mirrored-bucket-class as a backend. All buckets created leveraging this StorageClass will mirror data between aws-eu-north and aws-eu-west (see the previous chapter).\nObjectBucketClaim Finally we are able to create ObjectBucketClaims for projects requiring object storage. An ObjectBucketClaim is similar to an PersistentVolumeClaim. Every time a claim is created the Noobaa operator will create a corresponding S3 bucket for us.\nLet\u0026#39;s start testing this out by creating a new OpenShift project\noc new-project obc-test Now we define a ObjectBucketClaim to create a new bucket for our application:\napiVersion: objectbucket.io/v1alpha1 kind: ObjectBucketClaim metadata: labels: app: noobaa name: aws-mirrored-claim spec: generateBucketName: aws-mirrored storageClassName: aws-mirrored-openshift-storage.noobaa.io We use the StorageClass created in the previous step. This will create\na S3 Bucket in the requested StorageClass a ConfigMap storing access information a Secret storing credentials for accessing the S3 Bucket For testing we will upload some data via s3cmd and use a pod to monitor data within the bucket.\nLet\u0026#39;s do the upload with s3cmd, we need the following config file:\n[default] check_ssl_certificate = False check_ssl_hostname = False access_key = \u0026lt;access key\u0026gt; secret_key = \u0026lt;secret key\u0026gt; host_base = s3-openshift-storage.apps.ocp.aws.tntinfra.net host_bucket = %(bucket).s3-openshift-storage.apps.ocp.aws.tntinfra.net Of course you must change host-base according to your cluster name. It\u0026#39;s a route in the openshift-storage namespace:\noc get route -n openshift-storage s3 -o jsonpath=\u0026#39;{.spec.host}\u0026#39; You can extract the access and secret key from the K8s secret via:\noc extract secret/aws-mirrored-claim --to=- Copy the access key and the secret key to the s3 command config file (we\u0026#39;ve called our config noobaa-s3.cfg). Now we can list all available buckets via:\n$ s3cmd ls -c noobaa-s3.cfg 2022-04-22 13:56 s3://aws-mirrored-c1087a17-5c84-4c62-9f36-29081a6cf5a4 Now we are going to upload a sample file:\n$ s3cmd -c noobaa-s3.cfg put simple-aws-mirrored-obc.yaml s3://aws-mirrored-c1087a17-5c84-4c62-9f36-29081a6cf5a4 upload: \u0026#39;simple-aws-mirrored-obc.yaml\u0026#39; -\u0026gt; \u0026#39;s3://aws-mirrored-c1087a17-5c84-4c62-9f36-29081a6cf5a4/simple-aws-mirrored-obc.yaml\u0026#39; [1 of 1] 226 of 226 100% in 0s 638.18 B/s done We can also list available files via\ns3cmd -c noobaa-s3.cfg ls s3://aws-mirrored-c1087a17-5c84-4c62-9f36-29081a6cf5a4 2022-04-22 13:57 226 s3://aws-mirrored-c1087a17-5c84-4c62-9f36-29081a6cf5a4/simple-aws-mirrored-obc.yaml Our we could use a Pod to list available files from within OpenShift. Note how we use the ConfigMap and the Secret the Noobaa operater created for us, when we created the ObjectBucketClaim:\napiVersion: batch/v1 kind: Job metadata: name: s3-test-job spec: template: metadata: name: s3-pod spec: containers: - image: d3fk/s3cmd:latest name: s3-pod env: - name: BUCKET_NAME valueFrom: configMapKeyRef: name: aws-mirrored-claim key: BUCKET_NAME - name: BUCKET_HOST valueFrom: configMapKeyRef: name: aws-mirrored-claim key: BUCKET_HOST - name: BUCKET_PORT valueFrom: configMapKeyRef: name: aws-mirrored-claim key: BUCKET_PORT - name: AWS_ACCESS_KEY_ID valueFrom: secretKeyRef: name: aws-mirrored-claim key: AWS_ACCESS_KEY_ID - name: AWS_SECRET_ACCESS_KEY valueFrom: secretKeyRef: name: aws-mirrored-claim key: AWS_SECRET_ACCESS_KEY command: - /bin/sh - -c - \u0026#39;s3cmd --host $BUCKET_HOST --host-bucket \u0026#34;%(bucket).$BUCKET_HOST\u0026#34; --no-check-certificate ls s3://$BUCKET_NAME\u0026#39; restartPolicy: Never That\u0026#39;s all for now. If time allows we are going to write a follow up blog post on\nReplicating Buckets and Functions "},{"uri":"https://blog.stderr.at/java/2022-02-25-jpa-disconnected-entity/","title":"Adventures in Java Land: JPA disconnected entities","tags":[],"description":"","content":" An old man tries to refresh his Java skills and does DO378. He fails spectacularly at the first real example but learns a lot on the way.\nThe exception There is this basic example where you build a minimal REST API for storing speaker data in a database. Quarkus makes this quite easy. You just have to define your database connection properties in resources/application.properties and off you go developing your Java Quarkus REST service:\nquarkus.datasource.db-kind=h2 quarkus.datasource.jdbc.url=jdbc:h2:mem:default quarkus.datasource.username=admin quarkus.hibernate-orm.database.generation=drop-and-create quarkus.hibernate-orm.log.sql=true So next we define our entity class to be stored in the database. I will skip the import statements and any other code not relevant for this post.\n// import statements skipped @Entity public class Speaker extends PanacheEntity { public UUID uuid; public String nameFirst; public String nameLast; public String organization; @JsonbTransient public String biography; public String picture; public String twitterHandle; // Constructors, getters and setters, toString and other methods skipped .... } We define an entity Speaker which extends the PanacheEntity class. Panache is a thin wrapper around Hibernate providing convince features. For example the base class PanacheEntity defines a autoincrement Id column for us. This inherited Id column is of importance for understanding the problem ahead of us.\nSo next you define your SpeakerService class which uses the entity. Once again I will skip the imports and any code not relevant for understanding the problem:\n// imports omitted @ApplicationScoped public class SpeakerService { // other code omitted public Speaker create(Speaker speaker) { speaker.persist(); return speaker; } We focus on the create method here because the call to speaker.persist() was the reason for all the headache.\nBut we are still in coding mode and last but not least we define our SpeakerResource class, again everything not relevant for understanding the problem was removed:\n// import statements omitted @Path(\u0026#34;/speaker\u0026#34;) @Produces(MediaType.APPLICATION_JSON) @Consumes(MediaType.APPLICATION_JSON) public class SpeakerResource { @Inject SpeakerService service; // other code omitted @POST @Transactional public Speaker create(Speaker newSpeaker) { service.create(newSpeaker); return newSpeaker; } } The root path for our SpeakerResource is /speaker. We inject the SpeakerService and define a method create() for creating a Speaker. We would like to be able to send @Post requests to this endpoint and Jsonb or Jackson, whichever we currently prefer, will deserialize the JSON body in a Speaker object for us.\nSplendid, time to switch from coding mode to testing.\nWe launch that Quarkus application in developer mode\nmvn quarkus:dev Quarkus is so friendly and provides a swagger-ui in dev mode for testing our endpoint. Super duper lets call the create() endpoint via Swagger:\nBecause we are lazy we accept the default Swagger provides for us and just click Execute.\nBOOM, 500 internal server error. And a beautiful Java exception:\norg.jboss.resteasy.spi.UnhandledException: javax.persistence.PersistenceException: org.hibernate.PersistentObjectException: detached entity passed to persist: org.acme.conference.speaker.Speaker What? Detached entity what does this mean and why?\nEnlightenment Behind the scenes Hibernate uses a so called EntityManager for managing entities. An Entity can be in the following states when managed by Hibernate:\nNEW: The entity object was just created and is not persisted to the database MANAGED: The entity is managed by a running Session and all changes to the entity will be propagated to the database. After call to entitymanager.persist() or in our case newSpeaker.persist() the entity is stored in the database and in the managed state. REMOVED: The entity is removed from the database. And finally DETACHED: The Entity was detached from the EntityManager, e.g. by calling entitymanager.detach() or entitymanager.close(). See this blog for a way better explanation what is going on with entity states.\nOk, cool but why the hell is our Speaker entity in the DETACHED state? It was just created and never saved to the database before!\nAfter checking the database (was empty), I started my Java debugger of choice (IntellJ, but use whatever fit\u0026#39;s your needs. I\u0026#39;m to old for IDE vs Editor and Editor vs Editor wars).\nSo looking at the Speaker entity before calling persist() revealed the following:\nThe Speaker object passed into create() has an Id of 0 and all the internal Hibernate fields are set to null. So this seems to indicate that this Speaker object is currently not attached to an EntityManager session. This might explain the DETACHED state.\nI started playing around with EntityManager and calling merge() on the speaker object. The code looked like this:\n@ApplicationScoped public class SpeakerService { @Inject EntityManager em; // lots of code skipped public Speaker create(Speaker speaker) { var newSpeaker = em.merge(speaker); newSpeaker.persist(); return speaker; } Looking at the newSpeaker object returned by calling entitymanager.merge() in the debugger revealed the following:\nnewSpeaker has an Id of 1 (hm, why no 0?) and some those special Hibernate fields starting with $$ have a value assigned. So for me this indicates that the object is now managed by an EntityManager session and in the MANAGED state.\nAnd the Id, already assigned to the original Speaker object, de-serialized form JSON is actually the reason for the beautiful exception above.\nExplanation So after a little bit of internet search magic I found an explanation for the exception:\nIf an Id is already assigned to an entity object, Hibernate assumes that this is an entity in the DETACHED state (if the Id is auto-generated). For an entity to be persisted to the database it has to be transferred in the MANAGED state by calling entitymanager.merge()\nFor more information see the Hibernate documentation.\nWe can only call persist() if the object is in the transient state, to quote the Hibernate documentation:\ntransient: the entity has just been instantiated and is not associated with a persistence context. It has no persistent representation in the database and typically no identifier value has been assigned (unless the assigned generator was used).\nAnd reading on we also get explanation for the detached state:\ndetached: the entity has an associated identifier but is no longer associated with a persistence context (usually because the persistence context was closed or the instance was evicted from the context)\nJust removing the Id from the POST request will solve the issue and the example started to work.\nThis is also why the Id column is different in the Speaker object (deserialized from JSON) and newSpeaker object (create by calling entitymanager.merge()). The Speaker Id got passed in from JSON, and has nothing to do with the auto generated primary key Id within our database. After calling entitymanager.merge() the entity is actually associated with a database session and the Id is auto generated.\nSo maybe this is basic stuff, but it took me quite a few hours to understand what was going on.\nMaybe this is also a bad example. Should one expose the Id if it is auto generated and only used internally? Or the code just needs to handle that case… But this needs me more learning about API design.\n"},{"uri":"https://blog.stderr.at/categories/java/","title":"Java","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/etcd/2022-01-29-automatedetcdbackup/","title":"Automated ETCD Backup","tags":["OCP","Day-2","OpenShift","etcd"],"description":"Create ETCD backups using cronjobs","content":" Securing ETCD is one of the major Day-2 tasks for a Kubernetes cluster. This article will explain how to create a backup using OpenShift Cronjob.\nThere is absolutely no warranty. Verify your backups regularly and perform restore tests. Prerequisites The following is required:\nOpenShift Cluster 4.x\nIntegrated Storage, might be NFS or anything. Best practice would be a RWX enabled storage.\nConfigure Project \u0026amp; Cronjob Create the following objects in OpenShift. This fill create:\nA Project called ocp-etcd-backup\nA PersistentVolumeClaim to store the backups. Change to your appropriate StorageClass and accessMode\nA ServiceAccount called openshift-backup\nA dedicated ClusterRole which is able to start (debug pods)\nA ClusterRoleBinding between the created ServiceAccount and the customer ClusterRole\nA 2nd ClusterRoleBinding, which gives our ServiceAccount the permission to start privileged containers. This is required to start a debug pod on a control plane node.\nA CronJob which performs the backup …​ see Callouts for inline explanations.\nA helm chart, which would create these objects below, can be found at: https://github.com/tjungbauer/ocp-auto-backup. This is probably a better way to manage the variables via the values.yaml file. kind: Namespace apiVersion: v1 metadata: name: ocp-etcd-backup annotations: openshift.io/description: Openshift Backup Automation Tool openshift.io/display-name: Backup ETCD Automation openshift.io/node-selector: \u0026#39;\u0026#39; spec: {} --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: etcd-backup-pvc namespace: ocp-etcd-backup spec: accessModes: - ReadWriteOnce (1) resources: requests: storage: 100Gi storageClassName: gp2 volumeMode: Filesystem --- kind: ServiceAccount apiVersion: v1 metadata: name: openshift-backup namespace: ocp-etcd-backup --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-etcd-backup rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - \u0026#34;nodes\u0026#34; verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: - \u0026#34;pods\u0026#34; - \u0026#34;pods/log\u0026#34; verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;watch\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: openshift-backup subjects: - kind: ServiceAccount name: openshift-backup namespace: ocp-etcd-backup roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-etcd-backup --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: etcd-backup-scc-privileged roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:openshift:scc:privileged subjects: - kind: ServiceAccount name: openshift-backup namespace: ocp-etcd-backup --- kind: CronJob apiVersion: batch/v1 metadata: name: cronjob-etcd-backup namespace: ocp-etcd-backup labels: purpose: etcd-backup spec: schedule: \u0026#39;*/5 * * * *\u0026#39; (2) startingDeadlineSeconds: 200 concurrencyPolicy: Forbid suspend: false jobTemplate: metadata: creationTimestamp: null spec: backoffLimit: 0 template: metadata: creationTimestamp: null spec: nodeSelector: node-role.kubernetes.io/master: \u0026#39;\u0026#39; (3) restartPolicy: Never activeDeadlineSeconds: 200 serviceAccountName: openshift-backup schedulerName: default-scheduler hostNetwork: true terminationGracePeriodSeconds: 30 securityContext: {} containers: - resources: requests: cpu: 300m memory: 250Mi terminationMessagePath: /dev/termination-log name: etcd-backup command: (4) - /bin/bash - \u0026#39;-c\u0026#39; - \u0026gt;- oc get no -l node-role.kubernetes.io/master --no-headers -o name | grep `hostname` | head -n 1 | xargs -I {} -- oc debug {} -- bash -c \u0026#39;chroot /host sudo -E /usr/local/bin/cluster-backup.sh /home/core/backup\u0026#39; ; echo \u0026#39;Moving Local Master Backups to target directory (from /home/core/backup to mounted PVC)\u0026#39;; mv /home/core/backup/* /etcd-backup/; echo \u0026#39;Deleting files older than 30 days\u0026#39; ; find /etcd-backup/ -type f -mtime +30 -exec rm {} \\; securityContext: privileged: true runAsUser: 0 imagePullPolicy: IfNotPresent volumeMounts: - name: temp-backup mountPath: /home/core/backup (5) - name: etcd-backup mountPath: /etcd-backup (6) terminationMessagePolicy: FallbackToLogsOnError image: registry.redhat.io/openshift4/ose-cli serviceAccount: openshift-backup volumes: - name: temp-backup hostPath: path: /home/core/backup type: \u0026#39;\u0026#39; - name: etcd-backup persistentVolumeClaim: claimName: etcd-backup-pvc dnsPolicy: ClusterFirst tolerations: - operator: Exists effect: NoSchedule - operator: Exists effect: NoExecute successfulJobsHistoryLimit: 5 failedJobsHistoryLimit: 5 1 RWO is used here, since I have no other available storage on my test cluster. 2 How often shall the job be executed. Here, every 5 minutes. 3 Bind the job to \u0026#34;Master\u0026#34; nodes. 4 Command to be executed…​ It fetches the actual local master nodename and starts a debugging Pod there. The backup script is called and moves the backup to /home/core/backup which is a folder on the control plane itself. The move command will move the backups from the local folder to the actual backup target volume. Finally, it will remove backups older than 30 days. 5 Mounted /home/core/backup on the master nodes, here the command will store the backups before they are moved 6 Target destination for the etcd backup on the mounted PVC Start a Job If you do not want to wait until the CronJob is triggered, you can manually start the Job using the following commands:\noc create job backup --from=cronjob/cronjob-etcd-backup -n ocp-etcd-backup This will start a Pod which will do the backup:\nStarting pod/ip-10-0-196-187us-east-2computeinternal-debug ... To use host binaries, run `chroot /host` found latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-15 found latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10 found latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-9 found latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-3 etcdctl is already installed {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:1638199790.980932,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:119\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;created temporary db file\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/home/core/backup/snapshot_2021-11-29_152949.db.part\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2021-11-29T15:29:50.991Z\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;clientv3/maintenance.go:200\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;opened snapshot stream; downloading\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:1638199790.9912837,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:127\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;fetching snapshot\u0026#34;,\u0026#34;endpoint\u0026#34;:\u0026#34;https://10.0.196.187:2379\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2021-11-29T15:29:53.306Z\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;clientv3/maintenance.go:208\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;completed snapshot read; closing\u0026#34;} Snapshot saved at /home/core/backup/snapshot_2021-11-29_152949.db {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:1638199793.3482974,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:142\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;fetched snapshot\u0026#34;,\u0026#34;endpoint\u0026#34;:\u0026#34;https://10.0.196.187:2379\u0026#34;,\u0026#34;size\u0026#34;:\u0026#34;180 MB\u0026#34;,\u0026#34;took\u0026#34;:2.367303503} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:1638199793.348459,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:152\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;saved\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/home/core/backup/snapshot_2021-11-29_152949.db\u0026#34;} {\u0026#34;hash\u0026#34;:1180914745,\u0026#34;revision\u0026#34;:10182252,\u0026#34;totalKey\u0026#34;:19360,\u0026#34;totalSize\u0026#34;:179896320} snapshot db and kube resources are successfully saved to /home/core/backup Removing debug pod ... Moving Local Master Backups to target directory (from /home/core/backup to mounted PVC) Verifying the Backup Let’s start a dummy Pod which can access the PVC to verify if the backup is really there.\napiVersion: v1 kind: Pod metadata: name: verify-etcd-backup spec: containers: - name: verify-etcd-backup image: registry.access.redhat.com/ubi8/ubi command: [\u0026#34;sleep\u0026#34;, \u0026#34;3000\u0026#34;] volumeMounts: - name: etcd-backup mountPath: /etcd-backup volumes: - name: etcd-backup persistentVolumeClaim: claimName: etcd-backup-pvc Logging into that Pod will show the available backups stored at /etcd-backup which is the mounted PVC.\noc rsh -n ocp-etcd-backup verify-etcd-backup ls -la etcd-backup total 1406196 drwxr-xr-x. 3 root root 4096 Nov 29 17:00 . dr-xr-xr-x. 1 root root 25 Nov 29 17:06 .. drwx------. 2 root root 16384 Nov 29 15:21 lost+found -rw-------. 1 root root 179896352 Nov 29 15:21 snapshot_2021-11-29_152150.db -rw-------. 1 root root 179896352 Nov 29 15:29 snapshot_2021-11-29_152949.db -rw-------. 1 root root 179896352 Nov 29 15:32 snapshot_2021-11-29_153159.db -rw-------. 1 root root 179896352 Nov 29 15:36 snapshot_2021-11-29_153618.db -rw-------. 1 root root 179896352 Nov 29 15:55 snapshot_2021-11-29_155513.db -rw-------. 1 root root 179896352 Nov 29 16:00 snapshot_2021-11-29_160020.db -rw-------. 1 root root 179896352 Nov 29 16:55 snapshot_2021-11-29_165521.db -rw-------. 1 root root 179896352 Nov 29 17:00 snapshot_2021-11-29_170020.db -rw-------. 1 root root 89875 Nov 29 15:21 static_kuberesources_2021-11-29_152150.tar.gz -rw-------. 1 root root 89875 Nov 29 15:29 static_kuberesources_2021-11-29_152949.tar.gz -rw-------. 1 root root 89875 Nov 29 15:32 static_kuberesources_2021-11-29_153159.tar.gz -rw-------. 1 root root 89875 Nov 29 15:36 static_kuberesources_2021-11-29_153618.tar.gz -rw-------. 1 root root 89875 Nov 29 15:55 static_kuberesources_2021-11-29_155513.tar.gz -rw-------. 1 root root 89875 Nov 29 16:00 static_kuberesources_2021-11-29_160020.tar.gz -rw-------. 1 root root 89875 Nov 29 16:55 static_kuberesources_2021-11-29_165521.tar.gz -rw-------. 1 root root 89875 Nov 29 17:00 static_kuberesources_2021-11-29_170020.tar.gz "},{"uri":"https://blog.stderr.at/categories/environments/","title":"Environments","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/etcd/","title":"etcd","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/etcd/","title":"etcd","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/annotations/","title":"Annotations","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/environments/","title":"Environments","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ingress/","title":"Ingress","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ingresscontroller/","title":"IngressController","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/labels/","title":"Labels","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/nodeselector/","title":"NodeSelector","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/pod-placement/","title":"Pod Placement","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/pod-placement/","title":"Pod Placement","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/labels_environmets/2022-01-12-creatingenvironment/","title":"Working with Environments","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector","Environments","Ingress","IngressController","Labels","Annotations"],"description":"Create separate environments on one OpenShift cluster","content":" Imagine you have one OpenShift cluster and you would like to create 2 or more environments inside this cluster, but also separate them and force the environments to specific nodes, or use specific inbound routers. All this can be achieved using labels, IngressControllers and so on. The following article will guide you to set up dedicated compute nodes for infrastructure, development and test environments as well as the creation of IngressController which are bound to the appropriate nodes.\nPrerequisites Before we start we need an OpenShift cluster of course. In this example we have a cluster with typical 3 control plane nodes (labelled as master) and 7 compute nodes (labelled as worker)\noc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-149-168.us-east-2.compute.internal Ready worker 13h v1.21.1+6438632 # \u0026lt;-- will become infra ip-10-0-154-244.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # \u0026lt;-- will become infra ip-10-0-158-44.us-east-2.compute.internal Ready worker 15m v1.21.1+6438632 # \u0026lt;-- will become infra ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-188-198.us-east-2.compute.internal Ready worker 13h v1.21.1+6438632 # \u0026lt;-- will become worker-test ip-10-0-191-9.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # \u0026lt;-- will become worker-test ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-195-201.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # \u0026lt;-- will become worker-dev ip-10-0-199-235.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # \u0026lt;-- will become worker-dev We will use the 7 nodes to create the environments for:\nInfrastructure Services (3 nodes) - will be labelled as infra\nDevelopment Environment (2 nodes) - will be labelled as worker-dev\nTest Environment (2 nodes) - will be labelled as worker-test\nTo do this, we will label the nodes and create dedicated roles for them.\nCreate Nodes Labels and MachineConfigPools Let’s create a maschine config pool for the different environments.\nThe pool inherits the configuration from worker nodes by default, which means that any new update on the worker configuration will also update custom labeled nodes. Or in other words: it is possible to remove the worker label from the custom pools.\nCreate the following objects:\n# Infrastructure apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: infra spec: machineConfigSelector: matchExpressions: - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,infra]} (1) maxUnavailable: 1 (2) nodeSelector: matchLabels: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; (3) paused: false --- # Worker-DEV apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: worker-dev spec: machineConfigSelector: matchExpressions: - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-dev]} nodeSelector: matchLabels: node-role.kubernetes.io/worker-dev: \u0026#34;\u0026#34; --- # Worker-TEST apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: worker-test spec: machineConfigSelector: matchExpressions: - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-test]} nodeSelector: matchLabels: node-role.kubernetes.io/worker-test: \u0026#34;\u0026#34; 1 User worker and infra so that the default worker configuration gets applied during upgrades 2 Whenever an update happens, do only 1 at a time 3 This pool is valid for nodes which are labelled as infra Now let’s label our nodes. First add the new, additional label:\n# Add the label \u0026#34;infra\u0026#34; oc label node ip-10-0-149-168.us-east-2.compute.internal ip-10-0-154-244.us-east-2.compute.internal ip-10-0-158-44.us-east-2.compute.internal node-role.kubernetes.io/infra= # Add the label \u0026#34;worker-dev\u0026#34; oc label node ip-10-0-195-201.us-east-2.compute.internal ip-10-0-199-235.us-east-2.compute.internal node-role.kubernetes.io/worker-dev= # Add the label \u0026#34;worker-test\u0026#34; oc label node ip-10-0-188-198.us-east-2.compute.internal ip-10-0-191-9.us-east-2.compute.internal node-role.kubernetes.io/worker-test= This will result in:\nNAME STATUS ROLES AGE VERSION ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-149-168.us-east-2.compute.internal Ready infra,worker 13h v1.21.1+6438632 ip-10-0-154-244.us-east-2.compute.internal Ready infra,worker 20m v1.21.1+6438632 ip-10-0-158-44.us-east-2.compute.internal Ready infra,worker 19m v1.21.1+6438632 ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-188-198.us-east-2.compute.internal Ready worker,worker-test 13h v1.21.1+6438632 ip-10-0-191-9.us-east-2.compute.internal Ready worker,worker-test 20m v1.21.1+6438632 ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-195-201.us-east-2.compute.internal Ready worker,worker-dev 20m v1.21.1+6438632 ip-10-0-199-235.us-east-2.compute.internal Ready worker,worker-dev 20m v1.21.1+6438632 We can remove the worker label from the worker-dev and worker-test nodes now.\nKeep the label \u0026#34;worker\u0026#34; for the infra nodes as this is the default worker label which is used when no nodeselector is in use. You can use any other node, just keep in mind that per default new applications will be started on nodes with the labels \u0026#34;worker\u0026#34;. As an alternative, you can also define a cluster-wide default node selector. oc label node ip-10-0-195-201.us-east-2.compute.internal ip-10-0-199-235.us-east-2.compute.internal node-role.kubernetes.io/worker- oc label node ip-10-0-188-198.us-east-2.compute.internal ip-10-0-191-9.us-east-2.compute.internal node-role.kubernetes.io/worker- The final node labels will look like the following:\nNAME STATUS ROLES AGE VERSION ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-149-168.us-east-2.compute.internal Ready infra,worker 13h v1.21.1+6438632 ip-10-0-154-244.us-east-2.compute.internal Ready infra,worker 22m v1.21.1+6438632 ip-10-0-158-44.us-east-2.compute.internal Ready infra,worker 21m v1.21.1+6438632 ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-188-198.us-east-2.compute.internal Ready worker-test 13h v1.21.1+6438632 ip-10-0-191-9.us-east-2.compute.internal Ready worker-test 21m v1.21.1+6438632 ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-195-201.us-east-2.compute.internal Ready worker-dev 21m v1.21.1+6438632 ip-10-0-199-235.us-east-2.compute.internal Ready worker-dev 22m v1.21.1+6438632 Since the custom pools (infra, worker-test and worker-dev) inherit their configuration from the default worker pool, no changes on the files on the nodes themselves are triggered at this point. Create Custom Configuration Let’s test our setup by deploying a configuration on specific nodes. The following MaschineConfig objects will create a file at /etc/myfile on the nodes labelled either infra, worker-dev or worker_test. Dependent on the node role the content of the file will vary.\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: infra (1) name: 55-infra spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:,infra (2) filesystem: root mode: 0644 path: /etc/myfile (3) --- apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: worker-dev name: 55-worker-dev spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:,worker-dev filesystem: root mode: 0644 path: /etc/myfile --- apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: worker-test name: 55-worker-test spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:,worker-test filesystem: root mode: 0644 path: /etc/myfile 1 Valid for node with the role xyz 2 Content of the file 3 File to be created Since this is a new configuration, all nodes will get reconfigured.\nNAME STATUS ROLES AGE VERSION ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-149-168.us-east-2.compute.internal Ready,SchedulingDisabled infra,worker 13h v1.21.1+6438632 ip-10-0-154-244.us-east-2.compute.internal Ready infra,worker 28m v1.21.1+6438632 ip-10-0-158-44.us-east-2.compute.internal Ready infra,worker 27m v1.21.1+6438632 ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-188-198.us-east-2.compute.internal Ready worker-test 13h v1.21.1+6438632 ip-10-0-191-9.us-east-2.compute.internal NotReady,SchedulingDisabled worker-test 27m v1.21.1+6438632 ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-195-201.us-east-2.compute.internal Ready worker-dev 27m v1.21.1+6438632 ip-10-0-199-235.us-east-2.compute.internal NotReady,SchedulingDisabled worker-dev 28m v1.21.1+6438632 Wait until all nodes are ready and test the configuration by verifying the content of /etc/myfile:\n### # infra nodes: ### oc get pods -n openshift-machine-config-operator -l k8s-app=machine-config-daemon --field-selector \u0026#34;spec.nodeName=ip-10-0-149-168.us-east-2.compute.internal\u0026#34; NAME READY STATUS RESTARTS AGE machine-config-daemon-f85kd 2/2 Running 6 16h # Get file content oc rsh -n openshift-machine-config-operator machine-config-daemon-f85kd chroot /rootfs cat /etc/myfile Defaulted container \u0026#34;machine-config-daemon\u0026#34; out of: machine-config-daemon, oauth-proxy infra ### #worker-dev: ### oc get pods -n openshift-machine-config-operator -l k8s-app=machine-config-daemon --field-selector \u0026#34;spec.nodeName=ip-10-0-195-201.us-east-2.compute.internal\u0026#34; NAME READY STATUS RESTARTS AGE machine-config-daemon-s6rr5 2/2 Running 4 3h5m # Get file content oc rsh -n openshift-machine-config-operator machine-config-daemon-s6rr5 chroot /rootfs cat /etc/myfile Defaulted container \u0026#34;machine-config-daemon\u0026#34; out of: machine-config-daemon, oauth-proxy worker-dev ### # worker-test: ### oc get pods -n openshift-machine-config-operator -l k8s-app=machine-config-daemon --field-selector \u0026#34;spec.nodeName=ip-10-0-188-198.us-east-2.compute.internal\u0026#34; NAME READY STATUS RESTARTS AGE machine-config-daemon-m22rf 2/2 Running 6 16h # Get file content oc rsh -n openshift-machine-config-operator machine-config-daemon-m22rf chroot /rootfs cat /etc/myfile Defaulted container \u0026#34;machine-config-daemon\u0026#34; out of: machine-config-daemon, oauth-proxy worker-test The file /etc/myfile exists on all nodes and depending on their role the files have a different content.\nBind an Application to a Specific Environment The following will label the nodes with a specific environment and will deploy an example application, which should only be executed on the appropriate nodes.\nLet’s label the nodes with environment=worker-dev and environment=worker-test:\noc label node ip-10-0-195-201.us-east-2.compute.internal ip-10-0-199-235.us-east-2.compute.internal environment=worker-dev oc label node ip-10-0-188-198.us-east-2.compute.internal ip-10-0-191-9.us-east-2.compute.internal environment=worker-test Create a namespace for the example application\noc new-project bookinfo Create an annotation and a label for the namespace. The annotation will make sure that the application will only be started on nodes with the same label. The label will be later used for the IngressController setup.\noc annotate namespace bookinfo environment=worker-dev oc annotate namespace bookinfo openshift.io/node-selector: environment=worker-test oc label namespace bookinfo environment=worker-dev Deploy the example application. In this article the sample application of Istio was used:\noc apply -f https://raw.githubusercontent.com/istio/istio/release-1.11/samples/bookinfo/platform/kube/bookinfo.yaml This will start the application on worker-dev` nodes only, because the annotation in the namespace was created accordingly.\noc get pods -n bookinfo -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES details-v1-86dfdc4b95-v8zfv 1/1 Running 0 9m19s 10.130.2.17 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; productpage-v1-658849bb5-8gcl7 1/1 Running 0 7m17s 10.128.4.21 ip-10-0-195-201.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ratings-v1-76b8c9cbf9-cc4js 1/1 Running 0 9m19s 10.130.2.19 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v1-58b8568645-mbgth 1/1 Running 0 7m44s 10.128.4.20 ip-10-0-195-201.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v2-5d8f8b6775-qkdmz 1/1 Running 0 9m19s 10.130.2.21 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v3-666b89cfdf-8zv8w 1/1 Running 0 9m18s 10.130.2.22 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Create Dedicated IngressController IngressController are responsible to bring the traffic into the cluster. OpenShift comes with one default controller, but it is possible to create more in order to use different domains and separate the incoming traffic to different nodes.\nBind the default ingress controller to the infra labeled nodes, so we can be sure that the default router pods are executed only on these nodes:\noc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch=\u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;nodePlacement\u0026#34;:{\u0026#34;nodeSelector\u0026#34;: {\u0026#34;matchLabels\u0026#34;:{\u0026#34;node-role.kubernetes.io/infra\u0026#34;:\u0026#34;\u0026#34;}}}}}\u0026#39; The pods will get restarted, to be sure they are running on infra:\noc get pods -n openshift-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES router-default-78f8dd6f69-dbtbv 0/1 ContainerCreating 0 2s \u0026lt;none\u0026gt; ip-10-0-149-168.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-78f8dd6f69-wwpgb 0/1 ContainerCreating 0 2s \u0026lt;none\u0026gt; ip-10-0-158-44.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-7bbbc8f9bd-vfh84 1/1 Running 0 22m 10.129.4.6 ip-10-0-158-44.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-7bbbc8f9bd-wggrx 1/1 Terminating 0 19m 10.128.2.8 ip-10-0-149-168.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Create the following IngressController objects for worker-dev and worker-test. Replace with the domain of your choice\napiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: ingress-worker-dev namespace: openshift-ingress-operator spec: domain: worker-dev.\u0026lt;yourdomain\u0026gt; (1) endpointPublishingStrategy: type: HostNetwork httpErrorCodePages: name: \u0026#39;\u0026#39; namespaceSelector: matchLabels: environment: worker-dev (2) nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker-dev: \u0026#39;\u0026#39; (3) replicas: 3 tuningOptions: {} unsupportedConfigOverrides: null --- apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: ingress-worker-test namespace: openshift-ingress-operator spec: domain: worker-test.\u0026lt;yourdomain\u0026gt; endpointPublishingStrategy: type: HostNetwork httpErrorCodePages: name: \u0026#39;\u0026#39; namespaceSelector: matchLabels: environment: worker-test nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker-test: \u0026#39;\u0026#39; replicas: 3 tuningOptions: {} unsupportedConfigOverrides: null 1 Domainname which is used by this Controller 2 Namespace selector …​ namespaces with such label will be handled by this IngressController 3 Node Placement …​ This Controller should run on nodes with this label/role This will spin up additional router pods on the collect labelled nodes:\noc get pods -n openshift-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES router-default-78f8dd6f69-dbtbv 1/1 Running 0 8m7s 10.128.2.11 ip-10-0-149-168.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-78f8dd6f69-wwpgb 1/1 Running 0 8m7s 10.129.4.10 ip-10-0-158-44.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-ingress-worker-dev-76b65cf558-mspvb 1/1 Running 0 113s 10.130.2.13 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-ingress-worker-dev-76b65cf558-p2jpg 1/1 Running 0 113s 10.128.4.12 ip-10-0-195-201.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-ingress-worker-test-6bbf9967f-4whfs 1/1 Running 0 113s 10.131.2.13 ip-10-0-191-9.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-ingress-worker-test-6bbf9967f-jht4w 1/1 Running 0 113s 10.131.0.8 ip-10-0-188-198.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Verify Ingress Configuration To test our new ingress router lets create a route object for our example application:\nkind: Route apiVersion: route.openshift.io/v1 metadata: name: productpage namespace: bookinfo spec: host: productpage-bookinfo.worker-dev.\u0026lt;yourdomain\u0026gt; to: kind: Service name: productpage weight: 100 port: targetPort: http wildcardPolicy: None --- kind: Route apiVersion: route.openshift.io/v1 metadata: name: productpage-worker-test namespace: bookinfo spec: host: productpage-bookinfo.worker-test.\u0026lt;yourdomain\u0026gt; to: kind: Service name: productpage weight: 100 port: targetPort: http wildcardPolicy: None Be sure that the name is resolvable and a load balancer is configured accordingly Verify that the router pod has the correct configuration in the file haproxy.config :\noc get pods -n openshift-ingress NAME READY STATUS RESTARTS AGE router-default-78f8dd6f69-dbtbv 1/1 Running 0 95m router-default-78f8dd6f69-wwpgb 1/1 Running 0 95m router-ingress-worker-dev-76b65cf558-mspvb 1/1 Running 0 88m router-ingress-worker-dev-76b65cf558-p2jpg 1/1 Running 0 88m router-ingress-worker-test-6bbf9967f-4whfs 1/1 Running 0 88m router-ingress-worker-test-6bbf9967f-jht4w 1/1 Running 0 88m Verify the content of the haproxy configuration for one of the worker-dev router\noc rsh -n openshift-ingress router-ingress-worker-dev-76b65cf558-mspvb cat haproxy.config | grep productpage backend be_http:bookinfo:productpage server pod:productpage-v1-658849bb5-8gcl7:productpage:http:10.128.4.21:9080 10.128.4.21:9080 cookie 3758caf21badd7e4f729209173eece08 weight 256 Compare with worker-test router\noc rsh -n openshift-ingress router-ingress-worker-test-6bbf9967f-jht4w cat haproxy.config | grep productpage --\u0026gt; Empty result, this router is not configured with that route. Compare with default router:\nbackend be_http:bookinfo:productpage server pod:productpage-v1-658849bb5-8gcl7:productpage:http:10.128.4.21:9080 10.128.4.21:9080 cookie 3758caf21badd7e4f729209173eece08 weight 256 Why does this happen? Why are the default router and the router for worker-dev configured? This happens because it is the default router and we must explicitly tell it to ignore certain labels.\nModify the default IngressController\noc edit ingresscontroller.operator default -n openshift-ingress-operator Add the following\nnamespaceSelector: matchExpressions: - key: environment operator: NotIn values: - worker-dev - worker-test This will tell the default IngressController to ignore selectors on worker-dev and worker-test\nWait a few seconds until the route pods have been restarted:\noc get pods -n openshift-ingress NAME READY STATUS RESTARTS AGE router-default-744998df46-8lh4t 1/1 Running 0 2m32s router-default-744998df46-hztgf 1/1 Running 0 2m31s router-ingress-worker-dev-76b65cf558-mspvb 1/1 Running 0 96m router-ingress-worker-dev-76b65cf558-p2jpg 1/1 Running 0 96m router-ingress-worker-test-6bbf9967f-4whfs 1/1 Running 0 96m router-ingress-worker-test-6bbf9967f-jht4w 1/1 Running 0 96m And test again\noc rsh -n openshift-ingress router-default-744998df46-8lh4t cat haproxy.config | grep productpage --\u0026gt; empty result At this point the new router feels responsible. Be sure to have a load balancer configured correctly. Appendix Bind other infra-workload to infrastructure nodes:\nInternal Registry oc patch configs.imageregistry.operator.openshift.io/cluster -n openshift-image-registry --type=merge --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;nodeSelector\u0026#34;:{\u0026#34;node-role.kubernetes.io/infra\u0026#34;:\u0026#34;\u0026#34;}}}\u0026#39; OpenShift Monitoring Workload Create the following file and apply it.\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; cluster-monitoring-config-cm.yaml apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; prometheusOperator: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; grafana: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; telemeterClient: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; EOF oc create -f cluster-monitoring-config-cm.yaml "},{"uri":"https://blog.stderr.at/categories/acs/","title":"ACS","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/acs/2021-12-11-acsauth/","title":"Advanced Cluster Security - Authentication","tags":["ACS","Advanced Cluster Security","OpenShift","Security","Keycloak","Authentication","SSO","Stackrox"],"description":"Red Hat Advanced Cluster Security - Authentication","content":" Red Hat Advanced Cluster Security (RHACS) Central is installed with one administrator user by default. Typically, customers request an integration with existing Identity Provider(s) (IDP). RHACS offers different options for such integration. In this article 2 IDPs will be configured as an example. First OpenShift Auth and second Red Hat Single Sign On (RHSSO) based on Keycloak\nPrerequisites OpenShift 4 Cluster\nAdvanced Cluster Security v3.66+\nRed Hat SSO Operator installed\nWhile RHSSO will be installed during this article, only default and example values are used. These are by no means examples for a production system. Introduction Advanced Cluster Security comes with several default roles, which can be assigned to users:\nSystem role Description Admin\nThis role is targeted for administrators. Use it to provide read and write access to all resources.\nAnalyst\nThis role is targeted for a user who cannot make any changes, but can view everything. Use it to provide read-only access for all resources.\nContinuous Integration\nThis role is targeted for CI (continuous integration) systems and includes the permission set required to enforce deployment policies.\nNone\nThis role has no read and write access to any resource. You can set this role as the minimum access role for all users.\nSensor Creator\nRed Hat Advanced Cluster Security for Kubernetes uses this role to automate new cluster setups. It includes the permission set to create Sensors in secured clusters.\nScope Manager\nThis role includes the minimum permissions required to create and modify access scopes.\nIt is possible to create custom roles. Configure RHACS Authentication: OpenShift Auth It is assumed that RHACS is already installed and login to the Central UI is available. Login to your RHACS and select “Platform Configuration” \u0026gt; “Access Control”\nFrom the drop down menu Add auth provider select OpenShift Auth\nFigure 1. ACS Auth Provider Enter a Name for your provider and select a default role which is assigned to any user who can authenticate.\nIt is recommended to select the role None, so new accounts will have no privileges in RHACS.\nWith Rules you can assign roles to specific users, based on their userid, name, mail address or groups.\nFor example the user with the name poweruser gets the role Admin assigned.\nVerify Authentication with OpenShift Auth Logout from the Central UI and reload the browser.\nSelect from the drop down OpenShift Auth\nFigure 2. ACS Login Try to login with a valid OpenShift user. Depending on the Rules which have been defined during previous steps the appropriate permissions should be assigned. For example: If you login as user poweruser the role Admin is assigned.\nConfigure Red Hat Single Sign On The following steps will create some basic example objects to an existing RHSSO or Keycloak to test the authentication at RHACS. Skip to step #5 if you have Keycloak already up and running and would like to reuse an existing client.\nThe RHSSO operator (or Keycloak) is installed at the namespace single-sign-on.\nCreate an instance of Keycloak\napiVersion: keycloak.org/v1alpha1 kind: Keycloak metadata: name: example-keycloak namespace: single-sign-on spec: externalAccess: enabled: true instances: 1 Create a Realm This will create a Realm called Basic\napiVersion: keycloak.org/v1alpha1 kind: KeycloakRealm metadata: name: example-keycloakrealm namespace: single-sign-on spec: instanceSelector: matchLabels: app: sso realm: displayName: Basic Realm enabled: true id: basic realm: basic Login into Red Hat SSO Get the route to your RHSSO instance:\noc get route keycloak -n single-sign-on --template=\u0026#39;{{ .spec.host }}\u0026#39; # keycloak-single-sign-on.apps.cluster-29t8z.29t8z.sandbox677.opentlc.com and log into the Administration Interface.\nExtract the admin password for Keycloak\nThe secret name is build from \u0026#34;credential\u0026#34;\u0026lt;keycloak-instance-name\u0026gt;\noc extract secret/credential-example-keycloak -n single-sign-on --to=- # ADMIN_PASSWORD \u0026lt;you password\u0026gt; # ADMIN_USERNAME admin Be sure to select your Realm (Basic in our case), goto Clients and select a ClientID.\nIn this example we select account\nFigure 3. ACS Login Of course you can create or use any other Client. Enable the option Implicit Flow\nGet the Issuer URL from your realm. This is typically your: https://\u0026lt;KEYCLOAK_URL\u0026gt;/auth/realms/\u0026lt;REALM_NAME\u0026gt;;\nFor Example: https://keycloak-single-sign-on.apps.cluster-29t8z.29t8z.sandbox677.opentlc.com/auth/realms/basic\nCreate Test Users In RHSSO create 2 user accounts to test the authentication later.\nGoto Users and create the users:\nUser: acsadmin\nFirst Name: acsadmin\nUser: user1\nFirst Name: user 1\nYou can set any other values for these users. However, be sure to set a password for both, after they have been created.\nConfigure RHACS Authentication: RHSSO It is assumed that RSACS is already installed and login to the Central UI is available. Login to your RHACS and select “Platform Configuration” \u0026gt; “Access Control”\nFrom the drop down menu Add auth provider select OpenID Connect\nEnter a “Name” for your provider i.e. “Single Sign On”\nLeave the “Callback Mode” to the “Auto-Select” setting\nEnter your Issuer URL\nAs Client ID enter account (or the ClientID you would like to use)\nLeave the Client Secret empty and select the checkbox Do not use Client Secret which is good enough for our tests.\nRemember the two callback URL from the blue box. They must be configured in Keycloak.\nSelect a default role which is assigned to any user who can authenticate.\nIt is recommended to select the role None, so new accounts will have no privileges in RHACS.\nWith Rules you can assign roles to specific users, based on their userid, name, mail address or groups.\nFor example the user with the name acsadmin (which have been created previously in our RHSSO) gets the role Admin assigned.\nThe final settings are depict in the following image:\nFigure 4. ACS Login Continue RHSSO Configuration What is left to do is the configuration of redirect URLs. These URLs are shown in the ACS Authentication Provider configuration (see blue field in the image above)\nLog back into RHSSO and select “Clients” \u0026gt; “account”\nInto Valid Redirect URLs enter the two URLs which you saved from the blue box in the RHACS configuration.\nTroubleshoot: Test Login In RHACS you can test the login to you SSO.\nGoto \u0026#34;Platform Configuration\u0026#34; \u0026gt; \u0026#34;Access Control\u0026#34;\nClick the button \u0026#34;Test login\u0026#34;\nA popup will appear which asks you to enter SSO credentials. The connection to RHSSO will be validated:\nFigure 5. ACS Test SSO Verify Authentication with OpenShift Auth Logout from the Central UI and reload the browser.\nSelect from the drop down Single Sign On\nFigure 6. ACS Login SSO Try to login with a valid SSO user. Depending on the Rules which have been defined during previous steps the appropriate permissions should be assigned. For example: If you login as user acsadmin the role Admin is assigned.\n"},{"uri":"https://blog.stderr.at/tags/authentication/","title":"Authentication","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/keycloak/","title":"Keycloak","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/security/","title":"Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/security/","title":"Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/sso/","title":"SSO","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/stackrox/","title":"stackrox","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ansible/","title":"Ansible","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/ansible/","title":"Ansible","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/2021/11/ansible-style-guide/","title":"Ansible Style Guide","tags":["Ansible","Controller","Automation Controller","Style","StyleGuide","Best Practices","Playbook","Role"],"description":"Howto write and debug Ansible playbooks","content":" You should always follow the Best Practices and Ansible Lint rules defined by the Ansible documentation when developing playbooks.\nAlthough very basic, the Best Practices document gives a few guidelines to be able to carry out well-structured playbooks and roles, it contains recommendations that evolve with the project, so it is recommended to review it regularly. It is advisable to review the organization of content in Ansible.\nThe Ansible Lint documentation shows us through this tool the syntax rules that will be checked in the testing of roles and playbooks, the rules that will be checked are indicated in this document in their respective section.\nThis style guide is meant as a base of playbook development. It is not the ultimate truth. Always verify and apply appropriate company rules. There are many additional references available, such as: https://github.com/whitecloud/ansible-styleguide or https://github.com/redhat-cop/automation-good-practices General Recommendations Treat your Ansible content like code: Any playbook, role, inventory or collection should be versionized using Git or a similar tool.\nIterate: start with basic playbook and refactor later\nUse multiple Git repositories: on per role/collection, separate inventory from other repositories\nUse human-meaningful names in inventory files\ngroup hosts in inventory files\nuse a consistent Directory Structure\nOptimize Playbook Execution Disable facts gathering if it is not required.\nTry not to use: ansible_facts[‘hostname’] (or ‘nodename’)\nTry to use inventory_hostname and inventory_hostname_short instead\nIt is possible to selectively gather facts (gather_subnet)\nConsider caching facts\nIncrease Parallelism\nAnsible runs 1st task on every host, then 2nd task on every host …\nUse “forks” (default is 5) to control how many connections can be active (load will increase, try first with conservative values)\nWhile testing forks analize Enable CPU and Memory Profiling\nIf you use the module \u0026#34;copy\u0026#34; for large files: do not use “copy” module. Use “synchronize” module instead (based on rsync)\nUse lists when ever a modules support it (i.e. yum) Instead of\ntasks: - name: Ensure the packages are installed yum: name: \u0026#34;{{ item }}\u0026#34; state: present loop: - httpd - mod_ssl - httpd-tools - mariadb-server - mariadb - php - php-mysqlnd use\ntasks: - name: Ensure the packages are installed yum: state: present name: - httpd - mod_ssl - httpd-tools - mariadb-server - mariadb - php - php-mysqlnd Optimize SSH Connections\nin ansible.cfg set\nControlMaster\nControlPersist\nPreferredAuthentications\nEnabling Pipelining\nNot enabled by default\nreduces number of SSH operations\nrequires to disable requiertty in sudo options\nName Tasks and Plays Every task or play should be explicitly named in a way that the purpose is easily understood and can be referred to if the playbook has to be restarted from a certain task.\nFor example the following is hard to read and debug:\nPLAY [localhost] ******************************** TASK [include_vars] ******************************** ok: [localhost] TASK [yum] ******************************** ok: [localhost] While with naming it is easier to follow what is happening:\nPLAY [Create a new virtual machine] ******************************** TASK [Include vmware-credentials] ******************************** ok: [localhost] TASK [Install required packages with yum] ******************************** ok: [localhost] # bad # set another variable - set_fact: my_second_var: \u0026#34;{{ my_var }}\u0026#34; # good - name: set another variable set_fact: my_second_var: \u0026#34;{{ my_var }}\u0026#34; Reason Better understanding what is currently happening in a play and better possibility to debug.\nVariables in Task Names Include as much information as necessary to explain the purpose of a task. Make usage of variables inside a task name to create dynamic output messages.\n#bad - name: \u0026#39;Change status\u0026#39; service: enabled: true name: \u0026#39;httpd\u0026#39; state: \u0026#39;{{ state }}\u0026#39; become: true #good - name: \u0026#39;Change status of httpd to {{ state }}\u0026#39; service: enabled: true name: \u0026#39;httpd\u0026#39; state: \u0026#39;{{ state }}\u0026#39; become: true Reason This will help to easily understand log outputs of playbooks.\nOmitting Unnecessary Information While name tasks in a playbook, do not include the name of the role which is currently executed, since Ansible will do this automatically.\nReason Avoiding the same output twice on the console will prevent confusions.\nNames All the newly created Ansible roles should follow the name convention using dashes if necessary: [company]-[action]-[function/technology]\n# bad lvm # good mycompany-setup-lvm Reason If using roles from Ansible Galaxy, it will keep consistency about which roles are created internally.\nUse Modules instead of command or shell Before using the command or shell module, verify if there is already a module available which can avoid the usage of raw shell command.\n# bad - name: install httpd tasks: - command: \u0026#34;yum install httpd\u0026#34; # good - name: install packages tasks: - name: \u0026#39;install httpd\u0026#39; yum: name: \u0026#39;httpd\u0026#39; state: \u0026#39;present\u0026#39; Reason While raw command could be seen as a security risk in general, another reason to avoid them is the loss of immutability of the ansible playbooks or roles. Ansible cannot verify if a command has been already executed before or not and will therefore execute it every time the playbook is running.\nDocumenting a Task/Play Every playbook, role or task should start with a documentation why this code has been written and what it does and should include an example usage if applicable. The comment should be followed by ---` with no blank lines around it, to indicate the actual start of the yaml definition\n#bad - name: \u0026#39;Change httpd status\u0026#39; service: enabled: true name: \u0026#39;httpd\u0026#39; state: \u0026#39;{{ state }}\u0026#39; become: true #good # Example usage: ansible-playbook -e state=started playbook.yml # This playbook changes the state of the httpd daemon --- - name: \u0026#39;Change httpd status\u0026#39; service: enabled: true name: \u0026#39;httpd\u0026#39; state: \u0026#39;{{ state }}\u0026#39; become: true Reason This common programmatic practice helps to quickly understand the purpose and usage of a playbook or role.\nLint rule Yamllint document-start rule\nEnd a File Files should be ended with a newline.\nReason This is common Unix best practice which avoids any prompt misalignment when printing files in a terminal.\nLint rule Yamllint new-line-at-end-of-file rule\nQuotes Strings should be quoted, while quotes for booleans (e.g. true/false) or integers (i.g. 42) should be avoided.\nDo NOT quote:\nhosts: targets (e.g. hosts: databases rather than hosts: ‘databases’)\ninclude_tasks: and include_roles: target file names\ntask and role names\nregistered variables\nnumber values\nboolean values\nIt is possible to use single or double quotes. In this document single quotes are used, as it seems to be more common. Double quotes are often seen in European countries, especially German speaking countries. The most important thing is to stick to one style. # bad - name: \u0026#39;start robot named my-robot\u0026#39; service: name: my-robot state: started enabled: true become: true # good - name: \u0026#39;start robot named my-robot\u0026#39; service: name: \u0026#39;my-robot\u0026#39; state: \u0026#39;started\u0026#39; enabled: true become: true # double quotes w/ nested single quotes - name: \u0026#39;start all robots\u0026#39; service: name: \u0026#39;{{ item[\u0026#34;robot_name\u0026#34;] }}\u0026#39; state: \u0026#39;started\u0026#39; enabled: true with_items: \u0026#39;{{ robots }}\u0026#39; become: true # double quotes to escape characters - name \u0026#39;print some text on two lines\u0026#39; debug: msg: \u0026#34;This text is on\\ntwo lines\u0026#34; # folded scalar style - name: \u0026#39;robot infos\u0026#39; debug: msg: \u0026gt; Robot {{ item[\u0026#39;robot_name\u0026#39;] }} is {{ item[\u0026#39;status\u0026#39;] }} and in {{ item[\u0026#39;az\u0026#39;] }} availability zone with a {{ item[\u0026#39;curiosity_quotient\u0026#39;] }} curiosity quotient. with_items: robots # folded scalar when the string has nested quotes already - name: \u0026#39;print some text\u0026#39; debug: msg: \u0026gt; \u0026#34;I haven\u0026#39;t the slightest idea,\u0026#34; said the Hatter. # do not quote booleans/numbers - name: \u0026#39;download google homepage\u0026#39; get_url: dest: \u0026#39;/tmp\u0026#39; timeout: 60 url: \u0026#39;https://google.com\u0026#39; validate_certs: true # variables example 1 - name: \u0026#39;set a variable\u0026#39; set_fact: my_var: \u0026#39;test\u0026#39; # variables example 2 - name: \u0026#39;print my_var\u0026#39; debug: var: my_var when: ansible_os_family == \u0026#39;Darwin\u0026#39; # variables example 3 - name: \u0026#39;set another variable\u0026#39; set_fact: my_second_var: \u0026#39;{{ my_var }}\u0026#39; Lint rule Yamllint quoted-strings rule\nSudo Use the new become syntax when designating that a task needs to be run with sudo privileges\n#bad - name: \u0026#39;template client.json to /etc/sensu/conf.d/\u0026#39; template: dest: \u0026#39;/etc/sensu/conf.d/client.json\u0026#39; src: \u0026#39;client.json.j2\u0026#39; sudo: true # good - name: \u0026#39;template client.json to /etc/sensu/conf.d/\u0026#39; template: dest: \u0026#39;/etc/sensu/conf.d/client.json\u0026#39; src: \u0026#39;client.json.j2\u0026#39; become: true Reason Using sudo was deprecated at Ansible version 1.9.1\nLint rule Ansible-lint E103 rule\nHosts Declarations Host sections be defined in such a way that it follows this general order:\nhost declaration\nhost options in alphabetical order\npre_tasks\nroles\ntasks\n# example - hosts: \u0026#39;webservers\u0026#39; remote_user: \u0026#39;centos\u0026#39; vars: tomcat_state: \u0026#39;started\u0026#39; pre_tasks: - name: \u0026#39;set the timezone to America/Boise\u0026#39; lineinfile: dest: \u0026#39;/etc/environment\u0026#39; line: \u0026#39;TZ=America/Boise\u0026#39; state: \u0026#39;present\u0026#39; become: true roles: - { role: \u0026#39;tomcat\u0026#39;, tags: \u0026#39;tomcat\u0026#39; } tasks: - name: \u0026#39;start the tomcat service\u0026#39; service: name: \u0026#39;tomcat\u0026#39; state: \u0026#39;{{ tomcat_state }}\u0026#39; Reason A global definition about the order of these items, will create an easy and consistent human readable code.\nTasks Declarations A task should be defined in such a way that it follows this general order:\ntask name\ntags\ntask map declaration (e.g. service:)\ntask parameters in alphabetical order (remember to always use multi-line map syntax)\nloop operators (e.g. with_items)\ntask options in alphabetical order (e.g. become, ignore_errors, register)\n# example - name: \u0026#39;create some ec2 instances\u0026#39; tags: \u0026#39;ec2\u0026#39; ec2: assign_public_ip: true image: \u0026#39;ami-c7d092f7\u0026#39; instance_tags: Name: \u0026#39;{{ item }}\u0026#39; key_name: \u0026#39;my_key\u0026#39; with_items: \u0026#39;{{ instance_names }}\u0026#39; ignore_errors: true register: ec2_output when: ansible_os_family == \u0026#39;Darwin\u0026#39; Reason A global definition about the order of these items, will create an easy and consistent human readable code.\nInclude Declaration For include statements, make sure to quote filenames and only use blank lines between include statements if they are multi-line (e.g. they have tags).\n# bad - include: other_file.yml - include: \u0026#39;second_file.yml\u0026#39; - include: third_file.yml tags=third # good - include: \u0026#39;other_file.yml\u0026#39; - include: \u0026#39;second_file.yml\u0026#39; - include: \u0026#39;third_file.yml\u0026#39; tags: \u0026#39;third\u0026#39; Reason Using such syntax, will create an easy and consistent human readable code.\nBooleans Use true/false instead of yes/no (or 1/0).\n# bad - name: \u0026#39;start httpd\u0026#39; service: name: \u0026#39;httpd\u0026#39; state: \u0026#39;restarted\u0026#39; enabled: 1 become: \u0026#39;yes\u0026#39; # good - name: \u0026#39;start httpd\u0026#39; service: name: \u0026#39;httpd\u0026#39; state: \u0026#39;restarted\u0026#39; enabled: true become: true Reason Ansible can read boolean values in many different ways, like: True/False, true/false, yes/no, 1/0. It makes sense to stick to one option, which is then equally used in any playbook or role. It is recommended to use true/false since Java and Javascript are using the same values for boolean values.\nLint rule Yamllint truthy rule\nRequired config: truthy: {allowed-values: [\u0026#34;true\u0026#34;, \u0026#34;false\u0026#34;]}`\nKey Value Pairs Only one space should be used after the colon, when defining key/value pairs.\n# bad - name : \u0026#39;start httpd\u0026#39; service: name : \u0026#39;httpd\u0026#39; state : \u0026#39;restarted\u0026#39; enabled : true become : true # good - name: \u0026#39;start httpd\u0026#39; service: name: \u0026#39;httpd\u0026#39; state: \u0026#39;restarted\u0026#39; enabled: true become: true Reason It increases human readability and reduces changeset collisions for version control.\nLint rule Yamllint colons rule\nRequired config: colons: {max-spaces-before: 0, max-spaces-after: 1}\nUsing Map Syntax Always use the map syntax for better readability.\n# bad - name: \u0026#39;create conf.d directory\u0026#39; file: \u0026#39;path=/etc/httpd/conf.d/ state=directory mode=0755 owner=httpd group=httpd\u0026#39; become: true - name: \u0026#39;copy mod_ssl.conf to /etc/httpd/conf.d\u0026#39; copy: \u0026#39;dest=/etc/httpd/conf.d/ src=mod_ssl.conf\u0026#39; become: true # good - name: \u0026#39;create conf.d directory\u0026#39; file: group: \u0026#39;httpd\u0026#39; mode: \u0026#39;0755\u0026#39; owner: \u0026#39;httpd\u0026#39; path: \u0026#39;/etc/httpd/conf.d\u0026#39; state: \u0026#39;directory\u0026#39; become: true - name: \u0026#39;copy mod_ssl.conf to /etc/httpd/conf.d\u0026#39; copy: dest: \u0026#39;/etc/httpd/conf.d/\u0026#39; src: \u0026#39;mod_ssl.conf\u0026#39; become: true Reason It increases human readability and reduces changeset collisions for version control.\nSpacing You should have blank lines between two host blocks, between two task blocks, and between host and include blocks. When indenting, you should use 2 spaces to represent sub-maps, and multi-line maps should start with a -. For a more in-depth example of how spacing (and other things) please take a look at the example below\n# Example: ansible-playbook --ask-become-pass --ask-vault-pass style.yml # # This is a sample Ansible script to showcase all of our style decisions. # Pay close attention to things like spacing, where we use quotes, etc. # The only thing you can ignore is where comments are, except this first comment: # It\u0026#39;s generally a good idea to include some good information like sample usage # at the beginning of your file, so that someone can run head on the script # to see what they should do. # # A good rule of thumb on quoting is to quote anything that represents a value # that does not represent either a primitive type, or something within the # playbook; e.g. do not quote integers, booleans, variable names, boolean logic # Variable names still need to be quoted when they are module parameters for # Ansible to properly resolve them. # You should also always have single quotes around the outer string, and # double quotes on the inside. # If for some reason this is not possible or it would require escaping quotes # (which you should avoid if you can), use the scalar string operator (shown # in this playbook). # # Directory structure style: # Your directory structure should match the structure described by the Ansible # developers: http://docs.ansible.com/ansible/playbooks_best_practices.html # # --- # # - include: \u0026#39;role_name.yml\u0026#39; # become: true # only if every task in the role requires super user # # The self-named yml file contains all of the actual role tasks. # # Header comments are followed by blank line, then --- to signify start of YAML, # then another blank line, then the script. --- - hosts: \u0026#39;localhost\u0026#39; tasks: - name: \u0026#39;fail if someone tries to run this\u0026#39; fail: msg: \u0026#39;this playbook was not meant to actually be ran. just inspect the source!\u0026#39; - include: \u0026#39;first_include.yml\u0026#39; # quote filenames - include: \u0026#39;second_include.yml\u0026#39; # no blank line needed between includes without tags - include: \u0026#39;third_include.yml\u0026#39; # includes with tags should have blank lines between tags: \u0026#39;third_include\u0026#39; - include: \u0026#39;fourth_include.yml\u0026#39; tags: \u0026#39;fourth_include\u0026#39; - hosts: \u0026#39;tag_environment_samplefruit\u0026#39; remote_user: \u0026#39;centos\u0026#39; # options in alphabetical order vars: sample_str: \u0026#39;dood\u0026#39; # use snake_case for variable names sample_bool: true # do not quote booleans or integers sample_int: 42 vars_files: - \u0026#39;group_vars/secrets.yml\u0026#39; pre_tasks: # then pre_tasks, roles, tasks - name: \u0026#39;this runs a command that involves both single and double quotes\u0026#39; command: \u0026gt; echo \u0026#34;I can\u0026#39;t even\u0026#34; args: chdir: \u0026#39;/tmp\u0026#39; - name: \u0026#39;this command just involves double quotes\u0026#39; command: \u0026#39;echo \u0026#34;Hey man\u0026#34;\u0026#39; roles: - { role: \u0026#39;sample_role\u0026#39;, tags: \u0026#39;sample_role\u0026#39; } # use this format for role listing tasks: - name: \u0026#39;get list of directory permissions in /tmp\u0026#39; command: \u0026#39;ls -l /tmp\u0026#39; register: tmp_listing # do not quote variable names when registering # A task should be defined in the following order: # name # tags # module # module arguments, alphabetical # loop operator (e.g. with_items, with_fileglob) # other options, alphabetical (e.g. become, ignore_errors, when) - name: \u0026#39;a more complicated task to show where everything goes: touch all items from /tmp\u0026#39; tags: \u0026#39;debug\u0026#39; # tags go immediately after name file: path: \u0026#39;{{ item }}\u0026#39; # use path for single file actions, dest/src for multi file actions state: \u0026#39;touch\u0026#39; # arguments go in alphabetical order with_items: tmp_listing.stdout_lines # loop things go immediately after module # the rest of the task options are in alphabetical order become: true # try to keep become only on the tasks that need it. If every task in a host uses become, then move it up to the host options ignore_errors: true when: ansible_os_family == \u0026#39;Darwin\u0026#39; and tmp_listing.stdout_lines | length \u0026gt; 1 - name: \u0026#39;some modules can have maps in their maps (woah man)\u0026#39; ec2: assign_public_ip: true group: [\u0026#39;wca_ssh\u0026#39;, \u0026#39;wca_tomcat\u0026#39;] image: \u0026#39;ami-c7d092f7\u0026#39; instance_tags: Name: \u0026#39;instance\u0026#39; service_tomcat: \u0026#39;\u0026#39; key_name: \u0026#39;ops\u0026#39; - hosts: \u0026#39;tag_environment_secondfruit\u0026#39; tasks: - name: \u0026#39;this task has multiple tags\u0026#39; tags: [\u0026#39;tagme\u0026#39;, \u0026#39;tagmetoo\u0026#39;] set_fact: mr_fact: \u0026#39;w\u0026#39; - name: \u0026#39;perform an action\u0026#39; action: ec2_facts delegate_to: \u0026#39;localhost\u0026#39; # newline at end of file Reason This produces nice looking code that is easy to read.\nLint rule Yamllint empty-lines rule\nVariable Names Names of variables should be as expressive as possible. snake_case for names will help to make the code human readable. The prefix should contain the name of the role.\n# bad - name: \u0026#39;set some facts\u0026#39; set_fact: myBoolean: true int: 20 MY_STRING: \u0026#39;test\u0026#39; # good - name: \u0026#39;set some facts\u0026#39; set_fact: rolename_my_boolean: true rolename_my_int: 20 rolename_my_string: \u0026#39;test\u0026#39; Reason Ansible uses snake_case for module names so it makes sense to extend this convention to variable names. Perfixing the variable with the role name, makes it immediately obvious where it is used.\nJinja Variables Use spaces around Jinja variable names to increase readability.\n# bad - name: set some facts set_fact: my_new_var: \u0026#34;{{my_old_var}}\u0026#34; # good - name: set some facts set_fact: my_new_var: \u0026#34;{{ my_old_var }}\u0026#34; Reason A proper definition for how to create Jinja variables produces consistent and easily readable code.\nLint rule Ansible-lint E206 rule\nComparing Do not compare to literal True/False. Use when: var rather than when: var == True (or conversely when: not var).\nDo not compare it to empty strings. Use when: var rather than when: var != \u0026#34;\u0026#34; (or conversely when: not var rather than when: var == \u0026#34;\u0026#34;)\n# bad - name: validate required variables fail: msg: \u0026#34;No value specified for \u0026#39;{{ item }}\u0026#39;\u0026#34; when: (vars[item] is undefined) or (vars[item] is defined and vars[item] | trim == \u0026#34;\u0026#34;) with_items: \u0026#34;{{ appd_required_variables }}\u0026#34; - name: Create an user and add to the global group include_tasks: user.yml when: - username is defined - username != \u0026#34;\u0026#34; # good - name: Validate required variables fail: msg: \u0026#34;No value specified for \u0026#39;{{ item }}\u0026#39;\u0026#34; when: (vars[item] is undefined) or (vars[item] is defined and not vars[item] | trim == \u0026#34;\u0026#34;) with_items: \u0026#34;{{ appd_required_variables }}\u0026#34; - name: Create an user and add to the global group include_tasks: user.yml when: - username is defined - username Reason Avoid code complexity using quotes and standardize the way literals and empty strings are used\nLint rule Ansible-lint E601 rule\nDelegation Do not use local_action, use delegate_to: localhost\n# bad - name: Send summary mail local_action: module: mail subject: \u0026#34;Summary Mail\u0026#34; to: \u0026#34;{{ mail_recipient }}\u0026#34; body: \u0026#34;{{ mail_body }}\u0026#34; run_once: true # good - name: Send summary mail mail: subject: \u0026#34;Summary Mail\u0026#34; to: \u0026#34;{{ mail_recipient }}\u0026#34; body: \u0026#34;{{ mail_body }}\u0026#34; delegate_to: localhost run_once: true Reason Avoid complexity, standardization, flexibility and code readability. The module and its parameters are easy to read and can be delegated even to a third party server.\nLint rule Ansible-lint E504 rule\nPlaybook File Extension All Ansible Yaml files should have a .yml extension (and NOT .YML, .yaml etc).\n# bad ~/tasks.yaml # good ~/tasks.yml Reason Ansible tooling (like ansible-galaxy init) creates files with a .yml extension. Also, the Ansible documentation website references files with a .yml extension several times. Because of this, it is normal in the Ansible community to use a .yml extension for all Ansible YAML files.\nLint rule Ansible-lint E205 rule\nTemplate File Extension All Ansible Template files should have a .j2 extension.\n# bad ~/template.conf # good ~/template.conf.j2 Reason Ansible Template files will usually have the .j2 extension, which denotes the Jinja2 templating engine used.\nVaults All Ansible Vault files should have a .vault extension (and NOT .yml, .YML, .yaml etc).\n# bad ~/secrets.yml # good ~/secrets.vault Reason It is easier to control unencrypted files automatically for the specific .vault extension.\nDebug and Comments Do not overuse debug and comments in final code as much as possible. Use task and role names to explain what the task or role does. Use the verbose option under ansible for debugging purposes. If debug is used, assign a verbosity option. This will display the message only on certain debugging levels.\n# bad - name: print my_var debug: var: my_var when: ansible_os_family == \u0026#34;Darwin\u0026#34; # good - name: print my_var debug: var: my_var verbosity: 2 when: ansible_os_family == \u0026#34;Darwin\u0026#34; Reason It will keep clean code and consistency avoiding extra debug and comments. Extra debug will spend extra time when running the playbook or role.\nUse Modules copy or templates instead of linefile or blockfile Instead of using the modules linefile and blockfile, which manage changes inside a file, it should be tried to use the modules copy or template instead, which will manage the whole file. For better future proof template should be preferred over copy.\nReason When using linefile/blockfile only a single line or a part of a file is managed. It is not easy to remember which part exactly is managed and which is not. Using the template module the whole file is managed by Ansible and there is no confusion about different parts of a file. Moreover, regular expressions can be avoided, which are often used using linefile.\nUse Module synchronize Instead of copy for Large Files Copying large files takes significantly longer than syncing it. The synchronize modules which are based on rsync can increase the time moving large files from one node to another (or even on the same node).\nDo not Show Sensitive Data in Ansible Output When using the template module and there are passwords or other sensitive data in the file, use the no_log option to hide this information.\nReason For obvious reasons the output of sensitive data on the screen (and logfile) should be prohibited.\nUse Block-Module Block can help to organize the code and can enable rollbacks.\n- block: copy: src: critical.conf dest: /etc/critical/crit.conf service: name: critical state: restarted rescue: command: shutdown -h now Reason Using blocks groups critical tasks together and allows better management when a single task of this block fails.\nEnable CPU and Memory Profiling Enabling profiling is extremely useful when testing forks or to analyse memory and cpu consumption in general. It will present a summary of used CPU/memory for the whole play and per task.\nTo enable it:\nCreate a new control group - which is required for CPU and memory profiling\ncgcreate -a root:root -t root:root -g cpuacct,memory,pids:ansible_profile Configure ansible.cfg\ncallback_whitelist=cgroup_perf_recap [callback_cgroup_perf_recap] control_group=ansible_profile Execute the playbook using cgexec\ncgexec -g cpuacct,memory,pids:ansible_profile ansible-playbook x.yaml Analyse the usage\nMemory Execution Maximum: 11146.29MB cpu Execution Maximum: 112.08% pids Execution Maximum: 35.00 memory: lab : creating network (b42e9945-0dc7-20b1-09d1-00000000000a): 11097.35MB ….. Enable Task and Role Profiling The following can be enabled as well, to help debugging playbooks and roles:\n[defaults] callback_whitelist = profile_tasks, profile_role, timer Timer: duration of playbook execution (activated by default) profile_tasks/role: displays execution time per tasks/role\nThis will generate an output like the following:\nDirectory Structure A consistent directory structure is important to easily understand all playbooks and roles which are written. Ansible knows many different folder structures, any can be used. However, it is important to stick to one structure.\nThe following is an example. Not all folders are usually used and working with collections will change such structure a little bit.\n. ├── ansible.cfg ├── ansible_modules ├── group_vars │ ├── webservers │ └── all ├── hosts │ ├── webserver01 │ └── webserver02 ├── host_vars ├── modules ├── playbooks │ └── ansible-cmdb.yml └── roles ├── requirements.yml ├── galaxy └── dev-sec.ssh-hardening └── auditd ├── files │ ├── auditd.conf │ ├── audit.yml ├── handlers │ └── main.yml ├── meta │ └── main.yml └── tasks └── main.yml "},{"uri":"https://blog.stderr.at/tags/automation-controller/","title":"Automation Controller","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/best-practices/","title":"Best Practices","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/controller/","title":"Controller","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/playbook/","title":"Playbook","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/role/","title":"Role","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/style/","title":"Style","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/styleguide/","title":"StyleGuide","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/aro/","title":"ARO","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/azure/","title":"Azure","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/azure/2021-10-29-private-aro/","title":"Stumbling into Azure Part II: Setting up a private ARO cluster","tags":[],"description":"","content":" In Part I of our blog post we covered setting up required resources in Azure. Now we are finally going to set up a private cluster. Private\nAs review from Part I here is our planned setup, this time including the ARO cluster.\nAzure Setup The diagram below depicts our planned setup:\nOn the right hand side can see the resources required for our lab:\na virtual network (vnet 192.168.128.0/19). This vnet will be split into 3 separate subnets a master subnet (192.168.129.0/24) holding the ARO control plane nodes a node subnet (192.168.130.0/24) holding ARO worker nodes and finally a subnet call GatewaySubnet where we are going to deploy our Azure VPN gateway (called a vnet-gateway) The subnet where the Azure VPN gateway is located needs to have the name GatewaySubnet. Otherwise creating the Azure VPN gateway will fail.\nwe also need a publicIP resource that we are going to connect to our vnet-gateway (the VPN gateway) and finally a local-gateway resource that tells the vnet-gateway which networks are reachable on the left, in our case the Hetzner server. Creating the private Azure Red Hat OpenShift cluster Register required resource providers\naz provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait First we are going to set some environment variable. Those variables are used in the upcoming commands:\nexport RESOURCEGROUP=aro-rg export CLUSTER=\u0026#34;aro1\u0026#34; export GATWAY_SUBNET=\u0026#34;192.168.128.0/24\u0026#34; export MASTER_SUBNET=\u0026#34;192.168.129.0/24\u0026#34; export WORKER_SUBNET=\u0026#34;192.168.130.0/24\u0026#34; export HETZNER_VM_NETWORKS=\u0026#34;10.0.0.0/24 192.168.122.0/24 172.16.100.0/24\u0026#34; Disable subnet private endpoint policies\naz network vnet subnet update \\ --name master-subnet \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --disable-private-link-service-network-policies true Create a private DNS zone for our cluster\naz network private-dns zone create -n private2.tntinfra.net -g aro-rg Create the cluster\naz aro create \\ --resource-group $RESOURCEGROUP \\ --name $CLUSTER \\ --vnet aro-vnet \\ --master-subnet master-subnet \\ --worker-subnet worker-subnet \\ --apiserver-visibility Private \\ --ingress-visibility Private \\ --domain private.tntinfra.net # --pull-secret @pull-secret.txt # [OPTIONAL] After successful cluster creating add DNS entry for the API and Ingress\nQuery the Azure API for the API server IP and the ingress IP addresses:\naz aro show -n aro1 -g aro-rg --query \u0026#39;{api:apiserverProfile.ip, ingress:ingressProfiles[0].ip}\u0026#39; Example output\nApi Ingress ------------- --------------- 192.168.129.4 192.168.130.254 Add entries to Azure private DNS\naz network private-dns record-set a add-record -g aro-rg -z private.tntinfra.net -a \u0026#34;192.168.129.4\u0026#34; -n api az network private-dns record-set a add-record -g aro-rg -z private.tntinfra.net -a \u0026#34;192.168.130.254\u0026#34; -n \u0026#34;*.apps\u0026#34; List entries to verify configuration\naz network private-dns record-set a list -g aro-rg -z private.tntinfra.net Output:\nName ResourceGroup Ttl Type AutoRegistered Metadata ------ --------------- ----- ------ ---------------- ---------- api aro-rg 3600 A False *.apps aro-rg 3600 A False List cluster credentials after successful setup\naz aro list-credentials \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP Get the console URL\naz aro show \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \u0026#34;consoleProfile.url\u0026#34; -o tsv DNS, curl this works, dunno why?\ndig @192.168.129.7 console-openshift-console.apps.xm7rdz4r.westeurope.aroapp.io use curl to access the internal API and see if it works:\ncurl -kv https://192.168.129.4:6443 Additional Resources Build an Azure site-to-site VPN for DevTest Create a virtual network with a Site-to-Site VPN connection using CLI Libreswan: Disable rp_filter for IPsec Libreswan: NAT and IPsec not working Libreswan: Subnet to subnet VPN "},{"uri":"https://blog.stderr.at/ansible/2021/10/automation-controller-and-ldap-authentication/","title":"Automation Controller and LDAP Authentication","tags":["Ansible","Controller","Automation Controller","LDAP","Authentication"],"description":"Enable LDAP authentication and create and example LDAP server","content":" The following article shall quickly, without huge background information, deploy an Identity Management Server (based on FreeIPA) and connect this IDM to an existing Automation Controller so authentication can be tested and verified based on LDAP.\nInstall FreeIPA Run the following command to deploy and configure the IPA Server:\nyum module enable idm:DL1\nyum distro-sync\nyum -y module install idm:DL1/server\nInstall the server by calling the command ipa-server-install. This will start an interactive installation modus which requires the basic information about the IPA server. The following uses tower.local as base domain\nDo you want to configure integrated DNS (BIND)? [no]: Server host name [node01.tower.local]: Please confirm the domain name [tower.local]: Please provide a realm name [TOWER.LOCAL]: Directory Manager password: \u0026lt;enter password\u0026gt; Password (confirm): \u0026lt;enter password\u0026gt; IPA admin password: \u0026lt;enter password\u0026gt; Password (confirm): \u0026lt;enter password\u0026gt; Do you want to configure chrony with NTP server or pool address? [no]: Continue to configure the system with these values? [no]: yes Once all information have been provided the installation/configuration process starts. This will take a while…​\nBe sure that the hostname, here node01.tower.local, is resolvable, at least from the Tower/Controller node and the node you are accessing the FreeIPA UI. You can use your local hosts file or a real domain name for that. Login to IPA server via Command Line For user admin use: kinit admin\nCreate a Binduser (BindDN) The Binduser (or BindDN) will be used by the Controller to authenticate the Controller against the LDAP server.\nCreate the actual user\nipa user-add --first=”BindUser” --last=”None” --password binduser Output:\nPassword: Enter Password again to verify: ------------------ Added user \u0026#34;binduser\u0026#34; ------------------ User login: binduser First name: ”BindUser” Last name: ”None” Full name: ”BindUser” ”None” Display name: ”BindUser” ”None” Initials: ”” Home directory: /home/binduser GECOS: ”BindUser” ”None” Login shell: /bin/sh Principal name: binduser@TOWER.LOCAL Principal alias: binduser@TOWER.LOCAL User password expiration: 20211015133112Z Email address: binduser@tower.local UID: 1573400003 GID: 1573400003 Password: True Member of groups: ipausers Kerberos keys available: True Assign the new user to the admin group\nipa group-add-member admins --users=binduser Output:\nGroup Name: admins Description: Account administrators group GID: 1573400000 Member users: admin, binduser ----------------------------------- Number of members added 1 ----------------------------------- Create a 2nd User to test the authentication later\nipa user-add --first=”User” --last=”Name” --password user1 Enable LDAP Auth in Automation Controller Login to Automation Controller ad go to \u0026#34;Settings \u0026gt; LDAP Settings \u0026gt; Default\u0026#34;\nadd a new connection:\nLDAP Service URI: ldap://node01.tower.local:389\nLDAP Bind Password: \u0026lt;password of user binduser\u0026gt;`\nLDAP Group Type: MemberDNGroupType\nLDAP Bind DN: uid=binduser,cn=users,cn=accounts,dc=tower,dc=local\nLDAP User Search:\n[ \u0026#34;cn=users,cn=accounts,dc=tower,dc=local\u0026#34;, \u0026#34;SCOPE_SUBTREE\u0026#34;, \u0026#34;(uid=%(user)s)\u0026#34; ] LDAP Group Search:\n[ \u0026#34;cn=groups,cn=accounts,dc=tower,dc=local\u0026#34;, \u0026#34;SCOPE_SUBTREE\u0026#34;, \u0026#34;(objectClass=posixgroup)\u0026#34; ] The configuration should look like the following image:\nFigure 1. Automation Controller LDAP Authentication Verify Login with user1 You can now test the login using user1. If it does not work, check the following files for errors:\nTower Node: /var/log/tower/tower.log\nIPA Node: /var/log/dirsrv/slapd-TOWER-LOCAL/access\nThe login should work, but since the user1 is not assigned to any Team/Organization inside the Automation Controller, no privileges are granted. The user can do nothing. Automatically assign permissions 2 roles can be automatically assigned to authenticated users:\nSuper User\nAuditor\nTo test this, 2 groups will be created in the LDAP server and a new user will be assigned to one of the groups.\nCreate the group for super users: ipa group-add tower_administrators\nCreate the group for auditors: ipa group-add tower_auditors\nCreate a new user: ipa user-add --first=”User” --last=”Name” --password user2\nAssign the user to one the the groups: ipa group-add-member tower_administrators --users=user2\nModify the Controller LDAP configuration and set LDAP User Flags by Group. This will assing any member of tower_administrators to is_superuser for example.\n{ \u0026#34;is_superuser\u0026#34;: [ \u0026#34;cn=tower_administrators,cn=groups,cn=accounts,dc=tower,dc=local\u0026#34; ], \u0026#34;is_system_auditor\u0026#34;: [ \u0026#34;cn=tower_auditors,cn=groups,cn=accounts,dc=tower,dc=local\u0026#34; ] } Test the authentication and authorization with the user2. This user should now gain super admin permissions.\nAllow Users From Specific Groups Only Not all LDAP users shall be able to authenticate. Only users, which are member of a specific group, shall be able to authenticate.\nCreate a 3rd user: ipa user-add --first=”User” --last=”Name” --password user3\nModify the LDAP Configuration in Automation Controller and set LDAP Require Groups:\n\u0026#34;cn=towerusers,cn=groups,cn=accounts,dc=tower,dc=local\u0026#34; Add the group toweruser: ipa group-add towerusers\nAssign the user user3 to that group: ipa group-add-member towerusers --users=user3\nAt this state only user3 will be able to login. In order to allow the other users as well, all must be assigned to the group towerusers\nipa group-add-member towerusers --users=user3 ipa group-add-member towerusers --users=user1 Additional Configuration It is possible to automatically map users to Controller Organization. I did not fully test this, but the following is an example:\n{ \u0026#34;LDAP Organization\u0026#34;: { \u0026#34;admins\u0026#34;: \u0026#34;cn=engineering_admins,ou=groups,dc=example,dc=com\u0026#34;, \u0026#34;remove_admins\u0026#34;: false, \u0026#34;users\u0026#34;: [ \u0026#34;cn=engineering,ou=groups,dc=example,dc=com\u0026#34;, \u0026#34;cn=sales,ou=groups,dc=example,dc=com\u0026#34;, \u0026#34;cn=it,ou=groups,dc=example,dc=com\u0026#34; ], \u0026#34;remove_users\u0026#34;: false }, \u0026#34;LDAP Organization 2\u0026#34;: { \u0026#34;admins\u0026#34;: [ \u0026#34;cn=Administrators,cn=Builtin,dc=example,dc=com\u0026#34; ], \u0026#34;remove_admins\u0026#34;: false, \u0026#34;users\u0026#34;: true, \u0026#34;remove_users\u0026#34;: false } } "},{"uri":"https://blog.stderr.at/tags/ldap/","title":"LDAP","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/quay/","title":"Quay","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/azure/2021-10-17-s2s-vpn/","title":"Stumbling into Azure Part I: Building a site-to-site VPN tunnel for testing","tags":[],"description":"","content":" So we want to play with ARO (Azure Red Hat OpenShift) private clusters. A private cluster is not reachable from the internet (surprise) and is only reachable via a VPN tunnel from other networks.\nThis blog post describes how we created a site-to-site VPN between a Hetzner dedicated server running multiple VM\u0026#39;s via libvirt and Azure.\nAn upcoming blog post is going to cover the setup of the private ARO cluster.\nAzure Setup The diagram below depicts our planned setup:\nOn the right hand side can see the resources required for our lab:\na virtual network (vnet 192.168.128.0/19). This vnet will be split into 3 separate subnets a master subnet (192.168.129.0/24) holding the ARO control plane nodes a node subnet (192.168.130.0/24) holding ARO worker nodes and finally a subnet call GatewaySubnet where we are going to deploy our Azure VPN gateway (called a vnet-gateway) The subnet where the Azure VPN gateway is located needs to have the name GatewaySubnet. Otherwise creating the Azure VPN gateway will fail.\nwe also need a publicIP resource that we are going to connect to our vnet-gateway (the VPN gateway) and finally a local-gateway resource that tells the vnet-gateway which networks are reachable on the left, in our case the Hetzner server. Creating the required Azure resources First we are going to set some environment variable. Those variables are used in the upcoming commands:\nexport RESOURCEGROUP=aro-rg export GATWAY_SUBNET=\u0026#34;192.168.128.0/24\u0026#34; export MASTER_SUBNET=\u0026#34;192.168.129.0/24\u0026#34; export WORKER_SUBNET=\u0026#34;192.168.130.0/24\u0026#34; export HETZNER_VM_NETWORKS=\u0026#34;10.0.0.0/24 192.168.122.0/24 172.16.100.0/24\u0026#34; Next create a VNET resource holding our sub networks:\naz network vnet create \\ --resource-group $RESOURCEGROUP \\ --name aro-vnet \\ --address-prefixes 192.168.128.0/18 Create the GatewaySubnet subnet\naz network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name GatewaySubnet \\ --address-prefixes $GATEWAY_SUBNET Create the master subnet\naz network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name master-subnet \\ --address-prefixes $MASTER_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Create the worker subnet\naz network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name worker-subnet \\ --address-prefixes $WORKER_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry Create a public IP resource\naz network public-ip create \\ --name GatewayIP \\ --resource-group $RESOURCEGROUP \\ --allocation-method Dynamic Create a local-gateway resource\naz network local-gateway create \\ --name playground \\ --resource-group $RESOURCEGROUP \\ --local-address-prefixes $HETZNER_VM_NETWORKS \\ --gateway-ip-address 95.217.42.98 Create a vnet-gateway resource (takes around 30 minutes)\naz network vnet-gateway create \\ --name vpn-gateway \\ --public-ip-address GatewayIP \\ --resource-group $RESOURCEGROUP \\ --vnet aro-vnet \\ --gateway-type Vpn \\ --vpn-type RouteBased \\ --sku Basic \\ --no-wait Define a vpn-connection\naz network vpn-connection create \\ --name VNet1toSite2 \\ --resource-group $RESOURCEGROUP \\ --vnet-gateway1 vpn-gateway \\ --local-gateway2 playground \\ --location westeurope \\ --shared-key thepassword Required iptables (nf tables) hacks for libvirt Skip NAT rules if the destination network is in Azure and the client network deploy via libvirt iptables -I LIBVIRT_PRT 2 -t nat -d 192.168.129.0/24 -j RETURN iptables -I LIBVIRT_PRT 2 -t nat -d 192.168.130.0/24 -j RETURN Skip NAT rules if the destination network is in Azure and the client is connected via tailscale iptables -I ts-postrouting 1 -t nat -d 192.168.129.0/24 -j RETURN iptables -I ts-postrouting 1 -t nat -d 192.168.130.0/24 -j RETURN Libreswan setup on CentOS Stream Install the Libreswan packages\ndnf install libreswan Create a Azure configuration for Libreswan in ~/etc/ipsec.d/azure.conf\nconn masterSubnet also=azureTunnel leftsubnet=192.168.129.0/24 rightsubnet=172.16.100.0/24 auto=start conn workerSubnet also=azureTunnel leftsubnet=192.168.130.0/24 rightsubnet=172.16.100.0/24 auto=start conn azureTunnel authby=secret auto=start dpdaction=restart dpddelay=30 dpdtimeout=120 ike=aes256-sha1;modp1024 ikelifetime=3600s ikev2=insist keyingtries=3 pfs=yes phase2alg=aes128-sha1 left=51.137.113.44 leftsubnets=192.168.128.0/24 right=%defaultroute rightsubnets=172.16.100.0/24 salifetime=3600s type=tunnel ipsec-interface=yes Create a Libreswan secrets file for Azure in /etc/ipsec.d/azure.secrets:\n%any %any : PSK \u0026#34;abc123\u0026#34; Enable and start the IPsec service\nsystemctl enable --now ipsec We had to explicitly load the IPsec configuration via\nipsec addconn --config /etc/ipsec.d/azure.conf azureTunnel Libreswan IPSEC debugging tips Check the state of the IPsec systemd service\nsystemctl status ipsec Check the full log of the IPsec systemd service\njournalctl -e -u ipsec Check the state of the tunnels with the ipsec command line tool\nipsec status Check for the following lines\n000 Total IPsec connections: loaded 5, active 2 000 000 State Information: DDoS cookies not required, Accepting new IKE connections 000 IKE SAs: total(1), half-open(0), open(0), authenticated(1), anonymous(0) 000 IPsec SAs: total(2), authenticated(2), anonymous(0) 000 000 #130: \u0026#34;azureTunnel/1x1\u0026#34;:500 STATE_V2_ESTABLISHED_CHILD_SA (IPsec SA established); EVENT_SA_REKEY in 2003s; newest IPSEC; eroute owner; isakmp#131; idle; 000 #130: \u0026#34;azureTunnel/1x1\u0026#34; esp.56cf4304@51.137.113.44 esp.6f49e8d3@95.217.42.98 tun.0@51.137.113.44 tun.0@95.217.42.98 Traffic: ESPin=0B ESPout=0B! ESPmax=0B 000 #129: \u0026#34;masterSubnet/0x0\u0026#34;:500 STATE_V2_ESTABLISHED_CHILD_SA (IPsec SA established); EVENT_SA_REKEY in 1544s; newest IPSEC; eroute owner; isakmp#131; idle; 000 #129: \u0026#34;masterSubnet/0x0\u0026#34; esp.6e81e8da@51.137.113.44 esp.6f72bbc8@95.217.42.98 tun.0@51.137.113.44 tun.0@95.217.42.98 Traffic: ESPin=0B ESPout=0B! ESPmax=0B 000 #131: \u0026#34;masterSubnet/0x0\u0026#34;:500 STATE_V2_ESTABLISHED_IKE_SA (established IKE SA); EVENT_SA_REKEY in 2121s; newest ISAKMP; idle; IPsec specifies properties of connections via security associations (SA). The parent SA is describes the IKEv2 connections, the child SA is the ESP (encapsulated security payload) connection.\nCheck IPsec transformation policies\nip xfrm policy Check the state of IPsec transformation policies\nip xfrm state Check for dropped packages on the IPsec interface (ipsec1 in our case)\nip -s link show dev ipsec1 Additonal Resources Build an Azure site-to-site VPN for DevTest Create a virtual network with a Site-to-Site VPN connection using CLI Libreswan: Disable rp_filter for IPsec Libreswan: NAT and IPsec not working Libreswan: Subnet to subnet VPN "},{"uri":"https://blog.stderr.at/quay/2021-09-07-quay-upgrade-3.4/","title":"Stumbling into Quay: Upgrading from 3.3 to 3.4 with the quay-operator","tags":[],"description":"","content":" We had the task of answering various questions related to upgrading Red Hat Quay 3.3 to 3.4 and to 3.5 with the help of the quay-operator.\nThankfully (sic!) everything changed in regards to the Quay operator between Quay 3.3 and Quay 3.4.\nSo this is a brain dump of the things to consider.\nOperator changes With Quay 3.4 the operator was completely reworked and it basically changed from opinionated to very opinionated. The upgrade works quite well but you have to be aware about the following points:\nThe name of the custom resource changed from QuayEcosystem to QuayRegistry The configHostname, used for providing the quay configuration UI, is no longer configurable The password for the configuration UI is always regenerated after a configuration re-deployment The volume size of the PostgreSQL PVC will change to 50G In my test cluster I was using a 10G Ceph block device and the StorageClass did not support volume expansion. So my upgrade stopped at this point and I had to allow volume expansion in the storage class.\nA horizontal pod autoscaler is also deploy during the upgrade. The default is to scale automatically to 20 pods, you might reconsider this… With the Quay operator version 3.5 the operator is monitoring all namespaces for custom resources and needs to be installed in the openshift-operators namespace, this is how we upgraded Quay to 3.5 including the operator:\nchange the quay operator channel to 3.5 trigger an upgrade now Quay gets upgraded to version 3.5 after the Quay upgrade you need to reinstall the operator:\ndeinstall the quay operator, Quay is not affected by this reinstall the Quay operator (3.5) in all-namespaces the re-installation of the operator triggers a quay deployment, all Quay pods are restarted! You have to manually cleanup old\npostgres-config-secrets and quay-config-bundle\u0026#39;s Backup and restore considerations Red Hat is working on providing documentation on how to backup and restore Quay in various scenarios. There\u0026#39;s an open task for this that provides more information https://issues.redhat.com/browse/PROJQUAY-2242.\n"},{"uri":"https://blog.stderr.at/tags/sealed-secret/","title":"Sealed Secret","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2021-09-25-sealed_secrets/","title":"Secure your secrets with Sealed Secrets","tags":["Storage","OpenShift","OCP","Sealed Secret"],"description":"Using Sealed Secrets to encrypt your secrets and be able to upload them to Git","content":" Working with a GitOps approach is a good way to keep all configurations and settings versioned and in sync on Git. Sensitive data, such as passwords to a database connection, will quickly come around. Obviously, it is not a idea to store clear text strings in a, maybe even public, Git repository. Therefore, all sensitive information should be stored in a secret object. The problem with secrets in Kubernetes is that they are actually not encrypted. Instead, strings are base64 encoded which can be decoded as well. Thats not good …​ it should not be possible to decrypt secured data. Sealed Secret will help here…​\nSealed Secrets by Bitnami[1] is one option to create real, encrypted secrets. It contains two parts:\nA cluster-side controller / operator, which decrypts the secrets server-side on OpenShift installed in a dedicated namespace usually called sealed secrets.\nkubeseal - a client-side command line tool\nPrerequisites An OpenShift 4 cluster with cluster-admin permissions.\nSealed Secrets Operator Goto OperatorHub and search for Sealed Secrets (This is a Community Operator)\nFigure 1. Search Sealed Secrets in OperatorHub Install the operator, using the default settings, into the namespace sealed-secrets\nFigure 2. Installed Sealed Secret Operator Install the CRD SealedSecretController Install the following object. For now the default values can be used.\napiVersion: bitnami.com/v1alpha1 kind: SealedSecretController metadata: name: controller (1) namespace: sealed-secrets spec: networkPolicy: false nodeSelector: {} podLabels: {} resources: {} affinity: {} securityContext: fsGroup: \u0026#39;\u0026#39; runAsUser: \u0026#39;\u0026#39; rbac: create: true pspEnabled: false crd: create: true keep: true ingress: annotations: {} enabled: false hosts: - chart-example.local path: /v1/cert.pem tls: [] serviceAccount: create: true name: \u0026#39;\u0026#39; image: pullPolicy: IfNotPresent repository: \u0026gt;- quay.io/bitnami/sealed-secrets-controller@sha256:8e9a37bb2e1a6f3a8bee949e3af0e9dab0d7dca618f1a63048dc541b5d554985 secretName: sealed-secrets-key tolerations: [] controller: create: true priorityClassName: \u0026#39;\u0026#39; podAnnotations: {} 1 Be aware of the name of the controller OBJECT (name: controller). It is used lated as part of the actual controller name Install the command line tool kubeseal The kubeseal binary can be easily installed using either\non Mac: brew install kubeseal or\non Linux:\nwget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.16.0/kubeseal-linux-amd64 -O kubeseal install -m 755 kubeseal /usr/local/bin/ Testing Sealed Secrets Create a new project oc new-project myproject\nCreate a secret\necho -n \u0026#34;my_super_secret_string\u0026#34; \\ | kubectl create secret generic mypasswords --dry-run=client --from-file=password=/dev/stdin -o json \\ | kubeseal --controller-namespace=sealed-secrets --controller-name=controller-sealed-secrets --format json \u0026gt; mysealedsecret.json (1) 1 The switches --controller-namespace define the namespace where the operator is installed, --controller-name is a combination of the SealedSecretController object name and the name of the namespace The password=my_super_secret_string is created and piped into kubeseal which is using the controller, where the server created a certificate for encryption, to create an encrypted json file mysealedsecret.json. It is important to note, that the actually Kubernetes secret object is not created at this stage.\nThe file mysealedsecret.json is encrypted now and it is safe to store this file on Github.\nIt looks like this:\n{ \u0026#34;kind\u0026#34;: \u0026#34;SealedSecret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;bitnami.com/v1alpha1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mypasswords\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;myproject\u0026#34;, (1) \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;spec\u0026#34;: { \u0026#34;template\u0026#34;: { \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mypasswords\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;myproject\u0026#34;, (1) \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: null }, \u0026#34;encryptedData\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;AgBsSZVcTfzfNFI7ZlCsH3/4b3L7m52/O9f70pMtn1myPWHeY1QJFoxpWkH0tWosfeIoko+iB0kCyFk/iJEYSvd31zgnr90hv4e2qVtEBmm6n5B7V40ZERdiy2Cz7UXakUKDdhTjA0BTjcf0f0b2FRDenGxCHJB7cyOVGOZ36jF6IdP2k6kbsZXklti/4MXK7oskDXGzU7rTsESK0ttk5uQgrpfWrhaUip5+Db5vcG1OlHhMJ7In3NlNr0mbl+YiXsKKDNvyw9T14L3rlfvHz1xe0lIqC72i5LSCarpGoSKNOr+Sev9+b/+no6P4VDPuSLORbwVXlP5kt+8xnpZJIEqnetwhr78dt8F3xmjXVBZncdwKk22Y/b9L+uUKWPAvOT78khpUIHQPo9dV/nmz1ldvu58fCFL4TjOOtyTBcUPD3qQJp+sEXgy63l8hEaMXuLUlk+srSnJfMtwkFhl0CG2fKsg4CsQoZlvq5oKOl50sujg3Trv4W9qVVCYHA7BUXEj6J0DxjOCqSQixHRr7Z7JqIyhhdLYdHwMH80scsIb6Ok7keC82v1yae770NWWxJJ4M7Ieb2ERzgwy825gkdq9nx9I6fVxYJkkZlpKKoTvL0uno4sKjC1yQjCgW1vpiZeLIJO2f9TpvVdK2nrag0/gXPMboAL2BGnMPMwjR7OZm+iHq3NXNKiIV1aWRO4wkd/spWziLjOpeS7T1k9w4XxoACwv3g4it\u0026#34; } } } 1 The sealed secret will be created in your project Upload the sealed secret oc create -f mysealedsecret.json\nVerify Secret The object SealedSecret is created:\noc get SealedSecret NAME AGE mypasswords 3s The SealedSecretController will decrypt the and store the secret in the namespace. This can take a few seconds:\noc get secret mypasswords NAME TYPE DATA AGE mypasswords Opaque 1 25s Extract the secret and verify that your string has been stored as \u0026#34;normal\u0026#34; secret\noc extract secret/mypasswords --to=- # password my_super_secret_string Updating or appending new values The process for updateing or appending a secret is similar. The only difference is that a new value for the key string is new.\n# Updaing string echo -n \u0026#34;my_NEW_super_secret_string\u0026#34; \\ | kubectl create secret generic mypasswords --dry-run=client --from-file=password=/dev/stdin -o json \\ | kubeseal --controller-namespace=sealed-secrets --controller-name=controller-sealed-secrets --format json --merge-into mysealedsecret.json # Appending echo -n \u0026#34;my_appended_string\u0026#34; \\ | kubectl create secret generic mypasswords --dry-run=client --from-file=appendedstring=/dev/stdin -o json \\ | kubeseal --controller-namespace=sealed-secrets --controller-name=controller-sealed-secrets --format json --merge-into mysealedsecret.json Be sure that you are in the namespace you want to install the secret Upload the sealed secret oc apply -f mysealedsecret.json and extract it again to validate:\noc extract secret/mypasswords --to=- # appendedstring my_appended_string # password my_NEW_super_secret_string Sources [1]: Bitname Readme on Github\n"},{"uri":"https://blog.stderr.at/tags/storage/","title":"Storage","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/affinity/","title":"Affinity","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/affinity/","title":"Affinity","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/anti-affinity/","title":"Anti-Affinity","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/anti-affinity/","title":"Anti-Affinity","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/descheduler/","title":"Descheduler","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/descheduler/","title":"Descheduler","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/","title":"Introduction","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector","Affinity","Anti-Affinity"],"description":"Introduction to Pod Placement","content":" Pod scheduling is an internal process that determines placement of new pods onto nodes within the cluster. It is probably one of the most important tasks for a Day-2 scenario and should be considered at a very early stage for a new cluster. OpenShift/Kubernetes is already shipped with a default scheduler which schedules pods as they get created accross the cluster, without any manual steps.\nHowever, there are scenarios where a more advanced approach is required, like for example using a specifc group of nodes for dedicated workload or make sure that certain applications do not run on the same nodes. Kubernetes provides different options:\nControlling placement with node selectors\nControlling placement with pod/node affinity/anti-affinity rules\nControlling placement with taints and tolerations\nControlling placement with topology spread constraints\nThis series will try to go into the detail of the different options and explains in simple examples how to work with pod placement rules. It is not a replacement for any official documentation, so always check out Kubernetes and or OpenShift documentations.\nPod Placement Series NodeSelector\nPod Affinity and Anti Affinity\nNode Affinity\nTaints and Tolerations\nTopology Spread Constraints\nDescheduler\nPrerequisites The following prerequisites are used for all examples. Let’s image that our cluster (OpenShift 4) has 4 compute nodes\noc get node --selector=\u0026#39;node-role.kubernetes.io/worker\u0026#39; NAME STATUS ROLES AGE VERSION compute-0 Ready worker 7h1m v1.19.0+d59ce34 compute-1 Ready worker 7h1m v1.19.0+d59ce34 compute-2 Ready worker 7h1m v1.19.0+d59ce34 compute-3 Ready worker 7h1m v1.19.0+d59ce34 An example application (from the catalog Django + Postgres) has been deployed in the namespace podtesting. It contains by default 1 pod for a PostGresql database and one pod for a frontend web application.\noc get pods -n podtesting -o wide | grep Running django-psql-example-1-h6kst 1/1 Running 0 20m 10.130.2.97 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-1-4pcm4 1/1 Running 0 21m 10.131.0.51 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Without any configuration the OpenShift scheduler will try to spread the pods evenly accross the cluster.\nLet’s increase the replica of the web frontend:\noc scale --replicas=4 dc/django-psql-example -n podtesting Eventually 4 additional pods will be started accross the compute nodes of the cluster:\noc get pods -n podtesting -o wide | grep Running django-psql-example-1-842fl 1/1 Running 0 2m7s 10.131.0.65 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-h6kst 1/1 Running 0 24m 10.130.2.97 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-pxhlv 1/1 Running 0 2m7s 10.128.2.13 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-xms7x 1/1 Running 0 2m7s 10.129.2.10 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-1-4pcm4 1/1 Running 0 26m 10.131.0.51 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; As you can see, the scheduler already tries to spread the pods evenly, so that every worker node will host one frontend pod.\nHowever, let’s try to apply a more advanced configuration for the pod placement, starting with the Pod Placement - NodeSelector\n"},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-29-node-affinity/","title":"Node Affinity","tags":["OCP","Day-2","OpenShift","Pod Placement","Affinity"],"description":"Placeing Pods Using the Node Affinity rules","content":" Node Affinity allows to place a pod to a specific group of nodes. For example, it is possible to run a pod only on nodes with a specific CPU or disktype. The disktype was used as an example for the nodeSelector and yes …​ Node Affinity is conceptually similar to nodeSelector but allows a more granular configuration.\nPod Placement Series Please check out other ways of pod placements:\nExpand me... NodeSelector\nPod Affinity and Anti Affinity\nNode Affinity\nTaints and Tolerations\nTopology Spread Constraints\nDescheduler\nUsing Node Affinity Currently two types of affinity settings are known:\nrequiredDuringSchedulingIgnoreDuringExecuption (short required) - a hard requirement which must be met before a pod can be scheduled\npreferredDuringSchedulingIgnoredDuringExecution (short preferred) - a soft requirement the scheduler tries to meet, but does not guarantee it\nBoth types can be specified. In such case the node must first meet the required rule and then attempt to meet the preferred rule. Preparing node labels Remember the prerequisites explained in the . Pod Placement - Introduction. We have 4 compute nodes and an example web application up and running. Before we start with affinity rules we need to label all nodes. Let’s create 2 zones (east and west) for our compute nodes.\nYou can skip this, if these labels are still set. Figure 1. Node Zones oc label nodes compute-0 compute-1 topology.kubernetes.io/zone=east oc label nodes compute-2 compute-3 topology.kubernetes.io/zone=west Configure node affinity rule Like pod affinity the node affinity is defined on the pod specification:\nkind: DeploymentConfig apiVersion: apps.openshift.io/v1 metadata: name: django-psql-example namespace: podtesting [...] spec: [...] template: [...] spec: [...] affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - west In this example the pods are started only on nodes of the zone \u0026#34;West\u0026#34;. Since the value is an array, multiple zones can be defined letting the web application be executed on West and East for example. With this setup you can control on which node a specific application shall be executed. For example: you have a group of nodes which provide a GPU and your GPU application must be started only on this group of nodes.\nLike with pod affinity you can combine required and preferred settings.\nWhat happened to Node Anti-Affinity? Unlike Pod Anti-Affinity, there is no concept to define a node Anti-Affinity. Instead you can use the NotIn and DoesNotExist operators to achieve this bahaviour.\nCleanup As cleanup simply remove the affinity specification from the DeploymentConf. The node labels can stay as they are since they do not hurt.\nSummary This concludes the quick overview of the node affinity. Further information can be found at Node Affinity\n"},{"uri":"https://blog.stderr.at/categories/nodeselector/","title":"nodeSelector","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/","title":"NodeSelector","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector"],"description":"Placeing Pods Using the NodeSelector","content":" One of the easiest ways to tell your Kubernetes cluster where to put certain pods is to use a nodeSelector specification. A nodeSelector defines a key-value pair and are defined inside the specification of the pods and as a label on one or multiple nodes (or machine set or machine config). Only if selector matches the node label, the pod is allowed to be scheduled on that node.\nKubernetes distingushes between 2 types of selectors:\ncluster-wide node selectors: defined by the cluster administrators and valid for the whole cluster\nproject node selectors: to place new pods inside projects into specific nodes.\nPod Placement Series Please check out other ways of pod placements:\nExpand me... NodeSelector\nPod Affinity and Anti Affinity\nNode Affinity\nTaints and Tolerations\nTopology Spread Constraints\nDescheduler\nUsing nodeSelector As previously described, we have a cluster with an example application scheduled accross the worker nodes evenly by the scheduler.\noc get pods -n podtesting -o wide | grep Running django-psql-example-1-842fl 1/1 Running 0 2m7s 10.131.0.65 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-h6kst 1/1 Running 0 24m 10.130.2.97 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-pxhlv 1/1 Running 0 2m7s 10.128.2.13 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-xms7x 1/1 Running 0 2m7s 10.129.2.10 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-1-4pcm4 1/1 Running 0 26m 10.131.0.51 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; However, our 4 compute nodes are assembled with different hardware specification and are using different harddisks (sdd vs hdd).\nFigure 1. Nodes with Different Specifications Since our web application must run on fast disks must configure the cluster to schedule the pods on nodes with SSD only.\nTo start using nodeSelectors we first label our nodes accordingly:\ncompute-0 and compute-1 are faster nodes with an SSD attached.\ncompute-2 and compute-2 have a HDD attached.\noc label nodes compute-0 compute-1 disktype=ssd (1) oc label nodes compute-2 compute-3 disktype=hdd 1 as key we are using disktype As crosscheck we can list nodes with a specific label:\noc get nodes -l disktype=ssd NAME STATUS ROLES AGE VERSION compute-0 Ready worker 7h32m v1.19.0+d59ce34 compute-1 Ready worker 7h31m v1.19.0+d59ce34 oc get nodes -l disktype=hdd NAME STATUS ROLES AGE VERSION compute-2 Ready worker 7h32m v1.19.0+d59ce34 compute-3 Ready worker 7h32m v1.19.0+d59ce34 If no matching label is found, the pod cannot be scheduled. Therefore, always label the nodes first. The 2nd step is to add the node selector to the specification of the pod. In our example we are using a DeploymentConfig, so let’s add it there:\noc patch dc django-psql-example -n podtesting --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;nodeSelector\u0026#34;:{\u0026#34;disktype\u0026#34;:\u0026#34;ssd\u0026#34;}}}}}\u0026#39; This adds the nodeSelector into: spec/template/spec\nnodeSelector: disktype: ssd Kubernetes will now trigger a restart of the pods on the supposed nodes.\noc get pods -n podtesting -o wide | grep Running django-psql-example-3-4j92k 1/1 Running 0 42s 10.129.2.7 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-3-d7hsd 1/1 Running 0 42s 10.129.2.8 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-3-fkbfm 1/1 Running 0 14m 10.128.2.18 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-3-psskb 1/1 Running 0 14m 10.128.2.17 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; As you can see, only nodes with a SSD (compute-0 and compute-1) are being used.\nControlling pod placement with project-wide selector Adding a nodeSelector to a deployment seems fine…​ until somebody forgets to add it. Then the pods would be started anywhere the scheduler finds suitable. Therefore, it might make sense to use a project-wide node selector, which will automatically be applied on all pods on that project. The project selector is added by the cluster administrator to the Namespace object (no matter what the OpenShift documentation says in it’s example) as openshift.io/node-selector parameter.\nLet’s remove our previous configuration and add the setting to our namespace podtesting:\nCleanup\nRemove the nodeSelector from the deployment configuration and wait until all pods have been reshuffeld\noc patch dc django-psql-example -n podtesting --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/nodeSelector\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;disktype=ssd\u0026#34; }]\u0026#39; Add the label to the project\noc annotate ns/podtesting openshift.io/node-selector=\u0026#34;disktype=ssd\u0026#34; The OpenShift scheduler will now spread the Pods accross compute-0 or compute-1 again, but not on compute-2 or 3.\nWe can prove that by stressing our cluster (and nodes) a little bit and scale our frontend application to 10:\noc get pods -n podtesting -o wide | grep Running django-psql-example-4-2jn2l 1/1 Running 0 27s 10.128.2.8 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-6g7ks 1/1 Running 0 7m47s 10.129.2.23 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-752nm 1/1 Running 0 7m47s 10.128.2.7 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-c5jvm 1/1 Running 0 27s 10.129.2.4 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-f5kwg 1/1 Running 0 27s 10.129.2.5 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-g7bcs 1/1 Running 0 7m47s 10.129.2.24 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-h5tgb 1/1 Running 0 27s 10.129.2.6 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-spvpp 1/1 Running 0 28s 10.128.2.5 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-v9qwj 1/1 Running 0 7m48s 10.129.2.22 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-zgwcv 1/1 Running 0 27s 10.128.2.6 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; As you can see compute-0 and compute-1 are the only nodes which are used.\nWell-Known Labels nodeSelector is one of the easiest ways to control where an application shall be started. Working with labels is therefore very important as soon as workload shall be added to the cluster. Kubernetes reserves some labels which can be leveraged and some are already predefined on the nodes, for example:\nbeta.kubernetes.io/arch=amd64\nkubernetes.io/hostname=compute-0\nkubernetes.io/os=linux\nnode-role.kubernetes.io/worker=\nnode.openshift.io/os_id=rhcos\nA list of all known can be found at: [1]\nTwo of them I would like to mention here, since they might become very important when designing the placement of pods:\ntopology.kubernetes.io/zone\ntopology.kubernetes.io/region\nWith these two labels you can create availability zones for your cluster. A zone can be seen a logical failure domain and a cluster is typically spanned across multiple zones. This could be a rack in a data center for example, hardware which is sharing the same switch or simply different data centers. Zones are seen as independent to each other.\nA region is made up of one or more zones. A cluster is usually not spanned across multiple region.\nKubernetes makes a few assumptions about the structure of zones and regions:\nregions and zones are hierarchical: zones are strict subsets of regions and no zone can be in 2 regions\nzone names are unique across regions; for example region \u0026#34;africa-east-1\u0026#34; might be comprised of zones \u0026#34;africa-east-1a\u0026#34; and \u0026#34;africa-east-1b\u0026#34;\nCleanup This concludes the chapter about nodeSelectors. For the next chapter of the Pod Placement Series (Pod Affinity and Anti Affinity) we need to cleanup our configuration.\nScale the frontend down to 2\noc scale --replicas=2 dc/django-psql-example -n podtesting Remove the label from the namespace\noc annotate ns/podtesting openshift.io/node-selector- (1) 1 The minus at the end defines that this annotation shall be removed And, just to be sure if you have not done this before, remove the nodeSelector from the DeploymentConfig\noc patch dc django-psql-example -n podtesting --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/nodeSelector\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;disktype=ssd\u0026#34; }]\u0026#39; Sources [1]: Well-Known Labels, Annotations and Taints\n"},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-28-affinity/","title":"Pod Affinity/Anti-Affinity","tags":["OCP","Day-2","OpenShift","Pod Placement","Affinity","Anti-Affinity"],"description":"Placeing Pods Using the Affinity rules","content":" While noteSelector provides a very easy way to control where a pod shall be scheduled, the affinity/anti-affinity feature, expands this configuration with more expressive rules like logical AND operators, constraints against labels on other pods or soft rules instead of hard requirements.\nThe feature comes with two types:\npod affinity/anti-affinity - allows constrains against other pod labels rather than node labels.\nnode affinity - allows pods to specify a group of nodes they can be placed on\nPod Placement Series Please check out other ways of pod placements:\nExpand me... NodeSelector\nPod Affinity and Anti Affinity\nNode Affinity\nTaints and Tolerations\nTopology Spread Constraints\nDescheduler\nPod Affinity and Anti-Affinity Affinity and Anti-Affinity controls the nodes on which a pod should (or should not) be scheduled based on labels on Pods that are already scheduled on the node. This is a different approach than nodeSelector, since it does not directly take the node labels into account. That said, one example for such setup would be: You have dedicated nodes for developement and production workload and you have to be sure that pods of dev or prod applications do not run on the same node.\nAffinity and Anti-Affinity are shortly defined as:\nPod affinity - tells scheduler to put a new pod onto the same node as other pods (selection is done using label selectors)\nFor example:\nI want to run where this other labelled pod is already running (Pods from same service shall be running on same node.)\nPod Anti-Affinity - prevents the scheduler to place a new pod onto the same nodes with pods with the same labels\nFor example:\nI definitely do not want to start a pod where this other pod with the defined label is running (to prevent that all Pods of same service are running in the same availability zone.)\nAs described in the official Kubernetes documention two things should be considered:\nThe affinity feature requires processing which can slow down the cluster. Therefore, it is not recommended for cluster with more than 100 nodes.\nThe feature requires that all nodes are consistently labelled (topologyKey). Unintended behaviour might happen if labels are missing.\nUsing Affinity/Anti-Affinity Currently two types of pod affinity/anti-affinity are known:\nrequiredDuringSchedulingIgnoreDuringExecuption (short required) - a hard requirement which must be met before a pod can be scheduled\npreferredDuringSchedulingIgnoredDuringExecution (short preferred) - a soft requirement the scheduler tries to meet, but does not guarantee it\nBoth types can be defined in the same specification. In such case the node must first meet the required rule and then attempt based on best effort to meet the preferred rule. Preparing node labels Remember the prerequisites explained in the . Pod Placement - Introduction. We have 4 compute nodes and an example web application up and running. Before we start with affinity rules we need to label all nodes. Let’s create 2 zones (east and west) for our compute nodes using the well-known label topology.kubernetes.io/zone\nFigure 1. Node Zones oc label nodes compute-0 compute-1 topology.kubernetes.io/zone=east oc label nodes compute-2 compute-3 topology.kubernetes.io/zone=west Configure pod affinity rule In our example we have one database pod and multiple web application pods. Let’s image we would like to always run these pods in the same zone.\nThe pod affinity defines that a pod can be scheduled onto a node ONLY if that node is in the same zone as at least one already-running pod with a certain label.\nThis means we must first label the postgres pod accordingly. Let’s labels the pod with security=zone1\noc patch dc postgresql -n podtesting --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/metadata/labels/security\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;zone1\u0026#34; }]\u0026#39; After a while the postgres pod is restarted on one of the 4 compute nodes (since we did not specify in which zone this single pod shall be started) - here compute-1 (zone == east):\noc get pods -n podtesting -o wide | grep Running postgresql-5-6v5h6 1/1 Running 0 119s 10.129.2.24 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; As a second step, the deployment configuration of the web application must be modified. Remember, we want to run web application pods only on nodes located in the same zone as the postgres pods. In our example this would be either compute-0 or compute-1.\nModify the config accordingly:\nkind: DeploymentConfig apiVersion: apps.openshift.io/v1 [...] namespace: podtesting spec: [...] template: [...] spec: [...] affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security (1) operator: In (2) values: - zone1 (3) topologyKey: topology.kubernetes.io/zone (4) 1 The key of the label of a pod which is already running on that node is \u0026#34;security\u0026#34; 2 As operator \u0026#34;In\u0026#34; is used the postgres pod must have a matching key (security) containing the value (zone1). Other options like \u0026#34;NotIn\u0026#34;, \u0026#34;DoesNotExist\u0026#34; or \u0026#34;Exact\u0026#34; are available as well 3 The value must be \u0026#34;zone1\u0026#34; 4 As topology the topology.kubernetes.io/zone is used. The application can be deployed on nodes with the same label Setting this (and maybe scaling the replicas up a little bit) will start all frontend pods either on compute-0 or on compute-1.\nIn other words: On nodes of the same zone, where the postgres pod with the label security=zone1 is running.\noc get pods -n podtesting -o wide | grep Running django-psql-example-13-4w6qd 1/1 Running 0 67s 10.128.2.58 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-655dj 1/1 Running 0 67s 10.129.2.28 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-9d4pj 1/1 Running 0 67s 10.129.2.27 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-bdwhb 1/1 Running 0 67s 10.128.2.61 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-d4jrw 1/1 Running 0 67s 10.128.2.57 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-dm9qk 1/1 Running 0 67s 10.128.2.60 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-ktmfm 1/1 Running 0 67s 10.129.2.25 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-ldm56 1/1 Running 0 77s 10.128.2.55 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-mh2f5 1/1 Running 0 67s 10.129.2.29 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-qfkhq 1/1 Running 0 67s 10.129.2.26 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-v88qv 1/1 Running 0 67s 10.128.2.56 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-vfgf4 1/1 Running 0 67s 10.128.2.59 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-5-6v5h6 1/1 Running 0 3m18s 10.129.2.24 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Configure pod anti-affinity rule For now the database pod and the web application pod are running on nodes of the same zone. However, somebody is asking us to configure it vice versa: the web application should not run in the same zone as postgresql.\nHere we can use the Anti-Affinity feature.\nAs an alternative, it would also be possible to change the operator in the affinity rule from \u0026#34;In\u0026#34; to \u0026#34;NotIn\u0026#34; kind: DeploymentConfig apiVersion: apps.openshift.io/v1 [...] namespace: podtesting spec: [...] template: [...] spec: [...] affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security (1) operator: In (2) values: - zone1 (3) topologyKey: topology.kubernetes.io/zone (4) This will force the web application pods to run only on \u0026#34;west\u0026#34; zone nodes.\ndjango-psql-example-16-4n9h5 1/1 Running 0 40s 10.131.1.53 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-blf8b 1/1 Running 0 29s 10.130.2.63 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-f9plb 1/1 Running 0 29s 10.130.2.64 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-tm5rm 1/1 Running 0 28s 10.131.1.55 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-x8lbh 1/1 Running 0 29s 10.131.1.54 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-zb5fg 1/1 Running 0 28s 10.130.2.65 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-5-6v5h6 1/1 Running 0 18m 10.129.2.24 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Combining required and preferred affinities It is possible to combine requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution. In such case the required affinity MUST be met, while the preferred affinity is tried to be met. The following examples combines these two types in an affinity and anti-affinity specification.\nThe podAffinity block defines the same as above: schedule the pod on a node of the same zone, where a pod with the label security=zone1 is running. The podAntiAffinity defines that the pod should not be started on a node if that node has a pod running with the label security=zone2. However, the scheduler might decide to do so as long the podAffinity rule is met.\nspec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - zone1 topologyKey: topology.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 (1) podAffinityTerm: labelSelector: matchExpressions: - key: security operator: In values: - zone2 topologyKey: topology.kubernetes.io/zone 1 The weight field is used by the scheduler to create a scoring. The higher the scoring the more preferred is that node. topologyKey It is important to understand the topologyKey setting. This is the key for the node label. If an affinity rule is met, Kubernetes will try to find suitable nodes which are labelled with the topologyKey. All nodes must be labelled consistently, otherwise unintended behaviour might occur.\nAs described in the Kubernetes documentation at Pod Affinity and Anit-Affinity, the topologyKey has some constraints:\nQuote Kubernetes:\nFor pod affinity, empty topologyKey is not allowed in both requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution.\nFor pod anti-affinity, empty topologyKey is also not allowed in both requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution.\nFor requiredDuringSchedulingIgnoredDuringExecution pod anti-affinity, the admission controller LimitPodHardAntiAffinityTopology was introduced to limit topologyKey to kubernetes.io/hostname. If you want to make it available for custom topologies, you may modify the admission controller, or disable it.\nExcept for the above cases, the topologyKey can be any legally label-key.\nEnd of quote\nCleanup As cleanup simply remove the affinity specification from the DeploymentConf. The node labels can stay as they are since they do not hurt.\nSummary This concludes the quick overview of the pod affinity. The next chapter will discuss Node Affinity rules, which allows affinity based on node specifications.\n"},{"uri":"https://blog.stderr.at/tags/taints/","title":"Taints","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/taints/","title":"Taints","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/","title":"Taints and Tolerations","tags":["OCP","Day-2","OpenShift","Pod Placement","Taints","Tolerations"],"description":"Placeing Pods Using Taints and Tolerations","content":" While Node Affinity is a property of pods that attracts them to a set of nodes, taints are the exact opposite. Nodes can be configured with one or more taints, which mark the node in a way to only accept pods that do tolerate the taints. The tolerations themselves are applied to pods, telling the scheduler to accept a taints and start the workload on a tainted node.\nA common use case would be to mark certain nodes as infrastructure nodes, where only specific pods are allowed to be executed or to taint nodes with a special hardware (i.e. GPU).\nPod Placement Series Please check out other ways of pod placements:\nExpand me... NodeSelector\nPod Affinity and Anti Affinity\nNode Affinity\nTaints and Tolerations\nTopology Spread Constraints\nDescheduler\nUnderstanding Taints and Tolerations Matching taints and tolerations is defined by a key/value pair and a taint effect.\nFor example, a node can be tained as:\nspec: [...] template: [...] spec: taints: - effect: NoExecute key: key1 value: value1 [...] And a pod can have the matching toleration:\nspec: [...] template: [...] spec: tolerations: - key: \u0026#34;key1\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value1\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 3600 [...] This means that the pod is tolerating the taint of the node with the pair key1=value1.\nParameter: effect Above example is using as effect NoExecute. The following effects are possible:\nNoSchedule - new pods are not scheduled, existing pods remain\nPreferNoSchedule - new pods are not preferred but can still be scheduled but the scheduler tries to avoid that, existing pods remain\nNoExecute - new pods are not scheduled, existing pods (without matching toleration) are removed! The setting tolerationSeconds is used to define a maximum time until a pod is allowed to stay.\nParameter: operator For the operator two options are possible:\nExists - simply checks if a key exists and key and effect matches. No value should be configured in this case.\nEqual (default) - with this option key, value and effect must match exactly.\nConfiguring Taints and Tolerations Our compute-0 node is a special node with a GPU installed. We would like that our web application is running on this node and ONLY our web application is running on that node. To achieve this, we will taint the node accordingly with the key/value gpu=enabled and the effect NoExecute and configure a toleration to the DeploymentConfig of our web fronted.\nFigure 1. Tainting Node Always configure tolerations before you taint a node. Otherwise the scheduler might not be able to start a pod until it has been configured to tolerate the taints. First we will set the toleration to the DeploymentConfig:\noc edit dc/django-psql-example -n podtesting And add a toleration for the key/value pair, the effect NoExecute and a tolerationSeconds of X seconds.\nspec: [...] template: [...] spec: tolerations: - key: \u0026#34;gpu\u0026#34; value: \u0026#34;enabled\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 30 (1) 1 I am setting the tolerationSeconds to 30 seconds to get it done quicker. Before we taint our node, let’s check which pods are currently running there:\noc get pods -A -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,STATE:.status.phase | grep compute-0 | grep Running tuned-hrvl4 compute-0 Running dns-default-ln9s4 compute-0 Running node-resolver-qk24n compute-0 Running node-ca-lxrvf compute-0 Running ingress-canary-fv5w6 compute-0 Running machine-config-daemon-558v4 compute-0 Running grafana-78ccdb8c9d-8rqsp compute-0 Running node-exporter-rn8h4 compute-0 Running prometheus-adapter-66976bf759-fxdcd compute-0 Running multus-additional-cni-plugins-29qgq compute-0 Running multus-mkd87 compute-0 Running network-metrics-daemon-64hrw compute-0 Running network-check-target-l6l7n compute-0 Running ovnkube-node-hrtln compute-0 Running django-psql-example-24-c599b compute-0 Running django-psql-example-24-l7znv compute-0 Running django-psql-example-24-zpg77 compute-0 Running Multiple different pods are running here, as well as two of our web frontend (django-psql-example)\nThe other django-psql-example pods are started accross the cluster:\noc get pods -n podtesting -o wide oc get pods -n podtesting -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES django-psql-example-25-8ppxc 1/1 Running 0 21s 10.128.0.61 master-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-8rv76 1/1 Running 0 21s 10.130.2.92 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-9m8k2 1/1 Running 0 21s 10.129.0.67 master-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-fhvxg 1/1 Running 0 31s 10.128.2.126 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-kqgwz 0/1 Running 0 21s 10.130.0.71 master-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-m66nn 1/1 Running 0 21s 10.130.0.70 master-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-ntjqb 1/1 Running 0 21s 10.128.0.60 master-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-nxxqh 1/1 Running 0 21s 10.130.2.93 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-p8nbz 1/1 Running 0 21s 10.131.1.183 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-ttr9g 1/1 Running 0 21s 10.129.2.57 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-xn4fp 1/1 Running 0 21s 10.129.2.56 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-xpqf4 1/1 Running 0 21s 10.128.2.127 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-xwmwv 1/1 Running 0 21s 10.131.1.184 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; To taint the node we can simply execute the following command. Be sure to use the same values as in the toleration:\noc adm taint nodes compute-0 gpu=enabled:NoExecute This will create the following specification in the node object:\nspec: [...] taints: - effect: NoExecute key: gpu value: enabled OpenShift will allow pods, which are not tolerating the taints, to keep on running for 30 seconds. After that, these pods will be evicted and started elsewhere.\nWhen we check after the tolerationSeconds time has passed which pods are running on the node compute-0, we will see that most pods have disappeared, except the pods for the webapplication and pods which are part of DaemonSets. (DaemonSets are defined to run on all or specific nodes and are not evicted. The cluster-DaemonSets are tolerating everything)\noc get pods -A -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,STATE:.status.phase | grep compute-0 | grep Running tuned-hrvl4 compute-0 Running node-resolver-qk24n compute-0 Running node-ca-lxrvf compute-0 Running machine-config-daemon-558v4 compute-0 Running node-exporter-rn8h4 compute-0 Running multus-additional-cni-plugins-29qgq compute-0 Running multus-mkd87 compute-0 Running network-metrics-daemon-64hrw compute-0 Running network-check-target-l6l7n compute-0 Running ovnkube-node-hrtln compute-0 Running django-psql-example-25-8dzqh compute-0 Running django-psql-example-25-9p4sb compute-0 Running django-psql-example-25-pqbvn compute-0 Running django-psql-example-25-sb6hr compute-0 Running Removing taints Taints can be simply removed with the oc command added a trailing -\noc adm taint nodes compute-0 gpu=enabled:NoExecute- Built-in Taints - Taint Nodes by Condition Several taints are built into OpenShift and are set during certain events (aka Taint Nodes by Condition) and cleared when the condition is resolved.\nThe following list is quoted from the OpenShift documentation and provides a list of taints which are automatically set. For example, when a node becomes unavailable:\nnode.kubernetes.io/not-ready: The node is not ready. This corresponds to the node condition Ready=False.\nnode.kubernetes.io/unreachable: The node is unreachable from the node controller. This corresponds to the node condition Ready=Unknown.\nnode.kubernetes.io/out-of-disk: The node has insufficient free space on the node for adding new pods. This corresponds to the node condition OutOfDisk=True.\nnode.kubernetes.io/memory-pressure: The node has memory pressure issues. This corresponds to the node condition MemoryPressure=True.\nnode.kubernetes.io/disk-pressure: The node has disk pressure issues. This corresponds to the node condition DiskPressure=True.\nnode.kubernetes.io/network-unavailable: The node network is unavailable.\nnode.kubernetes.io/unschedulable: The node is unschedulable.\nnode.cloudprovider.kubernetes.io/uninitialized: When the node controller is started with an external cloud provider, this taint is set on a node to mark it as unusable. After a controller from the cloud-controller-manager initializes this node, the kubelet removes this taint.\nDepending on the condition, the node will either have the effect NoSchedule, which means no new pods will be started there (unless a toleration is configured) or the effect NoExecute, which will evict pods with no tolerations from the node. Typical examples for NoSchedule condition would be: memory-pressure or disk-pressure. If a node is unreachable or not-ready, then it will be automatically tainted with the effect NoExecute. tolerationSeconds (default 300) will be respected.\nTolerating all taints Tolerations can be configured to tolerate all possible taints. In such case no value or key is configured and operator: \u0026#34;Exists\u0026#34; is used:\nspec: [...] template: [...] spec: tolerations: - operator: “Exists” Some cluster daemonsets (i.e. tuned) are configured this way. Cleanup Remove the taint from the node:\noc adm taint nodes compute-0 gpu=enabled:NoExecute- And remove the toleration specification from the DeploymentConfig\nspec: [...] template: [...] spec: tolerations: - key: \u0026#34;gpu\u0026#34; value: \u0026#34;enabled\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 30 (1) "},{"uri":"https://blog.stderr.at/tags/tolerations/","title":"Tolerations","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/tolerations/","title":"Tolerations","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/topology-spread-constraints/","title":"Topology Spread Constraints","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/topology-spread-constraints/","title":"Topology Spread Constraints","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/","title":"Topology Spread Constraints","tags":["OCP","Day-2","OpenShift","Pod Placement","Topology Spread Constraints"],"description":"Placeing Pods Using Topology Spread Constraints","content":" Topology spread constraints is a new feature since Kubernetes 1.19 (OpenShift 4.6) and another way to control where pods shall be started. It allows to use failure-domains, like zones or regions or to define custom topology domains. It heavily relies on configured node labels, which are used to define topology domains. This feature is a more granular approach than affinity, allowing to achieve higher availability.\nPod Placement Series Please check out other ways of pod placements:\nExpand me... NodeSelector\nPod Affinity and Anti Affinity\nNode Affinity\nTaints and Tolerations\nTopology Spread Constraints\nDescheduler\nUnderstanding Topology Spread Constraints Imagine, like for pod affinity, we have two zones (east and west) in our 4 compute node cluster.\nFigure 1. Node Zones These zones are defined by node labels, which can be created with the following commands:\noc label nodes compute-0 compute-1 topology.kubernetes.io/zone=east oc label nodes compute-2 compute-3 topology.kubernetes.io/zone=west Now let’s scale our web application to 3 pods. The OpenShift scheduler will try to evenly spread the pods accross the cluster:\noc get pods -n podtesting -o wide django-psql-example-31-jrhsr 0/1 Running 0 8s 10.130.2.112 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-31-q66hk 0/1 Running 0 8s 10.131.1.219 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-31-xv7jc 0/1 Running 0 8s 10.128.3.115 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Figure 2. Running Pods If a new pod is started the scheduler may try to start it on compute-1. However, this is done based on best effort. The scheduler does not guarantee that and may try to start it on one of the nodes in zone \u0026#34;West\u0026#34;. With the configuration topologySpreadConstraints this can be controlled and incoming pods can only be scheduled on a node of zone \u0026#34;East\u0026#34; (either on compute-0 or compute-1).\nConfigure TopologySpreadContraint Let’s configure our topology by adding the following into the DeploymentConfig:\nspec: [...] template: [...] topologySpreadConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: name: django-psql-example This defines the following parameter:\nmaxSkew - defines the degree to which pods may be unevenly distributed (must be greater than zero). Depending on the whenUnsatisfiable parameter the bahaviour differs:\nwhenUnsatisfiable == DoNotSchedule: would not schedule if the maxSkew is is not met.\nwhenUnsatisfiable == ScheduleAnyway: the scheduler will still schedule and gives the topology which would decrease the skew a better score\ntopologyKey - key of node lables\nwhenUnsatisfiable - defines what to do with a pod if the spread constraint is not met. Can be either DoNotSchedule (default) or ScheduleAnyway\nlableSelector - used to find matching pods. Found pods are considered to be part of the topology domain. In our example we simply use a default label of the application: name: django-psql-example\nIf now a 4th pod shall be started the topologySpreadConstraints will allow this pod on one of the nodes of zone=east (either compute-0 or compute-1). It cannot be scheduled on nodes in zone=west since it would violate the maxSkew: 1\nPod distribution would be 1 in zone east and 3 on zone west. --\u0026gt; the skew would then be 3 - 1 = 2 which is not equal to 1 Configure multiple TopologySpreadContraint It is possible to configure multiple TopologySpreadConstraints. In such a case all contrains must meet the requirement (logical AND). For example:\nspec: [...] template: [...] topologySpreadConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: name: django-psql-example - maxSkew: 1 topologyKey: kubernetes.io/hostname whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: name: django-psql-example The first part is identical to our first example and would allow the schedule to start a pod in topology.kubernetes.io/zone: east (compute-0 or compute-1). The second configuration defines not a zone but a node hostname. Now the scheduler can only deploy into zone=east AND onto node=compute-1\nConflicts with multiple TopologySpreadConstraints If you use multiple constraints conflicts are possible. For example, you have a 3 node cluster and the 2 zones east and west. In such cases the maxSkew might be increased or the whenUnsatisfiable might be set to ScheduleAnyway\nSee https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#example-multiple-topologyspreadconstraints for further information.\nCleanup Remove the topologySpreadConstraint\nspec: [...] template: [...] topologySpreadConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: name: django-psql-example "},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-09-01-descheduler/","title":"Using Descheduler","tags":["OCP","Day-2","OpenShift","Descheduler"],"description":"Evicting pods using the Descheduler","content":" Descheduler is a new feature which is GA since OpenShift 4.7. It can be used to evict pods from nodes based on specific strategies. The evicted pod is then scheduled on another node (by the Scheduler) which is more suitable.\nThis feature can be used when:\nnodes are under/over-utilized\npod or node affinity, taints or labels have changed and are no longer valid for a running pod\nnode failures\npods have been restarted too many times\nPod Placement Series Please check out other ways of pod placements:\nExpand me... NodeSelector\nPod Affinity and Anti Affinity\nNode Affinity\nTaints and Tolerations\nTopology Spread Constraints\nDescheduler\nDescheduler Profiles The following descheduler profiles are known and one or multiple can be configured for the Descheduler. Each profiles enables certain strategies which the Descheduler is leveraging. Since the strategies names are more or less self explaining, I did not add their full description here. Instead detailed information can be found at: Descheduler Profiles\nAffinityAndTaints - removes pods that violates affinity and anti-affinity rules or taints\nRemovePodsViolatingInterPodAntiAffinity\nRemovePodsViolatingNodeAffinity\nRemovePodsViolatingNodeTaints\nTopologyAndDuplicates - evicts pods which are not evenly spreaded or which are violating the topology domain\nRemovePodsViolatingTopologySpreadConstraint\nRemoveDuplicates\nLifecycleAndUtilization - evicts long-running pods to balance resource usage of nodes\nRemovePodsHavingTooManyRestarts - Pods that are restarted more than 100 times\nLowNodeUtilization - removes pods from overutilized nodes.\nA node is considered underutilized if its usage is below 20% for all thresholds (CPU, memory, and number of pods).\nA node is considered overutilized if its usage is above 50% for any of the thresholds (CPU, memory, and number of pods).\nPodLifeTime - evicts pods that are too old\nDescheduler mechanism The following rules are followed by the Descheduler to ensure that eviction of pods does not go wild. Therefore the following pods will never be evicted:\npods in openshift-* or kube-system namespaces\npods with priorityClassName equal to system-cluster-critical or system-node-critical\npods which cannot be recreated, for example: static or stand-alone pods/jobs or pods without a replication controller or replica set\npods of a daemon set\npods with local storage\npods which are violating the pod disruption budget\nInstalling the Descheduler The Descheduler is not installed by default and must be installed after the cluster has been initiated. This is done by installed the Kube Descheduler Operator.\nFirst we create a separate namespace for our operator, including a label:\noc adm new-project openshift-kube-descheduler-operator oc label ns/openshift-kube-descheduler-operator openshift.io/cluster-monitoring=true Then we search for the Kube Descheduler operator and install it, using the newly created namespace:\nFigure 1. Install Descheduler Operator After a few moments the operator will be installed.\nYou can now create a Descheduler instance either via UI (wizard) or by using the following specification:\napiVersion: operator.openshift.io/v1 kind: KubeDescheduler metadata: name: cluster namespace: openshift-kube-descheduler-operator spec: deschedulingIntervalSeconds: 3600 (1) logLevel: Normal (2) managementState: Managed operatorLogLevel: Normal (3) profiles: (4) - AffinityAndTaints - TopologyAndDuplicates - LifecycleAndUtilization 1 Defines the time interval the descheduler is running. Default is 3600 seconds 2 Defines logging for overall component. Can be Normal, Debug, Trace or TraceAll 3 Defines logging for the operator itself. Can be Normal, Debug, Trace or TraceAll 4 Enables on or multiple profiles the Descheduler should consider "},{"uri":"https://blog.stderr.at/tags/ansible-tower/","title":"Ansible Tower","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/2021/07/ansible-tower-and-downloading-collections/","title":"Ansible Tower and downloading collections","tags":["Ansible","Ansible Tower"],"description":"Getting Ansible Tower to download collections can be hard","content":" Every wondered why Ansible Tower does not start downloading required collections when you synchronize a project? Here are the stumbling blocks we discovered so far:\nWrong name for requirements.yml When downloading collections Ansible Tower searches for a file requirements.yml in the collections directory.\nBe careful with the file extension: requirements.yml has to end with the extension .yml and not .yaml.\nCollections download is disabled in Ansible Tower Within Ansible Tower there is a setting called ENABLE COLLECTION(S) DOWNLOAD under Settings/Jobs. This has to be set to true, which is also the default.\nNo Ansible Galaxy credential defined for the organization Last but not least an Ansible Galaxy credential needs to be defined for the organization where the project is defined. With the default installation of Ansible Tower, when the sample playbooks are installed there is a credential called Ansible Galaxy defined. You need to assign this credential to the organization.\nIf you skip installing the sample playbooks, no Ansible Galaxy credential will be defined for you and you have to create it manually.\nHow does this actually work? Ansible Tower uses a Python virtual environment for running Ansible. The default environment is installed in /var/lib/awx/venv/awx. You can also create custom environments, see Using virtualenv with Ansible Tower.\nIn the default setup the following files define how collections are downloaded:\nlib/python3.6/site-packages/awx/main/tasks.py\nlib/python3.6/site-packages/awx/playbooks/project_update.yml\ntask.py task.py defines various internal tasks Tower has to run on various occasions. For example in line number 1930 (Ansible Tower 3.8.3) the task RunProjectUpdate gets defined. This is the task Tower has to run whenever a project update is required.\nIn our case the function build_extra_vars_file (line 2083 with Ansible Tower 3.8.3) defines the variable galaxy_creds_are_defined only if the organization has a galaxy credential defined (line 2099 Ansible Tower 3.8.3).\nLine 2120 (Ansible Tower 3.8.3) finally defines the Ansible extra variable collections_enabled depending on galaxy_creds_are_defined.\nproject_update.yml So task.py defines the extra variable collections_enabled (see above). Finally the playbook project_update.yml consumes this extra variable and only downloads collections if collections_enabled is set to true, see the block string at line 192 (Ansible Tower 3.8.3) in `project_update.yml.\nSo long and thanks for all the fish!\n"},{"uri":"https://blog.stderr.at/tags/compliance/","title":"Compliance","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/hardening/","title":"Hardening","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/compliance/2021/07/oc-compliance-command-line-plugin/","title":"oc compliance command line plugin","tags":["Hardening","OpenShift","OCP","security","compliance","plugin"],"description":"Using the oc client plugin os-compliance","content":" As described at Compliance Operator the Compliance Operator can be used to scan the OpenShift cluster environment against security benchmark, like CIS. Fetching the actual results might be a bit tricky tough.\nWith OpenShift 4.8 plugins to the oc command are allowed. One of these plugin os oc compliance, which allows you to easily fetch scan results, re-run scans and so on. Let’s install and try it out.\nInstallation An oc plugin must be deployed into the same directory as the oc command itself.\nThe following describes the building and installation of the plugin.\nYou need Go installed on your node. Clone the Git repository:\ngit clone https://github.com/openshift/oc-compliance.git Build and install the plugin\nmake; make install go build -o ./bin/oc-compliance ./cmd which oc | xargs dirname | xargs -n1 cp ./bin/oc-compliance The plugin allows the use of oc compliance\noc compliance You must specify a sub-command. Usage: oc-compliance [flags] oc-compliance [command] Available Commands: bind Creates a ScanSettingBinding for the given parameters controls Get a report of what controls you\\\u0026#39;re complying with fetch-fixes Download the fixes/remediations fetch-raw Download raw compliance results help Help about any command rerun-now Force a re-scan for one or more ComplianceScans view-result View a ComplianceCheckResult Flags: -h, --help help for oc-compliance Use \u0026#34;oc-compliance [command] --help\u0026#34; for more information about a command. Fetch Raw Results Without the oc-compliance plugin it was required to manually spin up a Pod and download the results from this Pod, where the PV is mounted. Now, with a simple command we can select the ScanSettingBinding and define an output folder. For example:\noc compliance fetch-raw \u0026lt;object-type\u0026gt; \u0026lt;object-name\u0026gt; -o \u0026lt;output-path\u0026gt; Assuming the the compliance operator was configured as in the previous article, we have the ScanSettingBinding called cis-compliance:\noc compliance fetch-raw scansettingbindings cis-compliance -n openshift-compliance -o /tmp/ This starts downloading the result archives into /tmp\nFetching results for cis-compliance scans: ocp4-cis-node-worker, ocp4-cis-node-master, ocp4-cis Fetching raw compliance results for pod \u0026#39;raw-result-extractor-fxbw8\u0026#39;.Fetching raw compliance results for scan \u0026#39;ocp4-cis-node-worker\u0026#39;......... The raw compliance results are avaliable in the following directory: /tmp/ocp4-cis-node-worker Fetching raw compliance results for pod \u0026#39;raw-result-extractor-kqrw5\u0026#39;.Fetching raw compliance results for scan \u0026#39;ocp4-cis-node-master\u0026#39;..... The raw compliance results are avaliable in the following directory: /tmp/ocp4-cis-node-master Fetching raw compliance results for pod \u0026#39;raw-result-extractor-pfrgk\u0026#39;.Fetching raw compliance results for scan \u0026#39;ocp4-cis\u0026#39;.. The raw compliance results are avaliable in the following directory: /tmp/ocp4-cis ls -la /tmp/ocp4-cis* /tmp/ocp4-cis: total 172 drwx------ 2 root root 4096 Jul 30 16:05 . drwxrwxrwt. 18 root root 4096 Jul 30 16:05 .. -rw-r--r-- 1 root root 166676 Jul 30 16:05 ocp4-cis-api-checks-pod.xml.bzip2 /tmp/ocp4-cis-node-master: total 504 drwx------ 2 root root 4096 Jul 30 16:05 . drwxrwxrwt. 18 root root 4096 Jul 30 16:05 .. -rw-r--r-- 1 root root 168256 Jul 30 16:05 ocp4-cis-node-master-master-0-pod.xml.bzip2 -rw-r--r-- 1 root root 165716 Jul 30 16:05 ocp4-cis-node-master-master-1-pod.xml.bzip2 -rw-r--r-- 1 root root 166945 Jul 30 16:05 ocp4-cis-node-master-master-2-pod.xml.bzip2 /tmp/ocp4-cis-node-worker: total 1112 drwx------ 2 root root 4096 Jul 30 16:05 . drwxrwxrwt. 18 root root 4096 Jul 30 16:05 .. -rw-r--r-- 1 root root 154943 Jul 30 16:05 ocp4-cis-node-worker-compute-0-pod.xml.bzip2 -rw-r--r-- 1 root root 154903 Jul 30 16:05 ocp4-cis-node-worker-compute-1-pod.xml.bzip2 -rw-r--r-- 1 root root 154939 Jul 30 16:05 ocp4-cis-node-worker-compute-2-pod.xml.bzip2 -rw-r--r-- 1 root root 154890 Jul 30 16:05 ocp4-cis-node-worker-compute-3-pod.xml.bzip2 -rw-r--r-- 1 root root 168175 Jul 30 16:05 ocp4-cis-node-worker-master-0-pod.xml.bzip2 -rw-r--r-- 1 root root 165603 Jul 30 16:05 ocp4-cis-node-worker-master-1-pod.xml.bzip2 -rw-r--r-- 1 root root 166914 Jul 30 16:05 ocp4-cis-node-worker-master-2-pod.xml.bzip2 Re-Run Scans Sometimes it is necessary to re-run scans. This can be done by annotating the appropriate scan as described at: Performing a Rescan\nWith the oc plugin you can simply trigger a re-scan with a single command:\noc compliance rerun-now scansettingbindings \u0026lt;name of scanbinding\u0026gt; For example:\noc compliance rerun-now scansettingbindings cis-compliance Example output:\nRerunning scans from \u0026#39;cis-compliance\u0026#39;: ocp4-cis-node-worker, ocp4-cis-node-master, ocp4-cis Re-running scan \u0026#39;openshift-compliance/ocp4-cis-node-worker\u0026#39; Re-running scan \u0026#39;openshift-compliance/ocp4-cis-node-master\u0026#39; Re-running scan \u0026#39;openshift-compliance/ocp4-cis\u0026#39; With the command oc get compliancescan -n openshift-compliance you can check when the scan has been done:\nNAME PHASE RESULT ocp4-cis RUNNING NOT-AVAILABLE ocp4-cis-node-master RUNNING NOT-AVAILABLE ocp4-cis-node-worker AGGREGATING NOT-AVAILABLE View Results on CLI Once a scan process has finished you can verify the check results quick and easy using the command line:\noc get ComplianceCheckResult -A This prints for example:\nNAMESPACE NAME STATUS SEVERITY [...] openshift-compliance ocp4-cis-audit-log-forwarding-enabled FAIL medium [...] The view-result can print a human readable output, for example:\noc compliance view-result ocp4-cis-audit-log-forwarding-enabled -n openshift-compliance Example:\n+----------------------+-----------------------------------------------------------------------------------------+ | KEY | VALUE | +----------------------+-----------------------------------------------------------------------------------------+ | Title | Ensure that Audit Log | | | Forwarding Is Enabled | +----------------------+-----------------------------------------------------------------------------------------+ | Status | FAIL | +----------------------+-----------------------------------------------------------------------------------------+ | Severity | medium | +----------------------+-----------------------------------------------------------------------------------------+ | Description | OpenShift audit works at the | | | API server level, logging | | | all requests coming to the | | | server. Audit is on by default | | | and the best practice is | | | to ship audit logs off the | | | cluster for retention. The | | | cluster-logging-operator is | | | able to do this with the | | | | | | | | | | | | ClusterLogForwarders | | | | | | | | | | | | resource. The forementioned resource can be configured to logs to different third party | | | systems. For more information on this, please reference the official documentation: | | | https://docs.openshift.com/container-platform/4.6/logging/cluster-logging-external.html | +----------------------+-----------------------------------------------------------------------------------------+ | Rationale | Retaining logs ensures the | | | ability to go back in time to | | | investigate or correlate any | | | events. Offloading audit logs | | | from the cluster ensures that | | | an attacker that has access | | | to the cluster will not be | | | able to tamper with the logs | | | because of the logs being | | | stored off-site. | +----------------------+-----------------------------------------------------------------------------------------+ | Instructions | Run the following command: | | | | | | oc get clusterlogforwarders | | | instance -n openshift-logging | | | -ojson | jq -r | | | \u0026#39;.spec.pipelines[].inputRefs | | | | contains([\u0026#34;audit\u0026#34;])\u0026#39; | | | | | | The output should return true. | +----------------------+-----------------------------------------------------------------------------------------+ | CIS-OCP Controls | 1.2.23 | +----------------------+-----------------------------------------------------------------------------------------+ | NIST-800-53 Controls | AC-2(12), AU-6, AU-6(1), | | | AU-6(3), AU-9(2), SI-4(16), | | | AU-4(1), AU-11, AU-7, AU-7(1) | +----------------------+-----------------------------------------------------------------------------------------+ | Available Fix | No | +----------------------+-----------------------------------------------------------------------------------------+ | Result Object Name | ocp4-cis-audit-log-forwarding-enabled | +----------------------+-----------------------------------------------------------------------------------------+ | Rule Object Name | ocp4-audit-log-forwarding-enabled | +----------------------+-----------------------------------------------------------------------------------------+ | Remediation Created | No | +----------------------+-----------------------------------------------------------------------------------------+ "},{"uri":"https://blog.stderr.at/tags/plugin/","title":"plugin","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/compliance-operator/","title":"Compliance Operator","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/compliance/2021/07/compliance-operator/","title":"Compliance Operator","tags":["Hardening","OpenShift","OCP","security","compliance","Compliance Operator"],"description":"Hardening OpenShift using the Compliance Operator","content":" OpenShift comes out of the box with a highly secure operating system, called Red Hat CoreOS. This OS is immutable, which means that no direct changes are done inside the OS, instead any configuration is managed by OpenShift itself using MachineConfig objects. Nevertheless, hardening certain settings must still be considered. Red Hat released a hardening guide (CIS Benchmark) which can be downloaded at https://www.cisecurity.org/.\nHowever, an automated way to perform such checks would be nice too. To achieve this the Compliance Operator can be leveraged, which runs an OpenSCAP check to create reports of the clusters is compliant or as the official documentation describes:\nThe Compliance Operator lets OpenShift Container Platform administrators describe the desired compliance state of a cluster and provides them with an overview of gaps and ways to remediate them. The Compliance Operator assesses compliance of both the Kubernetes API resources of OpenShift Container Platform, as well as the nodes running the cluster. The Compliance Operator uses OpenSCAP, a NIST-certified tool, to scan and enforce security policies provided by the content.\nThis article shall show how to quickly install the operator and retrieve the first result. It is not a full documentation, which is written by other people at: Compliance Operator, especially remediation is not covered here.\nAs prerequisites we have:\nInstalled OpenShift 4.6+ cluster\nThe Compliance Operator is available for Red Hat Enterprise Linux CoreOS (RHCOS) deployments only. Install the Compliance Operator The easiest way to deploy the Compliance Operator is by searching the OperatorHub which is available inside OpenShift.\nFigure 1. Install Compliance Operator Keep the default settings and wait until the operator has been installed.\nFigure 2. Install Compliance Operator Custom Resources (CRDs) The operator brings a ton of new CRDs into the system:\nScanSetting …​ defines when and on which roles (worker, master …​) a check shall be executed. It also defines a persistent volume (PV) to store the scan results. Two ScanSettings are created during the installation:\ndefault: just scans without automatically apply changes\ndefault-auto-apply: can automatically remediate without extra steps\nScanSettingBinding …​ binds one or more profiles to a scan\nProfile …​ Represent different compliance benchmarks with a set of rules. For this blog we will use CIS Benchmark profiles\nProfileBundle …​ Bundles a security image, which is later used by Profiles.\nRule …​ Rules which are used by profiles to verify the state of the cluster.\nTailoredProfile …​ Customized profile\nComplianceScan …​ scans which have been performed\nComplianceCheckResult …​ The results of a scan. Each ComplianceCheckResult represents the result of one compliance rule check\nComplianceRemediation …​ If a rule ca be remediated automatically, this object is created.\nCreate a ScanBinding object The first step to do is to create a ScanBiding objects. (We reuse the default ScanSetting)\nLet’s create the following object, which is using the profiles ocp4-cis and ocp4-cis-node\napiVersion: compliance.openshift.io/v1alpha1 kind: ScanSettingBinding metadata: name: cis-compliance profiles: - name: ocp4-cis-node (1) kind: Profile apiGroup: compliance.openshift.io/v1alpha1 - name: ocp4-cis (2) kind: Profile apiGroup: compliance.openshift.io/v1alpha1 settingsRef: name: default (3) kind: ScanSetting apiGroup: compliance.openshift.io/v1alpha1 1 use the profile ocp4-cis-node 2 use the profile ocp4-cis 3 reference to the default scansetting As soon as the object is created the cluster is scan is started. The objects ComplianceSuite and ComplianceScan are created automatically and will eventually reach the phase \u0026#34;DONE\u0026#34; when the scan is completed.\nThe following command will show the results of the scans\noc get compliancescan -n openshift-compliance NAME PHASE RESULT ocp4-cis DONE NON-COMPLIANT ocp4-cis-node-master DONE NON-COMPLIANT ocp4-cis-node-worker DONE INCONSISTENT Three different checks have been done. One overall cluster check and 2 separated for master and worker nodes.\nAs we used the default ScanSetting the next check will run a 1 am.\nProfiles The operator comes with a set of standard profiles which represent different compliance benchmarks.\nTo view available profiles:\noc get profiles.compliance -n openshift-compliance NAME AGE ocp4-cis 28m ocp4-cis-node 28m ocp4-e8 28m ocp4-moderate 28m rhcos4-e8 28m rhcos4-moderate 28m Each profile contains a description which explains the intention and a list of rules which used in this profile.\nFor example the profile \u0026#39;ocp4-cis-node\u0026#39; used above is containing:\noc get profiles.compliance -n openshift-compliance -oyaml ocp4-cis-node # Output description: This profile defines a baseline that aligns to the Center for Internet Security® Red Hat OpenShift Container Platform 4 Benchmark™, V0.3, currently unreleased. This profile includes Center for Internet Security® Red Hat OpenShift Container Platform 4 CIS Benchmarks™ content. Note that this part of the profile is meant to run on the Operating System that Red Hat OpenShift Container Platform 4 runs on top of. This profile is applicable to OpenShift versions 4.6 and greater. [...] name: ocp4-cis-node namespace: openshift-compliance [...] rules: - ocp4-etcd-unique-ca - ocp4-file-groupowner-cni-conf - ocp4-file-groupowner-controller-manager-kubeconfig - ocp4-file-groupowner-etcd-data-dir - ocp4-file-groupowner-etcd-data-files - ocp4-file-groupowner-etcd-member - ocp4-file-groupowner-etcd-pki-cert-files - ocp4-file-groupowner-ip-allocations [...] Like the profiles the different rules can be inspected:\noc get rules.compliance -n openshift-compliance ocp4-file-groupowner-etcd-member -o jsonpath=\u0026#39;{\u0026#34;Title: \u0026#34;}{.title}{\u0026#34;\\nDescription: \\n\u0026#34;}{.description}\u0026#39; # Output Title: Verify Group Who Owns The etcd Member Pod Specification File Description: To properly set the group owner of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml , run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml Profile Customization Sometimes is it required to modify (tailor) a profile to fit specific needs. With the TailoredProfile object it is possible to enable or disable rules.\nIn this blog, I just want to share a quick example from the official documentaiton: https://docs.openshift.com/container-platform/4.7/security/compliance_operator/compliance-operator-tailor.html\nThe following TailoredProfile disables 2 rules and sets a value for another rule:\napiVersion: compliance.openshift.io/v1alpha1 kind: TailoredProfile metadata: name: nist-moderate-modified spec: extends: rhcos4-moderate title: My modified NIST moderate profile disableRules: - name: rhcos4-file-permissions-node-config rationale: This breaks X application. - name: rhcos4-account-disable-post-pw-expiration rationale: No need to check this as it comes from the IdP setValues: - name: rhcos4-var-selinux-state rationale: Organizational requirements value: permissive Working with scan results Once a scan finished you probably want to see what the status of the scan is.\nAs you sse above the cluster failed to be compliant.\noc get compliancescan -n openshift-compliance NAME PHASE RESULT ocp4-cis DONE NON-COMPLIANT ocp4-cis-node-master DONE NON-COMPLIANT ocp4-cis-node-worker DONE INCONSISTENT Retrieving results via oc command List all results which can be remediated automatically:\noc get compliancecheckresults -l \u0026#39;compliance.openshift.io/check-status=FAIL,compliance.openshift.io/automated-remediation\u0026#39; -n openshift-compliance NAME STATUS SEVERITY ocp4-cis-api-server-encryption-provider-cipher FAIL medium ocp4-cis-api-server-encryption-provider-config FAIL medium Further information about remediation can be found at: Compliance Operator Remediation List all results which cannot be remediated automatically and must be fixed manually instead:\noc get compliancecheckresults -l \u0026#39;compliance.openshift.io/check-status=FAIL,!compliance.openshift.io/automated-remediation\u0026#39; -n openshift-compliance NAME STATUS SEVERITY ocp4-cis-audit-log-forwarding-enabled FAIL medium ocp4-cis-file-permissions-proxy-kubeconfig FAIL medium ocp4-cis-node-master-file-groupowner-ip-allocations FAIL medium ocp4-cis-node-master-file-groupowner-openshift-sdn-cniserver-config FAIL medium ocp4-cis-node-master-file-owner-ip-allocations FAIL medium ocp4-cis-node-master-file-owner-openshift-sdn-cniserver-config FAIL medium ocp4-cis-node-master-kubelet-configure-event-creation FAIL medium ocp4-cis-node-master-kubelet-configure-tls-cipher-suites FAIL medium ocp4-cis-node-master-kubelet-enable-protect-kernel-defaults FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-imagefs-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-imagefs-inodesfree FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-memory-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-nodefs-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-nodefs-inodesfree FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-imagefs-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-imagefs-inodesfree FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-memory-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-nodefs-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-nodefs-inodesfree FAIL medium ocp4-cis-node-worker-file-groupowner-ip-allocations FAIL medium ocp4-cis-node-worker-file-groupowner-openshift-sdn-cniserver-config FAIL medium ocp4-cis-node-worker-file-owner-ip-allocations FAIL medium ocp4-cis-node-worker-file-owner-openshift-sdn-cniserver-config FAIL medium ocp4-cis-node-worker-kubelet-configure-event-creation FAIL medium ocp4-cis-node-worker-kubelet-configure-tls-cipher-suites FAIL medium ocp4-cis-node-worker-kubelet-enable-protect-kernel-defaults FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-imagefs-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-imagefs-inodesfree FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-memory-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-nodefs-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-nodefs-inodesfree FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-imagefs-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-imagefs-inodesfree FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-memory-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-nodefs-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-nodefs-inodesfree FAIL medium Retrieving RAW results Let’s first retrieve the raw result of the scan. For each of the ComplianceScans a volume claim (PVC) is created to store he results. We can use a Pod to mount the volume to download the scan results.\nThe following PVC have been created on our example:\noc get pvc -n openshift-compliance NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE ocp4-cis Bound pvc-cc026ae3-2f42-4e19-bc55-016c6dd31d22 1Gi RWO managed-nfs-storage 4h17m ocp4-cis-node-master Bound pvc-3bd47c5e-2008-4759-9d53-ba41b568688d 1Gi RWO managed-nfs-storage 4h17m ocp4-cis-node-worker Bound pvc-77200e5f-0f15-410c-a4ee-f2fb3e316f84 1Gi RWO managed-nfs-storage 4h17m Now we can create a Pod which mounts all PVCs at once:\napiVersion: \u0026#34;v1\u0026#34; kind: Pod metadata: name: pv-extract namespace: openshift-compliance spec: containers: - name: pv-extract-pod image: registry.access.redhat.com/ubi8/ubi command: [\u0026#34;sleep\u0026#34;, \u0026#34;3000\u0026#34;] volumeMounts: (1) - mountPath: \u0026#34;/workers-scan-results\u0026#34; name: workers-scan-vol - mountPath: \u0026#34;/masters-scan-results\u0026#34; name: masters-scan-vol - mountPath: \u0026#34;/ocp4-scan-results\u0026#34; name: ocp4-scan-vol volumes: (2) - name: workers-scan-vol persistentVolumeClaim: claimName: ocp4-cis-node-worker - name: masters-scan-vol persistentVolumeClaim: claimName: ocp4-cis-node-master - name: ocp4-scan-vol persistentVolumeClaim: claimName: ocp4-cis 1 mount paths 2 volumesclaims to mount This creates a Pod with the PVCs mounted inside:\nsh-4.4# ls -la | grep scan drwxrwxrwx. 3 root root 4096 Jul 20 05:20 master-scan-results drwxrwxrwx. 3 root root 4096 Jul 20 05:20 ocp4-scan-results drwxrwxrwx. 3 root root 4096 Jul 20 05:20 workers-scan-results We can download the result-files to our local machine for further auditing. Therefore, we create the folder scan_results in which we copy everything:\nmkdir scan-results; cd scan-results oc -n openshift-compliance cp pv-extract:ocp4-scan-results ocp4-scan-results/. oc -n openshift-compliance cp pv-extract:workers-scan-results workers-scan-results/. oc -n openshift-compliance cp pv-extract:masters-scan-results masters-scan-results/. This will download several bzip2 archives for the appropriate scan result.\nOnce done, you can delete the \u0026#34;download pod\u0026#34; using: oc delete pod pv-extract -n openshift-compliance\nWork wth RAW results So above section described the download of the bzip2 files but what to do with it? First, you can import it into a tool which is able to read openScap reports. Or, secondly, you can use the oscap command to create a html output.\nWe have downloaded the following files:\n./ocp4-scan-results/0/ocp4-cis-api-checks-pod.xml.bzip2 ./masters-scan-results/0/ocp4-cis-node-master-master-0-pod.xml.bzip2 ./masters-scan-results/0/ocp4-cis-node-master-master-2-pod.xml.bzip2 ./masters-scan-results/0/ocp4-cis-node-master-master-1-pod.xml.bzip2 ./workers-scan-results/0/ocp4-cis-node-worker-compute-0-pod.xml.bzip2 ./workers-scan-results/0/ocp4-cis-node-worker-compute-1-pod.xml.bzip2 ./workers-scan-results/0/ocp4-cis-node-worker-compute-3-pod.xml.bzip2 ./workers-scan-results/0/ocp4-cis-node-worker-compute-2-pod.xml.bzip2 To create the html output (be sure that open-scap is installed on you host):\nmkdir html oscap xccdf generate report ocp4-scan-results/0/ocp4-cis-api-checks-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-api-checks.html oscap xccdf generate report masters-scan-results/0/ocp4-cis-node-master-master-0-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-node-master-master-0.html oscap xccdf generate report masters-scan-results/0/ocp4-cis-node-master-master-1-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-node-master-master-1.html oscap xccdf generate report masters-scan-results/0/ocp4-cis-node-master-master-2-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-node-master-master-2.html oscap xccdf generate report workers-scan-results/0/ocp4-cis-node-worker-compute-0-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-node-worker-compute-0.html ... The resulted html files are too big to be show here, but some snippets should give an overview:\nTo view the html output as an example I have linked the html files:\nOCP4 - CIS\nExample Master Node Results\nExample Worker Node Results\nOverall Scoring of the result:\nFigure 3. Scoring A list if passed or failed checks:\nFigure 4. Scan Result list Scan details with a link to the CIS Benchmark section and further explainations on how to fix the issue:\nFigure 5. Scan details Performing a rescan If it is necessary to run a rescan, the ComplianceScan object is simply annotated with:\noc annotate compliancescans/\u0026lt;scan_name\u0026gt; compliance.openshift.io/rescan= If default-auto-apply is enabled, remediation which changes MachineConfigs will trigger a cluster reboot. "},{"uri":"https://blog.stderr.at/tags/block-devices/","title":"Block devices","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2021-02-27-understanding-block-devices/","title":"Understanding RWO block device handling in OpenShift","tags":["Storage","OpenShift","OCP","Block devices"],"description":"A basic introduction into block device usage","content":" In this blog post we would like to explore OpenShift / Kubernetes block device handling. We try to answer the following questions:\nWhat happens if multiple pods try to access the same block device?\nWhat happens if we scale a deployment using block devices to more than one replica?\nAnd finally we want to give a short, high level overview about how the container storage interface (CSI) actually works.\nA block device provides Read-Write-Once (RWO) storage. This basically means a local file system mounted by a single node. Do not confuse this with a cluster (CephFS, GlusterFS) or network file system (NFS). These file systems provide Read-Write-Many (RWX) storage mountable on more than one node. Test setup For running our tests we need the following resources\nA new namespace/project for running our tests\nA persistent volume claim (PVC) to be mounted in our test pods\nTwo pods definitions for mounting the PVC\nStep 1: Creating a new namespace/project To run our test cases we created a new project with OpenShift\noc new-project blockdevices Step 2: Defining a block PVC Our cluster is running the rook operator (https://rook.io) and provides a ceph-block storage class for creating block devices:\n$ oc get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate false 4d14h Let’s take a look a the details of the storage class:\n$ oc get sc -o yaml ceph-block apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-block parameters: clusterID: rook-ceph csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph csi.storage.k8s.io/fstype: ext4 (1) csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph imageFeatures: layering imageFormat: \u0026#34;2\u0026#34; pool: blockpool provisioner: rook-ceph.rbd.csi.ceph.com reclaimPolicy: Delete volumeBindingMode: Immediate 1 So whenever we create a PVC using this storage class the Ceph provisioner will also create an EXT4 file system on the block device. To test block device handling we create the following persistent volume claim (PVC):\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: block-claim spec: accessModes: - ReadWriteOnce (1) resources: requests: storage: 1Gi storageClassName: ceph-block 1 The access mode is set to ReadWriteOnce (RWO), as block devices oc create -f pvc.yaml $ oc get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE block-claim Bound pvc-bd68be5d-c312-4c31-86a8-63a0c22de844 1Gi RWO ceph-block 91s To test our shiny new block device we are going to use the following three pod definitions:\nblock-pod-a apiVersion: v1 kind: Pod metadata: labels: run: block-pod-a name: block-pod-a spec: containers: - image: registry.redhat.io/ubi8/ubi:8.3 name: block-pod-a command: - sh - -c - \u0026#39;df -h /block \u0026amp;\u0026amp; findmnt /block \u0026amp;\u0026amp; sleep infinity\u0026#39; volumeMounts: - name: blockdevice mountPath: /block volumes: - name: blockdevice persistentVolumeClaim: claimName: block-claim block-pod-b apiVersion: v1 kind: Pod metadata: labels: run: block-pod-b name: block-pod-b spec: affinity: podAntiAffinity: (1) requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: run operator: In values: - block-pod-a topologyKey: kubernetes.io/hostname containers: - image: registry.redhat.io/ubi8/ubi:8.3 name: block-pod-b command: - sh - -c - \u0026#39;df -h /block \u0026amp;\u0026amp; findmnt /block \u0026amp;\u0026amp; sleep infinity\u0026#39; volumeMounts: - name: blockdevice mountPath: /block volumes: - name: blockdevice persistentVolumeClaim: claimName: block-claim 1 We use an AntiAffinity rule for making sure that block-pod-b runs on a different node than block-pod-a. block-pod-c apiVersion: v1 kind: Pod metadata: labels: run: block-pod-c name: block-pod-c spec: affinity: podAffinity: (1) preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: run operator: In values: - block-pod-a topologyKey: kubernetes.io/hostname containers: - image: registry.redhat.io/ubi8/ubi:8.3 name: block-pod-c command: - sh - -c - \u0026#39;df -h /block \u0026amp;\u0026amp; findmnt /block \u0026amp;\u0026amp; sleep infinity\u0026#39; volumeMounts: - name: blockdevice mountPath: /block volumes: - name: blockdevice persistentVolumeClaim: claimName: block-claim 1 We use an Affinity rule for making sure that block-pod-c runs on the same node as block-pod-a. In our first test we want to make sure that both pods are running on separate cluster nodes. So we create block-pod-a and block-pod-b:\n$ oc create -f block-pod-a.yml $ oc create -f block-pod-b.yml After a few seconds we can check the state of our pods:\n$ oc get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES block-pod-a 1/1 Running 0 46s 10.130.6.4 infra02.lan.stderr.at \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; block-pod-b 0/1 ContainerCreating 0 16s \u0026lt;none\u0026gt; infra01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Hm, block-pod-b is in the state ContainerCreating, let’s check the events. Also note that it is running on another node (infra01) then block-pod-a (infra02).\n10s Warning FailedAttachVolume pod/block-pod-b Multi-Attach error for volume \u0026#34;pvc-bd68be5d-c312-4c31-86a8-63a0c22de844\u0026#34; Volume is already used by pod(s) block-pod-a Ah, so because of our block device with RWO access mode and block-pod-b running on separate cluster node, OpenShift or K8s can’t attach the volume to our block-pod-b.\nBut let’s try another test and let’s create a third pod block-pod-c that should run on the same node as block-pod-a:\n$ oc create -f block-pod-c.yml Now let’s check the status of block-pod-c:\n$ oc get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES block-pod-a 1/1 Running 0 6m49s 10.130.6.4 infra02.lan.stderr.at \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; block-pod-b 0/1 ContainerCreating 0 6m19s \u0026lt;none\u0026gt; infra01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; block-pod-c 1/1 Running 0 14s 10.130.6.5 infra02.lan.stderr.at \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Oh, block-pod-c is running on node infra02 and mounted the RWO volume. Let’s check the events for block-pod-c:\n3m6s Normal Scheduled pod/block-pod-c Successfully assigned blockdevices/block-pod-c to infra02.lan.stderr.at 2m54s Normal AddedInterface pod/block-pod-c Add eth0 [10.130.6.5/23] 2m54s Normal Pulled pod/block-pod-c Container image \u0026#34;registry.redhat.io/ubi8/ubi:8.3\u0026#34; already present on machine 2m54s Normal Created pod/block-pod-c Created container block-pod-c 2m54s Normal Started pod/block-pod-c Started container block-pod-c When we compare this with the events for block-pod-a:\n9m41s Normal Scheduled pod/block-pod-a Successfully assigned blockdevices/block-pod-a to infra02.lan.stderr.at 9m41s Normal SuccessfulAttachVolume pod/block-pod-a AttachVolume.Attach succeeded for volume \u0026#34;pvc-bd68be5d-c312-4c31-86a8-63a0c22de844\u0026#34; 9m34s Normal AddedInterface pod/block-pod-a Add eth0 [10.130.6.4/23] 9m34s Normal Pulled pod/block-pod-a Container image \u0026#34;registry.access.redhat.com/ubi8/ubi:8.3\u0026#34; already present on machine 9m34s Normal Created pod/block-pod-a Created container block-pod-a 9m34s Normal Started pod/block-pod-a Started container block-pod-a So the AttachVolume.Attach message is missing in the events for block-pod-c. Because the volume is already attached to the node, interesting.\nEven with RWO block device volumes it is possible to use the same volume in multiple pods if the pods a running on the same node. I was not aware of this possibility and always had the believe with an RWO block device only one pod can access the volume. That’s the problem with believing :-)\nThanks or reading this far.\n"},{"uri":"https://blog.stderr.at/tags/kubernetes/","title":"Kubernetes","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2021-01-27-writingoperatoransible/","title":"Writing Operator using Ansible","tags":["Operator","OpenShift","OCP","Kubernetes"],"description":"Example of an Kubernetes Operator based on Ansible","content":" This quick post shall explain, without any fancy details, how to write an Operator based on Ansible. It is assumed that you know what purpose an Operator has.\nAs a short summary: Operators are a way to create custom controllers in OpenShift or Kubernetes. It watches for custom resource objects and creates the application based on the parameters in such custom resource object. Often written in Go, the SDK supports Ansible, Helm and (new) Java as well.\nIn this example we will install Gogs, a painless self-hosted Git services.\nAs general prerequisites we have:\nInstalled OpenShift 4.6+ cluster (could be Minicube)\nPossibility to execute Ansible scripts and oc/kubectl commands\nCommands: make, docker (or podman)\nInstall Operator SDK As explained at https://sdk.operatorframework.io/docs/installation/ the following prerequisites must be met prior installing the SDK at least:\nDocker v17.03+ or podman v1.9.3+ or buildah v1.7+\nOpenShift CLU v4.6+\nKubernetes/OpenShift cluster\nAccess to container registry, for example quay.io\nOptional: Go v1.13+ (for Operators based on Golang)\nAnsible v2.9.0+\nFollowing the instructions of the SDK documentation, the operator-sdk command will be installed.\nCreating the Operator To begin we create a new folder for the Operator and initialize the Operator project.\nmkdir gogs-operator cd gogs-operator operator-sdk init --plugins=ansible --domain=example.com.at operator-sdk create api --group gogs --version=v1alpha1 --kind Gogs --generate-playbook This will create a new project structure with the following parameters:\n--plugin: Type of Operator (Ansible or Helm)\n--domain: Defines the api endpoint together with group and version.\n--group: Usually short product name\n--version: Defines version of API endpoint\nThe folder structure which will be created automatically looks as follows:\n. |-- Dockerfile |-- Makefile |-- PROJECT |-- config | |-- crd | | |-- bases | | | |-- gogs.example.com.at_gogs.yaml | | |-- kustomization.yaml | |-- default | | |-- kustomization.yaml | | |-- manager_auth_proxy_patch.yaml | |-- manager | | |-- kustomization.yaml | | |-- manager.yaml | |-- prometheus | | |-- kustomization.yaml | | |-- monitor.yaml | |-- rbac | | |-- auth_proxy_client_clusterrole.yaml | | |-- auth_proxy_role.yaml | | |-- auth_proxy_role_binding.yaml | | |-- auth_proxy_service.yaml | | |-- gogs_editor_role.yaml | | |-- gogs_viewer_role.yaml | | |-- kustomization.yaml | | |-- leader_election_role.yaml | | |-- leader_election_role_binding.yaml | | |-- role.yaml | | |-- role_binding.yaml | |-- samples | | |-- gogs_v1alpha1_gogs.yaml | | |-- kustomization.yaml | |-- scorecard | | |-- bases | | | |-- config.yaml | | |-- kustomization.yaml | | |-- patches | | |-- basic.config.yaml | | |-- olm.config.yaml | |-- testing | |-- debug_logs_patch.yaml | |-- kustomization.yaml | |-- manager_image.yaml | |-- pull_policy | |-- Always.yaml | |-- IfNotPresent.yaml | |-- Never.yaml |-- molecule | |-- default | | |-- converge.yml | | |-- create.yml | | |-- destroy.yml | | |-- kustomize.yml | | |-- molecule.yml | | |-- prepare.yml | | |-- tasks | | | |-- gogs_test.yml | | |-- verify.yml | |-- kind | |-- converge.yml | |-- create.yml | |-- destroy.yml | |-- molecule.yml |-- playbooks | |-- gogs.yml |-- requirements.yml |-- roles |-- watches.yaml The watches.yaml file maps Custom Resources (identified by Group, Version, and Kind [GVK]) to Ansible Roles and Playbooks. It tells the Operator where to find the actual Ansible playbook.\n--- # Use the \u0026#39;create api\u0026#39; subcommand to add watches to this file. - version: v1alpha1 group: gogs.example.com.at kind: Gogs playbook: playbooks/gogs.yml # +kubebuilder:scaffold:watch Other files, especially inside playbooks and roles are created as placeholders. These files (or folders) are waiting for you to add the Ansible logic.\nDefining Roles and Playbook With the folder structure above, a playbook and different roles can be created in order to tell the Operator what it needs to do.\nSince the Operator will constantly watch for changes, all tasks must be idempotent In our example we will try to install Gogs, a Git service. It contains a Postgres database system and a webservice. To use some example roles and not fully start from scratch let’s clone the following repository and copy the folders to our Operator.\ncd .. https://github.com/tjungbauer/ansible-operator-roles cd gogs-operator # Remove placeholder rm -Rf roles/ # Copy Postgres deployment role cp -R ../ansible-operator-roles/roles/postgresql-ocp ./roles # Copy Gogs Deplyoment role cp -R ../ansible-operator-roles/roles/gogs-ocp ./roles When we examine the folder, we see 2 typical Ansible roles. The simple purpose is, to create all required OpenShift objects, like Deployment, Route, Service and so on, fully automated by the Operator.\n|-- playbooks | |-- gogs.yaml |-- roles |-- gogs-ocp | |-- README.adoc | |-- defaults | | |-- main.yml | |-- meta | | |-- main.yml | |-- tasks | | |-- main.yml | |-- templates | |-- config_map.j2 | |-- deployment.j2 | |-- persistent_volume_claim.j2 | |-- route.j2 | |-- service.j2 | |-- service_account.j2 |-- postgresql-ocp |-- README.adoc |-- defaults | |-- main.yml |-- meta | |-- main.yml |-- tasks | |-- main.yml |-- templates |-- deployment.j2 |-- persistent_volume_claim.j2 |-- secret.j2 |-- service.j2 Copy (or create) the following playbook under playbooks/gogs.yaml. As you can see there are 2 tasks: the first one will create the postgres application, the seconds one the Gogs service.\n--- # Persistent Gogs deployment playbook. # # The Playbook expects the following variables to be set in the CR: # (Note that Camel case gets converted by the ansible-operator to Snake case) # - PostgresqlVolumeSize # - GogsVolumeSize # - GogsSSL # The following variables come from the ansible-operator # - ansible_operator_meta.namespace # - ansible_operator_meta.name (from the name of the CR) - hosts: localhost gather_facts: no tasks: - name: Set up PostgreSQL include_role: name: ../roles/postgresql-ocp (1) vars: (2) _postgresql_namespace: \u0026#34;{{ ansible_operator_meta.namespace }}\u0026#34; _postgresql_name: \u0026#34;postgresql-gogs-{{ ansible_operator_meta.name }}\u0026#34; _postgresql_database_name: \u0026#34;gogsdb\u0026#34; _postgresql_user: \u0026#34;gogsuser\u0026#34; _postgresql_password: \u0026#34;gogspassword\u0026#34; _postgresql_volume_size: \u0026#34;{{ postgresql_volume_size|d(\u0026#39;4Gi\u0026#39;) }}\u0026#34; _postgresql_image: \u0026#34;{{ postgresql_image|d(\u0026#39;registry.redhat.io/rhscl/postgresql-10-rhel7\u0026#39;) }}\u0026#34; _postgresql_image_tag: \u0026#34;{{ postgresql_image_tag|d(\u0026#39;latest\u0026#39;) }}\u0026#34; _postgresql_size: 1 - name: Set Gogs Service name to default value set_fact: gogs_service_name: \u0026#34;gogs-{{ ansible_operator_meta.name }}\u0026#34; when: gogs_service_name is not defined - name: Set up Gogs include_role: name: ../roles/gogs-ocp (3) vars: (4) _gogs_namespace: \u0026#34;{{ ansible_operator_meta.namespace }}\u0026#34; _gogs_name: \u0026#34;{{ gogs_service_name }}\u0026#34; _gogs_ssl: \u0026#34;{{ gogs_ssl|d(False)|bool }}\u0026#34; _gogs_route: \u0026#34;{{ gogs_route | d(\u0026#39;\u0026#39;) }}\u0026#34; _gogs_image_tag: \u0026#34;{{ gogs_image_tag | d(\u0026#39;latest\u0026#39;) }}\u0026#34; _gogs_volume_size: \u0026#34;{{ gogs_volume_size|d(\u0026#39;4Gi\u0026#39;) }}\u0026#34; _gogs_postgresql_service_name: \u0026#34;postgresql-gogs-{{ ansible_operator_meta.name }}\u0026#34; _gogs_postgresql_database_name: gogsdb _gogs_postgresql_user: gogsuser _gogs_postgresql_password: gogspassword _gogs_size: 1 1 Path to Postgres Role 2 Parameters for Postgres service 3 Path to Gogs Role 4 Parameters for Gogs service Operator Permissions The Operator will require correct permissions in order to create objects like Routes or Services in OpenShift. The SDK automatically created a default role.yaml which can be modified. Open the file config/rbac/role.yaml and add permissions for:\nfor apiGroups \u0026#34;\u0026#34;\nservices\nroutes\nperistentvlumeclaims\nserviceaccounts\nconfigmaps\nfor apiGroups: route.operanshift.io the resource routes\nAt the end, the role.yaml should look like this:\n--- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: manager-role rules: ## ## Base operator rules ## - apiGroups: - \u0026#34;\u0026#34; resources: - secrets - pods - pods/exec - pods/log - services - routes - configmaps - persistentvolumeclaims - serviceaccounts verbs: - create - delete - get - list - patch - update - watch - apiGroups: - apps resources: - deployments - daemonsets - replicasets - statefulsets verbs: - create - delete - get - list - patch - update - watch ## ## Rules for gogs.example.com.at/v1alpha1, Kind: Gogs ## - apiGroups: - gogs.example.com.at resources: - gogs - gogs/status - gogs/finalizers verbs: - create - delete - get - list - patch - update - watch - apiGroups: - route.openshift.io resources: - routes verbs: - create - update - delete - get - list - watch - patch - apiGroups: - route.openshift.io resources: - routes verbs: - create - update - delete - get - list - watch - patch Building and Deploy the Operator Now it is time to build the Operator and push it to a repository. In this example a repository was created at quay.io and is called gogs-operator. The SDK will automatically create a Makefile during the initialization, which we will use now.\nThe Makefile is prepared for docker. If you use podman some modifications must be done first. Run the command sed -i \u0026#39;s/docker/podman/g\u0026#39; Makefile to replace all docker commands inside the Makefile. The next commands will build, push, install and deploy the Operator. Before we start we must be logged in to you Registry of choice (i.e. docker login …​) as well as into our OpenShift cluster. Moreover, it is required that the IMG environment variable is exported with the correct value.\nBuild the Operator and push into the registry\n# export IMG, be sure that the correct tag is used export IMG=quay.io/tjungbau/gogs-operator:v1.0.0 # Build and push into registry make podman-build podman-push podman build . -t quay.io/tjungbau/gogs-operator:v1.0.0 STEP 1: FROM quay.io/operator-framework/ansible-operator:v1.3.0 STEP 2: COPY requirements.yml ${HOME}/requirements.yml --\u0026gt; Using cache 4f84e7064b066c2cac5179b56490a0ef85591170c501ec8a480b617d6e91cff3 STEP 3: RUN ansible-galaxy collection install -r ${HOME}/requirements.yml \u0026amp;\u0026amp; chmod -R ug+rwx ${HOME}/.ansible --\u0026gt; Using cache 2a3a5d44451a45a4c38e1c314e8887c6c45f2551cbef87ef0d1ce518c1969c0d STEP 4: COPY watches.yaml ${HOME}/watches.yaml --\u0026gt; Using cache 642f8361a7b358b89d2e4e5211c1c7a1e22488c53bba0bf1ba2ba275fd56ee69 STEP 5: COPY roles/ ${HOME}/roles/ --\u0026gt; Using cache 93c1af8782bad84d8b81d2d2294c405caab70e2d01c232440f7eb8e5001746c1 STEP 6: COPY playbooks/ ${HOME}/playbooks/ --\u0026gt; Using cache 1cdeee1456ac67d70d4233b0f9ed8052465aaa2cded6bd8ae962dfcc848e5b92 STEP 7: COMMIT quay.io/tjungbau/gogs-operator:v1.0.0 --\u0026gt; 1cdeee1456a 1cdeee1456ac67d70d4233b0f9ed8052465aaa2cded6bd8ae962dfcc848e5b92 podman push quay.io/tjungbau/gogs-operator:v1.0.0 Getting image source signatures Copying blob d5ca8c3b3d34 skipped: already exists Copying blob 4b036ae478b7 skipped: already exists Copying blob 5cfcd0621ffc skipped: already exists Copying blob c6f3d1432bd0 skipped: already exists Copying blob 92538e92de29 skipped: already exists Copying blob eb7bf34352ca skipped: already exists Copying blob 80c43a11288f done Copying blob 803eb2035c9a done Copying blob 40d943ae1834 done Copying blob f4d9024614ee done Copying blob 5143a36c6002 done Copying blob 5050e1080446 skipped: already exists Copying config 1cdeee1456 done Writing manifest to image destination Copying config 1cdeee1456 [--------------------------------------] 0.0b / 6.2KiB Writing manifest to image destination Writing manifest to image destination Storing signatures Install the CRD into OpenShift\n# Install the custom resource definition make install /root/projects/gogs-operator/bin/kustomize build config/crd | kubectl apply -f - customresourcedefinition.apiextensions.k8s.io/gogs.gogs.example.com.at created Deploy the Operator and all required objects into OpenShift\n# Deploy the Operator into OpenShift make deploy cd config/manager \u0026amp;\u0026amp; /root/projects/gogs-operator/bin/kustomize edit set image controller=quay.io/tjungbau/gogs-operator:v1.0.0 /root/projects/gogs-operator/bin/kustomize build config/default | kubectl apply -f - namespace/gogs-operator-system created customresourcedefinition.apiextensions.k8s.io/gogs.gogs.example.com.at unchanged role.rbac.authorization.k8s.io/gogs-operator-leader-election-role created clusterrole.rbac.authorization.k8s.io/gogs-operator-manager-role created clusterrole.rbac.authorization.k8s.io/gogs-operator-metrics-reader created clusterrole.rbac.authorization.k8s.io/gogs-operator-proxy-role created rolebinding.rbac.authorization.k8s.io/gogs-operator-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-proxy-rolebinding created service/gogs-operator-controller-manager-metrics-service created deployment.apps/gogs-operator-controller-manager created This will create a new project in OpenShift called gogs-operator-system. Here, the Operator is running and waiting that somebody creates a CRD of the kind Gogs. Once this happens the Operator will execute the playbooks and therefore create a Postgres and a Gogs pod.\n# Operator Namespace oc get pods -n gogs-operator-system NAME READY STATUS RESTARTS AGE gogs-operator-controller-manager-6747bb6c6-s8794 2/2 Running 0 6m8s Using the Operator Now we need to create a CRD of the kind Gogs. This will happen in a new project, where the Gogs service shall be hosted.\nCreate a new OpenShift project\noc new-project gogs Verify the sample resource\ncat config/samples/gogs_v1alpha1_gogs.yaml apiVersion: gogs.example.com.at/v1alpha1 kind: Gogs metadata: name: gogs-sample spec: foo: bar Apply the sample resource\noc apply -f config/samples/gogs_v1alpha1_gogs.yaml -n gogs This will create two services:\npostgresql\nGogs\nThe Operator will be responsible to roll out all required objects. This includes the Deployments for the container, the Openshift service and the route.\noc get all -n gogs NAME READY STATUS RESTARTS AGE pod/gogs-gogs-sample-57778fd76-ghg8j 1/1 Running 0 74s pod/postgresql-gogs-gogs-sample-bbc49b794-mnltb 1/1 Running 0 115s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/gogs-gogs-sample ClusterIP 172.30.47.31 \u0026lt;none\u0026gt; 3000/TCP 80s service/postgresql-gogs-gogs-sample ClusterIP 172.30.47.158 \u0026lt;none\u0026gt; 5432/TCP 117s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/gogs-gogs-sample 1/1 1 1 74s deployment.apps/postgresql-gogs-gogs-sample 1/1 1 1 115s NAME DESIRED CURRENT READY AGE replicaset.apps/gogs-gogs-sample-57778fd76 1 1 1 74s replicaset.apps/postgresql-gogs-gogs-sample-bbc49b794 1 1 1 115s NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/gogs-gogs-sample gogs-gogs-sample-gogs.apps.ocp.ispworld.at gogs-gogs-sample \u0026lt;all\u0026gt; None At the end, all Pods are alive, ready and are fully controlled by the Operator. We can access the Gogs web interface via the route and start using our own Git service.\nUpdating Operator While the Operator is running fine now, at some point you might want to do some changes. For example, let’s run the Gog service with a replica of 3.\nPerform the following actions:\nSet the variable _gogs_size to 3 in playbooks/gogs.yml\nBuild and push the new version\nexport IMG=quay.io/tjungbau/gogs-operator:v1.0.8 make podman-build podman-push podman build . -t quay.io/tjungbau/gogs-operator:v1.0.8 STEP 1: FROM quay.io/operator-framework/ansible-operator:v1.3.0 STEP 2: COPY requirements.yml ${HOME}/requirements.yml --\u0026gt; Using cache 4f84e7064b066c2cac5179b56490a0ef85591170c501ec8a480b617d6e91cff3 STEP 3: RUN ansible-galaxy collection install -r ${HOME}/requirements.yml \u0026amp;\u0026amp; chmod -R ug+rwx ${HOME}/.ansible --\u0026gt; Using cache 2a3a5d44451a45a4c38e1c314e8887c6c45f2551cbef87ef0d1ce518c1969c0d STEP 4: COPY watches.yaml ${HOME}/watches.yaml --\u0026gt; Using cache 642f8361a7b358b89d2e4e5211c1c7a1e22488c53bba0bf1ba2ba275fd56ee69 STEP 5: COPY roles/ ${HOME}/roles/ --\u0026gt; Using cache 55785493e215d933ef7a93fe000afa6fbb088d87eeffcdddeea4e7fd1896f5b5 STEP 6: COPY playbooks/ ${HOME}/playbooks/ STEP 7: COMMIT quay.io/tjungbau/gogs-operator:v1.0.8 --\u0026gt; bb9d6a995d0 bb9d6a995d059eab7758f9ac17d3ce12f8759518e231f77d32a4b820e4b14396 podman push quay.io/tjungbau/gogs-operator:v1.0.8 Getting image source signatures Copying blob 5cfcd0621ffc skipped: already exists Copying blob d5ca8c3b3d34 skipped: already exists Copying blob eb7bf34352ca skipped: already exists Copying blob 4b036ae478b7 skipped: already exists Copying blob c6f3d1432bd0 skipped: already exists Copying blob 92538e92de29 skipped: already exists Copying blob 41e53e538a36 done Copying blob 5050e1080446 skipped: already exists Copying blob 40d943ae1834 skipped: already exists Copying blob 803eb2035c9a skipped: already exists Copying blob 80c43a11288f skipped: already exists Copying blob ee0361a14e3b skipped: already exists Copying config bb9d6a995d done Writing manifest to image destination Copying config bb9d6a995d [--------------------------------------] 0.0b / 6.2KiB Writing manifest to image destination Writing manifest to image destination Storing signatures Deploy the new version\nmake deploy cd config/manager \u0026amp;\u0026amp; /root/projects/gogs-operator/bin/kustomize edit set image controller=quay.io/tjungbau/gogs-operator:v1.0.8 /root/projects/gogs-operator/bin/kustomize build config/default | kubectl apply -f - namespace/gogs-operator-system unchanged customresourcedefinition.apiextensions.k8s.io/gogs.gogs.example.com.at unchanged role.rbac.authorization.k8s.io/gogs-operator-leader-election-role unchanged clusterrole.rbac.authorization.k8s.io/gogs-operator-manager-role unchanged clusterrole.rbac.authorization.k8s.io/gogs-operator-metrics-reader unchanged clusterrole.rbac.authorization.k8s.io/gogs-operator-proxy-role unchanged rolebinding.rbac.authorization.k8s.io/gogs-operator-leader-election-rolebinding unchanged clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-manager-rolebinding unchanged clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-proxy-rolebinding unchanged service/gogs-operator-controller-manager-metrics-service unchanged deployment.apps/gogs-operator-controller-manager configured The Operator will restart with a new version. After a while the changes will take affect and 3 Gogs pods will run.\noc get pods -n gogs NAME READY STATUS RESTARTS AGE gogs-gogs-sample-57778fd76-4m98m 1/1 Running 0 12m gogs-gogs-sample-57778fd76-5hrdn 1/1 Running 0 6m23s gogs-gogs-sample-57778fd76-xgh2f 1/1 Running 0 6m24s postgresql-gogs-gogs-sample-bbc49b794-z84wt 1/1 Running 0 13m What Else? - References Above example is a very quick overview about what can be done. There are many other options. You can create Operators using Go or Helm.\nThe best starting points are the following websites:\nCertified Operator Build Guide\nOperator SDK Documentation\n"},{"uri":"https://blog.stderr.at/tags/grafana/","title":"Grafana","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/thanos/","title":"Thanos","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2020-12-10-thanos-vs-thanos/","title":"Thanos Querier vs Thanos Querier","tags":["Grafana","Thanos","OpenShift","OCP"],"description":"How to leverage different Thanos Querier services.","content":" OpenShift comes per default with a static Grafana dashboard, which will present cluster metrics to cluster administrators. It is not possible to customize this Grafana instance.\nHowever, many customers would like to create their own dashboards, their own monitoring and their own alerting while leveraging the possibilities of OpenShift at the same time and without installing a completely separated monitoring stack.\nSo how can you create your own queries? How can you visualize them on custom dashboards, without the need to install Prometheus or Alertmanager a second time?\nThe solution is simple: Since OpenShift 4.5 (as TechPreview) and since OpenShift 4.6 (as GA) the default monitoring stack of OpenShift has been extended to support monitoring of user-defined projects. This additional configuration will help to observe your own projects.\nIn this article we will see how to deploy the Grafana operator and what the possible issues can occur, when simply connecting Grafana to the OpenShift monitoring.\nOverview As a developer in OpenShift, you can create an application which provides your custom statistics of your application at the endpoint /metrics. Here the example from the official OpenShift documentation:\n# HELP http_requests_total Count of all HTTP requests # TYPE http_requests_total counter http_requests_total{code=\u0026#34;200\u0026#34;,method=\u0026#34;get\u0026#34;} 4 http_requests_total{code=\u0026#34;404\u0026#34;,method=\u0026#34;get\u0026#34;} 2 # HELP version Version information about this binary # TYPE version gauge version{version=\u0026#34;v0.1.0\u0026#34;} 1 This metric can then be viewed inside OpenShift in the developer view under the menu Monitoring. If you go to \u0026#34;Monitoring \u0026gt; Metrics\u0026#34; and select \u0026#34;Custom Query\u0026#34; from the drop down, you can enter, for example, the following PromQL query:\nsum(rate(http_requests_total[2m])) The following graph will be the result:\nFigure 1. Custom Query This is great! But …​ what happens if a customer would like to see his very own super fancy Grafana dashboard? You cannot change the cluster dashboard. However, you can install your own Grafana instance and one way to do so is using the Custom Grafana Operator.\nBefore we begin Before we start the following should be prepared already:\nOpenShift 4.5+\nEnabled user-define workload monitoring\nA project with user-defined workload monitoring. This is explained in the official documentation at https://docs.openshift.com/container-platform/4.6/monitoring/enabling-monitoring-for-user-defined-projects.html.\nDuring this blog, we will use the namespace ns1 with a custom metric\nDeploy Custom Grafana Operator As for any community operator the following must be considered:\nCommunity Operators are operators which have not been vetted or verified by Red Hat. Community Operators should be used with caution because their stability is unknown. Red Hat provides no support for Community Operators. The community Grafana operator must be deployed to its own namespace, for example grafana. Create this namespace first (oc new-project grafana) and search and intall the Grafana Operator from the OperatorHub. You can use the default values, just be sure to select the wanted namespace.\nAfter a few minutes, the operator should be available:\nFigure 2. Installed Community Grafana Operator Setup Grafana Operator Before we can use Grafana to draw beautiful images it must be configured. We need to create an instance of Grafana. Ideally, OpenShift OAuth is already leveraged, to avoid the need to creating user account manually, inside Grafana.\nOAuth requires some objects, which must be created before the actual Grafana instance. The following YAMLs are taken from the operator documentation. Create the following inside the Grafana namespace:\nSession secret for the proxy …​ change the password!!\na cluster role grafana-proxy\na cluster role binding for the role\na config map injecting trusted CA bundles\napiVersion: v1 data: session_secret: Y2hhbmdlIG1lCg== kind: Secret metadata: name: grafana-k8s-proxy type: Opaque --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: grafana-proxy rules: - apiGroups: - authentication.k8s.io resources: - tokenreviews verbs: - create - apiGroups: - authorization.k8s.io resources: - subjectaccessreviews verbs: - create --- apiVersion: authorization.openshift.io/v1 kind: ClusterRoleBinding metadata: name: grafana-proxy roleRef: name: grafana-proxy subjects: - kind: ServiceAccount name: grafana-serviceaccount namespace: grafana userNames: - system:serviceaccount:grafana:grafana-serviceaccount --- apiVersion: v1 kind: ConfigMap metadata: labels: config.openshift.io/inject-trusted-cabundle: \u0026#34;true\u0026#34; name: ocp-injected-certs Now you can create the following instance under: \u0026#34;Installed Operators \u0026gt; Grafana Operator \u0026gt; Grafana \u0026gt; Create Grafana \u0026gt; YAML View\u0026#34; (or, as an alternative, via the CLI)\napiVersion: integreatly.org/v1alpha1 kind: Grafana metadata: name: grafana-oauth namespace: grafana spec: config: (1) auth: disable_login_form: false disable_signout_menu: true auth.anonymous: enabled: false auth.basic: enabled: true log: level: warn mode: console security: (2) admin_password: secret admin_user: root secrets: - grafana-k8s-tls - grafana-k8s-proxy client: preferService: true dataStorage: (3) accessModes: - ReadWriteOnce class: managed-nfs-storage size: 10Gi containers: (4) - args: - \u0026#39;-provider=openshift\u0026#39; - \u0026#39;-pass-basic-auth=false\u0026#39; - \u0026#39;-https-address=:9091\u0026#39; - \u0026#39;-http-address=\u0026#39; - \u0026#39;-email-domain=*\u0026#39; - \u0026#39;-upstream=http://localhost:3000\u0026#39; - \u0026#39;-tls-cert=/etc/tls/private/tls.crt\u0026#39; - \u0026#39;-tls-key=/etc/tls/private/tls.key\u0026#39; - \u0026gt;- -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token - \u0026#39;-cookie-secret-file=/etc/proxy/secrets/session_secret\u0026#39; - \u0026#39;-openshift-service-account=grafana-serviceaccount\u0026#39; - \u0026#39;-openshift-ca=/etc/pki/tls/cert.pem\u0026#39; - \u0026#39;-openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\u0026#39; - \u0026#39;-openshift-ca=/etc/grafana-configmaps/ocp-injected-certs/ca-bundle.crt\u0026#39; - \u0026#39;-skip-auth-regex=^/metrics\u0026#39; - \u0026gt;- -openshift-sar={\u0026#34;namespace\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;verb\u0026#34;: \u0026#34;get\u0026#34;} image: \u0026#39;quay.io/openshift/origin-oauth-proxy:4.8\u0026#39; name: grafana-proxy ports: - containerPort: 9091 name: grafana-proxy resources: {} volumeMounts: - mountPath: /etc/tls/private name: secret-grafana-k8s-tls readOnly: false - mountPath: /etc/proxy/secrets name: secret-grafana-k8s-proxy readOnly: false ingress: enabled: true targetPort: grafana-proxy termination: reencrypt service: annotations: service.alpha.openshift.io/serving-cert-secret-name: grafana-k8s-tls ports: - name: grafana-proxy port: 9091 protocol: TCP targetPort: grafana-proxy serviceAccount: annotations: serviceaccounts.openshift.io/oauth-redirectreference.primary: \u0026gt;- {\u0026#34;kind\u0026#34;:\u0026#34;OAuthRedirectReference\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;reference\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;Route\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;grafana-route\u0026#34;}} configMaps: - ocp-injected-certs dashboardLabelSelector: - matchExpressions: - key: app operator: In values: - grafana 1 Some default settings, which can be modified if required 2 A default administrative user 3 A datastore to use a persistent volume. Other options would be to use ephemeral storage, or another database. This might be especially important, if you would like HA for your Grafana. 4 Container arguments, most important the openshift-sar line which is important for the OAuth After a few moments, the operator will pick up the change and creates a Grafana pod.\nAdding a Data Source The next step is to connect your custom Grafana to Prometheus, or actually to the Thanos Querier. To do so, you will need to add a role to the Grafana service account and to create a CRD GrafanaDataSource.\nAt this moment, we will work with the cluster role cluster-monitoring-view. The problem this might bring is discussed later.\nAdd the role to the Grafana serviceaccount\noc adm policy add-cluster-role-to-user cluster-monitoring-view -z grafana-serviceaccount Retrieve the token of the service account\noc serviceaccounts get-token grafana-serviceaccount -n grafana Create the following Grafana Data Source, either via UI or via CLI. Be sure to change \u0026lt;TOKEN\u0026gt; with the token from step #2.\napiVersion: integreatly.org/v1alpha1 kind: GrafanaDataSource metadata: name: prometheus-grafanadatasource namespace: grafana spec: datasources: - access: proxy editable: true isDefault: true jsonData: httpHeaderName1: Authorization timeInterval: 5s tlsSkipVerify: true name: Prometheus secureJsonData: httpHeaderValue1: \u0026gt;- Bearer \u0026lt;TOKEN\u0026gt; (1) type: prometheus url: \u0026#39;https://thanos-querier.openshift-monitoring.svc.cluster.local:9091\u0026#39; (2) name: prometheus-grafanadatasource.yaml 1 enter token from step #2 2 Thanos default querier URL…​. this might cause problems (see below) The operator will now restart the Grafana pod to add the newest changes, which should not take more than a few seconds. Grafana can be used now. Dashboards can be created …​ but lets run some tests with PromQL queries instead.\nLet’s Test Log in to your Grafana using OAuth and a cluster administrator.\nYou could also use a non cluster administrator, if the user is able to GET the services of the Grafana namespace. The reason is the following line in the Grafana CRD: -openshift-sar={\u0026#34;namespace\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;services\u0026#34;,\u0026#34;verb\u0026#34;: \u0026#34;get\u0026#34;} which defines, that OAuth will work for everybody who can get the service. This might be changed according to personal needs, but for this test it is good enough. Then use the credentials for the admin account, which have been defined while creating the Grafana instance.\nYou will be logged in now and since there are no Dashboards, lets go to Explore to enter some custom PromQL queries, for instance our example from above:\nsum(rate(http_requests_total[2m])) Figure 3. First Query This is looking good.\nLet’s give it another try and sort by namespaces.\nsum(rate(http_requests_total[2m])) by (namespace) Figure 4. Second Query - showing internal namespace What is this? I see a namespace which is actually meant for the cluster (openshift-monitoring).\nLet’s try another query using a different metric:\nsum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate) by (namespace) Figure 5. Third Query - shows even more namespaces Ok, so we have access to all namespaces on the cluster.\nWhy do I see all namespaces? What does this mean? Well, it means that we have access to all namespaces of the cluster. We see everything. This makes sense, since we assign the cluster role \u0026#34;cluster-monitoring-view\u0026#34; to the serviceaccount of Grafana. But what if we want to show only objects from a specific namespace? If we want, for example, give the developers the possibility to create their own dashboards, without having view access to the whole cluster.\nThe first test might be to remove the cluster-monitoring-view privileges from the Grafana serviceaccount. This will lead to an error on Grafana itself, since it cannot access the Thanos Querier, which we configured with: https://thanos-querier.openshift-monitoring.svc.cluster.local:9091\nHow does the Openshift WebUI actually work, when you are a developer and would like to search one of the above queries. Let’s try that:\nFigure 6. Query using the OpenShift UI It works! It shows the namespace of the developer and only this namespace. When you inspect the actual network traffic, you will see that OpenShift automatically adds the URL parameter namespace=ns1 to the request URL:\nhttps://your-cluster/api/prometheus-tenancy/api/v1/query?namespace=ns1\u0026amp;query=sum%28node_namespace_pod_container%3Acontainer_cpu_usage_seconds_total%3Asum_rate%29+by+%28namespace%29 This is good information, let’s try this using the Grafana Data Source.\nIt is currently not possible to perform this configuration using the GrafanaDataSource CRD. Instead, it must be done directly at the Grafana Dashboard configuration. There is an open ticket at: https://github.com/integr8ly/grafana-operator/issues/309 Login to Grafana as administrator and switch to \u0026#34;Configuration \u0026gt; Data Source \u0026gt; Prometheus \u0026gt;\u0026#34;. At the very bottom add namespace=ns1 to the Custom query parameters\nFigure 7. Configure Grafana Data Source At this point the Grafana serviceaccount has cluster_monitoring_view privileges. As you can see in the following image, this configuration did not help.\nFigure 8. Query after Data Source has manually been modified Thanos Querier vs. Thanos Querier To summarize, in the OpenShift UI everything works, but when using the Grafana dashboard, we see all namespaces from the cluster. Let’s try to find out how OpenShift does this.\nWhen we check the Thanos services we will see 3 ports:\nports: - name: web protocol: TCP port: 9091 targetPort: web - name: tenancy protocol: TCP port: 9092 targetPort: tenancy - name: tenancy-rules protocol: TCP port: 9093 targetPort: tenancy-rules Currently we configured port 9091, but there is another one, which is called tenancy, maybe this is what we need? Let’s try it:\nChange the CRD GrafanaDataSource to use port 9092 (instead of 9091). This will restart the pod and remove the custom query parameter we configured earlier.\nRemove the cluster-role\noc adm policy remove-cluster-role-from-user cluster-monitoring-view -z grafana-serviceaccount The serviceaccount of Grafana, must be able to view the project we want to show in the dashboards. Therefore, allow the Grafana serviceaccount to view the project ns1:\noc adm policy add-role-to-user view system:serviceaccount:grafana:grafana-serviceaccount -n ns1 Log into Grafana as administrator and manually change the Data Source and add namespace=ns1 to the setting Custom query parameters\nRerun the Query …​ as you see you will now see one namespace only.\nFigure 9. Query with Thanos Querier on port 9092 What happened? So what actually happened here? We have two ports for our Thanos Querier which are important: 9091 and 9092.\nWhen we check the Deployment of the Thanos Querier for these ports we will see:\nFor the port 9091 it looks like the following:\nspec: [...] containers: [...] - resources: [...] ports: - name: web containerPort: 9091 protocol: TCP [...] args: [...] - \u0026#39;-openshift-sar={\u0026#34;resource\u0026#34;: \u0026#34;namespaces\u0026#34;, \u0026#34;verb\u0026#34;: \u0026#34;get\u0026#34;}\u0026#39; There is an OAuth setting which says: you have to have the privilege to GET the objects \u0026#34;namespace\u0026#34;.\nThe only cluster role which has exactly this privilege and which is also mentioned by the official OpenShift documentation is cluster-monitoring-view\n- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-monitoring-view rules: - apiGroups: - \u0026#34;\u0026#34; resources: - namespaces verbs: - get As we have seen above, this will show you all namespaces available on the cluster.\nWhen you check port 9092 there is no such OAuth configuration. This service is actually in front of the container kube-rbac-proxy. It does not require OAuth, but instead the namespace URL parameter.\nDetails can be found at: https://github.com/openshift/enhancements/blob/master/enhancements/monitoring/user-workload-monitoring.md\nIn short the whole setup looks like this:\nFigure 10. Thanos interconnecting containers While port 9091 goes directly to Thanos it will require that you have the cluster-monitoring-view role. Port 9092 does not require this, but instead you MUST send the URL parameter namespace=.\nSummary While both options are valid, some considerations must be done when using the Grafana Operator.\nCurrently the URL parameter can be set in Grafana directly only. The operator will ignore it. The ticket in the project shall address this, but is not yet implemented: https://github.com/integr8ly/grafana-operator/issues/309\nThe URL parameter setting will be gone, when the Grafana pods is restarted, which might lead to a problem.\nWhile the Grafana serviceaccount does not require cluster permissions, it will require permission to view the appropriate namespace\nAll above also means, that you actually would need to create a new DataSource for every project you want to monitor. I was not able to find a way, to send multiple namespaces in the URL parameter.\nIs it useful to leverage the Grafana operator then at all? Probably yes, since Operators are the future and it is actively developed. Nevertheless, it is always possible to deploy Grafana manually.\n"},{"uri":"https://blog.stderr.at/tags/argocd/","title":"ArgoCD","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2020-08-06-argocd/","title":"GitOps - Argo CD","tags":["Gitops","OpenShift","OCP","ArgoCD"],"description":"Using Argo CD to manage OpenShift resources","content":" Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. GitOps itself uses Git pull request to manager infrastructure and application configuration.\nLet’s try to install and use a simple usecase in order to demonstrate the basic possibilities.\nWithout going into the very detail, typical GitOps usecases are:\nApply configurations from Git\nDetect, (auto-)sync and notify configuration drifts\nManage multiple clusters and keep the configuration equal\n…​\nand much more. Further information about the theory behind can be found at https://www.openshift.com/blog/introduction-to-gitops-with-openshift.\nIn short: there is no reason why not to use GitOps and to leverage tools like Argo CD to manage configurations. In this tutorial, the Argo CD operator gets installed and a simple use case is shown to demonstrate the possibilities of this software.\nAs for architectural overview of Argo CD, please read the official documentation: https://argoproj.github.io/argo-cd/operator-manual/architecture/, which explains very well the core components. No need to rewrite it here.\nPrerequisites You need an Openshift 4 cluster. :)\nWatch the video at: https://demo.openshift.com/en/latest/argocd/\nInstall Argo CD operator Before you begin, create a new project:\noc new-project argocd In this project the operator will be deployed.\nLook for the Operatorhub in the OpenShift WebUI and \u0026#34;argocd\u0026#34; and select the \u0026#34;Argo CD Community\u0026#34; operator. Subscribe to this operator. Just be sure that the newly created project is selected. Other settings can stay as default.\nFigure 1. Argo CD: Operator This will install the operator. You can monitor this process by clicking \u0026#34;Installed Operators\u0026#34;. After a while it should switch from \u0026#34;Installing\u0026#34; to \u0026#34;Succeeded\u0026#34;.\nDeploy ArgoCD instance Select the installed operator \u0026#34;Argo CD\u0026#34;, select the tab \u0026#34;ArgoCD\u0026#34; and hit the button \u0026#34;Create ArgoCD\u0026#34;\nEnter the following yaml:\napiVersion: argoproj.io/v1alpha1 kind: ArgoCD metadata: name: argocd namespace: argocd spec: dex: image: quay.io/redhat-cop/dex openShiftOAuth: true version: v2.22.0-openshift rbac: policy: | g, argocdadmins, role:admin scopes: \u0026#39;[groups]\u0026#39; server: route: enabled: true This yaml extends the default example by:\nusing OpenShift authentication\nAllow all users from the group \u0026#34;argocdadmins\u0026#34; admin permissions inside Argo CD\ncreate a route to access argocd web interface\nOnce this configuration is created, the operator will automatically start to roll out the different pods, which are required. No worries, it will take quite long until everything is up and running.\nCreate a new group and assign a user to it In the ArgoCD resource we have defined the group argocdadmins and all users in this group will get administrator privileges in Argo CD. This group must be created and in addition we assign the user admin to it.\nFor example with the following commands:\noc adm groups new argocdadmins oc adm groups add-users argocdadmins admin Login to Argo CD Now it is time to login to Argo CD. Just fetch the route which was created by the operator (for example with: oc get routes -n argocd).\nOn the login page select \u0026#34;Login via OpenShift\u0026#34; and enter the credentials of the user you would like to use. (well, the one which you can admin permissions in the step above).\nFigure 2. Argo CD: Login Screen This will open the Argo CD Interface.\nFirst test with Argo CD Let’s create an application in Argo CD to demonstrate the possibilities about application management with GitOps. We will use a simple application which draws a blue (or green) box in your browser.\nClick on the button \u0026#34;Create App\u0026#34; and enter the following parameters:\nName: bgd\nProject: default (This is the project inside Argo CD, not OpenShift)\nSync Policy: Can stay at manual for now\nRepository URL: https://github.com/tjungbauer/gitops-examples (This is a fork of christianh814/gitops-examples)\nRevision: master\nPath: bgd/\nCluster: https://kubernetes.devault.svc (This is the local default cluster Argo CD created. Other Clusters may be defined)\nNamespace: bgd (This is the OpenShift namespace which will be created)\nAt the end, it should look like this:\nFigure 3. Argo CD: Create an Application Press the \u0026#34;Create\u0026#34; button and your application is ready to be synchronized. Since no synchronization happens yet, Argo CD will complain that the application is out of sync.\nSync application Since we set the Sync Policy to manual, the synchronization process must be started, guess what, manually. Click on the \u0026#34;Sync\u0026#34; button and Argo CD will open a side panel, which shows the resources are out of sync and other options.\nFigure 4. Argo CD: Sync an Application One notable option is the \u0026#34;Prune\u0026#34; setting. By selecting this, changes which have been done directly on OpenShift, are removed and replaced by the ones which are stored at Git.\nThis is a very good option, to force everyone to follow the GitOps process :) Press the \u0026#34;Synchronize\u0026#34; button and select the application. As you see the sync process has started and after a while, all resources are synced to OpenShift.\nFigure 5. Argo CD: Application Syncing Figure 6. Argo CD: Application Synced Verifying objects Now that Argo CD says that the application has been synchronized, we should check the objects, which have been created in OpenShift.\nAs you can see in the Git repository, there are 4 objects which should exist now:\na namespace (bgd)\na deployment\na service\na route\nFigure 7. Argo CD: Git Repo To verify the existence either check via the WebUI or simply try:\noc get all -n bgd NAME READY STATUS RESTARTS AGE pod/bgd-6b9b64d94d-5fqdg 1/1 Running 0 6m2s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/bgd ClusterIP 172.30.233.30 \u0026lt;none\u0026gt; 8080/TCP 6m7s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/bgd 1/1 1 1 6m4s NAME DESIRED CURRENT READY AGE replicaset.apps/bgd-6b9b64d94d 1 1 1 6m3s NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/bgd bgd-bgd.apps.ocp.example.test bgd 8080 None Obviously, the namespace exists and with it also the other objects, which hae been synchronized.\nWhen you now open the route http://bgd-bgd.apps.ocp.example.test in your browser, you will see a nice blue box.\nFigure 8. Argo CD: The Blue Box As you can see all objects have been synchronized and the application has been deployed correctly. The source of truth is in Git and all changes should be done there.\nI want a green box So you want a green box? Maybe you think of doing this:\nModify the Deployment and change the environment COLOR from blue to green:\n... spec: containers: - name: bgd image: \u0026#39;quay.io/redhatworkshops/bgd:latest\u0026#39; env: - name: COLOR value: green # change from blue to green ... This will trigger a re-deployment and …​ fine …​ you have a green box:\nFigure 9. Argo CD: The Green Box But is this the correct way to do that? NO, it is not. Argo CD will immediately complain that the application is out of sync.\nFigure 10. Argo CD: Out of Sync When you sync the application it will end up with a blue box again.\nFigure 11. Argo CD: The Blue Box But you really really want a green box? Fair enough, the correct way would be to change the deployment configuration on Git. Simply change the file bgd/bgd-deployment.yaml and set the COLOR to green:\n... spec: containers: - image: quay.io/redhatworkshops/bgd:latest name: bgd env: - name: COLOR value: \u0026#34;green\u0026#34; resources: {} Again Argo CD will complain that it is out of sync.\nFigure 12. Argo CD: Git Update By synchronizing the changes, it will deploy the latest version found at Git and …​ yes, you have a green box now (When deployment on OpenShift side has finished).\nFigure 13. Argo CD: The Green Box "},{"uri":"https://blog.stderr.at/tags/container-security/","title":"Container Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/05/enable-automatic-route-creation/","title":"Enable Automatic Route Creation","tags":["Istio","Service Mesh","OpenShift","OCP","Route"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 13 - Automatic route creation","content":" Red Hat Service Mesh 1.1 allows you to enable a \u0026#34;Automatic Route Creation\u0026#34; which will take care about the routes for a specific Gateway. Instead of defining * for hosts, a list of domains can be defined. The Istio OpenShift Routing (ior) synchronizes the routes and creates them inside the Istio namespace. If a Gateway is deleted, the routes will also be removed again.\nThis new features makes the manual creation of the route obsolete, as it was explained here: Openshift 4 and Service Mesh 4 - Ingress with custom domain\nEnable Automatic Route Creation Before this feature can be used, it must be enabled. To do so the ServiceMeshContolPlace, typically found in the namespace istio-system must be modified. Add the line ior_enabled: true to the istio-ingressgate configuration.\n... spec: istio: gateways: istio-egressgateway: autoscaleEnabled: false istio-ingressgateway: autoscaleEnabled: false ior_enabled: true ... Verify current service Let’s check our tutorial application, if it is still working.\noc project tutorial export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) curl $GATEWAY_URL/customer customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 30 Let’s review and remove the current used Gateway. As you can see the hosts is set to \u0026#39;*\u0026#39;\noc get istio-io oc get gateway.networking.istio.io/customer-gateway -o yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;networking.istio.io/v1alpha3\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Gateway\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;customer-gateway\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;tutorial\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;selector\u0026#34;:{\u0026#34;istio\u0026#34;:\u0026#34;ingressgateway\u0026#34;},\u0026#34;servers\u0026#34;:[{\u0026#34;hosts\u0026#34;:[\u0026#34;*\u0026#34;],\u0026#34;port\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;http\u0026#34;,\u0026#34;number\u0026#34;:80,\u0026#34;protocol\u0026#34;:\u0026#34;HTTP\u0026#34;}}]}} creationTimestamp: \u0026#34;2020-05-13T07:52:20Z\u0026#34; generation: 1 name: customer-gateway namespace: tutorial resourceVersion: \u0026#34;41370056\u0026#34; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/tutorial/gateways/customer-gateway uid: 96e82ed9-e870-493c-941f-bfa83c892b94 spec: selector: istio: ingressgateway servers: - hosts: - \u0026#39;*\u0026#39; port: name: http number: 80 protocol: HTTP Create a new Gateway First let’s remove the current Gateway\noc delete gateway.networking.istio.io/customer-gateway Now lets create a new Gateway, but this time we define some names for the hosts section:\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; Gateway-ior.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: customer-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - www.example.com - svc.example.com EOF oc apply -f Gateway-ior.yaml -n tutorial When you now check the routes, 2 new routes have been added:\noc get routes -n istio-system NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD ... tutorial-customer-gateway-kmqrl www.example.com istio-ingressgateway http2 None tutorial-customer-gateway-ks7q7 svc.example.com istio-ingressgateway http2 None To test the connectivity, you need to be sure that the hosts, used in the Gateway, are resolvable. If they are then you can access your service:\ncurl www.example.com/customer customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 31 curl svc.example.com/customer customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 32 "},{"uri":"https://blog.stderr.at/tags/istio/","title":"istio","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/quay/","title":"Quay","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/quay/2020-05-13-quay-tutorial1/","title":"Red Hat Quay Registry - Overview and Installation","tags":["Quay","Registry","OpenShift","Container Security"],"description":"Red Hat Quay Registry - Overview and Installation","content":" Red Hat Quay is an enterprise-quality container registry, which is responsible to build, scan, store and deploy containers. The main features of Quay include:\nHigh Availability\nSecurity Scanning (with Clair)\nRegistry mirroring\nDocker v2\nContinuous integration\nand much more.\nQuay can be installed as:\nStandalone single service\nHigh Availibility service\nOn OpenShift with an Operator\nOn OpenShift without an Operator.\nFor the following Quay Tutorial Quay version 3.3 on OpenShift 4.3/4.4 with an Operator is used. Moreover, a local storage is used, to make the deployment easier and independent to a storage provider. In any case, be sure that for a production environment a \u0026#34;real\u0026#34; storage is used.\nFurther details can be found at the official Red Hat documentation at: Deploy Red Hat Quay on OpenShift with an Operator\nQuay Architecture Several components are used to build Quay\nDatabase: used to store metadata (not images)\nRedis: stores live builder logs\nQuay container registry: Runs Quay as a service\nTwo types of storages are supported:\nPublic cloud storage, like Amazon S3, Google Cloud Storage\nPrivate cloud storage, S3 compliant Object Store, Like Ceph RADOS or OpenStack Swift.\nSince I perform the example setup in a limited lab environment, I used local storage. This is not supported for production installations. Installation This example installation, covers the configuration of all credentials, but it ignores the configuration of a storage engine. Instead it is using local storage.\nCreate a new project and install the Quay Operator Note: Before you begin create a new project called \u0026#34;quay\u0026#34;.\nFrom the OpenShift console goto Operators \u0026gt; OperationHub and search for \u0026#34;Red Hat Quay Operator\u0026#34;. Be sure NOT to select the community version. Install the Operator using the following settings:\nInstallation Mode: select the namespace \u0026#34;quay\u0026#34;\nLeave the other settings as default.\nFigure 1. Operator Installation Once you select \u0026#34;Subscribe\u0026#34;, the installation process will take a few minutes. At the end, the operator pods should run inside your namespace and is ready to bse used.\nFigure 2. Operator Running Configure Quay To configure Quay the Custom Resource Definition \u0026#34;QuayEcosystem\u0026#34; must be defined. Below example shows a ready to use example to test the deployment. Several secrets are used in this QuayEcosystem which must be created first. If they are not specified in the QuayEcosystem, then default values would be used. However, it always makes sense to specify passwords :)\nCreate a Pull Secret to pull from Quay.io Check the article Accessing Red Hat Quay to find the appropriate credentials you need to fetch images from quay.io. Find the section \u0026#34;Red Hat Quay v3 on OpenShift\u0026#34;, copy the secret into the file redhat_secret.yaml and create the object in OpenShift:\noc create -f redhat_secret.yaml Create Quay Superuser Credentials The Quay superuser will be able to manage other users or projects. Create the secret by defining username, password and e-mail address:\nThe password must have a minimum length of 8 characters. oc create secret generic quay-credentials \\ --from-literal=superuser-username=\u0026lt;username\u0026gt; \\ --from-literal=superuser-password=\u0026lt;password\u0026gt; \\ --from-literal=superuser-email=\u0026lt;email\u0026gt; Create Quay Configuration Credentials The Quay configuration is done, by a separate pod, with a separate accessible route. To set the password, create the following secret:\nThe password must have a minimum length of 8 characters. oc create secret generic quay-config-app \\ --from-literal=config-app-password=\u0026lt;password\u0026gt; Specify database credentials As next let’s create the credentials for the database:\noc create secret generic \u0026lt;secret_name\u0026gt; \\ --from-literal=database-username=\u0026lt;username\u0026gt; \\ --from-literal=database-password=\u0026lt;password\u0026gt; \\ --from-literal=database-root-password=\u0026lt;root-password\u0026gt; \\ --from-literal=database-name=\u0026lt;database-name\u0026gt; It is also possible to use and existing database. To configure this, create the secret as described and add the server parameter, containing the hostname, to the QuayEcosystem definition+ Setting Redis password By default, the operator would install Redis without any password. To specify a password, create the following secret:\noc create secret generic quay-redis-password \\ --from-literal=password=\u0026lt;password\u0026gt; Create QuayEcosystem Resource With all the secrets created above, it is time to create the QuayEcosystem. Once it is defined, the operator will automatically start all required services.\nThe following is an example, using the different secret names (The names should be self explaining) In addition, the following has been defined:\nvolumeSize = 10GI for the database\nkeepConfigDeployment to false, this will remove the configuration pod after the deployment.\nhostname: to reach the Quay registry under a defined hostname (otherwise a default name would be created)\nClair container scanning is enabled\napiVersion: redhatcop.redhat.io/v1alpha1 kind: QuayEcosystem metadata: name: quayecosystem spec: quay: imagePullSecretName: redhat-quay-pull-secret superuserCredentialsSecretName: quay-credentials configSecretName: quay-config-app deploymentStrategy: Recreate skipSetup: false keepConfigDeployment: false externalAccess: hostname: quay.apps.ocp.ispworld.at database: volumeSize: 10Gi credentialsSecretName: quay-database-credential registryBackends: - name: local local: storagePath: /opt/quayregistry redis: credentialsSecretName: quay-redis-password imagePullSecretName: redhat-quay-pull-secret clair: enabled: true imagePullSecretName: redhat-quay-pull-secret Quay WebUI Once the Quay Operator has deployed all containers, you should see one route (or 2 if you kept Configuration Deployment Container) and can access your Quay installation.\nFigure 3. Quay WebUI Optional: Disable self account creation Many customers want to disable the \u0026#34;Create Account\u0026#34; link on the login page (see Figure #3), to prevent that anybody could create a new account. To remove this option, the configuration pod must run.\nVerify if the Configuration pod is running If the following does not return anything, then the container is not running:\noc get routes -n quay | grep config If this is the case, modify the resource QuayEcosystem to enable the Configuration UI.\nEdit:\noc edit QuayEcosystem/quayecosystem and set \u0026#34;KeepConfigDeployment\u0026#34; to true:\nquay: [...] keepConfigDeployment: true After a few minutes another pod, called \u0026#34;quayecosystem-quay-config\u0026#34; will be started and a new route is created:\noc get routes -n quay | grep config quayecosystem-quay-config quayecosystem-quay-config-quay.apps.ocp.ispworld.at quayecosystem-quay-config 8443 passthrough/Redirect None Configure Account Creation and Anonymous Access Login to the Configuration Web Interface with the credentials you specified during the deployment and scroll down to the section \u0026#34;Access Settings\u0026#34;.\nFigure 4. Quay Configuration There remove the checkbox from:\nAnonymous Access\nUser Creation\nand save and build the configuration.\nThis will trigger a change on the Quay pod. After it has been recreated (this will take a few minutes), the feature to create a new account is removed from the Login page.\nWorking with Quay The following quick steps through Quay are the steps of the Quay tutorial, which can be seen at the Quay WebUI at the \u0026#34;Tutorial\u0026#34; tab.\nLogin via Docker CLI To login via docker CLI simply use:\ndocker login \u0026lt;your selected hostname for quay\u0026gt; Docker expects a valid certificate. Such certificate could be added to the definition of QuayEcosystem. However, I did not create a certificate for this lab. To allow untrusted certificates, on a Mac, simply download the certificates (For Chrome: you can drag and drop the certificate from the browser to your Desktop, for Firefox, you need to open the Options menu and export the certificates.). After that double click both certificates (the root and the site certificate), which will install them on you local Keychain. Open the Keychain on you Mac, find the appropriate certificates and set both to \u0026#34;Always trust\u0026#34; Create an example container The next step to create a new image is to create a container. For this example the busybox base image is used.\ndocker run busybox echo \u0026#34;fun\u0026#34; \u0026gt; newfile This will pull the latest image of busybox and create a container:\nUnable to find image \u0026#39;busybox:latest\u0026#39; locally latest: Pulling from library/busybox d9cbbca60e5f: Pulling fs layer d9cbbca60e5f: Verifying Checksum d9cbbca60e5f: Download complete d9cbbca60e5f: Pull complete Digest: sha256:836945da1f3afe2cfff376d379852bbb82e0237cb2925d53a13f53d6e8a8c48c Status: Downloaded newer image for busybox:latest With \u0026#34;docker ps\u0026#34; the running container is shown. Remember the Container ID for further steps. In this case fc3e9bb1e9da.\ndocker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fc3e9bb1e9da busybox \u0026#34;echo fun\u0026#34; 3 minutes ago Exited (0) 3 minutes ago relaxed_proskuriakova Create the image Once a container has terminated in Docker, the next step is to commit the container to an image. To do so we will use \u0026#34;docker commit\u0026#34; command. As name for the repository I took superapp.\ndocker commit fc3e9bb1e9da quay.apps.ocp.ispworld.at/quay/superapp Push the image to Red Hat Quay The final step is to push the image to our repository, where it will be stored for future use.\ndocker push quay.apps.ocp.ispworld.at/quay/superapp Figure 5. Quay Repository Test Container Security Scanner Clair is used to scan containers about possible security risks. It imports vulnerability data permanently from a known source and creates a list of threats for an image.\nTo test such scanning, we pull the \u0026#34;Universal Base Image RHEL 7\u0026#34; from Red Hat. I am using version 7.6 since this is already quite old and we expect some known vulnerabilities for this image.\nFirst you need to login to Red Hat Registry:\ndocker login registry.redhat.io Username: \u0026lt;Username\u0026gt; Password: \u0026lt;Password\u0026gt; Login Succeeded Then let’s pull the UBI Image 7.6 (instead of the latest)\ndocker pull registry.redhat.io/ubi7/ubi:7.6 Before we can push it to our Quay registry, we need to tag it:\ndocker tag registry.redhat.io/ubi7/ubi:7.6 quay.apps.ocp.ispworld.at/quay/ubi7:7.6 If we now check the local images, we see that there are two UBI images.\ndocker images REPOSITORY TAG IMAGE ID CREATED SIZE quay.apps.ocp.ispworld.at/quay/ubi7 7.6 247ee58855fd 10 months ago 204MB registry.redhat.io/ubi7/ubi 7.6 247ee58855fd 10 months ago 204MB Now it is possible to push the image to the Quay repository.\ndocker push quay.apps.ocp.ispworld.at/quay/ubi7:7.6 Finally the image is available inside our Registry and Clair will queue it for a security scan.\nOnce the scan is finished, possible found issues are shown under the \u0026#34;Repository Tags\u0026#34;.\nFigure 6. Clair Security Scanning When you click on then, you will see a detailed result page, with all vulnerabilities found:\nFigure 7. Clair Security Scanning Result "},{"uri":"https://blog.stderr.at/tags/registry/","title":"Registry","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/route/","title":"Route","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/service-mesh/","title":"Service Mesh","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/service-mesh/","title":"Service Mesh","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/authorization/","title":"Authorization","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/05/authorization-rbac/","title":"Authorization (RBAC)","tags":["Istio","Service Mesh","OpenShift","OCP","Authorization","Security"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 12 - Authorization Role Based Access Control (RBAC)","content":" Per default all requests inside a Service Mesh are allowed, which can be a problem security-wise. To solve this, authorization, which verifies if the user is allowed to perform a certain action, is required. Istio’s authorization provides access control on mesh-level, namespace-level and workload-level.\nWith the resource AuthorizationPolicy granular policies can be defined. These policies are loaded to and verified by the Envoy Proxy which then authorizes a request.\nImplicit enablement To enable authorization the only thing you need is to do is to define the AuthorizationPolicy. If the resource is not defined, then no access control will be used, instead any traffic is allowed. If AuthorizationPolicy is applied to a workload, then by default any traffic is denied unless it is explicitly allowed.\nThis is applicable to Service Mesh version 1.1+ Preparing Environment The following steps will configure an example Role Based Access Control (RBAC). It will start from scratch. If you just want to quickly configure the authorization and have anything else in place, you can start form here: Configure Authentication Policy\nCreate a new project\noc new-project tutorial Be sure that a Service Mesh Member Roll exists for this new project\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; memberroll.yaml apiVersion: maistra.io/v1 kind: ServiceMeshMemberRoll metadata: name: default spec: members: - tutorial EOF oc apply -f memberroll.yaml -n istio-system Clone and install the example application\ngit clone https://github.com/redhat-developer-demos/istio-tutorial/ istio-tutorial oc apply -f istio-tutorial/customer/kubernetes/Deployment.yml -n tutorial oc apply -f istio-tutorial/customer/kubernetes/Service.yml -n tutorial oc expose service customer oc apply -f istio-tutorial/preference/kubernetes/Deployment.yml -n tutorial oc apply -f istio-tutorial/preference/kubernetes/Service.yml -n tutorial oc apply -f istio-tutorial/recommendation/kubernetes/Deployment.yml -n tutorial oc apply -f istio-tutorial/recommendation/kubernetes/Service.yml -n tutorial Wait until all pods are running. There should be 2 containers for all pods:\noc get pods -w Create Gateway and VirtualService\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; Gateway_VirtualService.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: customer-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: customer-gateway spec: hosts: - \u0026#34;*\u0026#34; gateways: - customer-gateway http: - match: - uri: prefix: /customer rewrite: uri: / route: - destination: host: customer port: number: 8080 EOF oc apply -f Gateway_VirtualService.yaml -n tutorial Verify if the application is working You can either use the run.sh from previous tutorials, or simply try the following curl\nexport GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;); echo $GATEWAY_URL curl $GATEWAY_URL/customer This should return the following line:\ncustomer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1 Optionally check the connection from inside the customer container\nget pods name and enter it:\noc get pods NAME READY STATUS RESTARTS AGE customer-6948b8b959-dhsm9 2/2 Running 0 177m preference-v1-7fdb89c86b-dvzs9 2/2 Running 0 177m recommendation-v1-69db8d6c48-cjcpn 2/2 Running 0 177m Connect into the container pod and try to reach the different microservices\noc rsh customer-6948b8b959-dhsm9 sh-4.4$ curl customer:8080 customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 2 sh-4.4$ curl preference:8080 preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3 sh-4.4$ curl recommendation:8080 recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 4 Configure Authentication Policy Enabling User-End authentication\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; authentication-policy.yaml apiVersion: \u0026#34;authentication.istio.io/v1alpha1\u0026#34; kind: \u0026#34;Policy\u0026#34; metadata: name: \u0026#34;customerjwt\u0026#34; spec: targets: - name: customer - name: preference - name: recommendation origins: - jwt: issuer: \u0026#34;testing@secure.istio.io\u0026#34; jwksUri: \u0026#34;https://gist.githubusercontent.com/lordofthejars/7dad589384612d7a6e18398ac0f10065/raw/ea0f8e7b729fb1df25d4dc60bf17dee409aad204/jwks.json\u0026#34; principalBinding: USE_ORIGIN EOF oc apply -f authentication-policy.yaml -n tutorial Access should be denied after a few seconds\ncurl $GATEWAY_URL/customer Origin authentication failed.% Use token to authenticate\ntoken=$(curl https://gist.githubusercontent.com/lordofthejars/a02485d70c99eba70980e0a92b2c97ed/raw/f16b938464b01a2e721567217f672f11dc4ef565/token.simple.jwt -s) curl -H \u0026#34;Authorization: Bearer $token\u0026#34; $GATEWAY_URL/customer This will result in a correct response\ncustomer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 5 Configure Role Based Access Control (RBAC) Create the resource AuthorizationPolicy\nThis is a new resources, supported since Service Mesh 1.1. It will allow GET method when the role equals to \u0026#34;customer\u0026#34;\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; AuthorizationPolicy.yaml apiVersion: \u0026#34;security.istio.io/v1beta1\u0026#34; kind: \u0026#34;AuthorizationPolicy\u0026#34; metadata: name: \u0026#34;customer\u0026#34; spec: rules: - to: - operation: methods: [\u0026#34;GET\u0026#34;] when: - key: request.auth.claims[role] values: [\u0026#34;customer\u0026#34;] EOF oc apply -f AuthorizationPolicy.yaml -n tutorial Get a token for the role and retry to connect to the service,\ntoken=$(curl https://gist.githubusercontent.com/lordofthejars/f590c80b8d83ea1244febb2c73954739/raw/21ec0ba0184726444d99018761cf0cd0ece35971/token.role.jwt -s) curl -H \u0026#34;Authorization: Bearer $token\u0026#34; $GATEWAY_URL/customer This results in:\ncustomer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 8 Let’s verify the setting and change the AuthorizationPolicy. This will break the authorization, since the token provides roles=customer and we set the Policy to \u0026#34;whereistherole\u0026#34;\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; AuthorizationPolicy-Hack.yaml apiVersion: \u0026#34;security.istio.io/v1beta1\u0026#34; kind: \u0026#34;AuthorizationPolicy\u0026#34; metadata: name: \u0026#34;customer\u0026#34; spec: rules: - to: - operation: methods: [\u0026#34;GET\u0026#34;] when: - key: request.auth.claims[role] values: [\u0026#34;whereistherole\u0026#34;] EOF oc replace -f AuthorizationPolicy-Hack.yaml -n tutorial If you now try to access the service, with the token, which provides \u0026#34;customer\u0026#34; as role, it will lead to an error:\ntoken=$(curl https://gist.githubusercontent.com/lordofthejars/f590c80b8d83ea1244febb2c73954739/raw/21ec0ba0184726444d99018761cf0cd0ece35971/token.role.jwt -s) curl -H \u0026#34;Authorization: Bearer $token\u0026#34; $GATEWAY_URL/customer RBAC: access denied "},{"uri":"https://blog.stderr.at/general/2020/05/basic-usage-of-git/","title":"Basic usage of git","tags":["git","github"],"description":"Using git for contributing to projects hosted on github.com","content":" This is a very short and hopefully simple introduction on how to use Git when you would like to contribute to projects hosted on github.com. The same workflow should also work for projects on gitlab.com.\nIntroduction There is this fancy mega application hosted on github called megaapp that you would like to contribute to. It’s perfect but there’s just this little feature missing to make it even more perfect.\nThis is how we would tackle this.\nrocket science ahead Glossary Term Definition fork\nA (personal) copy of a repository you created on github or gitlab.\nupstream\nWhen creating forks of repositories on github or gitlab, the original repository hosting the project\nindex\nThe staging area git uses before you can commit to a repository\nremote repository\nA repository hosted on a server shared by developers\nlocal repository\nA local copy of a repository stored on you machine.\nStep 1: Fork the repository on github.com Login to you Github account and navigate to the project you would like to fork, megaapp in our example.\nClick on the the fork button, as depicted in the image below:\nIf you are a member of several projects on github.com, github is going to ask you into which project you would like to clone this repository.\nAfter selecting the project or your personal account, github is going to clone the repository into the project you selected. For this example I’m going to use my personal github account \u0026#34;tosmi\u0026#34;.\nStep 2: Clone the repository to you workstation Next we are going to clone our fork from Step 1: Fork the repository on github.com to our workstation and start working on the new feature.\nAfter forking the upstream project you are redirect to your personal copy of the project. Click on the \u0026#34;Clone or download\u0026#34; button and select the link. You can choose between SSH and HTTPS protocols for downloading the project. We are going to use SSH.\nCopy the link into a terminal and execute the git clone command:\n$ git clone git@github.com:tosmi/megaapp.git Step 3: Create a feature branch for your new fancy feature Change into the directory of the project you downloaded in Step 2: Clone the repository to you workstation\ncd megaapp Now we create a feature branch with a short name that describes our new feature:\ngit checkout -b tosmi/addoption Because we would like to add a new option to megaapp we call this feature branch addoption.\nWe are also prefixing the feature branch with our github username so that it is clear for the upstream project maintainer(s) who is contributing this.\nHow you name you branches is opinionated, so we would search for upstream project guidelines and if there are none maybe look at some existing pull request how other people are naming there branches. If we find no clue upstream we sticking with \u0026lt;github username\u0026gt;/\u0026lt;branch name\u0026gt;.\nWe can now start adding our mega feature to the project.\nStep 4: Add you changes to the Git index Before we can commit our changes, we have to place the changes made in the so called index or staging area:\n$ git add \u0026lt;path to file you have changed\u0026gt; If we would like to place all of our changes onto the index we could execute\n$ git add -A Step 5: Commit your changes After adding our changes to the Git index we can commit with\n$ git commit This will open our favorite editor and we can type a commit message. The first line should be a short description of our change, probably not longer than 70 to 80 characters. After two newlines we can enter a detailed explanation of your changes.\nThis is an example commit message\nAdded a `version` option to output the current version of megaapp This change introduces a `version` option to megaapp. The purpose is to output the current version of megaapp for users. This might be helpful when users open a bug report so we can see what version is affected. After saving the message and we have successfully created a commit.\nRemember this is now only stored in the local copy of the repository! We still have to push our changes to github. There is also the option to add the commit comment directly on the command line\n$ git commit -m \u0026#39;Added a `version` option to output the current version of megaapp This change introduces a `version` option to megaapp. The purpose is to output the current version of megaapp for users. This might be helpful when users open a bug report so we can see what version is affected.\u0026#39; Step 6: Pushing our local changes to our forked repo on github.com We execute\n$ git push to push our local changes to the forked repository hosted on github.com.\nStep 7: Creating a pull request on github.com We navigate to our personal project page of the forked repository on github. For the fork we are using in this example this is http://github.com/tosmi/megaapp.\nGithub is going to show us a button \u0026#34;Compare \u0026amp; pull request\u0026#34;:\nAfter clicking on that button we are able to review the changes we would like to include in this pull request.\nIf we are happy with our changes we click on \u0026#34;Create pull request\u0026#34;. The upstream owner of the repository will get notified and we can see our open pull request on the upstream project page under \u0026#34;Pull requests\u0026#34;.\nIf there are CI test configured for that project they will start to run and we can see if our pull request is going to pass all test configured.\nRebasing to current upstream if required Sometimes a upstream project maintainer asks you to rebase your work on the current upstream master branch. The following steps explain the basic workflow.\nFirst we are going to create a new remote location of our repository called upstream. Upstream points to the upstream project repository. We will not push to this location, in most cases this is not possible because you do not have write access to a remote upstream repository. It is just used for pulling upstream changes in our forked repository.\nExecute the following commands to add the upstream repository as a new remote location and display all remote locations currently defined.\n$ git remote add upstream https://github.com/rhatservices/megaapp.git $ git remote -v origin git@github.com:tosmi/megaapp.git (fetch) origin git@github.com:tosmi/megaapp.git (push) upstream https://github.com/rhatservices/megaapp.git (fetch) upstream https://github.com/rhatservices/megaapp.git (push) As we hopefully implemented our new feature in feature branch, we can pull changes from the upstream master branch into our local copy of the master branch. Remember we are using a feature branch and master should be kept clean from local changes.\n$ git checkout master Switched to branch \u0026#39;master\u0026#39; Your branch is up to date with \u0026#39;origin/master\u0026#39;. So now we have this older copy of the upstream master branch checked out and we would like to update it to the latest and greatest from the upstream master branch.\n$ git pull upstream master remote: Enumerating objects: 10, done. remote: Counting objects: 100% (10/10), done. remote: Compressing objects: 100% (3/3), done. remote: Total 6 (delta 2), reused 6 (delta 2), pack-reused 0 Unpacking objects: 100% (6/6), 630 bytes | 157.00 KiB/s, done. From https://github.com/rhatservices/megaapp * branch master -\u0026gt; FETCH_HEAD * [new branch] master -\u0026gt; upstream/master Updating 4d8584e..ddfd077 Fast-forward cmd/megaapp/main.go | 2 ++ cmd/megaapp/rule.go | 20 ++++++++++++++++++++ 2 files changed, 22 insertions(+) create mode 100644 cmd/megaapp/rule.go With the pull command above you pulled all changes from the upstream master branch into you local copy of master. Just to be sure let’s display all available branches, local and remote ones.\nBranches with a name remote/\u0026lt;remote name\u0026gt;/\u0026lt;branch name\u0026gt; are remote branches that git knows about. Origin points to our forked repository and is also the default location for push operations.\n$ git branch -a master * tosmi/megafeature remotes/origin/HEAD -\u0026gt; origin/master remotes/origin/master remotes/origin/tosmi/megafeature remotes/upstream/master So finally to rebase our feature branch to the upstream master branch we first need to checkout our feature branch via\n$ git checkout tosmi/megafeature Now we are able to rebase our changes to upstream master. Git basically pulls in all changes from the master branch and re-applies the changes we did in our feature branch.\ngit rebase upstream/master Successfully rebased and updated refs/heads/tosmi/megafeature. There might be merge conflicts when git tries to apply you changes from your feature branch. You have to fix those changes, git add the fixed files and execute git rebase continue. Luckily this is not the case for your megafeature.\nAs we have successfully rebased our feature branch to upstream master we can now try to push changes made to our forked github repository.\n$ git push To github.com:tosmi/megaapp.git ! [rejected] tosmi/megafeature -\u0026gt; tosmi/megafeature (non-fast-forward) error: failed to push some refs to \u0026#39;git@github.com:tosmi/megaapp.git\u0026#39; hint: Updates were rejected because the tip of your current branch is behind hint: its remote counterpart. Integrate the remote changes (e.g. hint: \u0026#39;git pull ...\u0026#39;) before pushing again. hint: See the \u0026#39;Note about fast-forwards\u0026#39; in \u0026#39;git push --help\u0026#39; for details. Oh, this fails of course! The reason is that our local feature branch and the remote feature branch have a different commit history. The remote feature branch is missing the commits from master that we applied when rebasing on the current master branch.\nSo let’s try again, this time using the --force-with-lease option. You could also use -f or --force but --force-with-lease will stop you if someone else (our you) has modified the remote feature branch meanwhile. If you push with -f or --force anyways you might loose changes.\n$ git push --force-with-lease Enumerating objects: 5, done. Counting objects: 100% (5/5), done. Delta compression using up to 8 threads Compressing objects: 100% (3/3), done. Writing objects: 100% (3/3), 295 bytes | 295.00 KiB/s, done. Total 3 (delta 2), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To github.com:tosmi/megaapp.git + acf66a3...39357b2 tosmi/megafeature -\u0026gt; tosmi/megafeature (forced update) But as no one modified the remote feature branch while we did our rebase the force push goes through.\nOur merge request (if we opened one already) is now updated to the latest upstream master branch and merging our feature should be a breeze. You might notify the upstream project maintainer that you feature branch is up to date and ready for merging\nUsing git’s interactive rebase to change you commit history When working with upstream projects it might be that a project maintainer requests that you rework your git history before he is willing to merge your changes. For example this could be that case if you have plenty of commits with very small changes (e.g. fixed typos).\nThe general rule is that one commit should implement one change. This is not a hard rule, but usually works.\nLet’s look at an example. For the implementation of our new feature that we would like to bring upstream we have the following commit history\n$ git log --oneline 0a5221d (HEAD -\u0026gt; tosmi/megafeature) fixed typo 0e60d12 update README bf2ef3c update We have updated README.md in the repository but there a three commits for this little change. Before bringing this upstream in our pull request, we would like to convert those three commits into a single one and also make the commit message a little more meaningful.\nWe execute the following command to start reworking our commit history\n$ git rebase -i Git will drop us into our beloved editor (vi in this case), under Linux you could change the editor git uses by modifying the $EDITOR environment variable. We are going to see the following output:\npick bf2ef3c update pick 0e60d12 update README pick 0a5221d fixed typo # Rebase 39357b2..0a5221d onto 39357b2 (3 commands) # # Commands: # p, pick \u0026lt;commit\u0026gt; = use commit # r, reword \u0026lt;commit\u0026gt; = use commit, but edit the commit message # e, edit \u0026lt;commit\u0026gt; = use commit, but stop for amending # s, squash \u0026lt;commit\u0026gt; = use commit, but meld into previous commit # f, fixup \u0026lt;commit\u0026gt; = like \u0026#34;squash\u0026#34;, but discard this commit\u0026#39;s log message # x, exec \u0026lt;command\u0026gt; = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with \u0026#39;git rebase --continue\u0026#39;) # d, drop \u0026lt;commit\u0026gt; = remove commit # l, label \u0026lt;label\u0026gt; = label current HEAD with a name # t, reset \u0026lt;label\u0026gt; = reset HEAD to a label # m, merge [-C \u0026lt;commit\u0026gt; | -c \u0026lt;commit\u0026gt;] \u0026lt;label\u0026gt; [# \u0026lt;oneline\u0026gt;] # . create a merge commit using the original merge commit\u0026#39;s # . message (or the oneline, if no original merge commit was # . specified). Use -c \u0026lt;commit\u0026gt; to reword the commit message. # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # Git automatically selected commit id bf2ef3c as the basis for our rebase. We could also have specified the commit id where we would like to start our rebase operation e.g.\ngit rebase -i bf2ef3c In our editor of choice we can now tell git what it should do with the selected commits. Please go ahead and read the helpfull explanation text in comments (prefixed with \u0026#39;#\u0026#39;) to get a better understanding of the operations supported.\nIn our case we would like to squash the last commits. So we change the lines with pick to squash until it looks like the following:\npick bf2ef3c update squash 0e60d12 update README squash 0a5221d fixed typo We would like to squash commits 0a5221d and 0e60d12 onto commit bf2ef3c. Keep in mind that git actually reverses the order of commits. So 0a5221d is the last commit we added.\nIf we save the file and quit our editor (I’m using vi here), git drops us into another buffer where we can finally modify the commits\nThis is a combination of 3 commits. # This is the 1st commit message: update # This is the commit message #2: update README # This is the commit message #3: fixed typo # Please enter the commit message for your changes. Lines starting # with \u0026#39;#\u0026#39; will be ignored, and an empty message aborts the commit. # # Date: Mon May 18 15:46:37 2020 +0200 # # interactive rebase in progress; onto 39357b2 # Last commands done (3 commands done): # squash 0e60d12 update README # squash 0a5221d fixed typo # No commands remaining. # You are currently rebasing branch \u0026#39;tosmi/megafeature\u0026#39; on \u0026#39;39357b2\u0026#39;. # # Changes to be committed: # modified: README.md # We can see all three commit message and we are going to modify those messages until we are happy\n# This is a combination of 3 commits. # This is the 1st commit message: updated README.md to megafeature as we added megafeature, it makes sense to include a short note about it also in README.md # Please enter the commit message for your changes. Lines starting # with \u0026#39;#\u0026#39; will be ignored, and an empty message aborts the commit. # # Date: Mon May 18 15:46:37 2020 +0200 # # interactive rebase in progress; onto 39357b2 # Last commands done (3 commands done): # squash 0e60d12 update README # squash 0a5221d fixed typo # No commands remaining. # You are currently rebasing branch \u0026#39;tosmi/megafeature\u0026#39; on \u0026#39;39357b2\u0026#39;. # # Changes to be committed: # modified: README.md # When we are happy with new commit message we just save and quit our editor. Git will now rewirte the history and when we take look at the commit history again we will see our changes:\n$ git log --oneline 91d1ae2 (HEAD -\u0026gt; tosmi/megafeature) updated README.md to megafeature 39357b2 (origin/tosmi/megafeature) added a mega feature ddfd077 (upstream/master, master) added rule command 4d8584e (origin/master, origin/HEAD) Update README.md eb6ccbc Create README.md 60fcabc start using cobra for argument parsing 5140ed0 import .gitignore d2b55d1 import a simple Makefile 2ecb412 initial import We only have commit 91d1ae2 now , which includes all three changes from the commits before.\nRewriting the history of a repository is a dangerous operation. Especially when you are working in a team. It is not advised to change the history of commits that got already pushed to a remote location. Otherwise your teammates will get confused next time they try to push or pull from the shared repository. So it’s OK to change the commit history of a feature branch that only you are using, but be careful when working on branches more than one developer is using.\n"},{"uri":"https://blog.stderr.at/categories/general/","title":"General","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/git/","title":"git","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/git/","title":"Git","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/github/","title":"github","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/bookinfo/","title":"Bookinfo","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/deploy-example-bookinfo-application/","title":"Deploy Example Bookinfo Application","tags":["Istio","Service Mesh","OpenShift","OCP","Bookinfo","Example"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 11 - Deploy Example Bookinfo Application","content":" To test a second application, a bookinfo application shall be deployed as an example.\nThe following section finds it’s origin at:\nIstio - Bookinfo Application\nOpenShift 4 - Example Application\nThe Bookinfo application displays information about a book, similar to a single catalog entry of an online book store. Displayed on the page is a description of the book, book details (ISBN, number of pages, and other information), and book reviews. The Bookinfo application consists of these microservices: * The productpage microservice calls the details and reviews microservices to populate the page. * The details microservice contains book information. * The reviews microservice contains book reviews. It also calls the ratings microservice. * The ratings microservice contains book ranking information that accompanies a book review. There are three versions of the reviews microservice: * Version v1 does not call the ratings Service. * Version v2 calls the ratings Service and displays each rating as one to five black stars. * Version v3 calls the ratings Service and displays each rating as one to five red stars. The end-to-end architecture of the application is shown below. Figure 1. Bookinfo Application End2End Overview To use the bookinfo application inside service mesh, no code changes are required. Instead an Envoy proxy is added as a sidecar container to all containers (product, review, details) which intercepts the traffic.\nInstallation Let’s start right away:\nCreate a new project\noc new-project bookinfo Add the new project to our Service Mesh\napiVersion: maistra.io/v1 kind: ServiceMeshMember metadata: name: default namespace: bookinfo spec: controlPlaneRef: name: basic-install namespace: istio-system Create the application\noc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-1.1/samples/bookinfo/platform/kube/bookinfo.yaml Create the Gateway and the VirtuaService\noc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-1.1/samples/bookinfo/networking/bookinfo-gateway.yaml Check if the services and pods are up and running\noc get svc,pods NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/details ClusterIP 172.30.178.172 \u0026lt;none\u0026gt; 9080/TCP 7m16s service/productpage ClusterIP 172.30.78.96 \u0026lt;none\u0026gt; 9080/TCP 7m13s service/ratings ClusterIP 172.30.154.12 \u0026lt;none\u0026gt; 9080/TCP 7m15s service/reviews ClusterIP 172.30.138.174 \u0026lt;none\u0026gt; 9080/TCP 7m14s NAME READY STATUS RESTARTS AGE pod/details-v1-d7db4d55b-mwzsk 2/2 Running 0 7m14s pod/productpage-v1-5f598fbbf4-svkbc 2/2 Running 0 7m11s pod/ratings-v1-85957d89d8-v2lrs 2/2 Running 0 7m11s pod/reviews-v1-67d9b4bcc-x6s2v 2/2 Running 0 7m11s pod/reviews-v2-67b465c497-zpz6z 2/2 Running 0 7m11s pod/reviews-v3-7bd659b757-j6rwn 2/2 Running 0 7m11s Verify that application is accessible Export the Gateway URL into a variable\nexport GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) Verify if the productpage is accessible\ncurl -s http://${GATEWAY_URL}/productpage | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; \u0026lt;title\u0026gt;Simple Bookstore App\u0026lt;/title\u0026gt; You can also access the Productpage in your browser. When you reload the page several times, you will see different results for the Reviews. This comes due to 3 different versions: one without any rating, one with black stars and one with red stars. http://${GATEWAY_URL}/productpage\nFigure 2. Bookinfo Application Adding default Destination Rule oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-1.1/samples/bookinfo/networking/destination-rule-all-mtls.yaml This will add default routing to all endpoints with same weight. As you can see in Kiali, the Reviews microservice is contacted equally.\nFigure 3. Kiali: Bookinfo Application Feel free to play with other DestinationRules to controll your traffic.\n"},{"uri":"https://blog.stderr.at/tags/example/","title":"Example","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/2020/04/ansible-azure-resource-manager-example/","title":"Ansible - Azure Resource Manager Example","tags":["Ansible","Azure"],"description":"Using Ansible Azure Resource Manager to create a Virtual Machine","content":" Using Ansible Resource Manager with an ARM template and a simple Ansible playbook to deploy a Virtual Machine with Disk, virtual network, public IP and so on.\nIntroduction Source: [1]\nIn order to deploy a Virtual Machine and all depended resources using the Azure Resource Manager (ARM) template with Ansible, you will need three things:\nThe ARM Template\nThe parameters you want to use\nThe Ansible playbook\nAll can be found below. Store them and simply call:\nansible-playbook Azure/create_azure_deployment.yml it will take several minutes, until everything has been deployment in Azure. Instead of using a json file locally you can upload the template file (as well as a parameters file) to a version control system and use it from there. Additional Resources Ansible Playbook: Create-Azure-Deplyoment.yml --- - name: Get facts of a VM hosts: localhost connection: local become: false gather_facts: false tasks: #- name: Destroy Azure Deploy # azure_rm_deployment: # resource_group: tju-ResourceGroup # name: tju-testDeployment # state: absent - name: Create Azure Resource Group deployment azure_rm_deployment: state: present resource_group_name: tju-ResourceGroup name: tju-testDeployment #template_link: \u0026#39;\u0026lt;YOUR RAW Github template file\u0026gt;\u0026#39; #parameters_link: \u0026#39;\u0026lt;YOUR RAW Github parameters file\u0026gt;\u0026#39; template: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;ResourceManagerTemplate.json\u0026#39;) }}\u0026#34; parameters: projectName: value: tjuProject location: value: \u0026#34;East US\u0026#34; adminUsername: value: tjungbauer adminPublicKey: value: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/Users/tjungbauer/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; operatingSystem: value: CentOS operatingSystemPublisher: value: OpenLogic operatingSystemSKU: value: \u0026#39;7.1\u0026#39; vmSize: value: Standard_D2s_v3 register: azure - name: Add new instance to host group add_host: hostname: \u0026#34;{{ item[\u0026#39;ips\u0026#39;][0].public_ip }}\u0026#34; groupname: azure_vms loop: \u0026#34;{{ azure.deployment.instances }}\u0026#34; ResourceManagerTemplate.json { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;projectName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies a name for generating resource names.\u0026#34; } }, \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the location for all resources.\u0026#34; } }, \u0026#34;adminUsername\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies a username for the Virtual Machine.\u0026#34; } }, \u0026#34;adminPublicKey\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the SSH rsa public key file as a string. Use \\\u0026#34;ssh-keygen -t rsa -b 2048\\\u0026#34; to generate your SSH key pairs.\u0026#34; } }, \u0026#34;operatingSystem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the Operating System. i.e. CentOS\u0026#34; } }, \u0026#34;operatingSystemPublisher\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the publisher. i.e. OpenLogic\u0026#34; } }, \u0026#34;operatingSystemSKU\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the version of the OS. i.e. 7.1\u0026#34; } }, \u0026#34;vmSize\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the the VM size. i.e. Standard_D2s_v3\u0026#34; } } }, \u0026#34;variables\u0026#34;: { \u0026#34;vNetName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-vnet\u0026#39;)]\u0026#34;, \u0026#34;vNetAddressPrefixes\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;vNetSubnetName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;vNetSubnetAddressPrefix\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;vmName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-vm\u0026#39;)]\u0026#34;, \u0026#34;publicIPAddressName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-ip\u0026#39;)]\u0026#34;, \u0026#34;networkInterfaceName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-nic\u0026#39;)]\u0026#34;, \u0026#34;networkSecurityGroupName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-nsg\u0026#39;)]\u0026#34;, \u0026#34;networkSecurityGroupName2\u0026#34;: \u0026#34;[concat(variables(\u0026#39;vNetSubnetName\u0026#39;), \u0026#39;-nsg\u0026#39;)]\u0026#34; }, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/networkSecurityGroups\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;networkSecurityGroupName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;securityRules\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ssh_rule\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Locks inbound down to ssh default port 22.\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;Tcp\u0026#34;, \u0026#34;sourcePortRange\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;destinationPortRange\u0026#34;: \u0026#34;22\u0026#34;, \u0026#34;sourceAddressPrefix\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;destinationAddressPrefix\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;access\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;priority\u0026#34;: 123, \u0026#34;direction\u0026#34;: \u0026#34;Inbound\u0026#34; } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/publicIPAddresses\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;publicIPAddressName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;publicIPAllocationMethod\u0026#34;: \u0026#34;Dynamic\u0026#34; }, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Basic\u0026#34; } }, { \u0026#34;comments\u0026#34;: \u0026#34;Simple Network Security Group for subnet [variables(\u0026#39;vNetSubnetName\u0026#39;)]\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/networkSecurityGroups\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-08-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;networkSecurityGroupName2\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;securityRules\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;default-allow-22\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;priority\u0026#34;: 1000, \u0026#34;access\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;direction\u0026#34;: \u0026#34;Inbound\u0026#34;, \u0026#34;destinationPortRange\u0026#34;: \u0026#34;22\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;Tcp\u0026#34;, \u0026#34;sourceAddressPrefix\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;sourcePortRange\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;destinationAddressPrefix\u0026#34;: \u0026#34;*\u0026#34; } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/virtualNetworks\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vNetName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkSecurityGroups\u0026#39;, variables(\u0026#39;networkSecurityGroupName2\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;addressSpace\u0026#34;: { \u0026#34;addressPrefixes\u0026#34;: [ \u0026#34;[variables(\u0026#39;vNetAddressPrefixes\u0026#39;)]\u0026#34; ] }, \u0026#34;subnets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnetName\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;addressPrefix\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnetAddressPrefix\u0026#39;)]\u0026#34;, \u0026#34;networkSecurityGroup\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkSecurityGroups\u0026#39;, variables(\u0026#39;networkSecurityGroupName2\u0026#39;))]\u0026#34; } } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/networkInterfaces\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;networkInterfaceName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;Microsoft.Network/publicIPAddresses\u0026#39;, variables(\u0026#39;publicIPAddressName\u0026#39;))]\u0026#34;, \u0026#34;[resourceId(\u0026#39;Microsoft.Network/virtualNetworks\u0026#39;, variables(\u0026#39;vNetName\u0026#39;))]\u0026#34;, \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkSecurityGroups\u0026#39;, variables(\u0026#39;networkSecurityGroupName\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;ipConfigurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ipconfig1\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;privateIPAllocationMethod\u0026#34;: \u0026#34;Dynamic\u0026#34;, \u0026#34;publicIPAddress\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/publicIPAddresses\u0026#39;, variables(\u0026#39;publicIPAddressName\u0026#39;))]\u0026#34; }, \u0026#34;subnet\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/virtualNetworks/subnets\u0026#39;, variables(\u0026#39;vNetName\u0026#39;), variables(\u0026#39;vNetSubnetName\u0026#39;))]\u0026#34; } } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-10-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vmName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkInterfaces\u0026#39;, variables(\u0026#39;networkInterfaceName\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;hardwareProfile\u0026#34;: { \u0026#34;vmSize\u0026#34;: \u0026#34;[parameters(\u0026#39;vmSize\u0026#39;)]\u0026#34; }, \u0026#34;osProfile\u0026#34;: { \u0026#34;computerName\u0026#34;: \u0026#34;[variables(\u0026#39;vmName\u0026#39;)]\u0026#34;, \u0026#34;adminUsername\u0026#34;: \u0026#34;[parameters(\u0026#39;adminUsername\u0026#39;)]\u0026#34;, \u0026#34;linuxConfiguration\u0026#34;: { \u0026#34;disablePasswordAuthentication\u0026#34;: true, \u0026#34;ssh\u0026#34;: { \u0026#34;publicKeys\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;[concat(\u0026#39;/home/\u0026#39;, parameters(\u0026#39;adminUsername\u0026#39;), \u0026#39;/.ssh/authorized_keys\u0026#39;)]\u0026#34;, \u0026#34;keyData\u0026#34;: \u0026#34;[parameters(\u0026#39;adminPublicKey\u0026#39;)]\u0026#34; } ] } } }, \u0026#34;storageProfile\u0026#34;: { \u0026#34;imageReference\u0026#34;: { \u0026#34;publisher\u0026#34;: \u0026#34;[parameters(\u0026#39;operatingSystemPublisher\u0026#39;)]\u0026#34;, \u0026#34;offer\u0026#34;: \u0026#34;[parameters(\u0026#39;operatingSystem\u0026#39;)]\u0026#34;, \u0026#34;sku\u0026#34;: \u0026#34;[parameters(\u0026#39;operatingSystemSKU\u0026#39;)]\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;osDisk\u0026#34;: { \u0026#34;createOption\u0026#34;: \u0026#34;fromImage\u0026#34; } }, \u0026#34;networkProfile\u0026#34;: { \u0026#34;networkInterfaces\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkInterfaces\u0026#39;, variables(\u0026#39;networkInterfaceName\u0026#39;))]\u0026#34; } ] } } } ], \u0026#34;outputs\u0026#34;: { \u0026#34;adminUsername\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;adminUsername\u0026#39;)]\u0026#34; } } } Sources [1]: azure_rm_deployment – Create or destroy Azure Resource Manager template deployments\n"},{"uri":"https://blog.stderr.at/tags/azure/","title":"Azure","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2020-04-16-tekton/","title":"OpenShift Pipelines - Tekton Introduction","tags":["Pipelines","OpenShift","OCP","Tekton"],"description":"Using OpenShift 4.x and OpenShift Pipelines using Tekton for your CI/CD process","content":" OpenShift Pipelines is a cloud-native, continuous integration and delivery (CI/CD) solution for building pipelines using Tekton. Tekton is a flexible, Kubernetes-native, open-source CI/CD framework that enables automating deployments across multiple platforms (Kubernetes, serverless, VMs, etc) by abstracting away the underlying details. [1]\nOpenShift Pipelines features Source: [1]\nStandard CI/CD pipeline definition based on Tekton\nBuild images with Kubernetes tools such as S2I, Buildah, Buildpacks, Kaniko, etc\nDeploy applications to multiple platforms such as Kubernetes, serverless and VMs\nEasy to extend and integrate with existing tools\nScale pipelines on-demand\nPortable across any Kubernetes platform\nDesigned for microservices and decentralized teams\nIntegrated with the OpenShift Developer Console\nPrerequisites OpenShift 4.x cluster. Try yourself at https://try.openshift.com\nOptional: Tekton CLI - Optional for now, since you could do everything via UI as well.\noc/kubectl CLI or WebUI\nThe Tekton CLI is optional in case you prefer to do everything via the OpenShift WebUI. However, below examples make use of the Tekton CLI and I personally would recommend to at least install it. (Like oc client it is good to have a CLI option as well). In any case, all described action can be done directly via the WebUI as well. Basic Concepts Tekton makes use of several custom resources (CRD).\nThese CRDs are:\nTask: each step in a pipeline is a task, while a task can contain several steps itself, which are required to perform a specific task. For each Task a pod will be allocated and for each step inside this Task a container will be used. This helps in better scalability and better performance throughout the pipeline process.\nPipeline: is a series of tasks, combined to work together in a defined (structured) way\nTaskRun: is the result of a Task, all combined TaskRuns are used in the PipelineRun\nPipelineRun: is the actual execution of a whole Pipeline, containing the results of the pipeline (success, failed…​)\nPipelines and Tasks should be generic and never define possible variables, like input git repository, directly in their definition. For this, the concept of PipelineResources has been created, which defines these parameters and which are used during a PipelineRun.\nInstallation The OpenShift Pipeline is an operator which can e installed using the following yaml:\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-OpenShift-Pipelines.yaml apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: openshift-pipelines-operator namespace: openshift-operators spec: channel: dev-preview name: openshift-pipelines-operator source: community-operators sourceNamespace: openshift-marketplace EOF oc create -f deploy-OpenShift-Pipelines.yaml As an alternative, you can also use the WebUI to rollout the operator:\nSearch for \u0026#34;OpenShift Pipeline\u0026#34; under OperatorHub and install it\nSelect:\nAll Namespaces: since the operator needs to watch for Tekton Custom Resources across all namespaces.\nChannel: Dev Preview\nApprove Strategy: Automatic\nPrepare a tutorial project To test our OpenShift Pipelines, we need to deploy an example application. This application let’s you vote what pet you like more: Cats or Dogs? It contais of a backend and a frontend part, which both will be deployed in a namespace.\nLet’s first create a new project:\noc new-project pipelines-tutorial The OpenShift Pipeline operator will automatically create a pipeline serviceaccount with all required permissions to build and push an image and which is used by PipelineRuns:\noc get sa pipeline NAME SECRETS AGE pipeline 2 15s Create a Task A Task is the smallest block of a Pipeline which by itself can contain one or more steps which are executed in order to process a specific element. For each Task a pod is allocated and each step is running in a container inside this pod. Tasks are reusable by other Pipelines. Input and Output specifications can be used to interact with other Tasks.\nLet’s create two tasks Source: Pipeline-Tutorial\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-Example-Tasks.yaml apiVersion: tekton.dev/v1alpha1 kind: Task metadata: name: apply-manifests spec: inputs: resources: - {type: git, name: source} params: - name: manifest_dir description: The directory in source that contains yaml manifests type: string default: \u0026#34;k8s\u0026#34; steps: - name: apply image: quay.io/openshift/origin-cli:latest workingDir: /workspace/source command: [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;] args: - |- echo Applying manifests in $(inputs.params.manifest_dir) directory oc apply -f $(inputs.params.manifest_dir) echo ----------------------------------- --- apiVersion: tekton.dev/v1alpha1 kind: Task metadata: name: update-deployment spec: inputs: resources: - {type: image, name: image} params: - name: deployment description: The name of the deployment patch the image type: string steps: - name: patch image: quay.io/openshift/origin-cli:latest command: [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;] args: - |- oc patch deployment $(inputs.params.deployment) --patch=\u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{ \u0026#34;containers\u0026#34;:[{ \u0026#34;name\u0026#34;: \u0026#34;$(inputs.params.deployment)\u0026#34;, \u0026#34;image\u0026#34;:\u0026#34;$(inputs.resources.image.url)\u0026#34; }] }}}}\u0026#39; EOF oc create -f deploy-Example-Tasks.yaml Verify that the two tasks have been created using the Tekton CLI:\ntkn task ls NAME AGE apply-manifests 52 seconds ago update-deployment 52 seconds ago Create a Pipeline A pipeline is a set of Tasks, which should be executed in a defined way to achieve a specific goal.\nThe example Pipeline below uses two resources:\ngit-repo: defines the Git-Source\nimage: Defines the target at a repository\nIt first uses the Task buildah, which is a standard Task the OpenShift operator created automatically. This task will build the image. The resulted image is pushed to an image registry, defined in the output parameter. After that our created tasks apply-manifest and update-deployment are executed. The execution order of these tasks is defined with the runAfter Parameter in the yaml definition.\nThe Pipeline should be re-usable accross multiple projects or environments, thats why the resources (git-repo and image) are not defined here. When a Pipeline is executed, these resources will get defined. cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-Example-Pipeline.yaml apiVersion: tekton.dev/v1alpha1 kind: Pipeline metadata: name: build-and-deploy spec: resources: - name: git-repo type: git - name: image type: image params: - name: deployment-name type: string description: name of the deployment to be patched tasks: - name: build-image taskRef: name: buildah kind: ClusterTask resources: inputs: - name: source resource: git-repo outputs: - name: image resource: image params: - name: TLSVERIFY value: \u0026#34;false\u0026#34; - name: apply-manifests taskRef: name: apply-manifests resources: inputs: - name: source resource: git-repo runAfter: - build-image - name: update-deployment taskRef: name: update-deployment resources: inputs: - name: image resource: image params: - name: deployment value: $(params.deployment-name) runAfter: - apply-manifests EOF oc create -f deploy-Example-Pipeline.yaml Verify that the Pipeline has been created using the Tekton CLI:\ntkn pipeline ls NAME AGE LAST RUN STARTED DURATION STATUS build-and-deploy 3 seconds ago --- --- --- --- Trigger Pipeline After the Pipeline has been created, it can be triggered to execute the Tasks.\nCreate PipelineResources Since the Pipeline is generic, we need to define 2 PipelineResources first, to execute a Pipepline. Our example application contains a frontend (vote-ui) AND a backend (vote-api), therefore 4 PipelineResources will be created. (2 times git repository to clone the source and 2 time output image)\nQuick overview:\nui-repo: will be used as git_repo in the Pipepline for the Frontend\nui-image: will be used as image in the Pipeline for the Frontend\napi-repo: will be used as git_repo in the Pipepline for the Backend\napi-image: will be used as image in the Pipeline for the Backend\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-Example-PipelineResources.yaml apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: ui-repo spec: type: git params: - name: url value: http://github.com/openshift-pipelines/vote-ui.git --- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: ui-image spec: type: image params: - name: url value: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-ui:latest --- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: api-repo spec: type: git params: - name: url value: http://github.com/openshift-pipelines/vote-api.git --- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: api-image spec: type: image params: - name: url value: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-api:latest EOF oc create -f deploy-Example-PipelineResources.yaml The resources can be listed with:\ntkn resource ls NAME TYPE DETAILS api-repo git url: http://github.com/openshift-pipelines/vote-api.git ui-repo git url: http://github.com/openshift-pipelines/vote-ui.git api-image image url: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-api:latest ui-image image url: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-ui:latest Execute Pipelines We start a PipelineRune for the backend and frontend of our application.\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-Example-PipelineRun.yaml apiVersion: tekton.dev/v1alpha1 kind: PipelineRun metadata: name: build-deploy-api-pipelinerun spec: pipelineRef: name: build-and-deploy resources: - name: git-repo resourceRef: name: api-repo - name: image resourceRef: name: api-image params: - name: deployment-name value: vote-api --- apiVersion: tekton.dev/v1alpha1 kind: PipelineRun metadata: name: build-deploy-ui-pipelinerun spec: pipelineRef: name: build-and-deploy resources: - name: git-repo resourceRef: name: ui-repo - name: image resourceRef: name: ui-image params: - name: deployment-name value: vote-ui EOF oc create -f deploy-Example-PipelineRun.yaml The PipelineRuns can be listed with\ntkn pipelinerun ls NAME STARTED DURATION STATUS build-deploy-api-pipelinerun 3 minutes ago --- Running build-deploy-ui-pipelinerun 3 minutes ago --- Running Moreover, the logs can be viewed with the following command and select the appropriate PipelineRun:\ntkn pipeline logs -f ? Select pipelinerun: [Use arrows to move, type to filter] \u0026gt; build-deploy-api-pipelinerun started 2 minutes ago build-deploy-ui-pipelinerun started 2 minutes ago Checking your application Now our Pipeline built and deployed the voting application, where you can vote if you prefere cats or dogs (Cats or course :) )\nGet the route of your project and open the URL in the browser. (Should be something like vote-ui-pipelines-tutorial.apps.yourclustername)\nFigure 1. Tekton: Example Application OpenShift WebUI With the OpenShift Pipeline operator a new menu item is introduced on the WebUI of OpenShift. All Tekton CLI command which are used above, can actually be replaced with the web interface, in case you prefere this. The big advantage is th graphical presentation of Pipelines and their lifetime.\nI will not create screenshots for every screen, but for example pipelines:\nUnder Pipelines a list of pipelines will be shown.\nFigure 2. OpenShift UI: List of Pipelines Additional Resources Sources [1]: OpenShift Pipelines Tutorial\nTekon\nTekton Task Catalog\n"},{"uri":"https://blog.stderr.at/tags/pipelines/","title":"Pipelines","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/general/2020/04/red-hat-satellite-cheat-sheet/","title":"Red Hat Satellite Cheat Sheet","tags":["Satellite"],"description":"Cheat sheet for Red Hat Satellite","content":" Cheat sheet for various Red Hat Satellite tasks from a newbie to a newbie.\nRequirements up to Satellite 6.7 RHEL 7.X\n4 CPU Cores\n20 GB of RAM\n300 GB disk space\nfor more info see the prerequistes guide\nInstallation Satellite up to version 6.7 uses puppet for installation. You can use\npuppet filebucket to restore files modified by puppet.\nSatellite requires the Red Hat Satellite Infrastructure Subscription, check if it’s available with\nsubscription-manager list --all --available --matches \u0026#39;Red Hat Satellite Infrastructure Subscription\u0026#39; If not attach it with\nsubscription-manager attach --pool=pool_id Next disable all repos and enable only supported repostories via\nsubscription-manager repos --disable \u0026#34;*\u0026#34; and enable required repositories\nsubscription-manager repos --enable=rhel-7-server-rpms \\ --enable=rhel-7-server-satellite-6.6-rpms \\ --enable=rhel-7-server-satellite-maintenance-6-rpms \\ --enable=rhel-server-rhscl-7-rpms \\ --enable=rhel-7-server-ansible-2.8-rpms then clean cached all repo data via\nyum clean all and install satellite packages via\nyum install satellite Install satellite with\nsatellite-installer --scenario satellite \\ --foreman-initial-organization \u0026#34;initial_organization_name\u0026#34; \\ --foreman-initial-location \u0026#34;initial_location_name\u0026#34; \\ --foreman-initial-admin-username admin_user_name \\ --foreman-initial-admin-password admin_password Backup / Restore / Cloning Use satellite-maintain for doing offline and online backups\nsatellite-maintain backup offline /backup/ when using the online option make sure that no new content view or content view versions should be created while the backup is running. basically satellite should be idle.\nCloning Satellite The online/offline options also backup /var/lib/pulp, which contains all downloaded packages. This could be huge. There’s an option to skip this so\nsatellite-maintain backup offline --skip-pulp-tar /backup/ For a restore you always need the content of /var/lib/pulp. This is mainly usefull for cloning satellite. You backup everything except /var/lib/pulp, copy the backup to a second system and rsync /var/lib/pulp to the new system. Then restore the backup and satellite should work as normal on the clone.\nSnaphot backups Satellite also supports backups via LVM snapshots. For more information see Snapshot backup\nUpgrades Read the Satellite release notes\nDo a offline backup see [Backup / Restore]\nYou could clone satellite to a other system\nIf there are local changes to dhcpd or dns configurations use\nsatellite-installer --foreman-proxy-dns-managed=false --foreman-proxy-dhcp-managed=false to stop satellite-install from overwriting those files.\ninstall the latest version of satellite-maintain via\nyum install rubygem-foreman_maintain check for available satellite versions with\nsatellite-maintain upgrade list-versions test the possible upgrade with\nsatellite-maintain upgrade check --target-version 6.7 and finally run the upgrade and PRAY!\nsatellite-maintain upgrade run --target-version 6.7 Various tips and tricks Installing packages via yum Satellite installs a yum plugin called foreman-protector. If you try to install a package via yum you get the following message\nWARNING: Excluding 12190 packages due to foreman-protector. Use foreman-maintain packages install/update \u0026lt;package\u0026gt; to safely install packages without restrictions. Use foreman-maintain upgrade run for full upgrade. so use\nsatellite-maintain install \u0026lt;package name\u0026gt; OS package upgrade This should be done via satellite-maintain because all packages are locked by default (see Installing packages via yum).\nThis basically comes down to running\noreman-maintain upgrade run --target-version 6.6.z for upgrading OS packages if you have satellite 6.6 installed.\n"},{"uri":"https://blog.stderr.at/tags/satellite/","title":"Satellite","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/satellite/","title":"Satellite","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/service-mesh-1.1-released/","title":"Service Mesh 1.1 released","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"OpenShift 4.x Service Mesh Update.","content":" April 10th 2020 Red Hat released Service Mesh version 1.1 which supports the following versions:\nIstio - 1.4.6\nKiali - 1.12.7\nJaeger - 1.17.1\nUpdate To update an operator like Service Mesh, the Operator Life Cycle Manager takes care and automatically updates everything (unless it was configured differently).\nFor the Service Mesh 1.1 update consult Upgrading Red Hat OpenShift Service Mesh It is important to add the version number to the ServiceMeshControlPlane object. The easiest way to do so is:\nLog into OpenShift\nSelect the Namespace istio-system\nGoto _\u0026#34;Installed Operators \u0026gt; Red Hat OpenShift Service Mesh \u0026gt; ServiceMeshControlPlanes \u0026gt; basic-install \u0026gt; YAML\u0026#34;\nUnder spec add the following:\nspec: version: v1.1 Notable Changes ServiceMeshMember Object With the ServiceMeshMember object it is now possible that a project administrator can add a service to the service mesh, instead relying on the cluster administrator to configure the ServiceMeshMemberRoll. To do so create the following object (i.e. under the namespace tutorial)\napiVersion: maistra.io/v1 kind: ServiceMeshMember metadata: name: default namespace: tutorial spec: controlPlaneRef: name: basic-install (1) namespace: istio-system (2) 1 Name of the ServiceMeshControlPlane object 2 name of the service mesh namespace "},{"uri":"https://blog.stderr.at/service-mesh/2020/04/authentication-jwt/","title":"Authentication JWT","tags":["Istio","Service Mesh","OpenShift","OCP","JWT"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 10 - Authentication with JWT","content":" Welcome to tutorial 10 of OpenShift 4 and Service Mesh, where we will discuss authentication with JWT. JSON Web Token (JWT) is an open standard that allows to transmit information between two parties securely as a JSON object. It is an authentication token, which is verified and signed and therefore trusted. The signing can be achieved by using a secret or a public/private key pair.\nService Mesh can be used to configure a policy which enables JWT for your services.\nPreparation Be sure that you have at least the Gateway and VirtualService configured:\noc get istio-io -n tutorial Which should return the following:\nNAME AGE gateway.networking.istio.io/ingress-gateway-exampleapp 45h NAME HOST AGE destinationrule.networking.istio.io/recommendation recommendation 29h NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/ingress-gateway-exampleapp [ingress-gateway-exampleapp] [*] 45h Run some texample traffic, to be sure that our application is still working as expected\nexport GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) sh ~/run.sh 1000 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 31622 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 33056 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 31623 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 33057 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 31624 # 5: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 33058 Enabling End-User Authentication To test this feature we will need a valid token (JWT). More details can be found at the Istio example\nAll we need to create a Policy object\napiVersion: \u0026#34;authentication.istio.io/v1alpha1\u0026#34; kind: \u0026#34;Policy\u0026#34; metadata: name: \u0026#34;jwt-example\u0026#34; spec: targets: - name: customer origins: - jwt: issuer: \u0026#34;testing@secure.istio.io\u0026#34; jwksUri: \u0026#34;https://raw.githubusercontent.com/istio/istio/release-1.2/security/tools/jwt/samples/jwks.json\u0026#34; (1) principalBinding: USE_ORIGIN 1 Path to test a public key After a few seconds the requests will fail with an \u0026#34;authentication failed\u0026#34; error:\nsh ~/run.sh 1000 $GATEWAY_URL # 0: Origin authentication failed. # 1: Origin authentication failed. # 2: Origin authentication failed. # 3: Origin authentication failed. # 4: Origin authentication failed. # 5: Origin authentication failed. In Kiali we see a 100% failure rate.\nFigure 1. Kiali: failing because of authentication error. To be able to connect to our application we first need to fetch a valid token and put this into the header while sending curl.\nexport GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) export TOKEN=$(curl https://raw.githubusercontent.com/istio/istio/release-1.1/security/tools/jwt/samples/demo.jwt -s) for x in $(seq 1 1000); do curl --header \u0026#34;Authorization: Bearer $TOKEN\u0026#34; $GATEWAY_URL -s; done In Kiali the traffic is now working again and authenticated.\nFigure 2. Kiali: Traffic authenticated. Clean Up Remove the policy again, to be ready for the next tutorial.\noc delete policy jwt-example -n tutorial "},{"uri":"https://blog.stderr.at/tags/jwt/","title":"JWT","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/mtls/","title":"mTLS","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/mutual-tls-authentication/","title":"Mutual TLS Authentication","tags":["Istio","Service Mesh","OpenShift","OCP","mTLS"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 9 - Mutual TLS/mTLS Authentication","content":" When more and more microservices are involved in an application, more and more traffic is sent on the network. It should be considered to secure this traffic, to prevent the possibility to inject malicious packets. Mutual TLS/mTLS authentication or two-way authentication offers a way to encrypt service traffic with certificates.\nWith Red Hat OpenShift Service Mesh, Mutual TLS can be used without the microservice knowing that it is happening. The TLS is managed completely by the Service Mesh Operator between two Envoy proxies using a defined mTLS policy.\nIssue 9 of OpenShift 4 and Service Mesh will explain how to enable Mutual TLS inside the Service Mesh to secure the traffic between the different microservices.\nHow does it work? If a microservice sends a request to a server, it must pass the local sidecar Envoy proxy first.\nThe proxy will intercept the outbound request and starts a mutual TLS handshake with the proxy at the server side. During this handshake the certificates are exchanged and loaded into the proxy containers by Service Mesh.\nThe client side Envoy starts a mutual TLS handshake with the server side Envoy.\nThe client proxy does a secure naming check on the server’s certificate to verify that the identity in the certificate is authorized.\nA mutual TLS connection is established between the client and the server.\nThe Envoy proxy at the server sides decrypts the traffic and forwards it to the application through a local TCP connection.\nPreparations Before we can start be sure that the services are setup like in Issue #3. In addition, be sure that the following DestinationRule already exists:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation subsets: - labels: version: v1 name: version-v1 - labels: version: v2 name: version-v2 Now we will create a pod, which is running outside of the Service Mesh. It will not have a sidecar proxy and will simply curl our application.\nStore the following yaml and create the object in our cluster.\napiVersion: extensions/v1beta1 kind: Deployment metadata: labels: app: curl version: v1 name: curl spec: replicas: 1 selector: matchLabels: app: curl version: v1 template: metadata: labels: app: curl version: v1 annotations: (1) sidecar.istio.io/proxyCPU: \u0026#34;500m\u0026#34; sidecar.istio.io/proxyMemory: 400Mi spec: containers: - image: quay.io/maistra_demos/curl:latest command: [\u0026#34;/bin/sleep\u0026#34;, \u0026#34;3650d\u0026#34;] imagePullPolicy: Always name: curl 1 since no sidecar is injected (sidecar.istio.io/inject: \u0026#34;true\u0026#34;), only 1 container will be started. The traffic coming from the microservice customer AND from the external client curl must be simulated. To achieve this the following shell script can be used:\n#!/bin/sh export CURL_POD=$(oc get pods -n tutorial -l app=curl | grep curl | awk \u0026#39;{ print $1}\u0026#39; ) export CUSTOMER_POD=$(oc get pods -n tutorial -l app=customer | grep customer | awk \u0026#39;{ print $1}\u0026#39; ) echo \u0026#34;A load generating script is running in the next step. Ctrl+C to stop\u0026#34; while :; do echo \u0026#34;Executing curl in curl pod\u0026#34; oc exec -n tutorial $CURL_POD -- curl -s http://preference:8080 \u0026gt; /dev/null sleep 0.5 echo \u0026#34;Executing curl in customer pod\u0026#34; oc exec -n tutorial $CUSTOMER_POD -c customer -- curl -s http://preference:8080 \u0026gt; /dev/null sleep 0.5 done By executing this, it will first execute a curl command out of the curl pod and then the same curl command out of the customer container. Kepp this script running\nEnabling Mutual TLS Lets execute the shell script above and verify Kiali. As you notice there are requests coming from the customer microservice and from the source called unknown, which is the curl-service running outside the Service Mesh.\nFigure 1. Kiali: traffic coming from customer microserver and external pod Enable the policy by creating the following object:\napiVersion: \u0026#34;authentication.istio.io/v1alpha1\u0026#34; kind: \u0026#34;Policy\u0026#34; metadata: name: \u0026#34;preference-mutualtls\u0026#34; spec: targets: - name: preference peers: - mtls: mode: STRICT (1) 1 We are enforcing mtls for the target preference After a few seconds the curl pod cannot reach the application anymore:\nExecuting curl in curl pod command terminated with exit code 56 Executing curl in customer pod Executing curl in curl pod command terminated with exit code 56 Executing curl in customer pod Executing curl in curl pod command terminated with exit code 5 This is expected, since the preference service allows traffic over mutual TLS only. This was enforced by the Policy object (STRICT mode). The customer service, which is running inside the Service Mesh receives the error \u0026#34;5053 Service Unavalable\u0026#34; since it tries to send traffic, but it does not know yet to use mTLS.\nIn Kiali you will see the following:\nFigure 2. Kiali: traffic is blocked The curl pod is greyed out, since the traffic it tries to send, never reaches the preference service and is therefor not counted in the metric. To make customer aware that mutual TLS shall be used, a DestinationRule must be configured:\napiVersion: \u0026#34;networking.istio.io/v1alpha3\u0026#34; kind: \u0026#34;DestinationRule\u0026#34; metadata: name: \u0026#34;preference-destination-rule\u0026#34; spec: host: \u0026#34;preference\u0026#34; trafficPolicy: tls: mode: ISTIO_MUTUAL (1) 1 Let’s use mTLS This defines that ISTIO_MUTUAL shall be used for the service preference. The customer service recognizes this and automatically enables mTLS. After a few minutes the traffic graph in Kiali will show \u0026#34;green\u0026#34; traffic from customer through preference to _recommendation:\nFigure 3. Kiali: traffic for Service Mesh components is fine again. Mutual TLS Migration As you can see in the previous section, the curl pod cannot reach the application inside the Service Mesh. This happens because prefernce is strictly enforcing encrypted traffic, but curl only sends plain text. Luckily, Istio provides a method to gradually monitor the traffic and migrate to mTLS. Instead of STRICT mode PERMISSIVE can be used. Enabling permissive mode, preference will accept both, encrypted and plain-text traffic.\nReplace the Policy object with the following configuration:\napiVersion: \u0026#34;authentication.istio.io/v1alpha1\u0026#34; kind: \u0026#34;Policy\u0026#34; metadata: name: \u0026#34;preference-mutualtls\u0026#34; spec: targets: - name: preference peers: - mtls: mode: PERMISSIVE oc replace -f Policy-permissive.yaml Now let’s wait a few minutes and observe Kiali, which should end up with:\nFigure 4. Kiali: Encrypted and Plain-Text traffic As you can see with the lock icon, the traffic between cunstomer and preference is encrypted, while the traffic from unknown (which is our curl pod), is plain-text.\nThe errors you may see in Kiali happen due a known issue: https://issues.jboss.org/browse/MAISTRA-1000 Cleanup Clean up your environment:\noc delete policy -n tutorial preference-mutualtls oc delete destinationrule -n tutorial preference-destination-rule "},{"uri":"https://blog.stderr.at/tags/fault-injection/","title":"Fault Injection","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/fault-injection/","title":"Fault Injection","tags":["Istio","Service Mesh","OpenShift","OCP","Fault Injection"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 8 - Faul Injection/Chaos Testing","content":" Tutorial 8 of OpenShift 4 and Service Mesh tries to cover Fault Injection by using Chaos testing method to verify if your application is running. This is done by adding the property HTTPFaultInjection to the VirtualService. The settings for this property can be for example: delay, to delay the access or abort, to completely abort the connection.\n\u0026#34;Adopting microservices often means more dependencies, and more services you might not control. It also means more requests on the network, increasing the possibility for errors. For these reasons, it’s important to test your services’ behavior when upstream dependencies fail.\u0026#34; [1]\nPreparation Before we start this tutorial, we need to clean up our cluster. This is especially important when you did the previous training Limit Egress/External Traffic.\noc delete deployment recommendation-v3 oc scale deployment recommendation-v2 --replicas=1 oc delete serviceentry worldclockapi-egress-rule oc delete virtualservice worldclockapi-timeout Verify that 2 pods for the recommendation services are running (with 2 containers)\noc get pods -l app=recommendation -n tutorial NAME READY STATUS RESTARTS AGE recommendation-v1-69db8d6c48-h8brv 2/2 Running 0 4d20h recommendation-v2-6c5b86bbd8-jnk8b 2/2 Running 0 4d19h Abort Connection with HTTP Error 503 For the first example, we will need to modify the VirtualService and the DestinationRule. The VirtualService must be extended with a http fault section, which will abort the traffic 50% of the time.\nCreate the VirtualService\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - fault: abort: httpStatus: 503 percent: 50 route: - destination: host: recommendation subset: app-recommendation Apply the change\noc replace -f VirtualService-abort.yaml Existing VirtualService with the name recommendation will be overwritten. Create the DestinationRule\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation subsets: - labels: app: recommendation name: app-recommendation Apply the change\noc replace -f destinationrule-faultinj.yaml Existing Destination with the name recommendation will be overwritten. Check the traffic and verify that 50% of the connections will end with a 503 error:\nexport INGRESS_GATEWAY=$(oc get route customer -n tutorial -o \u0026#39;jsonpath={.spec.host}\u0026#39;) sh ~/run.sh 1000 $GATEWAY_URL Clean Up oc delete virtualservice recommendation Test slow connection with Delay More interesting, in my opinion, to test is a slow connection. This can be tested by adding the fixedDelay property into the VirtualService. Like in the example below, we will use a VirtualService. This time delay instead of abort is used. The fixDelay defines a delay of 7 seconds for 50% of the traffic.\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - fault: delay: fixedDelay: 7.000s percent: 50 route: - destination: host: recommendation subset: app-recommendation If you now send traffic into the application, you will see that some answers will have a delay of 7 seconds. Keep sending traffic in a loop.\nEven more visible it will be, when you goto \u0026#34;Distributed Tracing\u0026#34; at the Kiali UI, select the service recommendation and a small lookback of maybe 5min. You will find that some requests are very fast, while other will tage about 7 seconds.\nFigure 1. Jaeger with delayed traffic. Retry on errors If a microservice is answering with an error, Service Mesh/Istio will automatically try to reach another pod providing the service. These retries can be modified. In order to make everything visible, we will use Kiali to monitor the traffic.\nWe start by sending traffic into the application. This should be split evenly between v1 and v2 of the recommendation microservice\nsh ~/run.sh 1000 $GATEWAY_URL # 8329: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 11145 # 8330: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 9712 # 8331: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 11146 # 8332: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 9713 # 8333: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 11147 In Kiali this ia visible in the Graphs, using the settings: \u0026#34;Versioned app graph\u0026#34; and \u0026#34;Requests percentage\u0026#34;\nFigure 2. Traffic is split by 50% between recommendation v1 nd v2 As second step we need to enable the nasty mode for the microservice v2. This will simulate an outage, respoding with error 503 all the time. This change must be done inside the container:\noc exec -it $(oc get pods|grep recommendation-v2|awk \u0026#39;{ print $1 }\u0026#39;|head -1) -c recommendation /bin/bash Inside the container use the following command and exit the container again\ncurl localhost:8080/misbehave Kiali will now show that v1 will get 100% of the traffic, while v2 is shown as red. When you select the red square of v2 and then move the mouse over the red cross for the failing application, you will see that the pd itself is ready, but that 100% of the traffic is currently failing.\nFigure 3. Traffic for v2 is failing revert the change and fix v2 service\noc exec -it $(oc get pods|grep recommendation-v2|awk \u0026#39;{ print $1 }\u0026#39;|head -1) -c recommendation /bin/bash curl localhost:8080/behave Verify in Kiali that everything is \u0026#34;green\u0026#34; again and that the traffic is split by 50% between v1 and v2.\nSources [1]: Istio By Example - Fault Injection\n"},{"uri":"https://blog.stderr.at/tags/do410/","title":"DO410","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/2020/04/do410-ansible-and-ansible-tower-training-notes/","title":"DO410 Ansible and Ansible Tower training notes","tags":["Ansible","Ansible Tower","DO410"],"description":"Notes taken during the DO410 online training course","content":" Notes taken during Red Hat course D410 Ansible and Ansible Tower.\nAnsible installation make sure that libselinux-python is installed\nAnsible 2.7 requires python 2.6 or 3.5\nyum list installed python windows modules implemented in powershell\nansible requires at least .net 4.0\nConfiguration files Ansible searches for ansible.cfg in the following order:\n$ANSIBLE_CFG\nansible.cfg in the current directory\n$HOME/ansible.cfg\n/etc/ansible/ansible.cfg\nwhichever it finds first will be used.\nuse\nansible --version to see which config file is currently used. you can view/dump/see what changed with\nansible-config [list|dump|view] Default modules List all available modules via\nansible-doc -l For getting help on a specific module use\nansible-doc ping Ad-hoc commmands To display ansible output on a single line per host for easier readablility use the -o option\nansible all -m command -a /bin/hostname -o Use the raw module for directly executing commands on remote systems that do not have python installed.\nansible -m raw Custom Facts Ansible uses custom facts from /etc/ansible/facts.d/. Facts can be stored in .ini style or you can place executable scripts in this directory. The script needs to output JSON. Custom facts are available via ansible_facts.ansible_local.\nMagic variables available hostvars: variables defined for this host\ngroup_names: list of groups this host is a member of\ngroups: list of all groups and hosts in the inventory\ninventory_hostname: host name of the current host as configured in the inventory\nMatching hosts in the inventory Some examples on how to match hosts defined in the inventory\n\u0026#39;*.lab.com\u0026#39;: match all hosts starting with lab.com\n\u0026#39;lab,datacenter\u0026#39;: match all hosts either in lab or datacenter\n\u0026#39;datacenter*\u0026#39;: match all host and host groups starting with datacenter\n\u0026#39;lab,\u0026amp;datacenter\u0026#39;: match hosts in the lab and datacenter group\n\u0026#39;datacenter,!test.lab.com\u0026#39;: match all hosts in datacenter, except test.lab.com\nDynamic inventory Example scripts for dynamic inventories can be found at https://github.com/ansible/ansible/tree/devel/contrib/inventory.\nYou can use ansible-inventory to take a look a the current inventory as json. This also works for static inventories.\nInventories can be combined. Just create a directory containing a static inventory and script to create a dynamic inventory, ansible will happily execute the scripts and merge everything together.\nDebugging The following might be useful when debugging ansible roles and playbooks\nansible-playbook play.yml --syntax-check ansible-playbook play.yml --step ansible-playbook play.yml --start-at-task=\u0026#34;start httpd service\u0026#34; ansible-playbook --check play.yml ansible-playbook --check --diff play.yml Ansible Tower Notes on deploying and working with ansible tower.\nInstallation System requirements:\nat least 4GB of RAM\nactual requirement depends on forks variable\nrecommendation is 100MB memory for each for + 2GB of memory for tower services\n20GB of disk storage, at least 10GB in /var\nSteps for installing:\ndownload setup tar.gz from http://releases.ansible.com/ansible-tower/setup/\nset passwords in inventory\nrun ./setup.sh\nAuthentication Authentication settings can be changed under Settings / Authentication. E.g for configuring Azure AD authentication we are going to need\nan Azure AD oauth2 key and\na Azure AD oauth2 secret\nRBAC separate roles for organizations and inventories\nyou need to assign roles to organizations and inventories\nThe Tower Flow These are the steps to run playbooks against managed nodes in Tower:\nCreate an organization if required\nCreate users\nCreate teams and assign users\nCreate credentials for accessing managed nodes\nAssign credential to organization\nCreate credentials for accessing SCM repositories (e.g. git)\nAssign credentials to users or teams\nCreate a project\nAssign Teams to project\nCreate a job template for executing playbooks\nAnsible Roles support If the project includes a requirements.txt file in the roles/ folder, tower will automatically run\nansible-galaxy install -r roles/requirements.yml -p ./roles/ --force at the end of an update. So this could be used to include external dependencies (like SAP ansible roles).\nJob Templates Ansible playbooks are stored in GIT repositories. A job template defines\nthe inventory used for this job template\nthe project for executing this job\nthis connects the GIT repository used in this project with the template\nthe playbook to execute\nthe credentials for executing jobs\npermissions for users / teams (e.g. admin, execute)\nTower creates jobs from those templates, which are ansible runs executed against managed nodes.\nFact Caching It might be a good idea to use the tower facts cache. To speed up playbook runs set gather_facts: no in the play. Then enable the facts cache in tower.\nIn tower settings set a timeout for the cache\nIn job templates enable Use facts cache\nCreate a playbook that runs on a regular basis to gather facts, e.g.\n- name: Refresh fact cache hosts: all gather_facts: yes Inventory options These are the options for creating inventories in Ansible Tower\nstatic inventory defined in tower\nimporting static inventories via awx-manage\nstatic inventory defined in git repository\ndynamic inventory via a custom script\ndynamic inventory provides by tower (e.g. satellite)\nA special feature in Tower are so called smart inventories. A smart inventory combines all static and dynamic inventories and allows filtering based on facts. Filtering requires a valid fact cache.\nTroubleshooting Tower uses the following components:\npostgresql\nnginx\nmemcached\nrabbitmq\nsupervisord\nUseful tools\nansible-tower-service (e.g. status / restart)\nsupervisorctl (e.g. status)\nawx-manage\nTower stores log files in\n/var/log/tower/ (e.g. tower.log).\n/var/log/supervisor/\n/var/log/nginx/\nOther important directories\n/var/lib/awx/public/static static files served by django\n/var/lib/awx/projects stores all project related files e.g. git checkouts)\n/var/lib/awx/jobs_status job status output\nby default playbook runs are confined to /tmp this might lead to problems with tasks running on the local system. In case of a lost admin password you can use awx-manage to reset the password or create a new superuser:\nawx-manage changepassword admin awx-manage createsuperuser Replacing the default TLS certificates Ansible tower uses nginx to service it’s web interface over TLS. Nginx uses the configuration file /etc/nginx/nginx.conf.\nTo deploy custom TLS certificates used by tower replace the certificate and private key in /etc/tower. You have to replace\n/etc/tower/tower.crt and\n/etc/tower/tower.key\nIt might be a good idea to create a backup copy before overwriting those files.\nBackup and restore Of course backup and restore are done via ansible. The ansible tower setup script setup.sh provides a wrapper around these playbooks. Execute\nsetup.sh -b to perform a backup. This creates a backup .tar.gz file in the current directory.\nTo restore a backup use\nsetup.sh -r this restores the latest backup per default.\nThings to remember Workflow job templates\nadd autocmd FileType yaml setlocal ai ts=2 sw=2 et to .vimrc\nuse sudo yum install python-cryptography if there are many vault files to speed up ansible\n"},{"uri":"https://blog.stderr.at/tags/egress/","title":"Egress","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/limit-egress/external-traffic/","title":"Limit Egress/External Traffic","tags":["Istio","Service Mesh","OpenShift","OCP","Egress"],"description":"OpenShift 4.x and Service Mesh/istio Tutorial 7 - Test and control your egress/external traffic.","content":" Sometimes services are only available from outside the OpenShift cluster (like external API) which must be reached. Part 7 of OpenShift 4 and Service Mesh takes care and explains how to control the egress or external traffic. All operations have been successdully tested on OpenShift 4.3.\nPreparation Before this tutorial can be started, ensure that 3 microservices are deployed (recommendation may have 2 versions) and that the objects Gateway and VirtualService are configured. The status should be like in Issue #4..6\nYou can verify this the following way:\nexport GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) curl $GATEWAY_URL which should simply print:\ncustomer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 7123 Setup recommendation-v3 We need to deploy version 3 of our recommendation microservice. This will perform an external API call to http://worldclockapi.com to retrieve the current time.\nTo deploy the Deployment v3:\ncd ~/istio-tutorial/recommendation oc apply -f kubernetes/Deployment-v3.yml -n tutorial If you list the pods at this moment, you will see that only one container (Ready 1/1) is started. This happens because the Deployment yaml file is missing an annotation. Fixing missing proxy sidecar container After you applied the Deployment-v3.yml, only 1 container is started. The proxy sidecar is not injected, because an annotation is missing in the configuration for the Deployment.\nTo fix this use the following command:\noc patch deployment recommendation-v3 -n tutorial -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;sidecar.istio.io/inject\u0026#34;:\u0026#34;true\u0026#34;}}}}}\u0026#39; This will automatically restart the pod with 2 containers.\nCreate DestinationRule and VirtualService Use the following definition to create (overwrite) the DestinationRule for recommendation-v3.\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation subsets: - labels: version: v3 name: version-v3 Only version 3 is used for now. The other versions are still there, but ignored for our tests. Apply the change\noc apply -f DestinationRule_v3.yaml Define the VirtualService and send 100% of the traffic to v3 of the recommendation microservice.\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - route: - destination: host: recommendation subset: version-v3 weight: 100 As an alternative, you can also edit the existing VirtualService and add the section for version-v3 with a weight of 100, while changing the weight of v1 and v2 to 0. Test egress traffic As usual we test our application by sending traffic to it. The following command should print successful connection requests:\nsh ~/run.sh 1000 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 1 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 2 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 3 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 4 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 5 As you can see 100% of the traffic is sent to v3 AND a new field enters the output. The current time is now shown as well. The information for this field is fetched with an external API call to http://worldclockapi.com.\nThe traffic is simply sent to an external destination. There is not limit yet. Readers of the Istio documentation will miss the object ServiceEntry which somebody should think is required. However, Openshift is currently(?) configured in a way to simply allow ANY traffic. This is defined in a ConfigMap which might be changed to modify the default behavior. However, as soon as ServiceEntry and the appropriate VirtualService is configured, the traffic will be limited as well. Limit/Control external access As you can see above you can simply send egress traffic without any control about what is allowed or not. In order to limit your outgoing traffic a new object called ServiceEntry must be defined as well as a change in your VirtualService will be required.\nDefine the ServiceEntry and apply it to your cluster:\napiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: worldclockapi-egress-rule spec: hosts: - worldclockapi.com ports: - name: http-80 number: 81 (1) protocol: http 1 Wrong port 81 is set on purpose for demonstration The port number: 81 is set on purpose, to prove that the traffic will not work with a wrong ServiceEntry. oc create -f ServiceEntry.yaml To actually limit the traffic a link between the ServiceEntry and a VirtualService, which defines the external destination, must be created. Moreover, a timeout is set for possible connection errors, to keep the application responding even when the external API is down.\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: worldclockapi-timeout (1) spec: hosts: - worldclockapi.com (2) http: - timeout: 3s (3) route: - destination: host: worldclockapi.com weight: 100 (4) 1 The name of the object 2 The external hostname we want to reach 3 The timeout setting in seconds 4 The destination route, which is sending 100% of the external traffic to the host above oc apply -f VirtualService-worldclockapi.yaml If you now run a connection test you will still get an error.\nsh ~/run.sh 1 $GATEWAY_URL # customer =\u0026gt; Error: 503 - preference =\u0026gt; Error: 500 ... Fix ServiceEntry This happens, because we misconfigured the ServiceEntry on purpose to demonstrate that the traffic is sent to worldclockapi.com:80.\nFix the ServiceEntry object and apply to your cluster:\napiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: worldclockapi-egress-rule spec: hosts: - worldclockapi.com ports: - name: http-80 number: 80 (1) protocol: http 1 Changed from 81 to 80 oc apply -f ServiceEntry.yaml Now the traffic should work and gives you back a connection to microservice and a current time:\nsh ~/run.sh 10 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 138 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 139 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 140 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 141 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 142 # 5: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 143 # 6: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 144 Verify Kiali Figure 1. Kiali shows traffic to the external service OPTIONAL: Disallow ANY connections This is a change in the default ConfigMap of the ServiceMesh. Do this on your own risk and always consult the latest documentation of OCP. As explained above, we are able to connect to an external service without any limitation. The ServiceEntry object together with the VirtualService define the actual destination and would disallow traffic if they are wrongly configured, but if you forget these entries, it would still be possible to establish an egress connection.\nIn OpenShift a ConfigMap in the istio-system namespace defines the default behavior. There are two possibilities:\nALLOW_ANY - outbound traffic to unknown destinations will be allowed, in case there are no services or ServiceEntries for the destination port\nREGISTRY_ONLY - restrict outbound traffic to services defined in the service registry as well\nLet’s Cleanup the ServiceEntry and the VirtualService which have been created above\noc delete serviceentry worldclockapi-egress-rule serviceentry.networking.istio.io \u0026#34;worldclockapi-egress-rule\u0026#34; deleted oc delete virtualservice worldclockapi-timeout virtualservice.networking.istio.io \u0026#34;worldclockapi-timeout\u0026#34; deleted Now traffic to the external service will be allowed again Modify the ConfigMap istio in the namespace istio-system\noc get configmap istio -n istio-system -o yaml | sed \u0026#39;s/mode: ALLOW_ANY/mode: REGISTRY_ONLY/g\u0026#39; | oc replace -n istio-system -f - Wait a few seconds and try to connect. You will see that the connection is not possible anymore.\nIf you now re-create the ServiceEntry the connection will be possible again, since the service is registered to the Service Mesh. "},{"uri":"https://blog.stderr.at/service-mesh/2020/04/advanced-routing-example/","title":"Advanced Routing Example","tags":["Istio","Service Mesh","OpenShift","OCP","Grayscale","Canary","DestinationRule","Mirror","Loadbalancer"],"description":"OpenShift 4.x and Service Mesh/istio Tutorial 6 - Advanced Routing. Try routing traffic based on canary, mirroring or loadbalancing traffic.","content":" Welcome to part 6 of OpenShift 4 and Service Mesh Advanced routing, like Canary Deployments, traffic mirroring and loadbalancing are discussed and tested. All operations have been successdully tested on OpenShift 4.3.\nAdvanced Routing During Issue #5 some simple routing was implemented. The traffic was split by 100% to a new version (v2) of the recommendation microservice. This section shall give a brief overview of advanced routing possibilities.\nCanary Deployments A canary deployment is a strategy to roll out a new version of your service by using traffic splitting. A small amount of traffic (10%) will be sent to the new version, while most of the traffic will be sent to the old version still. The traffic to the new version can be analysed and if everything works as expected more and more traffic can be sent to the new version.\nTo enable split traffic, the VirtualService must be update:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - route: - destination: host: recommendation subset: version-v1 weight: 90 - destination: host: recommendation subset: version-v2 weight: 10 Apply the change\noc apply -f VitualService_split_v1_and_v1.yaml Test the traffic and verify that 10% will be sent to v2\nsh ~/run.sh 100 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1060 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1061 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 2060 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1062 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1063 ... If an error is shown, then you most probably forget to configure the DestinationRule as described here. Figure 1. Kiali split traffic 90/10 Routing based on user-agent header It is possible to send traffic to different versions based on the browser type which is calling the application. In our test application the service customer is setting the header baggage-user-agent and propagates it to the other services.\n\u0026gt;\u0026gt; headers.putSingle(\u0026#34;baggage-user-agent\u0026#34;, userAgent); Create the following file\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - match: - headers: baggage-user-agent: regex: .*Safari.* route: - destination: host: recommendation subset: version-v2 - route: - destination: host: recommendation subset: version-v1 and apply the change\noc apply -f VitualService_safari.yaml In order to test the result, either use the appropriate browser or use curl to set the user-agent. As expected, request from Safari are sent to v2, other are sent to v1.\nSafari\ncurl -v -A Safari $GATEWAY_URL [...] \u0026gt; User-Agent: Safari [...] customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 2365 Firefox\ncurl -v -A Firefox $GATEWAY_URL [...] \u0026gt; User-Agent: Firefox [...] customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3762 Mirroring Traffic Mirroring Traffic, aka Dark Launch, will duplicate the traffic to another service, allowing you to analyse it before sending production data to it. Responses of the mirrored requests are ignored.\nRun the following command and be sure that recommendation-v1 and recommendation-v2 are both running:\noc get pod -n tutorial| grep recommendation recommendation-v1-69db8d6c48-h8brv 2/2 Running 0 24h recommendation-v2-6c5b86bbd8-jnk8b 2/2 Running 0 23h Update the VirtualService, so that version v2 will receive mirrored traffic, while the actual request will be sent to v1:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - route: - destination: host: recommendation subset: version-v1 mirror: (1) host: recommendation subset: version-v2 1 This must be set to \u0026#39;mirror\u0026#39; Apply the change\noc apply -f VitualService_mirrored-traffic.yaml Now lets open and follow the logs of recommandation-v2 in order to see that traffic will reach this service, but responses are ignored:\noc logs -f $(oc get pods|grep recommendation-v2|awk \u0026#39;{ print $1 }\u0026#39;) -c recommendation In a second terminal window send some traffic to our service.\nsh ~/run.sh 100 $GATEWAY_URL You will see that only v1 answers, while in the 2nd window, v2 gets the same traffic.\nLoad Balancing In the default OpenShift environment the kube-proxy forwards all requests to pods randomly. With Red Hat ServiceMesh it is possible to add more complexity and let the Envoy proxy handle load balancing for your services.\nThree methods are supported:\nrandom\nround-robin\nleast connection\nThe round robin function is used by default, when there is no DestinationRule configured. We can use the DestinationRule to use the least connection option to see how the traffic is sent.\nBefore we start we need to delete the VirtualService for the recommendation microservice\noc delete virtualservice recommendation The we scale version v2 to 3:\noc scale deployment recommendation-v2 --replicas=3 After a few seconds the folling pods should run now:\nNAME READY STATUS RESTARTS AGE customer-6948b8b959-jdjlg 2/2 Running 1 25h preference-v1-7fdb89c86b-nktqn 2/2 Running 0 25h recommendation-v1-69db8d6c48-h8brv 2/2 Running 0 25h recommendation-v2-6c5b86bbd8-6lgz6 2/2 Running 0 91s recommendation-v2-6c5b86bbd8-dnc8b 2/2 Running 0 91s recommendation-v2-6c5b86bbd8-jnk8b 2/2 Running 0 24h If you send traffic to the application, you would see that 3 quarter are sent to v1 and one is sent to v1.\nWith the following DestinationRule the traffic will be sent randomly to the application\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation trafficPolicy: loadBalancer: simple: RANDOM If you now sent traffic to the service, you will see that the traffic is sent randomly to the versions. (verify the serial number)\nsh ~/run.sh 100 $GATEWAY_URL # 140: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 5729 # 141: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 7119 # 142: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 361 # 143: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 362 # 144: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 5730 # 145: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 362 # 146: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 7120 # 147: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 7121 # 148: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 363 ... "},{"uri":"https://blog.stderr.at/tags/canary/","title":"Canary","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/destinationrule/","title":"DestinationRule","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/grayscale/","title":"Grayscale","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/loadbalancer/","title":"Loadbalancer","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/mirror/","title":"Mirror","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2020-04-01-oc-commands/","title":"Helpful oc / kubectl commands","tags":["oc","kubectl","OpenShift","OCP"],"description":"Openshift 4.x - Collection of usefull oc/kubectl commands.","content":" This is a list of useful oc and/or kubectl commands so they won’t be forgotton. No this is not a joke…​\nList all pods in state Running oc get pods --field-selector=status.phase=Running List all pods in state Running and show there resource usage oc get pods --field-selector=status.phase=Running -o json|jq \u0026#34;.items[] | {name: .metada ta.name, res: .spec.containers[].resources}\u0026#34; List events sort by time created oc get events --sort-by=\u0026#39;.lastTimestamp\u0026#39; Explain objects while specifing the api version Sometimes when you run oc explain you get a message in DESCRIPTION that this particular version is deprecated, e.g. you are running oc explain deployment and get\nDESCRIPTION: DEPRECATED - This group version of Deployment is deprecated by apps/v1/Deployment. See the release notes for more information. Deployment enables declarative updates for Pods and ReplicaSets. Note the DEPRECTATED message above if you want to see the documentation for the object that has not been deprecated you can use\noc explain deployment --api-version=apps/v1 Magic with oc set oc set is actually a very versatile command. Studying oc set -h is a good idea, here are some examples\nSet route weights when alternateBackends in a route are defined oc set route-backends bluegreen blue=1 green=9 Set resources on the command line oc set resources dc cakephp-mysql-example --limits=memory=1Gi,cpu=200m "},{"uri":"https://blog.stderr.at/service-mesh/2020/04/routing-example/","title":"Routing Example","tags":["Istio","Service Mesh","OpenShift","OCP","Grayscale","Canary","DestinationRule"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 5 - Routing Examples. Learn about routing and create your first definitions for VirtualService and DesitnationRule","content":" In part 5 of the OpenShift 4 and Service Mesh tutorials, basic routing, using the objects VirtualService and DesitnationRule, are described. All operations have been successfully tested on OpenShift 4.3.\nSome Theory In this section another version of the recommendation microservice will be deployed. The traffic to the new version will be controlled with different settings of the VirtualService. Multiple scenarios can be realized with ServiceMesh. In general these are defined as follows ([1]):\nBlue-Green Deployments In a Blue-Green deployment the old version (green) is kept running, while a new version (blue) is deployed and tested. When testing is successful, the 100% of the traffic is switched to the new version. If there is any error, the traffic could be switched back to the green version.\nA/B Deployments A/B deployments, in difference to Blue-Green deployments, will enable you to try a new version of the application in a limited way in the production environment. It is possible to specify that the production version gets most of the user requests, while a limited number of requests is sent to the new version. This could be specified by location of the user for example, so that all users from Vienna are sent to the new version, while all others are still using the old version.\nCanary Deployments Canary releases can be used to allow a small, minimum amount of traffic to the new version of your application. This traffic can be increased gradually until all traffic is sent to the new version. If any issues are found, you can roll back and send the traffic to the old version.\nPrerequisites It is assumed that an OpenShift environment is up and running and that Issues #1 - #3 are done at least:\nOpenshift 4 and ServiceMesh 1 - Installation\nOpenshift 4 and ServiceMesh 2 - Deploy Microservices\nOpenshift 4 and ServiceMesh 3 - Ingress Traffic\nPrepare Simple Routing OPTIONAL: Build the recommendation microservice If you want to locally build the microservice, you must change the source code from version v1 to v2 the following way:\nOpen the file:\nistio-tutorial/recommendation/java/vertx/src/main/java/com/redhat/developer/demos/recommendation/RecommendationVerticle.java and change the following line from v1 to v2\nprivate static final String RESPONSE_STRING_FORMAT = \u0026#34;recommendation v2 from \u0026#39;%s\u0026#39;: %d\\n\u0026#34;; Now you can build the image:\ncd istio-tutorial/recommendation/java/vertx mvn package podman build -t example/recommendation:v2 . (1) 1 Note the v2 tag Create second deployment with version2 A deployment with our recommendation:v2 microservice must be created. A service object must not be created this time, as it already exists.\ncd ~/istio-tutorial/recommendation/ oc apply -f kubernetes/Deployment-v2.yml -n tutorial oc get pods -w If you want to diff v1 and v2 deployment, you will notice that the main change is the image which gets pulled.\ndiff recommendation/kubernetes/Deployment-v2.yml recommendation/kubernetes/Deployment.yml 6,7c6,7 \u0026lt; version: v2 \u0026lt; name: recommendation-v2 --- \u0026gt; version: v1 \u0026gt; name: recommendation-v1 13c13 \u0026lt; version: v2 --- \u0026gt; version: v1 18c18 \u0026lt; version: v2 --- \u0026gt; version: v1 27c27 \u0026lt; image: quay.io/rhdevelopers/istio-tutorial-recommendation:v2.1 --- \u0026gt; image: quay.io/rhdevelopers/istio-tutorial-recommendation:v1.1 Call application Execute the test command to access the application. Since no rules are defined yet, the traffic is split by 50% to version 1 and version 2 (round robin):\nsh ~/run.sh 10 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 27 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 27 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 28 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 28 ... In Kiali presents this as well:\nFigure 1. Kiali sends 50% to v1 and v2 Send all traffic to recommendation:v2 To route the traffic accordingly a DestinationRule and a VirtualService must be created for recommendation. While the DesinationRule will add a name to each version, VirtualService specifies the actual destination of the traffic.\nDefine DestinationRule for recommendation The object DestinationRule will define the versions in subsets.\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation subsets: - labels: version: v1 name: version-v1 - labels: version: v2 name: version-v2 Create the object with the command: oc create -f \u0026lt;filename\u0026gt;\nDefine VirtualService for recommendation The VirtualService defines that 100% (weight) of the traffic for recomendation (host) will be sent to the subset (version-v2), which is defined in the DefinationRule\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - route: - destination: host: recommendation subset: version-v2 weight: 100 Create the object with the command: oc create -f \u0026lt;filename\u0026gt;\nCall application If you now call the application, only traffic to v2 should be shown:\nsh ~/run.sh 1000 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 27 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 27 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 28 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 28 ... In Kiali presents this as well and send 100% of the traffic to recommendation:v2:\nFigure 2. Kiali sends 100% to v2 Sources [1]: DZone: Traffic Management With Istio\n"},{"uri":"https://blog.stderr.at/service-mesh/2020/03/ingress-with-custom-domain/","title":"Ingress with custom domain","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 4 - Use a custom domain to get your traffic into your Service Mesh","content":" Since Service Mesh 1.1, there is a better way to achieve the following. Especially the manual creation of the route is not required anymore. Check the following article to Enable Automatic Route Creation. Often the question is how to get traffic into the Service Mesh when using a custom domains. Part 4 our our tutorials series OpenShift 4 and Service Mesh will use a dummy domain \u0026#34;hello-world.com\u0026#34; and explains the required settings which must be done.\nModify Gateway and VirtualService Issue #3 explains how to get ingress traffic into the Service Mesh, by defining the Gateway and the VirtualService. We are currently using the default ingress route defined in the istio_system project. But what if a custom domain shall be used? In such case another route must be defined in the istio-system project and small configuration changes must be applied.\nFirst lets create a slightly modified Gateway.yaml:\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: ingress-gateway-exampleapp spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;hello-world.com\u0026#34; (1) 1 add you custom domain here The only difference is at the hosts which was changed from \u0026#39;*\u0026#39; to \u0026#39;hello-world.com\u0026#39;\nAs second change, the VirtualService must be modified as well with the custom domain:\nVirtualService.yaml:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: ingress-gateway-exampleapp spec: hosts: - \u0026#34;hello-world.com\u0026#34; (1) gateways: - ingress-gateway-exampleapp http: - match: - uri: exact: / route: - destination: host: customer port: number: 8080 1 add you custom domain here Replace current objects in OpenShift:\noc replace -f Gateway.yaml -n tutorial oc replace -f VirtualService.yaml -n tutorial Create a new route under the project istio-system:\napiVersion: route.openshift.io/v1 kind: Route metadata: name: hello-world.com (1) namespace: istio-system (2) spec: host: hello-world.com (3) to: kind: Service name: istio-ingressgateway (4) port: targetPort: 8080 1 add you custom domain here 2 the route must be created at istio-system 3 add you custom domain here 4 this is the service as it was created by the operator OPTIONAL: Add custom domain to local hosts file The custom domain hello-world.com must be resolvable somehow, pointing to the ingress router of OpenShift. This can be done, by adding the domain into the local hosts file (with all limitations this brings with it)\n# Get IP address of: oc -n istio-system get route istio-ingressgateway echo \u0026#34;x.x.x.x hello-world.com\u0026#34; \u0026gt;\u0026gt; /etc/hosts Create some example traffic We will reuse the script of Issue #3 to simulate traffic. Since we changed the domain, the connection will go to hello-world.com\nsh run-check.sh 1000 hello-world.com This will send 1000 requests to our application:\n# 0: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6626 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6627 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6628 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6629 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6630 # 5: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6631 ... "},{"uri":"https://blog.stderr.at/service-mesh/2020/03/ingress-traffic/","title":"Ingress Traffic","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 3 - Get traffic into your Service Mesh and use Kiali to make your traffic visible.","content":" Part 3 of tutorial series OpenShift 4 and Service Mesh will show you how to create a Gateway and a VirtualService, so external traffic actually reaches your Mesh. It also provides an example script to run some curl in a loop.\nConfigure Gateway and VirtualService Example With the microservices deployed during Issue #2, it makes sense to test the access somehow. In order to bring traffic into the application a Gateway object and a VirtualService object must be created.\nThe Gateway will be the entry point which forward the traffic to the istio ingressgateway\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: ingress-gateway-exampleapp spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; As 2nd object a VirtualService must be created:\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: ingress-gateway-exampleapp spec: hosts: - \u0026#34;*\u0026#34; gateways: - ingress-gateway-exampleapp http: - match: - uri: exact: / route: - destination: host: customer port: number: 8080 Get all istio-io related objects of your project. These objects represent the network objects of Service Mesh, like Gateway, VirtualService and DestinationRule (explained later)\noc get istio-io -n tutorial NAME HOST AGE destinationrule.networking.istio.io/recommendation recommendation 3d21h NAME AGE gateway.networking.istio.io/ingress-gateway 4d15h NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/ingress-gateway [ingress-gateway] [*] 4d15h Create some example traffic Before we start, lets fetch the default route of our Service Mesh:\nexport GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) This should return: istio-ingressgateway-istio-system.apps.\u0026lt;clustername\u0026gt;\nNow, let’s create a shell script to run some curl commands in a loop and can be easily reused for other scenarios:\n#!/bin/bash numberOfRequests=$1 host2check=$2 if [ $# -eq 0 ]; then echo \u0026#34;better define: \u0026lt;script\u0026gt; #ofrequests hostname2check\u0026#34; echo \u0026#34;Example: run.sh 100 hello.com\u0026#34; let \u0026#34;numberOfRequests=100\u0026#34; else let \u0026#34;i = 0\u0026#34; while [ $i -lt $numberOfRequests ]; do echo -n \u0026#34;# $i: \u0026#34;; curl $2 let \u0026#34;i=$((i + 1))\u0026#34; done fi Run the script and check the output:\nsh run-check.sh 1000 $GATEWAY_URL This will send 1000 requests to our application:\n# 0: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3622 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3623 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3624 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3625 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3626 # 5: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3627 ... Verify in Kiali To verify in Kiali our application, open the URL in your browser and login using your OpenShift credentials.\nIf you do not know the URL for Kiali, execute the following command oc get route kiali -n istio-system Switch the the Graph view and you should see the following picture:\nFigure 1. Kiali Graph "},{"uri":"https://blog.stderr.at/service-mesh/2020/03/deploy-microservices/","title":"Deploy Microservices","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 2 - Deploy some example microservices, which are used in future tutorials.","content":" The second tutorials explains how to install an example application containing thee microservices. All operations have been successfully tested on OpenShift 4.3.\nIntroduction As quickly explained at Issue #1, OpenShift 4.3 and the Service Mesh shall be installed already. At this point you should have all 4 operators installed and ready. The test application used in this scenario is based on Java and contains 3 microservices in the following traffic flow:\nCustomer ⇒ Preference ⇒ Recomandation\nThe microservices are the same as used at the Interactive Learning Portal.\nDeploy microservices First lets create a new project for our tests\noc new-project tutorial EITHER: Add tutorial to ServiceMeshMemberRoll\nService Mesh 1.1 now supports the object ServiceMember which can created under the application namespace and which automatically configures the ServiceMemberRoll. However, below description still works. + OpenShift will auto-inject the sidecar proxy to pods of a namespace, if the namespace is configured in the ServiceMeshMemberRoll object which was created for the operator.\nCreate the file memberroll.yaml:\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; memberroll.yaml apiVersion: maistra.io/v1 kind: ServiceMeshMemberRoll metadata: name: default spec: members: - tutorial EOF Add the namespace to the member roll:\noc apply -f memberroll.yaml -n istio-system If you already have namespaces configured for ServiceMeshMemberRoll, better modify the object manually. Custom Resource Definitions (CRD) do not like to be modified on the fly (currently?) OR: Create ServiceMeshMember object: Available since ServiceMesh 1.1\napiVersion: maistra.io/v1 kind: ServiceMeshMember metadata: name: default namespace: tutorial spec: controlPlaneRef: name: basic-install namespace: istio-system Download the tutorial application locally\ngit clone https://github.com/redhat-developer-demos/istio-tutorial/ Deploy microservice: customer To deploy the first microservice 2 objects (Deployment and Service) must be created. Moreover, the service will be exposed, since the service customer is our entry point. Verify ~/istio-tutorial/customer/kubernetes/Deployment.yml to check where the actual image is coming from: quay.io/rhdevelopers/istio-tutorial-customer:v1.1\ncd ~/istio-tutorial/customer oc apply -f kubernetes/Deployment.yml -n tutorial oc apply -f kubernetes/Service.yml -n tutorial oc expose service customer Check if pods are running with two containers:\noc get pods -w The result should look somehow like this:\nNAME READY STATUS RESTARTS AGE customer-6948b8b959-g77bs 2/2 Running 0 52m If there is only 1 container running, indicated by READY = 1/1, then most likely the ServiceMeshMemberRoll was not updated with the name tutorial or contains a wrong project name. Deploy microservice: preference The deployment of the microservice preference is exactly like it is done for customer, except that no service must be exposed:\ncd ~/istio-tutorial/preference/ oc apply -f kubernetes/Deployment.yml -n tutorial oc apply -f kubernetes/Service.yml -n tutorial Check if pods are running with two containers:\noc get pods -w The result should look somehow like this:\nNAME READY STATUS RESTARTS AGE customer-6948b8b959-g77bs 2/2 Running 0 4d15h preference-v1-7fdb89c86b-gkk5g 2/2 Running 0 4d14h Deploy microservice: recommendation The deployment of the microservice recommendation is exactly like it is done for preference:\ncd ~/istio-tutorial/recommendation/ oc apply -f kubernetes/Deployment.yml -n tutorial oc apply -f kubernetes/Service.yml -n tutorial Check if pods are running with two containers:\noc get pods -w The result should look somehow like this:\nNAME READY STATUS RESTARTS AGE customer-6948b8b959-g77bs 2/2 Running 0 4d15h preference-v1-7fdb89c86b-gkk5g 2/2 Running 0 4d14h recommendation-v1-69db8d6c48-p9w2b 2/2 Running 0 4d14h Optional: build the images It is possible (and probably a good training) to build the microservices locally to understand how this works. In order to achieve this the packages maven and podman must be installed.\nBuild customer Go to the source folder of customer application and build it:\ncd ~/projects/istio-tutorial/customer/java/springboot mvn package It will take a few seconds, but it should give \u0026#34;BUILD SUCCESS\u0026#34; as output, if everything worked.\nNow the image will be built using podman\npodman build -t example/customer . Build preference The image build process of the second microservice follows the same flow as customer:\ncd ~/istio-tutorial/preference/java/springboot mvn package podman build -t example/preference:v1 . the \u0026#34;v1\u0026#34; tag at the image name is important and must be used. Build recommendation The image build process of the third microservice follows the same flow as preference:\ncd ~/projects/istio-tutorial/recommendation/java/vertx mvn package podman build -t example/recommendation:v1 . the \u0026#34;v1\u0026#34; tag at the image name is important and must be used. Later other versions will be deployed. "},{"uri":"https://blog.stderr.at/service-mesh/2020/03/installation/","title":"Installation","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 1 - Installation and First Steps","content":" Everything has a start, this blog as well as the following tutorials. This series of tutorials shall provide a brief and working overview about OpenShift Service Mesh. It is starting with the installation and the first steps, and will continue with advanced settings and configuration options.\nOpenShift 4.x and ServiceMesh UPDATE: At 10th April 2020 Red Hat released Service Mesh version 1.1 which supports: Istio - 1.4.6\nKiali - 1.12.7\nJaeger - 1.17.1\nThe following tutorials for OpenShift Service Mesh are based on the official documentation: OpenShift 4.3 Service Mesh and on the Interactive Learning Portal. All operations have been successfully tested on OpenShift 4.3. Currently OpenShift supports Istio 1.4.6, which shall be updated in one of the future releases.\nTo learn the basics of Service Mesh, please consult the documentation as they are not repeated here.\nIt is assumed that OpenShift has access to external registries, like quay.io.\nOther resource I can recommend are:\nIstio By Example: A very good and brief overview of different topics by Megan O’Keefe.\nIstio: The Istio documentation\nPrerequisites At the very beginning OpenShift must be installed. This tutorial is based on OpenShift 4.3 and a Lab installation on Hetzner was used. Moreover, it is assumed that the OpenShift Client and Git are installed on the local system.\nDuring the tutorials an example application in the namespace tutorial will be deployed. This application will contain 3 microservices:\ncustomer (the entry point)\npreference\nrecommendation\nThis application is also used at the Interactive Learning Portal which can be tested there interactively. However, the training is still based on OpenShift version 3.\nInstall Red Hat Service Mesh To deploy Service Mesh on Openshift 4 follow the guide at Installing Red Hat OpenShift Service Mesh.\nIn short, multiple operators must be installed:\nElasticsearch\nJaeger\nKiali\nService Mesh\nElasticsearch is a very memory intensive application. Per default it will request 16GB of memory which can be reduced on Lab environments. Link Jaeger and Grafana to Kiali In the lab environment it happened that Kiali was not able to auto detect Grafana or Jaeger. This is visible when the link Distributed Tracing is missing in the left menu.\nTo fix this the ServiceMeshControlPlane object in the istio-system namespace must be updated with 3 lines:\noc edit ServiceMeshControlPlane -n istio-system kiali: # ADD THE FOLLOWING LINES dashboard: grafanaURL: https://grafana-istio-system.apps.\u0026lt;your clustername\u0026gt; jaegerURL: https://jaeger-istio-system.apps.\u0026lt;your clustername\u0026gt; This change will take a few minutes to be effective.\n"},{"uri":"https://blog.stderr.at/tags/aap/","title":"AAP","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/anit-affinity/","title":"Anit-Affinity","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/aro/","title":"ARO","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/automation/","title":"Automation","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ci/cd/","title":"CI/CD","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/cicd/","title":"CICD","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/compliance/security/","title":"Compliance/Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/rss/","title":"RSS Feeds","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/sealed-secrets/","title":"Sealed Secrets","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/style-guide/","title":"Style Guide","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/tollerations/","title":"Tollerations","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/topology-spread-contstraints/","title":"Topology Spread Contstraints","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/tower/","title":"Tower","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/vpn/","title":"VPN","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/yaml/","title":"YAML","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/","title":"YAUB Yet Another Useless Blog","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector","Taints","Tollerations","Topology Spread Contstraints","Descheduler","Affinity","Anit-Affinity","Pipelines","OCP","Tekton","GitOps","Operator","Grafana","Thanos","Sealed Secrets","Storage","Vault","oc","kubectl","SSL","Cert Manager","Pipleines","CI/CD","Supply Chain","Rekor","cosign","SBOM","ACS","stackrox","SSL","SSO","Ansible","Automation","AAP","istio","Service Mesh","Azure","Compliance","Security"],"description":"","content":"Welcome to YAUB - Yet Another Useless Blog The articles in this blog shall help to easily tests and understand specific issues so they can be reproduced and tested on local environments.\nYou can find the most recent posts on this site or walk through the different categories via the left navigation.\n"}]