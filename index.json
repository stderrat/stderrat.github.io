[{"uri":"https://blog.stderr.at/day-2/etcd/","title":"etcd","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/labels_environmets/","title":"Labels &amp; Environments","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/","title":"OpenShift","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/","title":"OpenShift Day-2","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/pod-placement/","title":"Pod Placement","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/compliance/","title":"Compliance","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/","title":"Service Mesh","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/general/","title":"General","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/","title":"Ansible","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/acs/","title":"Advanced Cluster Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/azure/","title":"Azure","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/java/","title":"Java","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/quay/","title":"Quay","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/archive/","title":"Archive","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/legaldisclosure/","title":"Legal Disclosure","tags":[],"description":"","content":" Legal Disclosure Information in accordance with Section 5 TMG\n Toni Schmidbauer \u0026amp; Thomas Jungbauer 1200 Vienna, Austria\n Contact Information E-Mail:\n       Internet address: https://blog.stderr.at/\n  Disclaimer Accountability for content The contents of our pages have been created with the utmost care. However, we cannot guarantee the contents\u0026#39; accuracy, completeness or topicality. According to statutory provisions, we are furthermore responsible for our own content on these web pages. In this matter, please note that we are not obliged to monitor the transmitted or saved information of third parties, or investigate circumstances pointing to illegal activity. Our obligations to remove or block the use of information under generally applicable laws remain unaffected by this as per §§ 8 to 10 of the Telemedia Act (TMG).\n Accountability for links Responsibility for the content of external links (to web pages of third parties) lies solely with the operators of the linked pages. No violations were evident to us at the time of linking. Should any legal infringement become known to us, we will remove the respective link immediately.\n Copyright Our web pages and their contents are subject to German copyright law. Unless expressly permitted by law, every form of utilizing, reproducing or processing works subject to copyright protection on our web pages requires the prior consent of the respective owner of the rights. Individual reproductions of a work are only allowed for private use. The materials from these pages are copyrighted and any unauthorized use may violate copyright laws.\n Quelle: http://translate-24h.de\n    "},{"uri":"https://blog.stderr.at/tags/acs/","title":"ACS","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/acs/","title":"ACS","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/advanced-cluster-security/","title":"Advanced Cluster Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/acs/2021-12-11-acsauth/","title":"Advanced Cluster Security - Authentication","tags":["ACS","Advanced Cluster Security","OpenShift","Security","Keycloak","Authentication","SSO","Stackrox"],"description":"Red Hat Advanced Cluster Security - Authentication","content":"Red Hat Advanced Cluster Security (RHACS) Central is installed with one administrator user by default. Typically, customers request an integration with existing Identity Provider(s) (IDP). RHACS offers different options for such integration. In this article 2 IDPs will be configured as an example. First OpenShift Auth and second Red Hat Single Sign On (RHSSO) based on Keycloak\n Prerequisites  OpenShift 4 Cluster\n  Advanced Cluster Security v3.66+\n  Red Hat SSO Operator installed\n       While RHSSO will be installed during this article, only default and example values are used. These are by no means examples for a production system.       Introduction Advanced Cluster Security comes with several default roles, which can be assigned to users:\n     System role Description     Admin\n This role is targeted for administrators. Use it to provide read and write access to all resources.\n   Analyst\n This role is targeted for a user who cannot make any changes, but can view everything. Use it to provide read-only access for all resources.\n   Continuous Integration\n This role is targeted for CI (continuous integration) systems and includes the permission set required to enforce deployment policies.\n   None\n This role has no read and write access to any resource. You can set this role as the minimum access role for all users.\n   Sensor Creator\n Red Hat Advanced Cluster Security for Kubernetes uses this role to automate new cluster setups. It includes the permission set to create Sensors in secured clusters.\n   Scope Manager\n This role includes the minimum permissions required to create and modify access scopes.\n        It is possible to create custom roles.       Configure RHACS Authentication: OpenShift Auth     It is assumed that RHACS is already installed and login to the Central UI is available.      Login to your RHACS and select “Platform Configuration” \u0026gt; “Access Control”\n  From the drop down menu Add auth provider select OpenShift Auth\n Figure 1. ACS Auth Provider    Enter a Name for your provider and select a default role which is assigned to any user who can authenticate.\nIt is recommended to select the role None, so new accounts will have no privileges in RHACS.\n With Rules you can assign roles to specific users, based on their userid, name, mail address or groups.\n For example the user with the name poweruser gets the role Admin assigned.\n      Verify Authentication with OpenShift Auth  Logout from the Central UI and reload the browser.\n  Select from the drop down OpenShift Auth\n Figure 2. ACS Login    Try to login with a valid OpenShift user. Depending on the Rules which have been defined during previous steps the appropriate permissions should be assigned. For example: If you login as user poweruser the role Admin is assigned.\n      Configure Red Hat Single Sign On The following steps will create some basic example objects to an existing RHSSO or Keycloak to test the authentication at RHACS. Skip to step #5 if you have Keycloak already up and running and would like to reuse an existing client.\n The RHSSO operator (or Keycloak) is installed at the namespace single-sign-on.\n  Create an instance of Keycloak\napiVersion: keycloak.org/v1alpha1 kind: Keycloak metadata: name: example-keycloak namespace: single-sign-on spec: externalAccess: enabled: true instances: 1     Create a Realm This will create a Realm called Basic\napiVersion: keycloak.org/v1alpha1 kind: KeycloakRealm metadata: name: example-keycloakrealm namespace: single-sign-on spec: instanceSelector: matchLabels: app: sso realm: displayName: Basic Realm enabled: true id: basic realm: basic     Login into Red Hat SSO Get the route to your RHSSO instance:\noc get route keycloak -n single-sign-on --template=\u0026#39;{{ .spec.host }}\u0026#39; # keycloak-single-sign-on.apps.cluster-29t8z.29t8z.sandbox677.opentlc.com   and log into the Administration Interface.\n   Extract the admin password for Keycloak\nThe secret name is build from \u0026#34;credential\u0026#34;\u0026lt;keycloak-instance-name\u0026gt;\n oc extract secret/credential-example-keycloak -n single-sign-on --to=- # ADMIN_PASSWORD \u0026lt;you password\u0026gt; # ADMIN_USERNAME admin     Be sure to select your Realm (Basic in our case), goto Clients and select a ClientID.\n In this example we select account\n Figure 3. ACS Login      Of course you can create or use any other Client.       Enable the option Implicit Flow\n     Get the Issuer URL from your realm. This is typically your: https://\u0026lt;KEYCLOAK_URL\u0026gt;/auth/realms/\u0026lt;REALM_NAME\u0026gt;;\nFor Example: https://keycloak-single-sign-on.apps.cluster-29t8z.29t8z.sandbox677.opentlc.com/auth/realms/basic\n      Create Test Users In RHSSO create 2 user accounts to test the authentication later.\n  Goto Users and create the users:\n User: acsadmin\nFirst Name: acsadmin\n   User: user1\nFirst Name: user 1\n       You can set any other values for these users. However, be sure to set a password for both, after they have been created.\n   Configure RHACS Authentication: RHSSO     It is assumed that RSACS is already installed and login to the Central UI is available.      Login to your RHACS and select “Platform Configuration” \u0026gt; “Access Control”\n  From the drop down menu Add auth provider select OpenID Connect\n Enter a “Name” for your provider i.e. “Single Sign On”\n  Leave the “Callback Mode” to the “Auto-Select” setting\n  Enter your Issuer URL\n  As Client ID enter account (or the ClientID you would like to use)\n  Leave the Client Secret empty and select the checkbox Do not use Client Secret which is good enough for our tests.\nRemember the two callback URL from the blue box. They must be configured in Keycloak.\n   Select a default role which is assigned to any user who can authenticate.\nIt is recommended to select the role None, so new accounts will have no privileges in RHACS.\n   With Rules you can assign roles to specific users, based on their userid, name, mail address or groups.\n  For example the user with the name acsadmin (which have been created previously in our RHSSO) gets the role Admin assigned.\n      The final settings are depict in the following image:\n  Figure 4. ACS Login    Continue RHSSO Configuration What is left to do is the configuration of redirect URLs. These URLs are shown in the ACS Authentication Provider configuration (see blue field in the image above)\n  Log back into RHSSO and select “Clients” \u0026gt; “account”\n  Into Valid Redirect URLs enter the two URLs which you saved from the blue box in the RHACS configuration.\n     Troubleshoot: Test Login In RHACS you can test the login to you SSO.\n  Goto \u0026#34;Platform Configuration\u0026#34; \u0026gt; \u0026#34;Access Control\u0026#34;\n  Click the button \u0026#34;Test login\u0026#34;\nA popup will appear which asks you to enter SSO credentials. The connection to RHSSO will be validated:\n  Figure 5. ACS Test SSO       Verify Authentication with OpenShift Auth  Logout from the Central UI and reload the browser.\n  Select from the drop down Single Sign On\n Figure 6. ACS Login SSO    Try to login with a valid SSO user. Depending on the Rules which have been defined during previous steps the appropriate permissions should be assigned. For example: If you login as user acsadmin the role Admin is assigned.\n    "},{"uri":"https://blog.stderr.at/tags/authentication/","title":"Authentication","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/keycloak/","title":"Keycloak","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/openshift/","title":"OpenShift","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/openshift/","title":"OpenShift","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/security/","title":"Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/security/","title":"Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/sso/","title":"SSO","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/stackrox/","title":"Stackrox","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ansible/","title":"Ansible","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/ansible/","title":"Ansible","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/2021/11/ansible-style-guide/","title":"Ansible Style Guide","tags":["Ansible","Controller","Automation Controller","Style","StyleGuide","Best Practices","Playbook","Role"],"description":"Howto write and debug Ansible playbooks","content":"You should always follow the Best Practices and Ansible Lint rules defined by the Ansible documentation when developing playbooks.\n Although very basic, the Best Practices document gives a few guidelines to be able to carry out well-structured playbooks and roles, it contains recommendations that evolve with the project, so it is recommended to review it regularly. It is advisable to review the organization of content in Ansible.\n The Ansible Lint documentation shows us through this tool the syntax rules that will be checked in the testing of roles and playbooks, the rules that will be checked are indicated in this document in their respective section.\n     This style guide is meant as a base of playbook development. It is not the ultimate truth. Always verify and apply appropriate company rules. There are many additional references available, such as: https://github.com/whitecloud/ansible-styleguide or https://github.com/redhat-cop/automation-good-practices     General Recommendations   Treat your Ansible content like code: Any playbook, role, inventory or collection should be versionized using Git or a similar tool.\n  Iterate: start with basic playbook and refactor later\n  Use multiple Git repositories: on per role/collection, separate inventory from other repositories\n  Use human-meaningful names in inventory files\n  group hosts in inventory files\n  use a consistent Directory Structure\n     Optimize Playbook Execution  Disable facts gathering if it is not required.\n Try not to use: ansible_facts[‘hostname’] (or ‘nodename’)\n  Try to use inventory_hostname and inventory_hostname_short instead\n  It is possible to selectively gather facts (gather_subnet)\n  Consider caching facts\n     Increase Parallelism\n Ansible runs 1st task on every host, then 2nd task on every host …\n  Use “forks” (default is 5) to control how many connections can be active (load will increase, try first with conservative values)\n  While testing forks analize Enable CPU and Memory Profiling\n     If you use the module \u0026#34;copy\u0026#34; for large files: do not use “copy” module. Use “synchronize” module instead (based on rsync)\n  Use lists when ever a modules support it (i.e. yum) Instead of\n   tasks: - name: Ensure the packages are installed yum: name: \u0026#34;{{ item }}\u0026#34; state: present loop: - httpd - mod_ssl - httpd-tools - mariadb-server - mariadb - php - php-mysqlnd   use\n tasks: - name: Ensure the packages are installed yum: state: present name: - httpd - mod_ssl - httpd-tools - mariadb-server - mariadb - php - php-mysqlnd    Optimize SSH Connections\n in ansible.cfg set\n ControlMaster\n  ControlPersist\n  PreferredAuthentications\n        Enabling Pipelining\n Not enabled by default\n  reduces number of SSH operations\n  requires to disable requiertty in sudo options\n         Name Tasks and Plays Every task or play should be explicitly named in a way that the purpose is easily understood and can be referred to if the playbook has to be restarted from a certain task.\n For example the following is hard to read and debug:\n PLAY [localhost] ******************************** TASK [include_vars] ******************************** ok: [localhost] TASK [yum] ******************************** ok: [localhost]   While with naming it is easier to follow what is happening:\n PLAY [Create a new virtual machine] ******************************** TASK [Include vmware-credentials] ******************************** ok: [localhost] TASK [Install required packages with yum] ******************************** ok: [localhost]   # bad # set another variable - set_fact: my_second_var: \u0026#34;{{ my_var }}\u0026#34;   # good - name: set another variable set_fact: my_second_var: \u0026#34;{{ my_var }}\u0026#34;   Reason Better understanding what is currently happening in a play and better possibility to debug.\n    Variables in Task Names Include as much information as necessary to explain the purpose of a task. Make usage of variables inside a task name to create dynamic output messages.\n #bad - name: \u0026#39;Change status\u0026#39; service: enabled: true name: \u0026#39;httpd\u0026#39; state: \u0026#39;{{ state }}\u0026#39; become: true   #good - name: \u0026#39;Change status of httpd to {{ state }}\u0026#39; service: enabled: true name: \u0026#39;httpd\u0026#39; state: \u0026#39;{{ state }}\u0026#39; become: true   Reason This will help to easily understand log outputs of playbooks.\n    Omitting Unnecessary Information While name tasks in a playbook, do not include the name of the role which is currently executed, since Ansible will do this automatically.\n Reason Avoiding the same output twice on the console will prevent confusions.\n    Names All the newly created Ansible roles should follow the name convention using dashes if necessary: [company]-[action]-[function/technology]\n # bad lvm   # good mycompany-setup-lvm   Reason If using roles from Ansible Galaxy, it will keep consistency about which roles are created internally.\n    Use Modules instead of command or shell Before using the command or shell module, verify if there is already a module available which can avoid the usage of raw shell command.\n # bad - name: install httpd tasks: - command: \u0026#34;yum install httpd\u0026#34;   # good - name: install packages tasks: - name: \u0026#39;install httpd\u0026#39; yum: name: \u0026#39;httpd\u0026#39; state: \u0026#39;present\u0026#39;   Reason While raw command could be seen as a security risk in general, another reason to avoid them is the loss of immutability of the ansible playbooks or roles. Ansible cannot verify if a command has been already executed before or not and will therefore execute it every time the playbook is running.\n    Documenting a Task/Play Every playbook, role or task should start with a documentation why this code has been written and what it does and should include an example usage if applicable. The comment should be followed by ---` with no blank lines around it, to indicate the actual start of the yaml definition\n #bad - name: \u0026#39;Change httpd status\u0026#39; service: enabled: true name: \u0026#39;httpd\u0026#39; state: \u0026#39;{{ state }}\u0026#39; become: true   #good # Example usage: ansible-playbook -e state=started playbook.yml # This playbook changes the state of the httpd daemon --- - name: \u0026#39;Change httpd status\u0026#39; service: enabled: true name: \u0026#39;httpd\u0026#39; state: \u0026#39;{{ state }}\u0026#39; become: true   Reason This common programmatic practice helps to quickly understand the purpose and usage of a playbook or role.\n Lint rule Yamllint document-start rule\n    End a File Files should be ended with a newline.\n Reason This is common Unix best practice which avoids any prompt misalignment when printing files in a terminal.\n Lint rule Yamllint new-line-at-end-of-file rule\n    Quotes Strings should be quoted, while quotes for booleans (e.g. true/false) or integers (i.g. 42) should be avoided.\n Do NOT quote:\n   hosts: targets (e.g. hosts: databases rather than hosts: ‘databases’)\n  include_tasks: and include_roles: target file names\n  task and role names\n  registered variables\n  number values\n  boolean values\n       It is possible to use single or double quotes. In this document single quotes are used, as it seems to be more common. Double quotes are often seen in European countries, especially German speaking countries. The most important thing is to stick to one style.     # bad - name: \u0026#39;start robot named my-robot\u0026#39; service: name: my-robot state: started enabled: true become: true   # good - name: \u0026#39;start robot named my-robot\u0026#39; service: name: \u0026#39;my-robot\u0026#39; state: \u0026#39;started\u0026#39; enabled: true become: true # double quotes w/ nested single quotes - name: \u0026#39;start all robots\u0026#39; service: name: \u0026#39;{{ item[\u0026#34;robot_name\u0026#34;] }}\u0026#39; state: \u0026#39;started\u0026#39; enabled: true with_items: \u0026#39;{{ robots }}\u0026#39; become: true # double quotes to escape characters - name \u0026#39;print some text on two lines\u0026#39; debug: msg: \u0026#34;This text is on\\ntwo lines\u0026#34; # folded scalar style - name: \u0026#39;robot infos\u0026#39; debug: msg: \u0026gt; Robot {{ item[\u0026#39;robot_name\u0026#39;] }} is {{ item[\u0026#39;status\u0026#39;] }} and in {{ item[\u0026#39;az\u0026#39;] }} availability zone with a {{ item[\u0026#39;curiosity_quotient\u0026#39;] }} curiosity quotient. with_items: robots # folded scalar when the string has nested quotes already - name: \u0026#39;print some text\u0026#39; debug: msg: \u0026gt; \u0026#34;I haven\u0026#39;t the slightest idea,\u0026#34; said the Hatter. # do not quote booleans/numbers - name: \u0026#39;download google homepage\u0026#39; get_url: dest: \u0026#39;/tmp\u0026#39; timeout: 60 url: \u0026#39;https://google.com\u0026#39; validate_certs: true # variables example 1 - name: \u0026#39;set a variable\u0026#39; set_fact: my_var: \u0026#39;test\u0026#39; # variables example 2 - name: \u0026#39;print my_var\u0026#39; debug: var: my_var when: ansible_os_family == \u0026#39;Darwin\u0026#39; # variables example 3 - name: \u0026#39;set another variable\u0026#39; set_fact: my_second_var: \u0026#39;{{ my_var }}\u0026#39;   Lint rule Yamllint quoted-strings rule\n    Sudo Use the new become syntax when designating that a task needs to be run with sudo privileges\n #bad - name: \u0026#39;template client.json to /etc/sensu/conf.d/\u0026#39; template: dest: \u0026#39;/etc/sensu/conf.d/client.json\u0026#39; src: \u0026#39;client.json.j2\u0026#39; sudo: true   # good - name: \u0026#39;template client.json to /etc/sensu/conf.d/\u0026#39; template: dest: \u0026#39;/etc/sensu/conf.d/client.json\u0026#39; src: \u0026#39;client.json.j2\u0026#39; become: true   Reason Using sudo was deprecated at Ansible version 1.9.1\n Lint rule Ansible-lint E103 rule\n    Hosts Declarations Host sections be defined in such a way that it follows this general order:\n  host declaration\n  host options in alphabetical order\n  pre_tasks\n  roles\n  tasks\n   # example - hosts: \u0026#39;webservers\u0026#39; remote_user: \u0026#39;centos\u0026#39; vars: tomcat_state: \u0026#39;started\u0026#39; pre_tasks: - name: \u0026#39;set the timezone to America/Boise\u0026#39; lineinfile: dest: \u0026#39;/etc/environment\u0026#39; line: \u0026#39;TZ=America/Boise\u0026#39; state: \u0026#39;present\u0026#39; become: true roles: - { role: \u0026#39;tomcat\u0026#39;, tags: \u0026#39;tomcat\u0026#39; } tasks: - name: \u0026#39;start the tomcat service\u0026#39; service: name: \u0026#39;tomcat\u0026#39; state: \u0026#39;{{ tomcat_state }}\u0026#39;   Reason A global definition about the order of these items, will create an easy and consistent human readable code.\n    Tasks Declarations A task should be defined in such a way that it follows this general order:\n  task name\n  tags\n  task map declaration (e.g. service:)\n  task parameters in alphabetical order (remember to always use multi-line map syntax)\n  loop operators (e.g. with_items)\n  task options in alphabetical order (e.g. become, ignore_errors, register)\n   # example - name: \u0026#39;create some ec2 instances\u0026#39; tags: \u0026#39;ec2\u0026#39; ec2: assign_public_ip: true image: \u0026#39;ami-c7d092f7\u0026#39; instance_tags: Name: \u0026#39;{{ item }}\u0026#39; key_name: \u0026#39;my_key\u0026#39; with_items: \u0026#39;{{ instance_names }}\u0026#39; ignore_errors: true register: ec2_output when: ansible_os_family == \u0026#39;Darwin\u0026#39;   Reason A global definition about the order of these items, will create an easy and consistent human readable code.\n    Include Declaration For include statements, make sure to quote filenames and only use blank lines between include statements if they are multi-line (e.g. they have tags).\n # bad - include: other_file.yml - include: \u0026#39;second_file.yml\u0026#39; - include: third_file.yml tags=third   # good - include: \u0026#39;other_file.yml\u0026#39; - include: \u0026#39;second_file.yml\u0026#39; - include: \u0026#39;third_file.yml\u0026#39; tags: \u0026#39;third\u0026#39;   Reason Using such syntax, will create an easy and consistent human readable code.\n    Booleans Use true/false instead of yes/no (or 1/0).\n # bad - name: \u0026#39;start httpd\u0026#39; service: name: \u0026#39;httpd\u0026#39; state: \u0026#39;restarted\u0026#39; enabled: 1 become: \u0026#39;yes\u0026#39;   # good - name: \u0026#39;start httpd\u0026#39; service: name: \u0026#39;httpd\u0026#39; state: \u0026#39;restarted\u0026#39; enabled: true become: true   Reason Ansible can read boolean values in many different ways, like: True/False, true/false, yes/no, 1/0. It makes sense to stick to one option, which is then equally used in any playbook or role. It is recommended to use true/false since Java and Javascript are using the same values for boolean values.\n Lint rule Yamllint truthy rule\n Required config: truthy: {allowed-values: [\u0026#34;true\u0026#34;, \u0026#34;false\u0026#34;]}`\n    Key Value Pairs Only one space should be used after the colon, when defining key/value pairs.\n # bad - name : \u0026#39;start httpd\u0026#39; service: name : \u0026#39;httpd\u0026#39; state : \u0026#39;restarted\u0026#39; enabled : true become : true   # good - name: \u0026#39;start httpd\u0026#39; service: name: \u0026#39;httpd\u0026#39; state: \u0026#39;restarted\u0026#39; enabled: true become: true   Reason It increases human readability and reduces changeset collisions for version control.\n Lint rule Yamllint colons rule\n Required config: colons: {max-spaces-before: 0, max-spaces-after: 1}\n    Using Map Syntax Always use the map syntax for better readability.\n # bad - name: \u0026#39;create conf.d directory\u0026#39; file: \u0026#39;path=/etc/httpd/conf.d/ state=directory mode=0755 owner=httpd group=httpd\u0026#39; become: true - name: \u0026#39;copy mod_ssl.conf to /etc/httpd/conf.d\u0026#39; copy: \u0026#39;dest=/etc/httpd/conf.d/ src=mod_ssl.conf\u0026#39; become: true   # good - name: \u0026#39;create conf.d directory\u0026#39; file: group: \u0026#39;httpd\u0026#39; mode: \u0026#39;0755\u0026#39; owner: \u0026#39;httpd\u0026#39; path: \u0026#39;/etc/httpd/conf.d\u0026#39; state: \u0026#39;directory\u0026#39; become: true - name: \u0026#39;copy mod_ssl.conf to /etc/httpd/conf.d\u0026#39; copy: dest: \u0026#39;/etc/httpd/conf.d/\u0026#39; src: \u0026#39;mod_ssl.conf\u0026#39; become: true   Reason It increases human readability and reduces changeset collisions for version control.\n    Spacing You should have blank lines between two host blocks, between two task blocks, and between host and include blocks. When indenting, you should use 2 spaces to represent sub-maps, and multi-line maps should start with a -. For a more in-depth example of how spacing (and other things) please take a look at the example below\n # Example: ansible-playbook --ask-become-pass --ask-vault-pass style.yml # # This is a sample Ansible script to showcase all of our style decisions. # Pay close attention to things like spacing, where we use quotes, etc. # The only thing you can ignore is where comments are, except this first comment: # It\u0026#39;s generally a good idea to include some good information like sample usage # at the beginning of your file, so that someone can run head on the script # to see what they should do. # # A good rule of thumb on quoting is to quote anything that represents a value # that does not represent either a primitive type, or something within the # playbook; e.g. do not quote integers, booleans, variable names, boolean logic # Variable names still need to be quoted when they are module parameters for # Ansible to properly resolve them. # You should also always have single quotes around the outer string, and # double quotes on the inside. # If for some reason this is not possible or it would require escaping quotes # (which you should avoid if you can), use the scalar string operator (shown # in this playbook). # # Directory structure style: # Your directory structure should match the structure described by the Ansible # developers: http://docs.ansible.com/ansible/playbooks_best_practices.html # # --- # # - include: \u0026#39;role_name.yml\u0026#39; # become: true # only if every task in the role requires super user # # The self-named yml file contains all of the actual role tasks. # # Header comments are followed by blank line, then --- to signify start of YAML, # then another blank line, then the script. --- - hosts: \u0026#39;localhost\u0026#39; tasks: - name: \u0026#39;fail if someone tries to run this\u0026#39; fail: msg: \u0026#39;this playbook was not meant to actually be ran. just inspect the source!\u0026#39; - include: \u0026#39;first_include.yml\u0026#39; # quote filenames - include: \u0026#39;second_include.yml\u0026#39; # no blank line needed between includes without tags - include: \u0026#39;third_include.yml\u0026#39; # includes with tags should have blank lines between tags: \u0026#39;third_include\u0026#39; - include: \u0026#39;fourth_include.yml\u0026#39; tags: \u0026#39;fourth_include\u0026#39; - hosts: \u0026#39;tag_environment_samplefruit\u0026#39; remote_user: \u0026#39;centos\u0026#39; # options in alphabetical order vars: sample_str: \u0026#39;dood\u0026#39; # use snake_case for variable names sample_bool: true # do not quote booleans or integers sample_int: 42 vars_files: - \u0026#39;group_vars/secrets.yml\u0026#39; pre_tasks: # then pre_tasks, roles, tasks - name: \u0026#39;this runs a command that involves both single and double quotes\u0026#39; command: \u0026gt; echo \u0026#34;I can\u0026#39;t even\u0026#34; args: chdir: \u0026#39;/tmp\u0026#39; - name: \u0026#39;this command just involves double quotes\u0026#39; command: \u0026#39;echo \u0026#34;Hey man\u0026#34;\u0026#39; roles: - { role: \u0026#39;sample_role\u0026#39;, tags: \u0026#39;sample_role\u0026#39; } # use this format for role listing tasks: - name: \u0026#39;get list of directory permissions in /tmp\u0026#39; command: \u0026#39;ls -l /tmp\u0026#39; register: tmp_listing # do not quote variable names when registering # A task should be defined in the following order: # name # tags # module # module arguments, alphabetical # loop operator (e.g. with_items, with_fileglob) # other options, alphabetical (e.g. become, ignore_errors, when) - name: \u0026#39;a more complicated task to show where everything goes: touch all items from /tmp\u0026#39; tags: \u0026#39;debug\u0026#39; # tags go immediately after name file: path: \u0026#39;{{ item }}\u0026#39; # use path for single file actions, dest/src for multi file actions state: \u0026#39;touch\u0026#39; # arguments go in alphabetical order with_items: tmp_listing.stdout_lines # loop things go immediately after module # the rest of the task options are in alphabetical order become: true # try to keep become only on the tasks that need it. If every task in a host uses become, then move it up to the host options ignore_errors: true when: ansible_os_family == \u0026#39;Darwin\u0026#39; and tmp_listing.stdout_lines | length \u0026gt; 1 - name: \u0026#39;some modules can have maps in their maps (woah man)\u0026#39; ec2: assign_public_ip: true group: [\u0026#39;wca_ssh\u0026#39;, \u0026#39;wca_tomcat\u0026#39;] image: \u0026#39;ami-c7d092f7\u0026#39; instance_tags: Name: \u0026#39;instance\u0026#39; service_tomcat: \u0026#39;\u0026#39; key_name: \u0026#39;ops\u0026#39; - hosts: \u0026#39;tag_environment_secondfruit\u0026#39; tasks: - name: \u0026#39;this task has multiple tags\u0026#39; tags: [\u0026#39;tagme\u0026#39;, \u0026#39;tagmetoo\u0026#39;] set_fact: mr_fact: \u0026#39;w\u0026#39; - name: \u0026#39;perform an action\u0026#39; action: ec2_facts delegate_to: \u0026#39;localhost\u0026#39; # newline at end of file   Reason This produces nice looking code that is easy to read.\n Lint rule Yamllint empty-lines rule\n    Variable Names Names of variables should be as expressive as possible. snake_case for names will help to make the code human readable. The prefix should contain the name of the role.\n # bad - name: \u0026#39;set some facts\u0026#39; set_fact: myBoolean: true int: 20 MY_STRING: \u0026#39;test\u0026#39;   # good - name: \u0026#39;set some facts\u0026#39; set_fact: rolename_my_boolean: true rolename_my_int: 20 rolename_my_string: \u0026#39;test\u0026#39;   Reason Ansible uses snake_case for module names so it makes sense to extend this convention to variable names. Perfixing the variable with the role name, makes it immediately obvious where it is used.\n    Jinja Variables Use spaces around Jinja variable names to increase readability.\n # bad - name: set some facts set_fact: my_new_var: \u0026#34;{{my_old_var}}\u0026#34;   # good - name: set some facts set_fact: my_new_var: \u0026#34;{{ my_old_var }}\u0026#34;   Reason A proper definition for how to create Jinja variables produces consistent and easily readable code.\n Lint rule Ansible-lint E206 rule\n    Comparing Do not compare to literal True/False. Use when: var rather than when: var == True (or conversely when: not var).\n Do not compare it to empty strings. Use when: var rather than when: var != \u0026#34;\u0026#34; (or conversely when: not var rather than when: var == \u0026#34;\u0026#34;)\n # bad - name: validate required variables fail: msg: \u0026#34;No value specified for \u0026#39;{{ item }}\u0026#39;\u0026#34; when: (vars[item] is undefined) or (vars[item] is defined and vars[item] | trim == \u0026#34;\u0026#34;) with_items: \u0026#34;{{ appd_required_variables }}\u0026#34; - name: Create an user and add to the global group include_tasks: user.yml when: - username is defined - username != \u0026#34;\u0026#34;   # good - name: Validate required variables fail: msg: \u0026#34;No value specified for \u0026#39;{{ item }}\u0026#39;\u0026#34; when: (vars[item] is undefined) or (vars[item] is defined and not vars[item] | trim == \u0026#34;\u0026#34;) with_items: \u0026#34;{{ appd_required_variables }}\u0026#34; - name: Create an user and add to the global group include_tasks: user.yml when: - username is defined - username   Reason Avoid code complexity using quotes and standardize the way literals and empty strings are used\n Lint rule Ansible-lint E601 rule\n    Delegation Do not use local_action, use delegate_to: localhost\n # bad - name: Send summary mail local_action: module: mail subject: \u0026#34;Summary Mail\u0026#34; to: \u0026#34;{{ mail_recipient }}\u0026#34; body: \u0026#34;{{ mail_body }}\u0026#34; run_once: true   # good - name: Send summary mail mail: subject: \u0026#34;Summary Mail\u0026#34; to: \u0026#34;{{ mail_recipient }}\u0026#34; body: \u0026#34;{{ mail_body }}\u0026#34; delegate_to: localhost run_once: true   Reason Avoid complexity, standardization, flexibility and code readability. The module and its parameters are easy to read and can be delegated even to a third party server.\n Lint rule Ansible-lint E504 rule\n    Playbook File Extension All Ansible Yaml files should have a .yml extension (and NOT .YML, .yaml etc).\n # bad ~/tasks.yaml   # good ~/tasks.yml   Reason Ansible tooling (like ansible-galaxy init) creates files with a .yml extension. Also, the Ansible documentation website references files with a .yml extension several times. Because of this, it is normal in the Ansible community to use a .yml extension for all Ansible YAML files.\n Lint rule Ansible-lint E205 rule\n    Template File Extension All Ansible Template files should have a .j2 extension.\n # bad ~/template.conf   # good ~/template.conf.j2   Reason Ansible Template files will usually have the .j2 extension, which denotes the Jinja2 templating engine used.\n    Vaults All Ansible Vault files should have a .vault extension (and NOT .yml, .YML, .yaml etc).\n # bad ~/secrets.yml   # good ~/secrets.vault   Reason It is easier to control unencrypted files automatically for the specific .vault extension.\n    Debug and Comments Do not overuse debug and comments in final code as much as possible. Use task and role names to explain what the task or role does. Use the verbose option under ansible for debugging purposes. If debug is used, assign a verbosity option. This will display the message only on certain debugging levels.\n # bad - name: print my_var debug: var: my_var when: ansible_os_family == \u0026#34;Darwin\u0026#34;   # good - name: print my_var debug: var: my_var verbosity: 2 when: ansible_os_family == \u0026#34;Darwin\u0026#34;   Reason It will keep clean code and consistency avoiding extra debug and comments. Extra debug will spend extra time when running the playbook or role.\n    Use Modules copy or templates instead of linefile or blockfile Instead of using the modules linefile and blockfile, which manage changes inside a file, it should be tried to use the modules copy or template instead, which will manage the whole file. For better future proof template should be preferred over copy.\n Reason When using linefile/blockfile only a single line or a part of a file is managed. It is not easy to remember which part exactly is managed and which is not. Using the template module the whole file is managed by Ansible and there is no confusion about different parts of a file. Moreover, regular expressions can be avoided, which are often used using linefile.\n    Use Module synchronize Instead of copy for Large Files Copying large files takes significantly longer than syncing it. The synchronize modules which are based on rsync can increase the time moving large files from one node to another (or even on the same node).\n   Do not Show Sensitive Data in Ansible Output When using the template module and there are passwords or other sensitive data in the file, use the no_log option to hide this information.\n Reason For obvious reasons the output of sensitive data on the screen (and logfile) should be prohibited.\n    Use Block-Module Block can help to organize the code and can enable rollbacks.\n - block: copy: src: critical.conf dest: /etc/critical/crit.conf service: name: critical state: restarted rescue: command: shutdown -h now   Reason Using blocks groups critical tasks together and allows better management when a single task of this block fails.\n    Enable CPU and Memory Profiling Enabling profiling is extremely useful when testing forks or to analyse memory and cpu consumption in general. It will present a summary of used CPU/memory for the whole play and per task.\n To enable it:\n  Create a new control group - which is required for CPU and memory profiling\ncgcreate -a root:root -t root:root -g cpuacct,memory,pids:ansible_profile     Configure ansible.cfg\ncallback_whitelist=cgroup_perf_recap [callback_cgroup_perf_recap] control_group=ansible_profile     Execute the playbook using cgexec\ncgexec -g cpuacct,memory,pids:ansible_profile ansible-playbook x.yaml     Analyse the usage\nMemory Execution Maximum: 11146.29MB cpu Execution Maximum: 112.08% pids Execution Maximum: 35.00 memory: lab : creating network (b42e9945-0dc7-20b1-09d1-00000000000a): 11097.35MB …..         Enable Task and Role Profiling The following can be enabled as well, to help debugging playbooks and roles:\n [defaults] callback_whitelist = profile_tasks, profile_role, timer   Timer: duration of playbook execution (activated by default) profile_tasks/role: displays execution time per tasks/role\n This will generate an output like the following:\n      Directory Structure A consistent directory structure is important to easily understand all playbooks and roles which are written. Ansible knows many different folder structures, any can be used. However, it is important to stick to one structure.\n The following is an example. Not all folders are usually used and working with collections will change such structure a little bit.\n . ├── ansible.cfg ├── ansible_modules ├── group_vars │ ├── webservers │ └── all ├── hosts │ ├── webserver01 │ └── webserver02 ├── host_vars ├── modules ├── playbooks │ └── ansible-cmdb.yml └── roles ├── requirements.yml ├── galaxy └── dev-sec.ssh-hardening └── auditd ├── files │ ├── auditd.conf │ ├── audit.yml ├── handlers │ └── main.yml ├── meta │ └── main.yml └── tasks └── main.yml    "},{"uri":"https://blog.stderr.at/tags/automation-controller/","title":"Automation Controller","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/best-practices/","title":"Best Practices","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/controller/","title":"Controller","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/playbook/","title":"Playbook","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/role/","title":"Role","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/style/","title":"Style","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/styleguide/","title":"StyleGuide","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/etcd/2021-11-29-automatedetcdbackup/","title":"Automated ETCD Backup","tags":["OCP","Day-2","OpenShift","etcd"],"description":"Create ETCD backups using cronjobs","content":"Securing ETCD is one of the major Day-2 tasks for a Kubernetes cluster. This article will explain how to create a backup using OpenShift Cronjob.\n     There is absolutely no warranty. Verify your backups regularly and perform restore tests.     Prerequisites The following is required:\n   OpenShift Cluster 4.x\n  Integrated Storage, might be NFS or anything. Best practice would be a RWX enabled storage.\n     Configure Project \u0026amp; Cronjob Create the following objects in OpenShift. This fill create:\n  A Project called ocp-etcd-backup\n  A PersistentVolumeClaim to store the backups. Change to your appropriate StorageClass and accessMode\n  A ServiceAccount called openshift-backup\n  A dedicated ClusterRole which is able to start (debug pods)\n  A ClusterRoleBinding between the created ServiceAccount and the customer ClusterRole\n  A 2nd ClusterRoleBinding, which gives our ServiceAccount the permission to start privileged containers. This is required to start a debug pod on a control plane node.\n  A CronJob which performs the backup …​ see Callouts for inline explanations.\n       A helm chart, which would create these objects below, can be found at: https://github.com/tjungbauer/ocp-auto-backup. This is probably a better way to manage the variables via the values.yaml file.     kind: Namespace apiVersion: v1 metadata: name: ocp-etcd-backup annotations: openshift.io/description: Openshift Backup Automation Tool openshift.io/display-name: Backup ETCD Automation spec: {} --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: etcd-backup-pvc namespace: ocp-etcd-backup spec: accessModes: - ReadWriteOnce (1) resources: requests: storage: 100Gi storageClassName: gp2 volumeMode: Filesystem --- kind: ServiceAccount apiVersion: v1 metadata: name: openshift-backup namespace: ocp-etcd-backup --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-etcd-backup rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - \u0026#34;nodes\u0026#34; verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: - \u0026#34;pods\u0026#34; - \u0026#34;pods/log\u0026#34; verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;watch\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: openshift-backup subjects: - kind: ServiceAccount name: openshift-backup namespace: ocp-etcd-backup roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-etcd-backup --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: etcd-backup-scc-privileged roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:openshift:scc:privileged subjects: - kind: ServiceAccount name: openshift-backup namespace: ocp-etcd-backup --- kind: CronJob apiVersion: batch/v1 metadata: name: cronjob-etcd-backup namespace: ocp-etcd-backup labels: purpose: etcd-backup spec: schedule: \u0026#39;*/5 * * * *\u0026#39; (2) startingDeadlineSeconds: 200 concurrencyPolicy: Forbid suspend: false jobTemplate: metadata: creationTimestamp: null spec: backoffLimit: 0 template: metadata: creationTimestamp: null spec: nodeSelector: node-role.kubernetes.io/master: \u0026#39;\u0026#39; (3) restartPolicy: Never activeDeadlineSeconds: 200 serviceAccountName: openshift-backup schedulerName: default-scheduler hostNetwork: true terminationGracePeriodSeconds: 30 securityContext: {} containers: - resources: requests: cpu: 300m memory: 250Mi terminationMessagePath: /dev/termination-log name: etcd-backup command: (4) - /bin/bash - \u0026#39;-c\u0026#39; - \u0026gt;- oc get no -l node-role.kubernetes.io/master --no-headers -o name | grep `hostname` | head -n 1 | xargs -I {} -- oc debug {} -- bash -c \u0026#39;chroot /host sudo -E /usr/local/bin/cluster-backup.sh /home/core/backup\u0026#39; ; echo \u0026#39;Moving Local Master Backups to target directory (from /home/core/backup to mounted PVC)\u0026#39;; mv /home/core/backup/* /etcd-backup/; echo \u0026#39;Deleting files older than 30 days\u0026#39; ; find /etcd-backup/ -type f -mtime +30 -exec rm {} \\; securityContext: privileged: true runAsUser: 0 imagePullPolicy: IfNotPresent volumeMounts: - name: temp-backup mountPath: /home/core/backup (5) - name: etcd-backup mountPath: /etcd-backup (6) terminationMessagePolicy: FallbackToLogsOnError image: registry.redhat.io/openshift4/ose-cli serviceAccount: openshift-backup volumes: - name: temp-backup hostPath: path: /home/core/backup type: \u0026#39;\u0026#39; - name: etcd-backup persistentVolumeClaim: claimName: etcd-backup-pvc dnsPolicy: ClusterFirst tolerations: - operator: Exists effect: NoSchedule - operator: Exists effect: NoExecute successfulJobsHistoryLimit: 5 failedJobsHistoryLimit: 5     1 RWO is used here, since I have no other available storage on my test cluster.   2 How often shall the job be executed. Here, every 5 minutes.   3 Bind the job to \u0026#34;Master\u0026#34; nodes.   4 Command to be executed…​ It fetches the actual local master nodename and starts a debugging Pod there. The backup script is called and moves the backup to /home/core/backup which is a folder on the control plane itself. The move command will move the backups from the local folder to the actual backup target volume. Finally, it will remove backups older than 30 days.   5 Mounted /home/core/backup on the master nodes, here the command will store the backups before they are moved   6 Target destination for the etcd backup on the mounted PVC      Start a Job If you do not want to wait until the CronJob is triggered, you can manually start the Job using the following commands:\n oc create job backup --from=cronjob/cronjob-etcd-backup -n ocp-etcd-backup   This will start a Pod which will do the backup:\n Starting pod/ip-10-0-196-187us-east-2computeinternal-debug ... To use host binaries, run `chroot /host` found latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-15 found latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10 found latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-9 found latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-3 etcdctl is already installed {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:1638199790.980932,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:119\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;created temporary db file\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/home/core/backup/snapshot_2021-11-29_152949.db.part\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2021-11-29T15:29:50.991Z\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;clientv3/maintenance.go:200\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;opened snapshot stream; downloading\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:1638199790.9912837,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:127\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;fetching snapshot\u0026#34;,\u0026#34;endpoint\u0026#34;:\u0026#34;https://10.0.196.187:2379\u0026#34;} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:\u0026#34;2021-11-29T15:29:53.306Z\u0026#34;,\u0026#34;caller\u0026#34;:\u0026#34;clientv3/maintenance.go:208\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;completed snapshot read; closing\u0026#34;} Snapshot saved at /home/core/backup/snapshot_2021-11-29_152949.db {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:1638199793.3482974,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:142\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;fetched snapshot\u0026#34;,\u0026#34;endpoint\u0026#34;:\u0026#34;https://10.0.196.187:2379\u0026#34;,\u0026#34;size\u0026#34;:\u0026#34;180 MB\u0026#34;,\u0026#34;took\u0026#34;:2.367303503} {\u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;,\u0026#34;ts\u0026#34;:1638199793.348459,\u0026#34;caller\u0026#34;:\u0026#34;snapshot/v3_snapshot.go:152\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;saved\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/home/core/backup/snapshot_2021-11-29_152949.db\u0026#34;} {\u0026#34;hash\u0026#34;:1180914745,\u0026#34;revision\u0026#34;:10182252,\u0026#34;totalKey\u0026#34;:19360,\u0026#34;totalSize\u0026#34;:179896320} snapshot db and kube resources are successfully saved to /home/core/backup Removing debug pod ... Moving Local Master Backups to target directory (from /home/core/backup to mounted PVC)     Verifying the Backup Let’s start a dummy Pod which can access the PVC to verify if the backup is really there.\n apiVersion: v1 kind: Pod metadata: name: verify-etcd-backup spec: containers: - name: verify-etcd-backup image: registry.access.redhat.com/ubi8/ubi command: [\u0026#34;sleep\u0026#34;, \u0026#34;3000\u0026#34;] volumeMounts: - name: etcd-backup mountPath: /etcd-backup volumes: - name: etcd-backup persistentVolumeClaim: claimName: etcd-backup-pvc   Logging into that Pod will show the available backups stored at /etcd-backup which is the mounted PVC.\n oc rsh -n ocp-etcd-backup verify-etcd-backup ls -la etcd-backup total 1406196 drwxr-xr-x. 3 root root 4096 Nov 29 17:00 . dr-xr-xr-x. 1 root root 25 Nov 29 17:06 .. drwx------. 2 root root 16384 Nov 29 15:21 lost+found -rw-------. 1 root root 179896352 Nov 29 15:21 snapshot_2021-11-29_152150.db -rw-------. 1 root root 179896352 Nov 29 15:29 snapshot_2021-11-29_152949.db -rw-------. 1 root root 179896352 Nov 29 15:32 snapshot_2021-11-29_153159.db -rw-------. 1 root root 179896352 Nov 29 15:36 snapshot_2021-11-29_153618.db -rw-------. 1 root root 179896352 Nov 29 15:55 snapshot_2021-11-29_155513.db -rw-------. 1 root root 179896352 Nov 29 16:00 snapshot_2021-11-29_160020.db -rw-------. 1 root root 179896352 Nov 29 16:55 snapshot_2021-11-29_165521.db -rw-------. 1 root root 179896352 Nov 29 17:00 snapshot_2021-11-29_170020.db -rw-------. 1 root root 89875 Nov 29 15:21 static_kuberesources_2021-11-29_152150.tar.gz -rw-------. 1 root root 89875 Nov 29 15:29 static_kuberesources_2021-11-29_152949.tar.gz -rw-------. 1 root root 89875 Nov 29 15:32 static_kuberesources_2021-11-29_153159.tar.gz -rw-------. 1 root root 89875 Nov 29 15:36 static_kuberesources_2021-11-29_153618.tar.gz -rw-------. 1 root root 89875 Nov 29 15:55 static_kuberesources_2021-11-29_155513.tar.gz -rw-------. 1 root root 89875 Nov 29 16:00 static_kuberesources_2021-11-29_160020.tar.gz -rw-------. 1 root root 89875 Nov 29 16:55 static_kuberesources_2021-11-29_165521.tar.gz -rw-------. 1 root root 89875 Nov 29 17:00 static_kuberesources_2021-11-29_170020.tar.gz    "},{"uri":"https://blog.stderr.at/tags/day-2/","title":"Day-2","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/day-2/","title":"Day-2","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/environments/","title":"Environments","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/etcd/","title":"etcd","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/etcd/","title":"etcd","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ocp/","title":"OCP","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/annotations/","title":"Annotations","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/environments/","title":"Environments","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ingress/","title":"Ingress","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/ingresscontroller/","title":"IngressController","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/labels/","title":"Labels","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/nodeselector/","title":"NodeSelector","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/pod-placement/","title":"Pod Placement","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/pod-placement/","title":"Pod Placement","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/labels_environmets/2021-11-12-creatingenvironment/","title":"Working with Environments","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector","Environments","Ingress","IngressController","Labels","Annotations"],"description":"Create separate environments on one OpenShift cluster","content":"Imagine you have one OpenShift cluster and you would like to create 2 or more environments inside this cluster, but also separate them and force the environments to specific nodes, or use specific inbound routers. All this can be achieved using labels, IngressControllers and so on. The following article will guide you to set up dedicated compute nodes for infrastructure, development and test environments as well as the creation of IngressController which are bound to the appropriate nodes.\n Prerequisites Before we start we need an OpenShift cluster of course. In this example we have a cluster with typical 3 control plane nodes (labelled as master) and 7 compute nodes (labelled as worker)\n oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-149-168.us-east-2.compute.internal Ready worker 13h v1.21.1+6438632 # \u0026lt;-- will become infra ip-10-0-154-244.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # \u0026lt;-- will become infra ip-10-0-158-44.us-east-2.compute.internal Ready worker 15m v1.21.1+6438632 # \u0026lt;-- will become infra ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-188-198.us-east-2.compute.internal Ready worker 13h v1.21.1+6438632 # \u0026lt;-- will become worker-test ip-10-0-191-9.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # \u0026lt;-- will become worker-test ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-195-201.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # \u0026lt;-- will become worker-dev ip-10-0-199-235.us-east-2.compute.internal Ready worker 16m v1.21.1+6438632 # \u0026lt;-- will become worker-dev   We will use the 7 nodes to create the environments for:\n   Infrastructure Services (3 nodes) - will be labelled as infra\n  Development Environment (2 nodes) - will be labelled as worker-dev\n  Test Environment (2 nodes) - will be labelled as worker-test\n   To do this, we will label the nodes and create dedicated roles for them.\n   Create Nodes Labels and MachineConfigPools Let’s create a maschine config pool for the different environments.\n The pool inherits the configuration from worker nodes by default, which means that any new update on the worker configuration will also update custom labeled nodes. Or in other words: it is possible to remove the worker label from the custom pools.\n Create the following objects:\n # Infrastructure apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: infra spec: machineConfigSelector: matchExpressions: - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,infra]} (1) maxUnavailable: 1 (2) nodeSelector: matchLabels: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; (3) paused: false --- # Worker-DEV apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: worker-dev spec: machineConfigSelector: matchExpressions: - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-dev]} nodeSelector: matchLabels: node-role.kubernetes.io/worker-dev: \u0026#34;\u0026#34; --- # Worker-TEST apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfigPool metadata: name: worker-test spec: machineConfigSelector: matchExpressions: - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-test]} nodeSelector: matchLabels: node-role.kubernetes.io/worker-test: \u0026#34;\u0026#34;     1 User worker and infra so that the default worker configuration gets applied during upgrades   2 Whenever an update happens, do only 1 at a time   3 This pool is valid for nodes which are labelled as infra    Now let’s label our nodes. First add the new, additional label:\n # Add the label \u0026#34;infra\u0026#34; oc label node ip-10-0-149-168.us-east-2.compute.internal ip-10-0-154-244.us-east-2.compute.internal ip-10-0-158-44.us-east-2.compute.internal node-role.kubernetes.io/infra= # Add the label \u0026#34;worker-dev\u0026#34; oc label node ip-10-0-195-201.us-east-2.compute.internal ip-10-0-199-235.us-east-2.compute.internal node-role.kubernetes.io/worker-dev= # Add the label \u0026#34;worker-test\u0026#34; oc label node ip-10-0-188-198.us-east-2.compute.internal ip-10-0-191-9.us-east-2.compute.internal node-role.kubernetes.io/worker-test=   This will result in:\n NAME STATUS ROLES AGE VERSION ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-149-168.us-east-2.compute.internal Ready infra,worker 13h v1.21.1+6438632 ip-10-0-154-244.us-east-2.compute.internal Ready infra,worker 20m v1.21.1+6438632 ip-10-0-158-44.us-east-2.compute.internal Ready infra,worker 19m v1.21.1+6438632 ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-188-198.us-east-2.compute.internal Ready worker,worker-test 13h v1.21.1+6438632 ip-10-0-191-9.us-east-2.compute.internal Ready worker,worker-test 20m v1.21.1+6438632 ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-195-201.us-east-2.compute.internal Ready worker,worker-dev 20m v1.21.1+6438632 ip-10-0-199-235.us-east-2.compute.internal Ready worker,worker-dev 20m v1.21.1+6438632   We can remove the worker label from the worker-dev and worker-test nodes now.\n     Keep the label \u0026#34;worker\u0026#34; for the infra nodes as this is the default worker label which is used when no nodeselector is in use. You can use any other node, just keep in mind that per default new applications will be started on nodes with the labels \u0026#34;worker\u0026#34;. As an alternative, you can also define a cluster-wide default node selector.     oc label node ip-10-0-195-201.us-east-2.compute.internal ip-10-0-199-235.us-east-2.compute.internal node-role.kubernetes.io/worker- oc label node ip-10-0-188-198.us-east-2.compute.internal ip-10-0-191-9.us-east-2.compute.internal node-role.kubernetes.io/worker-   The final node labels will look like the following:\n NAME STATUS ROLES AGE VERSION ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-149-168.us-east-2.compute.internal Ready infra,worker 13h v1.21.1+6438632 ip-10-0-154-244.us-east-2.compute.internal Ready infra,worker 22m v1.21.1+6438632 ip-10-0-158-44.us-east-2.compute.internal Ready infra,worker 21m v1.21.1+6438632 ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-188-198.us-east-2.compute.internal Ready worker-test 13h v1.21.1+6438632 ip-10-0-191-9.us-east-2.compute.internal Ready worker-test 21m v1.21.1+6438632 ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-195-201.us-east-2.compute.internal Ready worker-dev 21m v1.21.1+6438632 ip-10-0-199-235.us-east-2.compute.internal Ready worker-dev 22m v1.21.1+6438632       Since the custom pools (infra, worker-test and worker-dev) inherit their configuration from the default worker pool, no changes on the files on the nodes themselves are triggered at this point.       Create Custom Configuration Let’s test our setup by deploying a configuration on specific nodes. The following MaschineConfig objects will create a file at /etc/myfile on the nodes labelled either infra, worker-dev or worker_test. Dependent on the node role the content of the file will vary.\n apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: infra (1) name: 55-infra spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:,infra (2) filesystem: root mode: 0644 path: /etc/myfile (3) --- apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: worker-dev name: 55-worker-dev spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:,worker-dev filesystem: root mode: 0644 path: /etc/myfile --- apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: worker-test name: 55-worker-test spec: config: ignition: version: 2.2.0 storage: files: - contents: source: data:,worker-test filesystem: root mode: 0644 path: /etc/myfile     1 Valid for node with the role xyz   2 Content of the file   3 File to be created    Since this is a new configuration, all nodes will get reconfigured.\n NAME STATUS ROLES AGE VERSION ip-10-0-138-104.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-149-168.us-east-2.compute.internal Ready,SchedulingDisabled infra,worker 13h v1.21.1+6438632 ip-10-0-154-244.us-east-2.compute.internal Ready infra,worker 28m v1.21.1+6438632 ip-10-0-158-44.us-east-2.compute.internal Ready infra,worker 27m v1.21.1+6438632 ip-10-0-160-91.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-188-198.us-east-2.compute.internal Ready worker-test 13h v1.21.1+6438632 ip-10-0-191-9.us-east-2.compute.internal NotReady,SchedulingDisabled worker-test 27m v1.21.1+6438632 ip-10-0-192-174.us-east-2.compute.internal Ready master 13h v1.21.1+6438632 ip-10-0-195-201.us-east-2.compute.internal Ready worker-dev 27m v1.21.1+6438632 ip-10-0-199-235.us-east-2.compute.internal NotReady,SchedulingDisabled worker-dev 28m v1.21.1+6438632   Wait until all nodes are ready and test the configuration by verifying the content of /etc/myfile:\n ### # infra nodes: ### oc get pods -n openshift-machine-config-operator -l k8s-app=machine-config-daemon --field-selector \u0026#34;spec.nodeName=ip-10-0-149-168.us-east-2.compute.internal\u0026#34; NAME READY STATUS RESTARTS AGE machine-config-daemon-f85kd 2/2 Running 6 16h # Get file content oc rsh -n openshift-machine-config-operator machine-config-daemon-f85kd chroot /rootfs cat /etc/myfile Defaulted container \u0026#34;machine-config-daemon\u0026#34; out of: machine-config-daemon, oauth-proxy infra ### #worker-dev: ### oc get pods -n openshift-machine-config-operator -l k8s-app=machine-config-daemon --field-selector \u0026#34;spec.nodeName=ip-10-0-195-201.us-east-2.compute.internal\u0026#34; NAME READY STATUS RESTARTS AGE machine-config-daemon-s6rr5 2/2 Running 4 3h5m # Get file content oc rsh -n openshift-machine-config-operator machine-config-daemon-s6rr5 chroot /rootfs cat /etc/myfile Defaulted container \u0026#34;machine-config-daemon\u0026#34; out of: machine-config-daemon, oauth-proxy worker-dev ### # worker-test: ### oc get pods -n openshift-machine-config-operator -l k8s-app=machine-config-daemon --field-selector \u0026#34;spec.nodeName=ip-10-0-188-198.us-east-2.compute.internal\u0026#34; NAME READY STATUS RESTARTS AGE machine-config-daemon-m22rf 2/2 Running 6 16h # Get file content oc rsh -n openshift-machine-config-operator machine-config-daemon-m22rf chroot /rootfs cat /etc/myfile Defaulted container \u0026#34;machine-config-daemon\u0026#34; out of: machine-config-daemon, oauth-proxy worker-test   The file /etc/myfile exists on all nodes and depending on their role the files have a different content.\n   Bind an Application to a Specific Environment The following will label the nodes with a specific environment and will deploy an example application, which should only be executed on the appropriate nodes.\n  Let’s label the nodes with environment=worker-dev and environment=worker-test:\noc label node ip-10-0-195-201.us-east-2.compute.internal ip-10-0-199-235.us-east-2.compute.internal environment=worker-dev oc label node ip-10-0-188-198.us-east-2.compute.internal ip-10-0-191-9.us-east-2.compute.internal environment=worker-test     Create a namespace for the example application\noc new-project bookinfo     Create an annotation and a label for the namespace. The annotation will make sure that the application will only be started on nodes with the same label. The label will be later used for the IngressController setup.\noc annotate namespace bookinfo environment=worker-dev oc annotate namespace bookinfo openshift.io/node-selector: environment=worker-test oc label namespace bookinfo environment=worker-dev     Deploy the example application. In this article the sample application of Istio was used:\noc apply -f https://raw.githubusercontent.com/istio/istio/release-1.11/samples/bookinfo/platform/kube/bookinfo.yaml      This will start the application on worker-dev` nodes only, because the annotation in the namespace was created accordingly.\n oc get pods -n bookinfo -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES details-v1-86dfdc4b95-v8zfv 1/1 Running 0 9m19s 10.130.2.17 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; productpage-v1-658849bb5-8gcl7 1/1 Running 0 7m17s 10.128.4.21 ip-10-0-195-201.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ratings-v1-76b8c9cbf9-cc4js 1/1 Running 0 9m19s 10.130.2.19 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v1-58b8568645-mbgth 1/1 Running 0 7m44s 10.128.4.20 ip-10-0-195-201.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v2-5d8f8b6775-qkdmz 1/1 Running 0 9m19s 10.130.2.21 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v3-666b89cfdf-8zv8w 1/1 Running 0 9m18s 10.130.2.22 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;     Create Dedicated IngressController IngressController are responsible to bring the traffic into the cluster. OpenShift comes with one default controller, but it is possible to create more in order to use different domains and separate the incoming traffic to different nodes.\n Bind the default ingress controller to the infra labeled nodes, so we can be sure that the default router pods are executed only on these nodes:\n oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch=\u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;nodePlacement\u0026#34;:{\u0026#34;nodeSelector\u0026#34;: {\u0026#34;matchLabels\u0026#34;:{\u0026#34;node-role.kubernetes.io/infra\u0026#34;:\u0026#34;\u0026#34;}}}}}\u0026#39;   The pods will get restarted, to be sure they are running on infra:\n oc get pods -n openshift-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES router-default-78f8dd6f69-dbtbv 0/1 ContainerCreating 0 2s \u0026lt;none\u0026gt; ip-10-0-149-168.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-78f8dd6f69-wwpgb 0/1 ContainerCreating 0 2s \u0026lt;none\u0026gt; ip-10-0-158-44.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-7bbbc8f9bd-vfh84 1/1 Running 0 22m 10.129.4.6 ip-10-0-158-44.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-7bbbc8f9bd-wggrx 1/1 Terminating 0 19m 10.128.2.8 ip-10-0-149-168.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Create the following IngressController objects for worker-dev and worker-test. Replace with the domain of your choice\n apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: ingress-worker-dev namespace: openshift-ingress-operator spec: domain: worker-dev.\u0026lt;yourdomain\u0026gt; (1) endpointPublishingStrategy: type: HostNetwork httpErrorCodePages: name: \u0026#39;\u0026#39; namespaceSelector: matchLabels: environment: worker-dev (2) nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker-dev: \u0026#39;\u0026#39; (3) replicas: 3 tuningOptions: {} unsupportedConfigOverrides: null --- apiVersion: operator.openshift.io/v1 kind: IngressController metadata: name: ingress-worker-test namespace: openshift-ingress-operator spec: domain: worker-test.\u0026lt;yourdomain\u0026gt; endpointPublishingStrategy: type: HostNetwork httpErrorCodePages: name: \u0026#39;\u0026#39; namespaceSelector: matchLabels: environment: worker-test nodePlacement: nodeSelector: matchLabels: node-role.kubernetes.io/worker-test: \u0026#39;\u0026#39; replicas: 3 tuningOptions: {} unsupportedConfigOverrides: null     1 Domainname which is used by this Controller   2 Namespace selector …​ namespaces with such label will be handled by this IngressController   3 Node Placement …​ This Controller should run on nodes with this label/role    This will spin up additional router pods on the collect labelled nodes:\n oc get pods -n openshift-ingress -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES router-default-78f8dd6f69-dbtbv 1/1 Running 0 8m7s 10.128.2.11 ip-10-0-149-168.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-default-78f8dd6f69-wwpgb 1/1 Running 0 8m7s 10.129.4.10 ip-10-0-158-44.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-ingress-worker-dev-76b65cf558-mspvb 1/1 Running 0 113s 10.130.2.13 ip-10-0-199-235.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-ingress-worker-dev-76b65cf558-p2jpg 1/1 Running 0 113s 10.128.4.12 ip-10-0-195-201.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-ingress-worker-test-6bbf9967f-4whfs 1/1 Running 0 113s 10.131.2.13 ip-10-0-191-9.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; router-ingress-worker-test-6bbf9967f-jht4w 1/1 Running 0 113s 10.131.0.8 ip-10-0-188-198.us-east-2.compute.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;     Verify Ingress Configuration To test our new ingress router lets create a route object for our example application:\n kind: Route apiVersion: route.openshift.io/v1 metadata: name: productpage namespace: bookinfo spec: host: productpage-bookinfo.worker-dev.\u0026lt;yourdomain\u0026gt; to: kind: Service name: productpage weight: 100 port: targetPort: http wildcardPolicy: None --- kind: Route apiVersion: route.openshift.io/v1 metadata: name: productpage-worker-test namespace: bookinfo spec: host: productpage-bookinfo.worker-test.\u0026lt;yourdomain\u0026gt; to: kind: Service name: productpage weight: 100 port: targetPort: http wildcardPolicy: None       Be sure that the name is resolvable and a load balancer is configured accordingly     Verify that the router pod has the correct configuration in the file haproxy.config :\n oc get pods -n openshift-ingress NAME READY STATUS RESTARTS AGE router-default-78f8dd6f69-dbtbv 1/1 Running 0 95m router-default-78f8dd6f69-wwpgb 1/1 Running 0 95m router-ingress-worker-dev-76b65cf558-mspvb 1/1 Running 0 88m router-ingress-worker-dev-76b65cf558-p2jpg 1/1 Running 0 88m router-ingress-worker-test-6bbf9967f-4whfs 1/1 Running 0 88m router-ingress-worker-test-6bbf9967f-jht4w 1/1 Running 0 88m   Verify the content of the haproxy configuration for one of the worker-dev router\n oc rsh -n openshift-ingress router-ingress-worker-dev-76b65cf558-mspvb cat haproxy.config | grep productpage backend be_http:bookinfo:productpage server pod:productpage-v1-658849bb5-8gcl7:productpage:http:10.128.4.21:9080 10.128.4.21:9080 cookie 3758caf21badd7e4f729209173eece08 weight 256   Compare with worker-test router\n oc rsh -n openshift-ingress router-ingress-worker-test-6bbf9967f-jht4w cat haproxy.config | grep productpage --\u0026gt; Empty result, this router is not configured with that route.   Compare with default router:\n backend be_http:bookinfo:productpage server pod:productpage-v1-658849bb5-8gcl7:productpage:http:10.128.4.21:9080 10.128.4.21:9080 cookie 3758caf21badd7e4f729209173eece08 weight 256   Why does this happen? Why are the default router and the router for worker-dev configured? This happens because it is the default router and we must explicitly tell it to ignore certain labels.\n Modify the default IngressController\n oc edit ingresscontroller.operator default -n openshift-ingress-operator   Add the following\n namespaceSelector: matchExpressions: - key: environment operator: NotIn values: - worker-dev - worker-test   This will tell the default IngressController to ignore selectors on worker-dev and worker-test\n Wait a few seconds until the route pods have been restarted:\n oc get pods -n openshift-ingress NAME READY STATUS RESTARTS AGE router-default-744998df46-8lh4t 1/1 Running 0 2m32s router-default-744998df46-hztgf 1/1 Running 0 2m31s router-ingress-worker-dev-76b65cf558-mspvb 1/1 Running 0 96m router-ingress-worker-dev-76b65cf558-p2jpg 1/1 Running 0 96m router-ingress-worker-test-6bbf9967f-4whfs 1/1 Running 0 96m router-ingress-worker-test-6bbf9967f-jht4w 1/1 Running 0 96m   And test again\n oc rsh -n openshift-ingress router-default-744998df46-8lh4t cat haproxy.config | grep productpage --\u0026gt; empty result       At this point the new router feels responsible. Be sure to have a load balancer configured correctly.       Appendix Bind other infra-workload to infrastructure nodes:\n Internal Registry oc patch configs.imageregistry.operator.openshift.io/cluster -n openshift-image-registry --type=merge --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;nodeSelector\u0026#34;:{\u0026#34;node-role.kubernetes.io/infra\u0026#34;:\u0026#34;\u0026#34;}}}\u0026#39;    OpenShift Monitoring Workload Create the following file and apply it.\n cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; cluster-monitoring-config-cm.yaml apiVersion: v1 kind: ConfigMap metadata: name: cluster-monitoring-config namespace: openshift-monitoring data: config.yaml: |+ alertmanagerMain: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; prometheusK8s: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; prometheusOperator: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; grafana: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; k8sPrometheusAdapter: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; kubeStateMetrics: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; telemeterClient: nodeSelector: node-role.kubernetes.io/infra: \u0026#34;\u0026#34; EOF   oc create -f cluster-monitoring-config-cm.yaml     "},{"uri":"https://blog.stderr.at/categories/aro/","title":"ARO","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/azure/","title":"Azure","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/azure/2021-10-29-private-aro/","title":"Stumbling into Azure Part II: Setting up a private ARO cluster","tags":[],"description":"","content":"  In Part I of our blog post we covered setting up required resources in Azure. Now we are finally going to set up a private cluster. Private\n As review from Part I here is our planned setup, this time including the ARO cluster.\nAzure Setup   The diagram below depicts our planned setup:\n On the right hand side can see the resources required for our lab:\n  a virtual network (vnet 192.168.128.0/19). This vnet will be split into 3 separate subnets\n  a master subnet (192.168.129.0/24) holding the ARO control plane nodes\n  a node subnet (192.168.130.0/24) holding ARO worker nodes\n  and finally a subnet call GatewaySubnet where we are going to deploy our Azure VPN gateway (called a vnet-gateway)\n  The subnet where the Azure VPN gateway is located needs to have the name GatewaySubnet. Otherwise creating the Azure VPN gateway will fail.\n   we also need a publicIP resource that we are going to connect to our vnet-gateway (the VPN gateway)\n  and finally a local-gateway resource that tells the vnet-gateway which networks are reachable on the left, in our case the Hetzner server.\n  Creating the private Azure Red Hat OpenShift cluster    Register required resource providers\naz provider register -n Microsoft.RedHatOpenShift --wait az provider register -n Microsoft.Compute --wait az provider register -n Microsoft.Storage --wait az provider register -n Microsoft.Authorization --wait      First we are going to set some environment variable. Those variables are used in the upcoming commands:\nexport RESOURCEGROUP=aro-rg export CLUSTER=\u0026#34;aro1\u0026#34; export GATWAY_SUBNET=\u0026#34;192.168.128.0/24\u0026#34; export MASTER_SUBNET=\u0026#34;192.168.129.0/24\u0026#34; export WORKER_SUBNET=\u0026#34;192.168.130.0/24\u0026#34; export HETZNER_VM_NETWORKS=\u0026#34;10.0.0.0/24 192.168.122.0/24 172.16.100.0/24\u0026#34;    Disable subnet private endpoint policies\naz network vnet subnet update \\ --name master-subnet \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --disable-private-link-service-network-policies true    Create a private DNS zone for our cluster\naz network private-dns zone create -n private2.tntinfra.net -g aro-rg    Create the cluster\naz aro create \\ --resource-group $RESOURCEGROUP \\ --name $CLUSTER \\ --vnet aro-vnet \\ --master-subnet master-subnet \\ --worker-subnet worker-subnet \\ --apiserver-visibility Private \\ --ingress-visibility Private \\ --domain private.tntinfra.net # --pull-secret @pull-secret.txt # [OPTIONAL]    After successful cluster creating add DNS entry for the API and Ingress\n Query the Azure API for the API server IP and the ingress IP addresses:\naz aro show -n aro1 -g aro-rg --query \u0026#39;{api:apiserverProfile.ip, ingress:ingressProfiles[0].ip}\u0026#39;   Example output\nApi Ingress ------------- --------------- 192.168.129.4 192.168.130.254   Add entries to Azure private DNS\naz network private-dns record-set a add-record -g aro-rg -z private.tntinfra.net -a \u0026#34;192.168.129.4\u0026#34; -n api az network private-dns record-set a add-record -g aro-rg -z private.tntinfra.net -a \u0026#34;192.168.130.254\u0026#34; -n \u0026#34;*.apps\u0026#34;   List entries to verify configuration\naz network private-dns record-set a list -g aro-rg -z private.tntinfra.net   Output:\nName ResourceGroup Ttl Type AutoRegistered Metadata ------ --------------- ----- ------ ---------------- ---------- api aro-rg 3600 A False *.apps aro-rg 3600 A False    List cluster credentials after successful setup\naz aro list-credentials \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP    Get the console URL\naz aro show \\ --name $CLUSTER \\ --resource-group $RESOURCEGROUP \\ --query \u0026#34;consoleProfile.url\u0026#34; -o tsv        DNS, curl   this works, dunno why?\ndig @192.168.129.7 console-openshift-console.apps.xm7rdz4r.westeurope.aroapp.io   use curl to access the internal API and see if it works:\ncurl -kv https://192.168.129.4:6443    Additional Resources    Build an Azure site-to-site VPN for DevTest\n  Create a virtual network with a Site-to-Site VPN connection using CLI\n  Libreswan: Disable rp_filter for IPsec\n  Libreswan: NAT and IPsec not working\n  Libreswan: Subnet to subnet VPN\n    "},{"uri":"https://blog.stderr.at/ansible/2021/10/automation-controller-ad-ldap-authentication/","title":"Automation Controller ad LDAP Authentication","tags":["Ansible","Controller","Automation Controller","LDAP","Authentication"],"description":"Enable LDAP authentication and create an example LDAP server","content":"The following article shall quickly, without huge background information, deploy an Identity Management Server (based on FreeIPA) and connect this IDM to an existing Automation Controller so authentication can be tested and verified based on LDAP.\n Install FreeIPA Run the following command to deploy and configure the IPA Server:\n  yum module enable idm:DL1\n  yum distro-sync\n  yum -y module install idm:DL1/server\n  Install the server by calling the command ipa-server-install. This will start an interactive installation modus which requires the basic information about the IPA server. The following uses tower.local as base domain\n   Do you want to configure integrated DNS (BIND)? [no]: Server host name [node01.tower.local]: Please confirm the domain name [tower.local]: Please provide a realm name [TOWER.LOCAL]: Directory Manager password: \u0026lt;enter password\u0026gt; Password (confirm): \u0026lt;enter password\u0026gt; IPA admin password: \u0026lt;enter password\u0026gt; Password (confirm): \u0026lt;enter password\u0026gt; Do you want to configure chrony with NTP server or pool address? [no]: Continue to configure the system with these values? [no]: yes   Once all information have been provided the installation/configuration process starts. This will take a while…​\n     Be sure that the hostname, here node01.tower.local, is resolvable, at least from the Tower/Controller node and the node you are accessing the FreeIPA UI. You can use your local hosts file or a real domain name for that.       Login to IPA server via Command Line  For user admin use: kinit admin\n     Create a Binduser (BindDN) The Binduser (or BindDN) will be used by the Controller to authenticate the Controller against the LDAP server.\n  Create the actual user\nipa user-add --first=”BindUser” --last=”None” --password binduser   Output:\n Password: Enter Password again to verify: ------------------ Added user \u0026#34;binduser\u0026#34; ------------------ User login: binduser First name: ”BindUser” Last name: ”None” Full name: ”BindUser” ”None” Display name: ”BindUser” ”None” Initials: ”” Home directory: /home/binduser GECOS: ”BindUser” ”None” Login shell: /bin/sh Principal name: binduser@TOWER.LOCAL Principal alias: binduser@TOWER.LOCAL User password expiration: 20211015133112Z Email address: binduser@tower.local UID: 1573400003 GID: 1573400003 Password: True Member of groups: ipausers Kerberos keys available: True     Assign the new user to the admin group\nipa group-add-member admins --users=binduser   Output:\n     Group Name: admins Description: Account administrators group GID: 1573400000 Member users: admin, binduser ----------------------------------- Number of members added 1 -----------------------------------    Create a 2nd User to test the authentication later\n   ipa user-add --first=”User” --last=”Name” --password user1     Enable LDAP Auth in Automation Controller  Login to Automation Controller ad go to \u0026#34;Settings \u0026gt; LDAP Settings \u0026gt; Default\u0026#34;\n  add a new connection:\n LDAP Service URI: ldap://node01.tower.local:389\n  LDAP Bind Password: \u0026lt;password of user binduser\u0026gt;`\n  LDAP Group Type: MemberDNGroupType\n  LDAP Bind DN: uid=binduser,cn=users,cn=accounts,dc=tower,dc=local\n  LDAP User Search:\n[ \u0026#34;cn=users,cn=accounts,dc=tower,dc=local\u0026#34;, \u0026#34;SCOPE_SUBTREE\u0026#34;, \u0026#34;(uid=%(user)s)\u0026#34; ]     LDAP Group Search:\n[ \u0026#34;cn=groups,cn=accounts,dc=tower,dc=local\u0026#34;, \u0026#34;SCOPE_SUBTREE\u0026#34;, \u0026#34;(objectClass=posixgroup)\u0026#34; ]         The configuration should look like the following image:\n  Figure 1. Automation Controller LDAP Authentication    Verify Login with user1 You can now test the login using user1. If it does not work, check the following files for errors:\n Tower Node: /var/log/tower/tower.log\n IPA Node: /var/log/dirsrv/slapd-TOWER-LOCAL/access\n     The login should work, but since the user1 is not assigned to any Team/Organization inside the Automation Controller, no privileges are granted. The user can do nothing.       Automatically assign permissions 2 roles can be automatically assigned to authenticated users:\n  Super User\n  Auditor\n   To test this, 2 groups will be created in the LDAP server and a new user will be assigned to one of the groups.\n  Create the group for super users: ipa group-add tower_administrators\n  Create the group for auditors: ipa group-add tower_auditors\n  Create a new user: ipa user-add --first=”User” --last=”Name” --password user2\n  Assign the user to one the the groups: ipa group-add-member tower_administrators --users=user2\n  Modify the Controller LDAP configuration and set LDAP User Flags by Group. This will assing any member of tower_administrators to is_superuser for example.\n{ \u0026#34;is_superuser\u0026#34;: [ \u0026#34;cn=tower_administrators,cn=groups,cn=accounts,dc=tower,dc=local\u0026#34; ], \u0026#34;is_system_auditor\u0026#34;: [ \u0026#34;cn=tower_auditors,cn=groups,cn=accounts,dc=tower,dc=local\u0026#34; ] }      Test the authentication and authorization with the user2. This user should now gain super admin permissions.\n   Allow Users From Specific Groups Only Not all LDAP users shall be able to authenticate. Only users, which are member of a specific group, shall be able to authenticate.\n  Create a 3rd user: ipa user-add --first=”User” --last=”Name” --password user3\n  Modify the LDAP Configuration in Automation Controller and set LDAP Require Groups:\n\u0026#34;cn=towerusers,cn=groups,cn=accounts,dc=tower,dc=local\u0026#34;     Add the group toweruser: ipa group-add towerusers\n  Assign the user user3 to that group: ipa group-add-member towerusers --users=user3\n   At this state only user3 will be able to login. In order to allow the other users as well, all must be assigned to the group towerusers\n ipa group-add-member towerusers --users=user3 ipa group-add-member towerusers --users=user1     Additional Configuration It is possible to automatically map users to Controller Organization. I did not fully test this, but the following is an example:\n  { \u0026#34;LDAP Organization\u0026#34;: { \u0026#34;admins\u0026#34;: \u0026#34;cn=engineering_admins,ou=groups,dc=example,dc=com\u0026#34;, \u0026#34;remove_admins\u0026#34;: false, \u0026#34;users\u0026#34;: [ \u0026#34;cn=engineering,ou=groups,dc=example,dc=com\u0026#34;, \u0026#34;cn=sales,ou=groups,dc=example,dc=com\u0026#34;, \u0026#34;cn=it,ou=groups,dc=example,dc=com\u0026#34; ], \u0026#34;remove_users\u0026#34;: false }, \u0026#34;LDAP Organization 2\u0026#34;: { \u0026#34;admins\u0026#34;: [ \u0026#34;cn=Administrators,cn=Builtin,dc=example,dc=com\u0026#34; ], \u0026#34;remove_admins\u0026#34;: false, \u0026#34;users\u0026#34;: true, \u0026#34;remove_users\u0026#34;: false } }    "},{"uri":"https://blog.stderr.at/tags/ldap/","title":"LDAP","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/quay/","title":"Quay","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/azure/2021-10-17-s2s-vpn/","title":"Stumbling into Azure Part I: Building a site-to-site VPN tunnel for testing","tags":[],"description":"","content":"  So we want to play with ARO (Azure Red Hat OpenShift) private clusters. A private cluster is not reachable from the internet (surprise) and is only reachable via a VPN tunnel from other networks.\n This blog post describes how we created a site-to-site VPN between a Hetzner dedicated server running multiple VM\u0026#39;s via libvirt and Azure.\n An upcoming blog post is going to cover the setup of the private ARO cluster.\nAzure Setup   The diagram below depicts our planned setup:\n On the right hand side can see the resources required for our lab:\n  a virtual network (vnet 192.168.128.0/19). This vnet will be split into 3 separate subnets\n  a master subnet (192.168.129.0/24) holding the ARO control plane nodes\n  a node subnet (192.168.130.0/24) holding ARO worker nodes\n  and finally a subnet call GatewaySubnet where we are going to deploy our Azure VPN gateway (called a vnet-gateway)\n  The subnet where the Azure VPN gateway is located needs to have the name GatewaySubnet. Otherwise creating the Azure VPN gateway will fail.\n   we also need a publicIP resource that we are going to connect to our vnet-gateway (the VPN gateway)\n  and finally a local-gateway resource that tells the vnet-gateway which networks are reachable on the left, in our case the Hetzner server.\n  Creating the required Azure resources    First we are going to set some environment variable. Those variables are used in the upcoming commands:\nexport RESOURCEGROUP=aro-rg export GATWAY_SUBNET=\u0026#34;192.168.128.0/24\u0026#34; export MASTER_SUBNET=\u0026#34;192.168.129.0/24\u0026#34; export WORKER_SUBNET=\u0026#34;192.168.130.0/24\u0026#34; export HETZNER_VM_NETWORKS=\u0026#34;10.0.0.0/24 192.168.122.0/24 172.16.100.0/24\u0026#34;    Next create a VNET resource holding our sub networks:\naz network vnet create \\ --resource-group $RESOURCEGROUP \\ --name aro-vnet \\ --address-prefixes 192.168.128.0/18    Create the GatewaySubnet subnet\naz network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name GatewaySubnet \\ --address-prefixes $GATEWAY_SUBNET    Create the master subnet\naz network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name master-subnet \\ --address-prefixes $MASTER_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry    Create the worker subnet\naz network vnet subnet create \\ --resource-group $RESOURCEGROUP \\ --vnet-name aro-vnet \\ --name worker-subnet \\ --address-prefixes $WORKER_SUBNET \\ --service-endpoints Microsoft.ContainerRegistry    Create a public IP resource\naz network public-ip create \\ --name GatewayIP \\ --resource-group $RESOURCEGROUP \\ --allocation-method Dynamic    Create a local-gateway resource\naz network local-gateway create \\ --name playground \\ --resource-group $RESOURCEGROUP \\ --local-address-prefixes $HETZNER_VM_NETWORKS \\ --gateway-ip-address 95.217.42.98    Create a vnet-gateway resource (takes around 30 minutes)\naz network vnet-gateway create \\ --name vpn-gateway \\ --public-ip-address GatewayIP \\ --resource-group $RESOURCEGROUP \\ --vnet aro-vnet \\ --gateway-type Vpn \\ --vpn-type RouteBased \\ --sku Basic \\ --no-wait    Define a vpn-connection\naz network vpn-connection create \\ --name VNet1toSite2 \\ --resource-group $RESOURCEGROUP \\ --vnet-gateway1 vpn-gateway \\ --local-gateway2 playground \\ --location westeurope \\ --shared-key thepassword        Required iptables (nf tables) hacks for libvirt  Skip NAT rules if the destination network is in Azure and the client network deploy via libvirt  iptables -I LIBVIRT_PRT 2 -t nat -d 192.168.129.0/24 -j RETURN iptables -I LIBVIRT_PRT 2 -t nat -d 192.168.130.0/24 -j RETURN    Skip NAT rules if the destination network is in Azure and the client is connected via tailscale  iptables -I ts-postrouting 1 -t nat -d 192.168.129.0/24 -j RETURN iptables -I ts-postrouting 1 -t nat -d 192.168.130.0/24 -j RETURN      Libreswan setup on CentOS Stream    Install the Libreswan packages\ndnf install libreswan    Create a Azure configuration for Libreswan in ~/etc/ipsec.d/azure.conf\nconn masterSubnet also=azureTunnel leftsubnet=192.168.129.0/24 rightsubnet=172.16.100.0/24 auto=start conn workerSubnet also=azureTunnel leftsubnet=192.168.130.0/24 rightsubnet=172.16.100.0/24 auto=start conn azureTunnel authby=secret auto=start dpdaction=restart dpddelay=30 dpdtimeout=120 ike=aes256-sha1;modp1024 ikelifetime=3600s ikev2=insist keyingtries=3 pfs=yes phase2alg=aes128-sha1 left=51.137.113.44 leftsubnets=192.168.128.0/24 right=%defaultroute rightsubnets=172.16.100.0/24 salifetime=3600s type=tunnel ipsec-interface=yes    Create a Libreswan secrets file for Azure in /etc/ipsec.d/azure.secrets:\n%any %any : PSK \u0026#34;abc123\u0026#34;    Enable and start the IPsec service\nsystemctl enable --now ipsec    We had to explicitly load the IPsec configuration via\nipsec addconn --config /etc/ipsec.d/azure.conf azureTunnel      Libreswan IPSEC debugging tips    Check the state of the IPsec systemd service\nsystemctl status ipsec    Check the full log of the IPsec systemd service\njournalctl -e -u ipsec    Check the state of the tunnels with the ipsec command line tool\nipsec status   Check for the following lines\n000 Total IPsec connections: loaded 5, active 2 000 000 State Information: DDoS cookies not required, Accepting new IKE connections 000 IKE SAs: total(1), half-open(0), open(0), authenticated(1), anonymous(0) 000 IPsec SAs: total(2), authenticated(2), anonymous(0) 000 000 #130: \u0026#34;azureTunnel/1x1\u0026#34;:500 STATE_V2_ESTABLISHED_CHILD_SA (IPsec SA established); EVENT_SA_REKEY in 2003s; newest IPSEC; eroute owner; isakmp#131; idle; 000 #130: \u0026#34;azureTunnel/1x1\u0026#34; esp.56cf4304@51.137.113.44 esp.6f49e8d3@95.217.42.98 tun.0@51.137.113.44 tun.0@95.217.42.98 Traffic: ESPin=0B ESPout=0B! ESPmax=0B 000 #129: \u0026#34;masterSubnet/0x0\u0026#34;:500 STATE_V2_ESTABLISHED_CHILD_SA (IPsec SA established); EVENT_SA_REKEY in 1544s; newest IPSEC; eroute owner; isakmp#131; idle; 000 #129: \u0026#34;masterSubnet/0x0\u0026#34; esp.6e81e8da@51.137.113.44 esp.6f72bbc8@95.217.42.98 tun.0@51.137.113.44 tun.0@95.217.42.98 Traffic: ESPin=0B ESPout=0B! ESPmax=0B 000 #131: \u0026#34;masterSubnet/0x0\u0026#34;:500 STATE_V2_ESTABLISHED_IKE_SA (established IKE SA); EVENT_SA_REKEY in 2121s; newest ISAKMP; idle;   IPsec specifies properties of connections via security associations (SA). The parent SA is describes the IKEv2 connections, the child SA is the ESP (encapsulated security payload) connection.\n Check IPsec transformation policies\nip xfrm policy   Check the state of IPsec transformation policies\nip xfrm state   Check for dropped packages on the IPsec interface (ipsec1 in our case)\nip -s link show dev ipsec1      Additonal Resources    Build an Azure site-to-site VPN for DevTest\n  Create a virtual network with a Site-to-Site VPN connection using CLI\n  Libreswan: Disable rp_filter for IPsec\n  Libreswan: NAT and IPsec not working\n  Libreswan: Subnet to subnet VPN\n    "},{"uri":"https://blog.stderr.at/quay/2021-09-07-quay-upgrade-3.4/","title":"Stumbling into Quay: Upgrading from 3.3 to 3.4 with the quay-operator","tags":[],"description":"","content":"  We had the task of answering various questions related to upgrading Red Hat Quay 3.3 to 3.4 and to 3.5 with the help of the quay-operator.\n Thankfully (sic!) everything changed in regards to the Quay operator between Quay 3.3 and Quay 3.4.\n So this is a brain dump of the things to consider.\nOperator changes   With Quay 3.4 the operator was completely reworked and it basically changed from opinionated to very opinionated. The upgrade works quite well but you have to be aware about the following points:\n  The name of the custom resource changed from QuayEcosystem to QuayRegistry\n  The configHostname, used for providing the quay configuration UI, is no longer configurable\n  The password for the configuration UI is always regenerated after a configuration re-deployment\n  The volume size of the PostgreSQL PVC will change to 50G\n   In my test cluster I was using a 10G Ceph block device and the StorageClass did not support volume expansion. So my upgrade stopped at this point and I had to allow volume expansion in the storage class.\n   A horizontal pod autoscaler is also deploy during the upgrade. The default is to scale automatically to 20 pods, you might reconsider this…\n  With the Quay operator version 3.5 the operator is monitoring all namespaces for custom resources and needs to be installed in the openshift-operators namespace, this is how we upgraded Quay to 3.5 including the operator:\n  change the quay operator channel to 3.5\n  trigger an upgrade\n  now Quay gets upgraded to version 3.5\n  after the Quay upgrade you need to reinstall the operator:\n  deinstall the quay operator, Quay is not affected by this\n  reinstall the Quay operator (3.5) in all-namespaces\n  the re-installation of the operator triggers a quay deployment, all Quay pods are restarted!\n      You have to manually cleanup old\n  postgres-config-secrets and\n  quay-config-bundle\u0026#39;s\n      Backup and restore considerations   Red Hat is working on providing documentation on how to backup and restore Quay in various scenarios. There\u0026#39;s an open task for this that provides more information https://issues.redhat.com/browse/PROJQUAY-2242.\n  "},{"uri":"https://blog.stderr.at/tags/sealed-secret/","title":"Sealed Secret","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2021-09-25-sealed_secrets/","title":"Secure your secrets with Sealed Secrets","tags":["Storage","OpenShift","OCP","Sealed Secret"],"description":"Using Sealed Secrets to encrypt your secrets and be able to upload them to Git","content":"Working with a GitOps approach is a good way to keep all configurations and settings versioned and in sync on Git. Sensitive data, such as passwords to a database connection, will quickly come around. Obviously, it is not a idea to store clear text strings in a, maybe even public, Git repository. Therefore, all sensitive information should be stored in a secret object. The problem with secrets in Kubernetes is that they are actually not encrypted. Instead, strings are base64 encoded which can be decoded as well. Thats not good …​ it should not be possible to decrypt secured data. Sealed Secret will help here…​\n Sealed Secrets by Bitnami[1] is one option to create real, encrypted secrets. It contains two parts:\n  A cluster-side controller / operator, which decrypts the secrets server-side on OpenShift installed in a dedicated namespace usually called sealed secrets.\n  kubeseal - a client-side command line tool\n   Prerequisites   An OpenShift 4 cluster with cluster-admin permissions.\n     Sealed Secrets Operator  Goto OperatorHub and search for Sealed Secrets (This is a Community Operator)\n    Figure 1. Search Sealed Secrets in OperatorHub   Install the operator, using the default settings, into the namespace sealed-secrets\n    Figure 2. Installed Sealed Secret Operator    Install the CRD SealedSecretController Install the following object. For now the default values can be used.\n apiVersion: bitnami.com/v1alpha1 kind: SealedSecretController metadata: name: controller (1) namespace: sealed-secrets spec: networkPolicy: false nodeSelector: {} podLabels: {} resources: {} affinity: {} securityContext: fsGroup: \u0026#39;\u0026#39; runAsUser: \u0026#39;\u0026#39; rbac: create: true pspEnabled: false crd: create: true keep: true ingress: annotations: {} enabled: false hosts: - chart-example.local path: /v1/cert.pem tls: [] serviceAccount: create: true name: \u0026#39;\u0026#39; image: pullPolicy: IfNotPresent repository: \u0026gt;- quay.io/bitnami/sealed-secrets-controller@sha256:8e9a37bb2e1a6f3a8bee949e3af0e9dab0d7dca618f1a63048dc541b5d554985 secretName: sealed-secrets-key tolerations: [] controller: create: true priorityClassName: \u0026#39;\u0026#39; podAnnotations: {}     1 Be aware of the name of the controller OBJECT (name: controller). It is used lated as part of the actual controller name      Install the command line tool kubeseal The kubeseal binary can be easily installed using either\n on Mac: brew install kubeseal or\n on Linux:\n wget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.16.0/kubeseal-linux-amd64 -O kubeseal install -m 755 kubeseal /usr/local/bin/     Testing Sealed Secrets  Create a new project oc new-project myproject\n  Create a secret\necho -n \u0026#34;my_super_secret_string\u0026#34; \\ | kubectl create secret generic mypasswords --dry-run=client --from-file=password=/dev/stdin -o json \\ | kubeseal --controller-namespace=sealed-secrets --controller-name=controller-sealed-secrets --format json \u0026gt; mysealedsecret.json (1)     1 The switches --controller-namespace define the namespace where the operator is installed, --controller-name is a combination of the SealedSecretController object name and the name of the namespace    The password=my_super_secret_string is created and piped into kubeseal which is using the controller, where the server created a certificate for encryption, to create an encrypted json file mysealedsecret.json. It is important to note, that the actually Kubernetes secret object is not created at this stage.\n The file mysealedsecret.json is encrypted now and it is safe to store this file on Github.\n It looks like this:\n { \u0026#34;kind\u0026#34;: \u0026#34;SealedSecret\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;bitnami.com/v1alpha1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mypasswords\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;myproject\u0026#34;, (1) \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;spec\u0026#34;: { \u0026#34;template\u0026#34;: { \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mypasswords\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;myproject\u0026#34;, (1) \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;data\u0026#34;: null }, \u0026#34;encryptedData\u0026#34;: { \u0026#34;password\u0026#34;: \u0026#34;AgBsSZVcTfzfNFI7ZlCsH3/4b3L7m52/O9f70pMtn1myPWHeY1QJFoxpWkH0tWosfeIoko+iB0kCyFk/iJEYSvd31zgnr90hv4e2qVtEBmm6n5B7V40ZERdiy2Cz7UXakUKDdhTjA0BTjcf0f0b2FRDenGxCHJB7cyOVGOZ36jF6IdP2k6kbsZXklti/4MXK7oskDXGzU7rTsESK0ttk5uQgrpfWrhaUip5+Db5vcG1OlHhMJ7In3NlNr0mbl+YiXsKKDNvyw9T14L3rlfvHz1xe0lIqC72i5LSCarpGoSKNOr+Sev9+b/+no6P4VDPuSLORbwVXlP5kt+8xnpZJIEqnetwhr78dt8F3xmjXVBZncdwKk22Y/b9L+uUKWPAvOT78khpUIHQPo9dV/nmz1ldvu58fCFL4TjOOtyTBcUPD3qQJp+sEXgy63l8hEaMXuLUlk+srSnJfMtwkFhl0CG2fKsg4CsQoZlvq5oKOl50sujg3Trv4W9qVVCYHA7BUXEj6J0DxjOCqSQixHRr7Z7JqIyhhdLYdHwMH80scsIb6Ok7keC82v1yae770NWWxJJ4M7Ieb2ERzgwy825gkdq9nx9I6fVxYJkkZlpKKoTvL0uno4sKjC1yQjCgW1vpiZeLIJO2f9TpvVdK2nrag0/gXPMboAL2BGnMPMwjR7OZm+iHq3NXNKiIV1aWRO4wkd/spWziLjOpeS7T1k9w4XxoACwv3g4it\u0026#34; } } }     1 The sealed secret will be created in your project      Upload the sealed secret oc create -f mysealedsecret.json\n     Verify Secret The object SealedSecret is created:\n oc get SealedSecret NAME AGE mypasswords 3s   The SealedSecretController will decrypt the and store the secret in the namespace. This can take a few seconds:\n oc get secret mypasswords NAME TYPE DATA AGE mypasswords Opaque 1 25s   Extract the secret and verify that your string has been stored as \u0026#34;normal\u0026#34; secret\n oc extract secret/mypasswords --to=- # password my_super_secret_string     Updating or appending new values The process for updateing or appending a secret is similar. The only difference is that a new value for the key string is new.\n # Updaing string echo -n \u0026#34;my_NEW_super_secret_string\u0026#34; \\ | kubectl create secret generic mypasswords --dry-run=client --from-file=password=/dev/stdin -o json \\ | kubeseal --controller-namespace=sealed-secrets --controller-name=controller-sealed-secrets --format json --merge-into mysealedsecret.json # Appending echo -n \u0026#34;my_appended_string\u0026#34; \\ | kubectl create secret generic mypasswords --dry-run=client --from-file=appendedstring=/dev/stdin -o json \\ | kubeseal --controller-namespace=sealed-secrets --controller-name=controller-sealed-secrets --format json --merge-into mysealedsecret.json       Be sure that you are in the namespace you want to install the secret     Upload the sealed secret oc apply -f mysealedsecret.json and extract it again to validate:\n oc extract secret/mypasswords --to=- # appendedstring my_appended_string # password my_NEW_super_secret_string     Sources   [1]: Bitname Readme on Github\n    "},{"uri":"https://blog.stderr.at/tags/storage/","title":"Storage","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/java/2022-02-23-jpa-disconnected-entity/","title":"Adventures in Java Land: JPA disconnected entities","tags":[],"description":"","content":" Prefix   An old man tries to refresh his Java skills and does DO378. He fails spectacularly at the first real example but learns a lot on the way.\n  The exception   There is this basic example where you build a minimal REST API for storing speaker data in a database. Quarkus makes this quite easy. You just have to define your database connection properties in resources/application.properties and off you go developing your Java Quarkus REST service:\nquarkus.datasource.db-kind=h2 quarkus.datasource.jdbc.url=jdbc:h2:mem:default quarkus.datasource.username=admin quarkus.hibernate-orm.database.generation=drop-and-create quarkus.hibernate-orm.log.sql=true   So next we define our entity class to be stored in the database. I will skip the import statements and any other code not relevant for this post.\n// import statements skipped @Entity public class Speaker extends PanacheEntity { public UUID uuid; public String nameFirst; public String nameLast; public String organization; @JsonbTransient public String biography; public String picture; public String twitterHandle; // Constructors, getters and setters, toString and other methods skipped  .... }   We define an entity Speaker which extends the PanacheEntity class. Panache is a thin wrapper around Hibernate providing convince features. For example the base class PanacheEntity defines a autoincrement Id column for us. This inherited Id column is of importance for understanding the problem ahead of us.\n So next you define your SpeakerService class which uses the entity. Once again I will skip the imports and any code not relevant for understanding the problem:\n// imports omitted  @ApplicationScoped public class SpeakerService { // other code omitted  public Speaker create(Speaker speaker) { speaker.persist(); return speaker; }   We focus on the create method here because the call to speaker.persist() was the reason for all the headache.\n But we are still in coding mode and last but not least we define our SpeakerResource class, again everything not relevant for understanding the problem was removed:\n// import statements omitted  @Path(\u0026#34;/speaker\u0026#34;) @Produces(MediaType.APPLICATION_JSON) @Consumes(MediaType.APPLICATION_JSON) public class SpeakerResource { @Inject SpeakerService service; // other code omitted  @POST @Transactional public Speaker create(Speaker newSpeaker) { service.create(newSpeaker); return newSpeaker; } }   The root path for our SpeakerResource is /speaker. We inject the SpeakerService and define a method create() for creating a Speaker. We would like to be able to send @Post requests to this endpoint and Jsonb orJackson, whichever we currently prefer, will deserialize the JSON body in a Speaker object for us.\n Splendid, time to switch from coding mode to testing.\n We launch that Quarkus application in developer mode\nmvn quarkus:dev   Quarkus is so friendly and provides a swagger-ui in dev mode for testing our endpoint. Super duper lets call the create() endpoint via Swagger:\n  Because we are lazy we accept the default Swagger provides for us and just click Execute.\n BOOM, 500 internal server error. And a beautiful Java exception:\norg.jboss.resteasy.spi.UnhandledException: javax.persistence.PersistenceException: org.hibernate.PersistentObjectException: detached entity passed to persist: org.acme.conference.speaker.Speaker   What? Detached entity what does this mean and why?\n  Enlightenment   Behind the scenes Hibernate uses a so called EntityManager for managing entities. An Entity can be in the following states when managed by Hibernate:\n  NEW: The entity object was just created and is not persisted to the database\n  MANAGED: The entity is managed by a running Session and all changes to the entity will be propagated to the database. After call to entitymanager.persist() or in our case newSpeaker.persist() the entity is stored in the database and in the managed state.\n  REMOVED: The entity is removed from the database. And finally\n  DETACHED: The Entity was detached from the EntityManager, e.g. by calling entitymanager.detach() or entitymanager.close().\n  See this blog for a way better explanation what is going on with entity states.\n Ok, cool but why the hell is our Speaker entity in the DETACHED state? It was just created and never saved to the database before!\n After checking the database (was empty), I started my Java debugger of choice (IntellJ, but use whatever fit\u0026#39;s your needs. I\u0026#39;m to old for IDE vs Editor and Editor vs Editor wars).\n So looking at the Speaker entity before calling persist() revealed the following:\n  The Speaker object passed into create() has an Id of 0 and all the internal Hibernate fields are set to null. So this seems to indicate that this Speaker object is currently not attached to an EntityManager session. This might explain the DETACHED state.\n I started playing around with EntityManager and calling merge() on the speaker object. The code looked like this:\n@ApplicationScoped public class SpeakerService { @Inject EntityManager em; // lots of code skipped  public Speaker create(Speaker speaker) { var newSpeaker = em.merge(speaker); newSpeaker.persist(); return speaker; }   Looking at the newSpeaker object returned by calling entitymanager.merge() in the debugger revealed the following:\n  newSpeaker has an Id of 1 (hm, why no 0?) and some those special Hibernate fields starting with $$ have a value assigned. So for me this indicates that the object is now managed by an EntityManager session and in the MANAGED state.\n And the Id, already assigned to the original Speaker object, de-serialized form JSON is actually the reason for the beautiful exception above.\n  Explanation   So after a little bit of internet search magic I found an explanation for the exception:\n  If an Id is already assigned to an entity object, Hibernate assumes that this is an entity in the DETACHED state (if the Id is auto-generated). For an entity to be persisted to the database it has to be transferred in the MANAGED state by calling entitymanager.merge()\n For more information see the Hibernate documentation.\n  We can only call persist() if the object is in the transient state, to quote the Hibernate documentation:\n transient: the entity has just been instantiated and is not associated with a persistence context. It has no persistent representation in the database and typically no identifier value has been assigned (unless the assigned generator was used).\n And reading on we also get explanation for the detached state:\n detached: the entity has an associated identifier but is no longer associated with a persistence context (usually because the persistence context was closed or the instance was evicted from the context)\n Just removing the Id from the POST request will solve the issue and the example started to work.\n This is also why the Id column is different in the Speaker object (deserialized from JSON) and newSpeaker object (create by calling entitymanager.merge()). The Speaker Id got passed in from JSON, and has nothing to do with the auto generated primary key Id within our database. After calling entitymanager.merge() the entity is actually associated with a database session and the Id is auto generated.\n So maybe this is basic stuff, but it took me quite a few hours to understand what was going on.\n Maybe this is also a bad example. Should one expose the Id if it is auto generated and only used internally? Or the code just needs to handle that case… But this needs me more learning about API design.\n  "},{"uri":"https://blog.stderr.at/categories/java/","title":"Java","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/affinity/","title":"Affinity","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/affinity/","title":"Affinity","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/anti-affinity/","title":"Anti-Affinity","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/anti-affinity/","title":"Anti-Affinity","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/descheduler/","title":"Descheduler","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/descheduler/","title":"Descheduler","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/","title":"Introduction","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector","Affinity","Anti-Affinity"],"description":"Introduction to Pod Placement","content":"Pod scheduling is an internal process that determines placement of new pods onto nodes within the cluster. It is probably one of the most important tasks for a Day-2 scenario and should be considered at a very early stage for a new cluster. OpenShift/Kubernetes is already shipped with a default scheduler which schedules pods as they get created accross the cluster, without any manual steps.\n However, there are scenarios where a more advanced approach is required, like for example using a specifc group of nodes for dedicated workload or make sure that certain applications do not run on the same nodes. Kubernetes provides different options:\n   Controlling placement with node selectors\n  Controlling placement with pod/node affinity/anti-affinity rules\n  Controlling placement with taints and tolerations\n  Controlling placement with topology spread constraints\n   This series will try to go into the detail of the different options and explains in simple examples how to work with pod placement rules. It is not a replacement for any official documentation, so always check out Kubernetes and or OpenShift documentations.\n Pod Placement Series  NodeSelector\n  Pod Affinity and Anti Affinity\n  Node Affinity\n  Taints and Tolerations\n  Topology Spread Constraints\n  Descheduler\n     Prerequisites     The following prerequisites are used for all examples.     Let’s image that our cluster (OpenShift 4) has 4 compute nodes\n oc get node --selector=\u0026#39;node-role.kubernetes.io/worker\u0026#39; NAME STATUS ROLES AGE VERSION compute-0 Ready worker 7h1m v1.19.0+d59ce34 compute-1 Ready worker 7h1m v1.19.0+d59ce34 compute-2 Ready worker 7h1m v1.19.0+d59ce34 compute-3 Ready worker 7h1m v1.19.0+d59ce34   An example application (from the catalog Django + Postgres) has been deployed in the namespace podtesting. It contains by default 1 pod for a PostGresql database and one pod for a frontend web application.\n oc get pods -n podtesting -o wide | grep Running django-psql-example-1-h6kst 1/1 Running 0 20m 10.130.2.97 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-1-4pcm4 1/1 Running 0 21m 10.131.0.51 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Without any configuration the OpenShift scheduler will try to spread the pods evenly accross the cluster.\n Let’s increase the replica of the web frontend:\n oc scale --replicas=4 dc/django-psql-example -n podtesting   Eventually 4 additional pods will be started accross the compute nodes of the cluster:\n oc get pods -n podtesting -o wide | grep Running django-psql-example-1-842fl 1/1 Running 0 2m7s 10.131.0.65 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-h6kst 1/1 Running 0 24m 10.130.2.97 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-pxhlv 1/1 Running 0 2m7s 10.128.2.13 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-xms7x 1/1 Running 0 2m7s 10.129.2.10 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-1-4pcm4 1/1 Running 0 26m 10.131.0.51 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   As you can see, the scheduler already tries to spread the pods evenly, so that every worker node will host one frontend pod.\n However, let’s try to apply a more advanced configuration for the pod placement, starting with the Pod Placement - NodeSelector\n  "},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-29-node-affinity/","title":"Node Affinity","tags":["OCP","Day-2","OpenShift","Pod Placement","Affinity"],"description":"Placeing Pods Using the Node Affinity rules","content":"Node Affinity allows to place a pod to a specific group of nodes. For example, it is possible to run a pod only on nodes with a specific CPU or disktype. The disktype was used as an example for the nodeSelector and yes …​ Node Affinity is conceptually similar to nodeSelector but allows a more granular configuration.\n Pod Placement Series Please check out other ways of pod placements:\n   Expand me...    NodeSelector\n  Pod Affinity and Anti Affinity\n  Node Affinity\n  Taints and Tolerations\n  Topology Spread Constraints\n  Descheduler\n        Using Node Affinity Currently two types of affinity settings are known:\n   requiredDuringSchedulingIgnoreDuringExecuption (short required) - a hard requirement which must be met before a pod can be scheduled\n  preferredDuringSchedulingIgnoredDuringExecution (short preferred) - a soft requirement the scheduler tries to meet, but does not guarantee it\n       Both types can be specified. In such case the node must first meet the required rule and then attempt to meet the preferred rule.     Preparing node labels     Remember the prerequisites explained in the . Pod Placement - Introduction. We have 4 compute nodes and an example web application up and running.     Before we start with affinity rules we need to label all nodes. Let’s create 2 zones (east and west) for our compute nodes.\n     You can skip this, if these labels are still set.      Figure 1. Node Zones  oc label nodes compute-0 compute-1 topology.kubernetes.io/zone=east oc label nodes compute-2 compute-3 topology.kubernetes.io/zone=west    Configure node affinity rule Like pod affinity the node affinity is defined on the pod specification:\n kind: DeploymentConfig apiVersion: apps.openshift.io/v1 metadata: name: django-psql-example namespace: podtesting [...] spec: [...] template: [...] spec: [...] affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - west   In this example the pods are started only on nodes of the zone \u0026#34;West\u0026#34;. Since the value is an array, multiple zones can be defined letting the web application be executed on West and East for example. With this setup you can control on which node a specific application shall be executed. For example: you have a group of nodes which provide a GPU and your GPU application must be started only on this group of nodes.\n Like with pod affinity you can combine required and preferred settings.\n    What happened to Node Anti-Affinity? Unlike Pod Anti-Affinity, there is no concept to define a node Anti-Affinity. Instead you can use the NotIn and DoesNotExist operators to achieve this bahaviour.\n   Cleanup As cleanup simply remove the affinity specification from the DeploymentConf. The node labels can stay as they are since they do not hurt.\n   Summary This concludes the quick overview of the node affinity. Further information can be found at Node Affinity\n  "},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/","title":"NodeSelector","tags":["OCP","Day-2","OpenShift","Pod Placement","NodeSelector"],"description":"Placeing Pods Using the NodeSelector","content":"One of the easiest ways to tell your Kubernetes cluster where to put certain pods is to use a nodeSelector specification. A nodeSelector defines a key-value pair and are defined inside the specification of the pods and as a label on one or multiple nodes (or machine set or machine config). Only if selector matches the node label, the pod is allowed to be scheduled on that node.\n Kubernetes distingushes between 2 types of selectors:\n  cluster-wide node selectors: defined by the cluster administrators and valid for the whole cluster\n  project node selectors: to place new pods inside projects into specific nodes.\n   Pod Placement Series Please check out other ways of pod placements:\n   Expand me...    NodeSelector\n  Pod Affinity and Anti Affinity\n  Node Affinity\n  Taints and Tolerations\n  Topology Spread Constraints\n  Descheduler\n        Using nodeSelector As previously described, we have a cluster with an example application scheduled accross the worker nodes evenly by the scheduler.\n oc get pods -n podtesting -o wide | grep Running django-psql-example-1-842fl 1/1 Running 0 2m7s 10.131.0.65 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-h6kst 1/1 Running 0 24m 10.130.2.97 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-pxhlv 1/1 Running 0 2m7s 10.128.2.13 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-1-xms7x 1/1 Running 0 2m7s 10.129.2.10 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-1-4pcm4 1/1 Running 0 26m 10.131.0.51 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   However, our 4 compute nodes are assembled with different hardware specification and are using different harddisks (sdd vs hdd).\n  Figure 1. Nodes with Different Specifications  Since our web application must run on fast disks must configure the cluster to schedule the pods on nodes with SSD only.\n To start using nodeSelectors we first label our nodes accordingly:\n   compute-0 and compute-1 are faster nodes with an SSD attached.\n  compute-2 and compute-2 have a HDD attached.\n   oc label nodes compute-0 compute-1 disktype=ssd (1) oc label nodes compute-2 compute-3 disktype=hdd     1 as key we are using disktype    As crosscheck we can list nodes with a specific label:\n oc get nodes -l disktype=ssd NAME STATUS ROLES AGE VERSION compute-0 Ready worker 7h32m v1.19.0+d59ce34 compute-1 Ready worker 7h31m v1.19.0+d59ce34 oc get nodes -l disktype=hdd NAME STATUS ROLES AGE VERSION compute-2 Ready worker 7h32m v1.19.0+d59ce34 compute-3 Ready worker 7h32m v1.19.0+d59ce34       If no matching label is found, the pod cannot be scheduled. Therefore, always label the nodes first.     The 2nd step is to add the node selector to the specification of the pod. In our example we are using a DeploymentConfig, so let’s add it there:\n oc patch dc django-psql-example -n podtesting --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;nodeSelector\u0026#34;:{\u0026#34;disktype\u0026#34;:\u0026#34;ssd\u0026#34;}}}}}\u0026#39;   This adds the nodeSelector into: spec/template/spec\n nodeSelector: disktype: ssd   Kubernetes will now trigger a restart of the pods on the supposed nodes.\n oc get pods -n podtesting -o wide | grep Running django-psql-example-3-4j92k 1/1 Running 0 42s 10.129.2.7 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-3-d7hsd 1/1 Running 0 42s 10.129.2.8 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-3-fkbfm 1/1 Running 0 14m 10.128.2.18 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-3-psskb 1/1 Running 0 14m 10.128.2.17 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   As you can see, only nodes with a SSD (compute-0 and compute-1) are being used.\n   Controlling pod placement with project-wide selector Adding a nodeSelector to a deployment seems fine…​ until somebody forgets to add it. Then the pods would be started anywhere the scheduler finds suitable. Therefore, it might make sense to use a project-wide node selector, which will automatically be applied on all pods on that project. The project selector is added by the cluster administrator to the Namespace object (no matter what the OpenShift documentation says in it’s example) as openshift.io/node-selector parameter.\n Let’s remove our previous configuration and add the setting to our namespace podtesting:\n  Cleanup\nRemove the nodeSelector from the deployment configuration and wait until all pods have been reshuffeld\n oc patch dc django-psql-example -n podtesting --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/nodeSelector\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;disktype=ssd\u0026#34; }]\u0026#39;     Add the label to the project\noc annotate ns/podtesting openshift.io/node-selector=\u0026#34;disktype=ssd\u0026#34;      The OpenShift scheduler will now spread the Pods accross compute-0 or compute-1 again, but not on compute-2 or 3.\n We can prove that by stressing our cluster (and nodes) a little bit and scale our frontend application to 10:\n oc get pods -n podtesting -o wide | grep Running django-psql-example-4-2jn2l 1/1 Running 0 27s 10.128.2.8 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-6g7ks 1/1 Running 0 7m47s 10.129.2.23 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-752nm 1/1 Running 0 7m47s 10.128.2.7 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-c5jvm 1/1 Running 0 27s 10.129.2.4 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-f5kwg 1/1 Running 0 27s 10.129.2.5 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-g7bcs 1/1 Running 0 7m47s 10.129.2.24 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-h5tgb 1/1 Running 0 27s 10.129.2.6 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-spvpp 1/1 Running 0 28s 10.128.2.5 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-v9qwj 1/1 Running 0 7m48s 10.129.2.22 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-4-zgwcv 1/1 Running 0 27s 10.128.2.6 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   As you can see compute-0 and compute-1 are the only nodes which are used.\n   Well-Known Labels nodeSelector is one of the easiest ways to control where an application shall be started. Working with labels is therefore very important as soon as workload shall be added to the cluster. Kubernetes reserves some labels which can be leveraged and some are already predefined on the nodes, for example:\n   beta.kubernetes.io/arch=amd64\n  kubernetes.io/hostname=compute-0\n  kubernetes.io/os=linux\n  node-role.kubernetes.io/worker=\n  node.openshift.io/os_id=rhcos\n   A list of all known can be found at: [1]\n Two of them I would like to mention here, since they might become very important when designing the placement of pods:\n   topology.kubernetes.io/zone\n  topology.kubernetes.io/region\n   With these two labels you can create availability zones for your cluster. A zone can be seen a logical failure domain and a cluster is typically spanned across multiple zones. This could be a rack in a data center for example, hardware which is sharing the same switch or simply different data centers. Zones are seen as independent to each other.\n A region is made up of one or more zones. A cluster is usually not spanned across multiple region.\n Kubernetes makes a few assumptions about the structure of zones and regions:\n   regions and zones are hierarchical: zones are strict subsets of regions and no zone can be in 2 regions\n  zone names are unique across regions; for example region \u0026#34;africa-east-1\u0026#34; might be comprised of zones \u0026#34;africa-east-1a\u0026#34; and \u0026#34;africa-east-1b\u0026#34;\n     Cleanup This concludes the chapter about nodeSelectors. For the next chapter of the Pod Placement Series (Pod Affinity and Anti Affinity) we need to cleanup our configuration.\n  Scale the frontend down to 2\noc scale --replicas=2 dc/django-psql-example -n podtesting     Remove the label from the namespace\noc annotate ns/podtesting openshift.io/node-selector- (1)     1 The minus at the end defines that this annotation shall be removed      And, just to be sure if you have not done this before, remove the nodeSelector from the DeploymentConfig\noc patch dc django-psql-example -n podtesting --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/nodeSelector\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;disktype=ssd\u0026#34; }]\u0026#39;        Sources   [1]: Well-Known Labels, Annotations and Taints\n    "},{"uri":"https://blog.stderr.at/categories/nodeselector/","title":"nodeSelector","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-28-affinity/","title":"Pod Affinity/Anti-Affinity","tags":["OCP","Day-2","OpenShift","Pod Placement","Affinity","Anti-Affinity"],"description":"Placeing Pods Using the Affinity rules","content":"While noteSelector provides a very easy way to control where a pod shall be scheduled, the affinity/anti-affinity feature, expands this configuration with more expressive rules like logical AND operators, constraints against labels on other pods or soft rules instead of hard requirements.\n The feature comes with two types:\n   pod affinity/anti-affinity - allows constrains against other pod labels rather than node labels.\n  node affinity - allows pods to specify a group of nodes they can be placed on\n   Pod Placement Series Please check out other ways of pod placements:\n   Expand me...    NodeSelector\n  Pod Affinity and Anti Affinity\n  Node Affinity\n  Taints and Tolerations\n  Topology Spread Constraints\n  Descheduler\n        Pod Affinity and Anti-Affinity Affinity and Anti-Affinity controls the nodes on which a pod should (or should not) be scheduled based on labels on Pods that are already scheduled on the node. This is a different approach than nodeSelector, since it does not directly take the node labels into account. That said, one example for such setup would be: You have dedicated nodes for developement and production workload and you have to be sure that pods of dev or prod applications do not run on the same node.\n Affinity and Anti-Affinity are shortly defined as:\n   Pod affinity - tells scheduler to put a new pod onto the same node as other pods (selection is done using label selectors)\nFor example:\n   I want to run where this other labelled pod is already running (Pods from same service shall be running on same node.)\n     Pod Anti-Affinity - prevents the scheduler to place a new pod onto the same nodes with pods with the same labels\nFor example:\n   I definitely do not want to start a pod where this other pod with the defined label is running (to prevent that all Pods of same service are running in the same availability zone.)\n          As described in the official Kubernetes documention two things should be considered:\n  The affinity feature requires processing which can slow down the cluster. Therefore, it is not recommended for cluster with more than 100 nodes.\n  The feature requires that all nodes are consistently labelled (topologyKey). Unintended behaviour might happen if labels are missing.\n         Using Affinity/Anti-Affinity Currently two types of pod affinity/anti-affinity are known:\n   requiredDuringSchedulingIgnoreDuringExecuption (short required) - a hard requirement which must be met before a pod can be scheduled\n  preferredDuringSchedulingIgnoredDuringExecution (short preferred) - a soft requirement the scheduler tries to meet, but does not guarantee it\n       Both types can be defined in the same specification. In such case the node must first meet the required rule and then attempt based on best effort to meet the preferred rule.     Preparing node labels     Remember the prerequisites explained in the . Pod Placement - Introduction. We have 4 compute nodes and an example web application up and running.     Before we start with affinity rules we need to label all nodes. Let’s create 2 zones (east and west) for our compute nodes using the well-known label topology.kubernetes.io/zone\n  Figure 1. Node Zones  oc label nodes compute-0 compute-1 topology.kubernetes.io/zone=east oc label nodes compute-2 compute-3 topology.kubernetes.io/zone=west    Configure pod affinity rule In our example we have one database pod and multiple web application pods. Let’s image we would like to always run these pods in the same zone.\n The pod affinity defines that a pod can be scheduled onto a node ONLY if that node is in the same zone as at least one already-running pod with a certain label.\n This means we must first label the postgres pod accordingly. Let’s labels the pod with security=zone1\n oc patch dc postgresql -n podtesting --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/metadata/labels/security\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;zone1\u0026#34; }]\u0026#39;   After a while the postgres pod is restarted on one of the 4 compute nodes (since we did not specify in which zone this single pod shall be started) - here compute-1 (zone == east):\n oc get pods -n podtesting -o wide | grep Running postgresql-5-6v5h6 1/1 Running 0 119s 10.129.2.24 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   As a second step, the deployment configuration of the web application must be modified. Remember, we want to run web application pods only on nodes located in the same zone as the postgres pods. In our example this would be either compute-0 or compute-1.\n Modify the config accordingly:\n kind: DeploymentConfig apiVersion: apps.openshift.io/v1 [...] namespace: podtesting spec: [...] template: [...] spec: [...] affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security (1) operator: In (2) values: - zone1 (3) topologyKey: topology.kubernetes.io/zone (4)     1 The key of the label of a pod which is already running on that node is \u0026#34;security\u0026#34;   2 As operator \u0026#34;In\u0026#34; is used the postgres pod must have a matching key (security) containing the value (zone1). Other options like \u0026#34;NotIn\u0026#34;, \u0026#34;DoesNotExist\u0026#34; or \u0026#34;Exact\u0026#34; are available as well   3 The value must be \u0026#34;zone1\u0026#34;   4 As topology the topology.kubernetes.io/zone is used. The application can be deployed on nodes with the same label    Setting this (and maybe scaling the replicas up a little bit) will start all frontend pods either on compute-0 or on compute-1.\n In other words: On nodes of the same zone, where the postgres pod with the label security=zone1 is running.\n oc get pods -n podtesting -o wide | grep Running django-psql-example-13-4w6qd 1/1 Running 0 67s 10.128.2.58 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-655dj 1/1 Running 0 67s 10.129.2.28 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-9d4pj 1/1 Running 0 67s 10.129.2.27 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-bdwhb 1/1 Running 0 67s 10.128.2.61 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-d4jrw 1/1 Running 0 67s 10.128.2.57 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-dm9qk 1/1 Running 0 67s 10.128.2.60 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-ktmfm 1/1 Running 0 67s 10.129.2.25 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-ldm56 1/1 Running 0 77s 10.128.2.55 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-mh2f5 1/1 Running 0 67s 10.129.2.29 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-qfkhq 1/1 Running 0 67s 10.129.2.26 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-v88qv 1/1 Running 0 67s 10.128.2.56 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-13-vfgf4 1/1 Running 0 67s 10.128.2.59 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-5-6v5h6 1/1 Running 0 3m18s 10.129.2.24 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;    Configure pod anti-affinity rule For now the database pod and the web application pod are running on nodes of the same zone. However, somebody is asking us to configure it vice versa: the web application should not run in the same zone as postgresql.\n Here we can use the Anti-Affinity feature.\n     As an alternative, it would also be possible to change the operator in the affinity rule from \u0026#34;In\u0026#34; to \u0026#34;NotIn\u0026#34;     kind: DeploymentConfig apiVersion: apps.openshift.io/v1 [...] namespace: podtesting spec: [...] template: [...] spec: [...] affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security (1) operator: In (2) values: - zone1 (3) topologyKey: topology.kubernetes.io/zone (4)   This will force the web application pods to run only on \u0026#34;west\u0026#34; zone nodes.\n django-psql-example-16-4n9h5 1/1 Running 0 40s 10.131.1.53 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-blf8b 1/1 Running 0 29s 10.130.2.63 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-f9plb 1/1 Running 0 29s 10.130.2.64 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-tm5rm 1/1 Running 0 28s 10.131.1.55 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-x8lbh 1/1 Running 0 29s 10.131.1.54 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-16-zb5fg 1/1 Running 0 28s 10.130.2.65 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; postgresql-5-6v5h6 1/1 Running 0 18m 10.129.2.24 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;      Combining required and preferred affinities It is possible to combine requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution. In such case the required affinity MUST be met, while the preferred affinity is tried to be met. The following examples combines these two types in an affinity and anti-affinity specification.\n The podAffinity block defines the same as above: schedule the pod on a node of the same zone, where a pod with the label security=zone1 is running. The podAntiAffinity defines that the pod should not be started on a node if that node has a pod running with the label security=zone2. However, the scheduler might decide to do so as long the podAffinity rule is met.\n spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - zone1 topologyKey: topology.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 (1) podAffinityTerm: labelSelector: matchExpressions: - key: security operator: In values: - zone2 topologyKey: topology.kubernetes.io/zone     1 The weight field is used by the scheduler to create a scoring. The higher the scoring the more preferred is that node.    topologyKey It is important to understand the topologyKey setting. This is the key for the node label. If an affinity rule is met, Kubernetes will try to find suitable nodes which are labelled with the topologyKey. All nodes must be labelled consistently, otherwise unintended behaviour might occur.\n As described in the Kubernetes documentation at Pod Affinity and Anit-Affinity, the topologyKey has some constraints:\n Quote Kubernetes:\n  For pod affinity, empty topologyKey is not allowed in both requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution.\n  For pod anti-affinity, empty topologyKey is also not allowed in both requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution.\n  For requiredDuringSchedulingIgnoredDuringExecution pod anti-affinity, the admission controller LimitPodHardAntiAffinityTopology was introduced to limit topologyKey to kubernetes.io/hostname. If you want to make it available for custom topologies, you may modify the admission controller, or disable it.\n  Except for the above cases, the topologyKey can be any legally label-key.\n   End of quote\n    Cleanup As cleanup simply remove the affinity specification from the DeploymentConf. The node labels can stay as they are since they do not hurt.\n   Summary This concludes the quick overview of the pod affinity. The next chapter will discuss Node Affinity rules, which allows affinity based on node specifications.\n  "},{"uri":"https://blog.stderr.at/tags/taints/","title":"Taints","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/taints/","title":"Taints","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/","title":"Taints and Tolerations","tags":["OCP","Day-2","OpenShift","Pod Placement","Taints","Tolerations"],"description":"Placeing Pods Using Taints and Tolerations","content":"While Node Affinity is a property of pods that attracts them to a set of nodes, taints are the exact opposite. Nodes can be configured with one or more taints, which mark the node in a way to only accept pods that do tolerate the taints. The tolerations themselves are applied to pods, telling the scheduler to accept a taints and start the workload on a tainted node.\n A common use case would be to mark certain nodes as infrastructure nodes, where only specific pods are allowed to be executed or to taint nodes with a special hardware (i.e. GPU).\n Pod Placement Series Please check out other ways of pod placements:\n   Expand me...    NodeSelector\n  Pod Affinity and Anti Affinity\n  Node Affinity\n  Taints and Tolerations\n  Topology Spread Constraints\n  Descheduler\n        Understanding Taints and Tolerations Matching taints and tolerations is defined by a key/value pair and a taint effect.\n For example, a node can be tained as:\n spec: [...] template: [...] spec: taints: - effect: NoExecute key: key1 value: value1 [...]   And a pod can have the matching toleration:\n spec: [...] template: [...] spec: tolerations: - key: \u0026#34;key1\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;value1\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 3600 [...]   This means that the pod is tolerating the taint of the node with the pair key1=value1.\n Parameter: effect Above example is using as effect NoExecute. The following effects are possible:\n   NoSchedule - new pods are not scheduled, existing pods remain\n  PreferNoSchedule - new pods are not preferred but can still be scheduled but the scheduler tries to avoid that, existing pods remain\n  NoExecute - new pods are not scheduled, existing pods (without matching toleration) are removed! The setting tolerationSeconds is used to define a maximum time until a pod is allowed to stay.\n    Parameter: operator For the operator two options are possible:\n   Exists - simply checks if a key exists and key and effect matches. No value should be configured in this case.\n  Equal (default) - with this option key, value and effect must match exactly.\n      Configuring Taints and Tolerations Our compute-0 node is a special node with a GPU installed. We would like that our web application is running on this node and ONLY our web application is running on that node. To achieve this, we will taint the node accordingly with the key/value gpu=enabled and the effect NoExecute and configure a toleration to the DeploymentConfig of our web fronted.\n  Figure 1. Tainting Node      Always configure tolerations before you taint a node. Otherwise the scheduler might not be able to start a pod until it has been configured to tolerate the taints.     First we will set the toleration to the DeploymentConfig:\n oc edit dc/django-psql-example -n podtesting   And add a toleration for the key/value pair, the effect NoExecute and a tolerationSeconds of X seconds.\n spec: [...] template: [...] spec: tolerations: - key: \u0026#34;gpu\u0026#34; value: \u0026#34;enabled\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 30 (1)     1 I am setting the tolerationSeconds to 30 seconds to get it done quicker.    Before we taint our node, let’s check which pods are currently running there:\n oc get pods -A -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,STATE:.status.phase | grep compute-0 | grep Running tuned-hrvl4 compute-0 Running dns-default-ln9s4 compute-0 Running node-resolver-qk24n compute-0 Running node-ca-lxrvf compute-0 Running ingress-canary-fv5w6 compute-0 Running machine-config-daemon-558v4 compute-0 Running grafana-78ccdb8c9d-8rqsp compute-0 Running node-exporter-rn8h4 compute-0 Running prometheus-adapter-66976bf759-fxdcd compute-0 Running multus-additional-cni-plugins-29qgq compute-0 Running multus-mkd87 compute-0 Running network-metrics-daemon-64hrw compute-0 Running network-check-target-l6l7n compute-0 Running ovnkube-node-hrtln compute-0 Running django-psql-example-24-c599b compute-0 Running django-psql-example-24-l7znv compute-0 Running django-psql-example-24-zpg77 compute-0 Running   Multiple different pods are running here, as well as two of our web frontend (django-psql-example)\n The other django-psql-example pods are started accross the cluster:\n oc get pods -n podtesting -o wide oc get pods -n podtesting -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES django-psql-example-25-8ppxc 1/1 Running 0 21s 10.128.0.61 master-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-8rv76 1/1 Running 0 21s 10.130.2.92 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-9m8k2 1/1 Running 0 21s 10.129.0.67 master-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-fhvxg 1/1 Running 0 31s 10.128.2.126 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-kqgwz 0/1 Running 0 21s 10.130.0.71 master-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-m66nn 1/1 Running 0 21s 10.130.0.70 master-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-ntjqb 1/1 Running 0 21s 10.128.0.60 master-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-nxxqh 1/1 Running 0 21s 10.130.2.93 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-p8nbz 1/1 Running 0 21s 10.131.1.183 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-ttr9g 1/1 Running 0 21s 10.129.2.57 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-xn4fp 1/1 Running 0 21s 10.129.2.56 compute-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-xpqf4 1/1 Running 0 21s 10.128.2.127 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-25-xwmwv 1/1 Running 0 21s 10.131.1.184 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   To taint the node we can simply execute the following command. Be sure to use the same values as in the toleration:\n oc adm taint nodes compute-0 gpu=enabled:NoExecute   This will create the following specification in the node object:\n spec: [...] taints: - effect: NoExecute key: gpu value: enabled   OpenShift will allow pods, which are not tolerating the taints, to keep on running for 30 seconds. After that, these pods will be evicted and started elsewhere.\n When we check after the tolerationSeconds time has passed which pods are running on the node compute-0, we will see that most pods have disappeared, except the pods for the webapplication and pods which are part of DaemonSets. (DaemonSets are defined to run on all or specific nodes and are not evicted. The cluster-DaemonSets are tolerating everything)\n oc get pods -A -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,STATE:.status.phase | grep compute-0 | grep Running tuned-hrvl4 compute-0 Running node-resolver-qk24n compute-0 Running node-ca-lxrvf compute-0 Running machine-config-daemon-558v4 compute-0 Running node-exporter-rn8h4 compute-0 Running multus-additional-cni-plugins-29qgq compute-0 Running multus-mkd87 compute-0 Running network-metrics-daemon-64hrw compute-0 Running network-check-target-l6l7n compute-0 Running ovnkube-node-hrtln compute-0 Running django-psql-example-25-8dzqh compute-0 Running django-psql-example-25-9p4sb compute-0 Running django-psql-example-25-pqbvn compute-0 Running django-psql-example-25-sb6hr compute-0 Running     Removing taints Taints can be simply removed with the oc command added a trailing -\n oc adm taint nodes compute-0 gpu=enabled:NoExecute-     Built-in Taints - Taint Nodes by Condition Several taints are built into OpenShift and are set during certain events (aka Taint Nodes by Condition) and cleared when the condition is resolved.\n The following list is quoted from the OpenShift documentation and provides a list of taints which are automatically set. For example, when a node becomes unavailable:\n   node.kubernetes.io/not-ready: The node is not ready. This corresponds to the node condition Ready=False.\n  node.kubernetes.io/unreachable: The node is unreachable from the node controller. This corresponds to the node condition Ready=Unknown.\n  node.kubernetes.io/out-of-disk: The node has insufficient free space on the node for adding new pods. This corresponds to the node condition OutOfDisk=True.\n  node.kubernetes.io/memory-pressure: The node has memory pressure issues. This corresponds to the node condition MemoryPressure=True.\n  node.kubernetes.io/disk-pressure: The node has disk pressure issues. This corresponds to the node condition DiskPressure=True.\n  node.kubernetes.io/network-unavailable: The node network is unavailable.\n  node.kubernetes.io/unschedulable: The node is unschedulable.\n  node.cloudprovider.kubernetes.io/uninitialized: When the node controller is started with an external cloud provider, this taint is set on a node to mark it as unusable. After a controller from the cloud-controller-manager initializes this node, the kubelet removes this taint.\n   Depending on the condition, the node will either have the effect NoSchedule, which means no new pods will be started there (unless a toleration is configured) or the effect NoExecute, which will evict pods with no tolerations from the node. Typical examples for NoSchedule condition would be: memory-pressure or disk-pressure. If a node is unreachable or not-ready, then it will be automatically tainted with the effect NoExecute. tolerationSeconds (default 300) will be respected.\n   Tolerating all taints Tolerations can be configured to tolerate all possible taints. In such case no value or key is configured and operator: \u0026#34;Exists\u0026#34; is used:\n spec: [...] template: [...] spec: tolerations: - operator: “Exists”       Some cluster daemonsets (i.e. tuned) are configured this way.       Cleanup Remove the taint from the node:\n oc adm taint nodes compute-0 gpu=enabled:NoExecute-   And remove the toleration specification from the DeploymentConfig\n spec: [...] template: [...] spec: tolerations: - key: \u0026#34;gpu\u0026#34; value: \u0026#34;enabled\u0026#34; operator: \u0026#34;Equal\u0026#34; effect: \u0026#34;NoExecute\u0026#34; tolerationSeconds: 30 (1)    "},{"uri":"https://blog.stderr.at/tags/tolerations/","title":"Tolerations","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/tolerations/","title":"Tolerations","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/topology-spread-constraints/","title":"Topology Spread Constraints","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/topology-spread-constraints/","title":"Topology Spread Constraints","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/","title":"Topology Spread Constraints","tags":["OCP","Day-2","OpenShift","Pod Placement","Topology Spread Constraints"],"description":"Placeing Pods Using Topology Spread Constraints","content":"Topology spread constraints is a new feature since Kubernetes 1.19 (OpenShift 4.6) and another way to control where pods shall be started. It allows to use failure-domains, like zones or regions or to define custom topology domains. It heavily relies on configured node labels, which are used to define topology domains. This feature is a more granular approach than affinity, allowing to achieve higher availability.\n Pod Placement Series Please check out other ways of pod placements:\n   Expand me...    NodeSelector\n  Pod Affinity and Anti Affinity\n  Node Affinity\n  Taints and Tolerations\n  Topology Spread Constraints\n  Descheduler\n        Understanding Topology Spread Constraints Imagine, like for pod affinity, we have two zones (east and west) in our 4 compute node cluster.\n  Figure 1. Node Zones  These zones are defined by node labels, which can be created with the following commands:\n oc label nodes compute-0 compute-1 topology.kubernetes.io/zone=east oc label nodes compute-2 compute-3 topology.kubernetes.io/zone=west   Now let’s scale our web application to 3 pods. The OpenShift scheduler will try to evenly spread the pods accross the cluster:\n oc get pods -n podtesting -o wide django-psql-example-31-jrhsr 0/1 Running 0 8s 10.130.2.112 compute-2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-31-q66hk 0/1 Running 0 8s 10.131.1.219 compute-3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; django-psql-example-31-xv7jc 0/1 Running 0 8s 10.128.3.115 compute-0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;    Figure 2. Running Pods  If a new pod is started the scheduler may try to start it on compute-1. However, this is done based on best effort. The scheduler does not guarantee that and may try to start it on one of the nodes in zone \u0026#34;West\u0026#34;. With the configuration topologySpreadConstraints this can be controlled and incoming pods can only be scheduled on a node of zone \u0026#34;East\u0026#34; (either on compute-0 or compute-1).\n   Configure TopologySpreadContraint Let’s configure our topology by adding the following into the DeploymentConfig:\n spec: [...] template: [...] topologySpreadConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: name: django-psql-example   This defines the following parameter:\n   maxSkew - defines the degree to which pods may be unevenly distributed (must be greater than zero). Depending on the whenUnsatisfiable parameter the bahaviour differs:\n  whenUnsatisfiable == DoNotSchedule: would not schedule if the maxSkew is is not met.\n  whenUnsatisfiable == ScheduleAnyway: the scheduler will still schedule and gives the topology which would decrease the skew a better score\n     topologyKey - key of node lables\n  whenUnsatisfiable - defines what to do with a pod if the spread constraint is not met. Can be either DoNotSchedule (default) or ScheduleAnyway\n  lableSelector - used to find matching pods. Found pods are considered to be part of the topology domain. In our example we simply use a default label of the application: name: django-psql-example\n   If now a 4th pod shall be started the topologySpreadConstraints will allow this pod on one of the nodes of zone=east (either compute-0 or compute-1). It cannot be scheduled on nodes in zone=west since it would violate the maxSkew: 1\n Pod distribution would be 1 in zone east and 3 on zone west. --\u0026gt; the skew would then be 3 - 1 = 2 which is not equal to 1     Configure multiple TopologySpreadContraint It is possible to configure multiple TopologySpreadConstraints. In such a case all contrains must meet the requirement (logical AND). For example:\n spec: [...] template: [...] topologySpreadConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: name: django-psql-example - maxSkew: 1 topologyKey: kubernetes.io/hostname whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: name: django-psql-example   The first part is identical to our first example and would allow the schedule to start a pod in topology.kubernetes.io/zone: east (compute-0 or compute-1). The second configuration defines not a zone but a node hostname. Now the scheduler can only deploy into zone=east AND onto node=compute-1\n Conflicts with multiple TopologySpreadConstraints If you use multiple constraints conflicts are possible. For example, you have a 3 node cluster and the 2 zones east and west. In such cases the maxSkew might be increased or the whenUnsatisfiable might be set to ScheduleAnyway\n See https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#example-multiple-topologyspreadconstraints for further information.\n    Cleanup Remove the topologySpreadConstraint\n spec: [...] template: [...] topologySpreadConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: name: django-psql-example    "},{"uri":"https://blog.stderr.at/day-2/pod-placement/2021-09-01-descheduler/","title":"Using Descheduler","tags":["OCP","Day-2","OpenShift","Descheduler"],"description":"Evicting pods using the Descheduler","content":"Descheduler is a new feature which is GA since OpenShift 4.7. It can be used to evict pods from nodes based on specific strategies. The evicted pod is then scheduled on another node (by the Scheduler) which is more suitable.\n This feature can be used when:\n   nodes are under/over-utilized\n  pod or node affinity, taints or labels have changed and are no longer valid for a running pod\n  node failures\n  pods have been restarted too many times\n   Pod Placement Series Please check out other ways of pod placements:\n   Expand me...    NodeSelector\n  Pod Affinity and Anti Affinity\n  Node Affinity\n  Taints and Tolerations\n  Topology Spread Constraints\n  Descheduler\n        Descheduler Profiles The following descheduler profiles are known and one or multiple can be configured for the Descheduler. Each profiles enables certain strategies which the Descheduler is leveraging. Since the strategies names are more or less self explaining, I did not add their full description here. Instead detailed information can be found at: Descheduler Profiles\n   AffinityAndTaints - removes pods that violates affinity and anti-affinity rules or taints\n  RemovePodsViolatingInterPodAntiAffinity\n  RemovePodsViolatingNodeAffinity\n  RemovePodsViolatingNodeTaints\n     TopologyAndDuplicates - evicts pods which are not evenly spreaded or which are violating the topology domain\n  RemovePodsViolatingTopologySpreadConstraint\n  RemoveDuplicates\n     LifecycleAndUtilization - evicts long-running pods to balance resource usage of nodes\n  RemovePodsHavingTooManyRestarts - Pods that are restarted more than 100 times\n  LowNodeUtilization - removes pods from overutilized nodes.\n  A node is considered underutilized if its usage is below 20% for all thresholds (CPU, memory, and number of pods).\n  A node is considered overutilized if its usage is above 50% for any of the thresholds (CPU, memory, and number of pods).\n     PodLifeTime - evicts pods that are too old\n        Descheduler mechanism The following rules are followed by the Descheduler to ensure that eviction of pods does not go wild. Therefore the following pods will never be evicted:\n   pods in openshift-* or kube-system namespaces\n  pods with priorityClassName equal to system-cluster-critical or system-node-critical\n  pods which cannot be recreated, for example: static or stand-alone pods/jobs or pods without a replication controller or replica set\n  pods of a daemon set\n  pods with local storage\n  pods which are violating the pod disruption budget\n     Installing the Descheduler The Descheduler is not installed by default and must be installed after the cluster has been initiated. This is done by installed the Kube Descheduler Operator.\n First we create a separate namespace for our operator, including a label:\n oc adm new-project openshift-kube-descheduler-operator oc label ns/openshift-kube-descheduler-operator openshift.io/cluster-monitoring=true   Then we search for the Kube Descheduler operator and install it, using the newly created namespace:\n  Figure 1. Install Descheduler Operator  After a few moments the operator will be installed.\n You can now create a Descheduler instance either via UI (wizard) or by using the following specification:\n apiVersion: operator.openshift.io/v1 kind: KubeDescheduler metadata: name: cluster namespace: openshift-kube-descheduler-operator spec: deschedulingIntervalSeconds: 3600 (1) logLevel: Normal (2) managementState: Managed operatorLogLevel: Normal (3) profiles: (4) - AffinityAndTaints - TopologyAndDuplicates - LifecycleAndUtilization     1 Defines the time interval the descheduler is running. Default is 3600 seconds   2 Defines logging for overall component. Can be Normal, Debug, Trace or TraceAll   3 Defines logging for the operator itself. Can be Normal, Debug, Trace or TraceAll   4 Enables on or multiple profiles the Descheduler should consider     "},{"uri":"https://blog.stderr.at/tags/ansible-tower/","title":"Ansible Tower","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/2021/07/ansible-tower-and-downloading-collections/","title":"Ansible Tower and downloading collections","tags":["Ansible","Ansible Tower"],"description":"Getting Ansible Tower to download collections can be hard","content":"Every wondered why Ansible Tower does not start downloading required collections when you synchronize a project? Here are the stumbling blocks we discovered so far:\n Wrong name for requirements.yml When downloading collections Ansible Tower searches for a file requirements.yml in the collections directory.\n Be careful with the file extension: requirements.yml has to end with the extension .yml and not .yaml.\n   Collections download is disabled in Ansible Tower Within Ansible Tower there is a setting called ENABLE COLLECTION(S) DOWNLOAD under Settings/Jobs. This has to be set to true, which is also the default.\n   No Ansible Galaxy credential defined for the organization Last but not least an Ansible Galaxy credential needs to be defined for the organization where the project is defined. With the default installation of Ansible Tower, when the sample playbooks are installed there is a credential called Ansible Galaxy defined. You need to assign this credential to the organization.\n If you skip installing the sample playbooks, no Ansible Galaxy credential will be defined for you and you have to create it manually.\n   How does this actually work? Ansible Tower uses a Python virtual environment for running Ansible. The default environment is installed in /var/lib/awx/venv/awx. You can also create custom environments, see Using virtualenv with Ansible Tower.\n In the default setup the following files define how collections are downloaded:\n   lib/python3.6/site-packages/awx/main/tasks.py\n  lib/python3.6/site-packages/awx/playbooks/project_update.yml\n   task.py task.py defines various internal tasks Tower has to run on various occasions. For example in line number 1930 (Ansible Tower 3.8.3) the task RunProjectUpdate gets defined. This is the task Tower has to run whenever a project update is required.\n In our case the function build_extra_vars_file (line 2083 with Ansible Tower 3.8.3) defines the variable galaxy_creds_are_defined only if the organization has a galaxy credential defined (line 2099 Ansible Tower 3.8.3).\n Line 2120 (Ansible Tower 3.8.3) finally defines the Ansible extra variable collections_enabled depending on galaxy_creds_are_defined.\n  project_update.yml So task.py defines the extra variable collections_enabled (see above). Finally the playbook project_update.yml consumes this extra variable and only downloads collections if collections_enabled is set to true, see the block string at line 192 (Ansible Tower 3.8.3) in `project_update.yml.\n So long and thanks for all the fish!\n   "},{"uri":"https://blog.stderr.at/tags/compliance/","title":"compliance","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/hardening/","title":"Hardening","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/compliance/2021/07/oc-compliance-command-line-plugin/","title":"oc compliance command line plugin","tags":["Hardening","OpenShift","OCP","security","compliance","plugin"],"description":"Using the oc client plugin os-compliance","content":"As described at Compliance Operator the Compliance Operator can be used to scan the OpenShift cluster environment against security benchmark, like CIS. Fetching the actual results might be a bit tricky tough.\n With OpenShift 4.8 plugins to the oc command are allowed. One of these plugin os oc compliance, which allows you to easily fetch scan results, re-run scans and so on. Let’s install and try it out.\n Installation An oc plugin must be deployed into the same directory as the oc command itself.\n The following describes the building and installation of the plugin.\n     You need Go installed on your node.      Clone the Git repository:\ngit clone https://github.com/openshift/oc-compliance.git     Build and install the plugin\nmake; make install go build -o ./bin/oc-compliance ./cmd which oc | xargs dirname | xargs -n1 cp ./bin/oc-compliance     The plugin allows the use of oc compliance\noc compliance You must specify a sub-command. Usage: oc-compliance [flags] oc-compliance [command] Available Commands: bind Creates a ScanSettingBinding for the given parameters controls Get a report of what controls you\\\u0026#39;re complying with fetch-fixes Download the fixes/remediations fetch-raw Download raw compliance results help Help about any command rerun-now Force a re-scan for one or more ComplianceScans view-result View a ComplianceCheckResult Flags: -h, --help help for oc-compliance Use \u0026#34;oc-compliance [command] --help\u0026#34; for more information about a command.        Fetch Raw Results Without the oc-compliance plugin it was required to manually spin up a Pod and download the results from this Pod, where the PV is mounted. Now, with a simple command we can select the ScanSettingBinding and define an output folder. For example:\n oc compliance fetch-raw \u0026lt;object-type\u0026gt; \u0026lt;object-name\u0026gt; -o \u0026lt;output-path\u0026gt;   Assuming the the compliance operator was configured as in the previous article, we have the ScanSettingBinding called cis-compliance:\n oc compliance fetch-raw scansettingbindings cis-compliance -n openshift-compliance -o /tmp/   This starts downloading the result archives into /tmp\n Fetching results for cis-compliance scans: ocp4-cis-node-worker, ocp4-cis-node-master, ocp4-cis Fetching raw compliance results for pod \u0026#39;raw-result-extractor-fxbw8\u0026#39;.Fetching raw compliance results for scan \u0026#39;ocp4-cis-node-worker\u0026#39;......... The raw compliance results are avaliable in the following directory: /tmp/ocp4-cis-node-worker Fetching raw compliance results for pod \u0026#39;raw-result-extractor-kqrw5\u0026#39;.Fetching raw compliance results for scan \u0026#39;ocp4-cis-node-master\u0026#39;..... The raw compliance results are avaliable in the following directory: /tmp/ocp4-cis-node-master Fetching raw compliance results for pod \u0026#39;raw-result-extractor-pfrgk\u0026#39;.Fetching raw compliance results for scan \u0026#39;ocp4-cis\u0026#39;.. The raw compliance results are avaliable in the following directory: /tmp/ocp4-cis ls -la /tmp/ocp4-cis* /tmp/ocp4-cis: total 172 drwx------ 2 root root 4096 Jul 30 16:05 . drwxrwxrwt. 18 root root 4096 Jul 30 16:05 .. -rw-r--r-- 1 root root 166676 Jul 30 16:05 ocp4-cis-api-checks-pod.xml.bzip2 /tmp/ocp4-cis-node-master: total 504 drwx------ 2 root root 4096 Jul 30 16:05 . drwxrwxrwt. 18 root root 4096 Jul 30 16:05 .. -rw-r--r-- 1 root root 168256 Jul 30 16:05 ocp4-cis-node-master-master-0-pod.xml.bzip2 -rw-r--r-- 1 root root 165716 Jul 30 16:05 ocp4-cis-node-master-master-1-pod.xml.bzip2 -rw-r--r-- 1 root root 166945 Jul 30 16:05 ocp4-cis-node-master-master-2-pod.xml.bzip2 /tmp/ocp4-cis-node-worker: total 1112 drwx------ 2 root root 4096 Jul 30 16:05 . drwxrwxrwt. 18 root root 4096 Jul 30 16:05 .. -rw-r--r-- 1 root root 154943 Jul 30 16:05 ocp4-cis-node-worker-compute-0-pod.xml.bzip2 -rw-r--r-- 1 root root 154903 Jul 30 16:05 ocp4-cis-node-worker-compute-1-pod.xml.bzip2 -rw-r--r-- 1 root root 154939 Jul 30 16:05 ocp4-cis-node-worker-compute-2-pod.xml.bzip2 -rw-r--r-- 1 root root 154890 Jul 30 16:05 ocp4-cis-node-worker-compute-3-pod.xml.bzip2 -rw-r--r-- 1 root root 168175 Jul 30 16:05 ocp4-cis-node-worker-master-0-pod.xml.bzip2 -rw-r--r-- 1 root root 165603 Jul 30 16:05 ocp4-cis-node-worker-master-1-pod.xml.bzip2 -rw-r--r-- 1 root root 166914 Jul 30 16:05 ocp4-cis-node-worker-master-2-pod.xml.bzip2     Re-Run Scans Sometimes it is necessary to re-run scans. This can be done by annotating the appropriate scan as described at: Performing a Rescan\n With the oc plugin you can simply trigger a re-scan with a single command:\n oc compliance rerun-now scansettingbindings \u0026lt;name of scanbinding\u0026gt;   For example:\n oc compliance rerun-now scansettingbindings cis-compliance   Example output:\n Rerunning scans from \u0026#39;cis-compliance\u0026#39;: ocp4-cis-node-worker, ocp4-cis-node-master, ocp4-cis Re-running scan \u0026#39;openshift-compliance/ocp4-cis-node-worker\u0026#39; Re-running scan \u0026#39;openshift-compliance/ocp4-cis-node-master\u0026#39; Re-running scan \u0026#39;openshift-compliance/ocp4-cis\u0026#39;   With the command oc get compliancescan -n openshift-compliance you can check when the scan has been done:\n NAME PHASE RESULT ocp4-cis RUNNING NOT-AVAILABLE ocp4-cis-node-master RUNNING NOT-AVAILABLE ocp4-cis-node-worker AGGREGATING NOT-AVAILABLE     View Results on CLI Once a scan process has finished you can verify the check results quick and easy using the command line:\n oc get ComplianceCheckResult -A   This prints for example:\n NAMESPACE NAME STATUS SEVERITY [...] openshift-compliance ocp4-cis-audit-log-forwarding-enabled FAIL medium [...]   The view-result can print a human readable output, for example:\n oc compliance view-result ocp4-cis-audit-log-forwarding-enabled -n openshift-compliance   Example:\n +----------------------+-----------------------------------------------------------------------------------------+ | KEY | VALUE | +----------------------+-----------------------------------------------------------------------------------------+ | Title | Ensure that Audit Log | | | Forwarding Is Enabled | +----------------------+-----------------------------------------------------------------------------------------+ | Status | FAIL | +----------------------+-----------------------------------------------------------------------------------------+ | Severity | medium | +----------------------+-----------------------------------------------------------------------------------------+ | Description | OpenShift audit works at the | | | API server level, logging | | | all requests coming to the | | | server. Audit is on by default | | | and the best practice is | | | to ship audit logs off the | | | cluster for retention. The | | | cluster-logging-operator is | | | able to do this with the | | | | | | | | | | | | ClusterLogForwarders | | | | | | | | | | | | resource. The forementioned resource can be configured to logs to different third party | | | systems. For more information on this, please reference the official documentation: | | | https://docs.openshift.com/container-platform/4.6/logging/cluster-logging-external.html | +----------------------+-----------------------------------------------------------------------------------------+ | Rationale | Retaining logs ensures the | | | ability to go back in time to | | | investigate or correlate any | | | events. Offloading audit logs | | | from the cluster ensures that | | | an attacker that has access | | | to the cluster will not be | | | able to tamper with the logs | | | because of the logs being | | | stored off-site. | +----------------------+-----------------------------------------------------------------------------------------+ | Instructions | Run the following command: | | | | | | oc get clusterlogforwarders | | | instance -n openshift-logging | | | -ojson | jq -r | | | \u0026#39;.spec.pipelines[].inputRefs | | | | contains([\u0026#34;audit\u0026#34;])\u0026#39; | | | | | | The output should return true. | +----------------------+-----------------------------------------------------------------------------------------+ | CIS-OCP Controls | 1.2.23 | +----------------------+-----------------------------------------------------------------------------------------+ | NIST-800-53 Controls | AC-2(12), AU-6, AU-6(1), | | | AU-6(3), AU-9(2), SI-4(16), | | | AU-4(1), AU-11, AU-7, AU-7(1) | +----------------------+-----------------------------------------------------------------------------------------+ | Available Fix | No | +----------------------+-----------------------------------------------------------------------------------------+ | Result Object Name | ocp4-cis-audit-log-forwarding-enabled | +----------------------+-----------------------------------------------------------------------------------------+ | Rule Object Name | ocp4-audit-log-forwarding-enabled | +----------------------+-----------------------------------------------------------------------------------------+ | Remediation Created | No | +----------------------+-----------------------------------------------------------------------------------------+    "},{"uri":"https://blog.stderr.at/tags/plugin/","title":"plugin","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/compliance/2021/07/compliance-operator/","title":"Compliance Operator","tags":["Hardening","OpenShift","OCP","security","compliance"],"description":"Hardening OpenShift using the Compliance Operator","content":"OpenShift comes out of the box with a highly secure operating system, called Red Hat CoreOS. This OS is immutable, which means that no direct changes are done inside the OS, instead any configuration is managed by OpenShift itself using MachineConfig objects. Nevertheless, hardening certain settings must still be considered. Red Hat released a hardening guide (CIS Benchmark) which can be downloaded at https://www.cisecurity.org/.\n However, an automated way to perform such checks would be nice too. To achieve this the Compliance Operator can be leveraged, which runs an OpenSCAP check to create reports of the clusters is compliant or as the official documentation describes:\n The Compliance Operator lets OpenShift Container Platform administrators describe the desired compliance state of a cluster and provides them with an overview of gaps and ways to remediate them. The Compliance Operator assesses compliance of both the Kubernetes API resources of OpenShift Container Platform, as well as the nodes running the cluster. The Compliance Operator uses OpenSCAP, a NIST-certified tool, to scan and enforce security policies provided by the content.\n This article shall show how to quickly install the operator and retrieve the first result. It is not a full documentation, which is written by other people at: Compliance Operator, especially remediation is not covered here.\n As prerequisites we have:\n   Installed OpenShift 4.6+ cluster\n       The Compliance Operator is available for Red Hat Enterprise Linux CoreOS (RHCOS) deployments only.     Install the Compliance Operator The easiest way to deploy the Compliance Operator is by searching the OperatorHub which is available inside OpenShift.\n  Figure 1. Install Compliance Operator  Keep the default settings and wait until the operator has been installed.\n  Figure 2. Install Compliance Operator     Custom Resources (CRDs) The operator brings a ton of new CRDs into the system:\n   ScanSetting …​ defines when and on which roles (worker, master …​) a check shall be executed. It also defines a persistent volume (PV) to store the scan results. Two ScanSettings are created during the installation:\n  default: just scans without automatically apply changes\n  default-auto-apply: can automatically remediate without extra steps\n     ScanSettingBinding …​ binds one or more profiles to a scan\n  Profile …​ Represent different compliance benchmarks with a set of rules. For this blog we will use CIS Benchmark profiles\n  ProfileBundle …​ Bundles a security image, which is later used by Profiles.\n  Rule …​ Rules which are used by profiles to verify the state of the cluster.\n  TailoredProfile …​ Customized profile\n  ComplianceScan …​ scans which have been performed\n  ComplianceCheckResult …​ The results of a scan. Each ComplianceCheckResult represents the result of one compliance rule check\n  ComplianceRemediation …​ If a rule ca be remediated automatically, this object is created.\n     Create a ScanBinding object The first step to do is to create a ScanBiding objects. (We reuse the default ScanSetting)\n Let’s create the following object, which is using the profiles ocp4-cis and ocp4-cis-node\n apiVersion: compliance.openshift.io/v1alpha1 kind: ScanSettingBinding metadata: name: cis-compliance profiles: - name: ocp4-cis-node (1) kind: Profile apiGroup: compliance.openshift.io/v1alpha1 - name: ocp4-cis (2) kind: Profile apiGroup: compliance.openshift.io/v1alpha1 settingsRef: name: default (3) kind: ScanSetting apiGroup: compliance.openshift.io/v1alpha1     1 use the profile ocp4-cis-node   2 use the profile ocp4-cis   3 reference to the default scansetting    As soon as the object is created the cluster is scan is started. The objects ComplianceSuite and ComplianceScan are created automatically and will eventually reach the phase \u0026#34;DONE\u0026#34; when the scan is completed.\n The following command will show the results of the scans\n oc get compliancescan -n openshift-compliance NAME PHASE RESULT ocp4-cis DONE NON-COMPLIANT ocp4-cis-node-master DONE NON-COMPLIANT ocp4-cis-node-worker DONE INCONSISTENT   Three different checks have been done. One overall cluster check and 2 separated for master and worker nodes.\n As we used the default ScanSetting the next check will run a 1 am.\n   Profiles The operator comes with a set of standard profiles which represent different compliance benchmarks.\n To view available profiles:\n oc get profiles.compliance -n openshift-compliance   NAME AGE ocp4-cis 28m ocp4-cis-node 28m ocp4-e8 28m ocp4-moderate 28m rhcos4-e8 28m rhcos4-moderate 28m   Each profile contains a description which explains the intention and a list of rules which used in this profile.\n For example the profile \u0026#39;ocp4-cis-node\u0026#39; used above is containing:\n oc get profiles.compliance -n openshift-compliance -oyaml ocp4-cis-node # Output description: This profile defines a baseline that aligns to the Center for Internet Security® Red Hat OpenShift Container Platform 4 Benchmark™, V0.3, currently unreleased. This profile includes Center for Internet Security® Red Hat OpenShift Container Platform 4 CIS Benchmarks™ content. Note that this part of the profile is meant to run on the Operating System that Red Hat OpenShift Container Platform 4 runs on top of. This profile is applicable to OpenShift versions 4.6 and greater. [...] name: ocp4-cis-node namespace: openshift-compliance [...] rules: - ocp4-etcd-unique-ca - ocp4-file-groupowner-cni-conf - ocp4-file-groupowner-controller-manager-kubeconfig - ocp4-file-groupowner-etcd-data-dir - ocp4-file-groupowner-etcd-data-files - ocp4-file-groupowner-etcd-member - ocp4-file-groupowner-etcd-pki-cert-files - ocp4-file-groupowner-ip-allocations [...]   Like the profiles the different rules can be inspected:\n oc get rules.compliance -n openshift-compliance ocp4-file-groupowner-etcd-member -o jsonpath=\u0026#39;{\u0026#34;Title: \u0026#34;}{.title}{\u0026#34;\\nDescription: \\n\u0026#34;}{.description}\u0026#39; # Output Title: Verify Group Who Owns The etcd Member Pod Specification File Description: To properly set the group owner of /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml , run the command: $ sudo chgrp root /etc/kubernetes/static-pod-resources/etcd-pod-*/etcd-pod.yaml   Profile Customization Sometimes is it required to modify (tailor) a profile to fit specific needs. With the TailoredProfile object it is possible to enable or disable rules.\n In this blog, I just want to share a quick example from the official documentaiton: https://docs.openshift.com/container-platform/4.7/security/compliance_operator/compliance-operator-tailor.html\n The following TailoredProfile disables 2 rules and sets a value for another rule:\n apiVersion: compliance.openshift.io/v1alpha1 kind: TailoredProfile metadata: name: nist-moderate-modified spec: extends: rhcos4-moderate title: My modified NIST moderate profile disableRules: - name: rhcos4-file-permissions-node-config rationale: This breaks X application. - name: rhcos4-account-disable-post-pw-expiration rationale: No need to check this as it comes from the IdP setValues: - name: rhcos4-var-selinux-state rationale: Organizational requirements value: permissive      Working with scan results Once a scan finished you probably want to see what the status of the scan is.\n As you sse above the cluster failed to be compliant.\n oc get compliancescan -n openshift-compliance NAME PHASE RESULT ocp4-cis DONE NON-COMPLIANT ocp4-cis-node-master DONE NON-COMPLIANT ocp4-cis-node-worker DONE INCONSISTENT   Retrieving results via oc command List all results which can be remediated automatically:\n oc get compliancecheckresults -l \u0026#39;compliance.openshift.io/check-status=FAIL,compliance.openshift.io/automated-remediation\u0026#39; -n openshift-compliance NAME STATUS SEVERITY ocp4-cis-api-server-encryption-provider-cipher FAIL medium ocp4-cis-api-server-encryption-provider-config FAIL medium       Further information about remediation can be found at: Compliance Operator Remediation     List all results which cannot be remediated automatically and must be fixed manually instead:\n oc get compliancecheckresults -l \u0026#39;compliance.openshift.io/check-status=FAIL,!compliance.openshift.io/automated-remediation\u0026#39; -n openshift-compliance NAME STATUS SEVERITY ocp4-cis-audit-log-forwarding-enabled FAIL medium ocp4-cis-file-permissions-proxy-kubeconfig FAIL medium ocp4-cis-node-master-file-groupowner-ip-allocations FAIL medium ocp4-cis-node-master-file-groupowner-openshift-sdn-cniserver-config FAIL medium ocp4-cis-node-master-file-owner-ip-allocations FAIL medium ocp4-cis-node-master-file-owner-openshift-sdn-cniserver-config FAIL medium ocp4-cis-node-master-kubelet-configure-event-creation FAIL medium ocp4-cis-node-master-kubelet-configure-tls-cipher-suites FAIL medium ocp4-cis-node-master-kubelet-enable-protect-kernel-defaults FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-imagefs-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-imagefs-inodesfree FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-memory-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-nodefs-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-hard-nodefs-inodesfree FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-imagefs-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-imagefs-inodesfree FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-memory-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-nodefs-available FAIL medium ocp4-cis-node-master-kubelet-eviction-thresholds-set-soft-nodefs-inodesfree FAIL medium ocp4-cis-node-worker-file-groupowner-ip-allocations FAIL medium ocp4-cis-node-worker-file-groupowner-openshift-sdn-cniserver-config FAIL medium ocp4-cis-node-worker-file-owner-ip-allocations FAIL medium ocp4-cis-node-worker-file-owner-openshift-sdn-cniserver-config FAIL medium ocp4-cis-node-worker-kubelet-configure-event-creation FAIL medium ocp4-cis-node-worker-kubelet-configure-tls-cipher-suites FAIL medium ocp4-cis-node-worker-kubelet-enable-protect-kernel-defaults FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-imagefs-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-imagefs-inodesfree FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-memory-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-nodefs-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-hard-nodefs-inodesfree FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-imagefs-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-imagefs-inodesfree FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-memory-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-nodefs-available FAIL medium ocp4-cis-node-worker-kubelet-eviction-thresholds-set-soft-nodefs-inodesfree FAIL medium    Retrieving RAW results Let’s first retrieve the raw result of the scan. For each of the ComplianceScans a volume claim (PVC) is created to store he results. We can use a Pod to mount the volume to download the scan results.\n The following PVC have been created on our example:\n oc get pvc -n openshift-compliance NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE ocp4-cis Bound pvc-cc026ae3-2f42-4e19-bc55-016c6dd31d22 1Gi RWO managed-nfs-storage 4h17m ocp4-cis-node-master Bound pvc-3bd47c5e-2008-4759-9d53-ba41b568688d 1Gi RWO managed-nfs-storage 4h17m ocp4-cis-node-worker Bound pvc-77200e5f-0f15-410c-a4ee-f2fb3e316f84 1Gi RWO managed-nfs-storage 4h17m   Now we can create a Pod which mounts all PVCs at once:\n apiVersion: \u0026#34;v1\u0026#34; kind: Pod metadata: name: pv-extract namespace: openshift-compliance spec: containers: - name: pv-extract-pod image: registry.access.redhat.com/ubi8/ubi command: [\u0026#34;sleep\u0026#34;, \u0026#34;3000\u0026#34;] volumeMounts: (1) - mountPath: \u0026#34;/workers-scan-results\u0026#34; name: workers-scan-vol - mountPath: \u0026#34;/masters-scan-results\u0026#34; name: masters-scan-vol - mountPath: \u0026#34;/ocp4-scan-results\u0026#34; name: ocp4-scan-vol volumes: (2) - name: workers-scan-vol persistentVolumeClaim: claimName: ocp4-cis-node-worker - name: masters-scan-vol persistentVolumeClaim: claimName: ocp4-cis-node-master - name: ocp4-scan-vol persistentVolumeClaim: claimName: ocp4-cis     1 mount paths   2 volumesclaims to mount    This creates a Pod with the PVCs mounted inside:\n sh-4.4# ls -la | grep scan drwxrwxrwx. 3 root root 4096 Jul 20 05:20 master-scan-results drwxrwxrwx. 3 root root 4096 Jul 20 05:20 ocp4-scan-results drwxrwxrwx. 3 root root 4096 Jul 20 05:20 workers-scan-results   We can download the result-files to our local machine for further auditing. Therefore, we create the folder scan_results in which we copy everything:\n mkdir scan-results; cd scan-results oc -n openshift-compliance cp pv-extract:ocp4-scan-results ocp4-scan-results/. oc -n openshift-compliance cp pv-extract:workers-scan-results workers-scan-results/. oc -n openshift-compliance cp pv-extract:masters-scan-results masters-scan-results/.   This will download several bzip2 archives for the appropriate scan result.\n Once done, you can delete the \u0026#34;download pod\u0026#34; using: oc delete pod pv-extract -n openshift-compliance\n  Work wth RAW results So above section described the download of the bzip2 files but what to do with it? First, you can import it into a tool which is able to read openScap reports. Or, secondly, you can use the oscap command to create a html output.\n We have downloaded the following files:\n ./ocp4-scan-results/0/ocp4-cis-api-checks-pod.xml.bzip2 ./masters-scan-results/0/ocp4-cis-node-master-master-0-pod.xml.bzip2 ./masters-scan-results/0/ocp4-cis-node-master-master-2-pod.xml.bzip2 ./masters-scan-results/0/ocp4-cis-node-master-master-1-pod.xml.bzip2 ./workers-scan-results/0/ocp4-cis-node-worker-compute-0-pod.xml.bzip2 ./workers-scan-results/0/ocp4-cis-node-worker-compute-1-pod.xml.bzip2 ./workers-scan-results/0/ocp4-cis-node-worker-compute-3-pod.xml.bzip2 ./workers-scan-results/0/ocp4-cis-node-worker-compute-2-pod.xml.bzip2   To create the html output (be sure that open-scap is installed on you host):\n mkdir html oscap xccdf generate report ocp4-scan-results/0/ocp4-cis-api-checks-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-api-checks.html oscap xccdf generate report masters-scan-results/0/ocp4-cis-node-master-master-0-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-node-master-master-0.html oscap xccdf generate report masters-scan-results/0/ocp4-cis-node-master-master-1-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-node-master-master-1.html oscap xccdf generate report masters-scan-results/0/ocp4-cis-node-master-master-2-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-node-master-master-2.html oscap xccdf generate report workers-scan-results/0/ocp4-cis-node-worker-compute-0-pod.xml.bzip2 \u0026gt;\u0026gt; html/ocp4-cis-node-worker-compute-0.html ...   The resulted html files are too big to be show here, but some snippets should give an overview:\n To view the html output as an example I have linked the html files:\n   OCP4 - CIS\n  Example Master Node Results\n  Example Worker Node Results\n   Overall Scoring of the result:\n  Figure 3. Scoring  A list if passed or failed checks:\n  Figure 4. Scan Result list  Scan details with a link to the CIS Benchmark section and further explainations on how to fix the issue:\n  Figure 5. Scan details     Performing a rescan If it is necessary to run a rescan, the ComplianceScan object is simply annotated with:\n oc annotate compliancescans/\u0026lt;scan_name\u0026gt; compliance.openshift.io/rescan=       If default-auto-apply is enabled, remediation which changes MachineConfigs will trigger a cluster reboot.      "},{"uri":"https://blog.stderr.at/tags/block-devices/","title":"Block devices","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2021-02-27-understanding-block-devices/","title":"Understanding RWO block device handling in OpenShift","tags":["Storage","OpenShift","OCP","Block devices"],"description":"A basic introduction into block device usage","content":"In this blog post we would like to explore OpenShift / Kubernetes block device handling. We try to answer the following questions:\n   What happens if multiple pods try to access the same block device?\n  What happens if we scale a deployment using block devices to more than one replica?\n   And finally we want to give a short, high level overview about how the container storage interface (CSI) actually works.\n     A block device provides Read-Write-Once (RWO) storage. This basically means a local file system mounted by a single node. Do not confuse this with a cluster (CephFS, GlusterFS) or network file system (NFS). These file systems provide Read-Write-Many (RWX) storage mountable on more than one node.     Test setup For running our tests we need the following resources\n   A new namespace/project for running our tests\n  A persistent volume claim (PVC) to be mounted in our test pods\n  Two pods definitions for mounting the PVC\n   Step 1: Creating a new namespace/project To run our test cases we created a new project with OpenShift\n oc new-project blockdevices    Step 2: Defining a block PVC Our cluster is running the rook operator (https://rook.io) and provides a ceph-block storage class for creating block devices:\n $ oc get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate false 4d14h   Let’s take a look a the details of the storage class:\n $ oc get sc -o yaml ceph-block apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-block parameters: clusterID: rook-ceph csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph csi.storage.k8s.io/fstype: ext4 (1) csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph imageFeatures: layering imageFormat: \u0026#34;2\u0026#34; pool: blockpool provisioner: rook-ceph.rbd.csi.ceph.com reclaimPolicy: Delete volumeBindingMode: Immediate     1 So whenever we create a PVC using this storage class the Ceph provisioner will also create an EXT4 file system on the block device.    To test block device handling we create the following persistent volume claim (PVC):\n apiVersion: v1 kind: PersistentVolumeClaim metadata: name: block-claim spec: accessModes: - ReadWriteOnce (1) resources: requests: storage: 1Gi storageClassName: ceph-block     1 The access mode is set to ReadWriteOnce (RWO), as block devices    oc create -f pvc.yaml   $ oc get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE block-claim Bound pvc-bd68be5d-c312-4c31-86a8-63a0c22de844 1Gi RWO ceph-block 91s   To test our shiny new block device we are going to use the following three pod definitions:\n block-pod-a apiVersion: v1 kind: Pod metadata: labels: run: block-pod-a name: block-pod-a spec: containers: - image: registry.redhat.io/ubi8/ubi:8.3 name: block-pod-a command: - sh - -c - \u0026#39;df -h /block \u0026amp;\u0026amp; findmnt /block \u0026amp;\u0026amp; sleep infinity\u0026#39; volumeMounts: - name: blockdevice mountPath: /block volumes: - name: blockdevice persistentVolumeClaim: claimName: block-claim   block-pod-b apiVersion: v1 kind: Pod metadata: labels: run: block-pod-b name: block-pod-b spec: affinity: podAntiAffinity: (1) requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: run operator: In values: - block-pod-a topologyKey: kubernetes.io/hostname containers: - image: registry.redhat.io/ubi8/ubi:8.3 name: block-pod-b command: - sh - -c - \u0026#39;df -h /block \u0026amp;\u0026amp; findmnt /block \u0026amp;\u0026amp; sleep infinity\u0026#39; volumeMounts: - name: blockdevice mountPath: /block volumes: - name: blockdevice persistentVolumeClaim: claimName: block-claim     1 We use an AntiAffinity rule for making sure that block-pod-b runs on a different node than block-pod-a.    block-pod-c apiVersion: v1 kind: Pod metadata: labels: run: block-pod-c name: block-pod-c spec: affinity: podAffinity: (1) preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: run operator: In values: - block-pod-a topologyKey: kubernetes.io/hostname containers: - image: registry.redhat.io/ubi8/ubi:8.3 name: block-pod-c command: - sh - -c - \u0026#39;df -h /block \u0026amp;\u0026amp; findmnt /block \u0026amp;\u0026amp; sleep infinity\u0026#39; volumeMounts: - name: blockdevice mountPath: /block volumes: - name: blockdevice persistentVolumeClaim: claimName: block-claim     1 We use an Affinity rule for making sure that block-pod-c runs on the same node as block-pod-a.    In our first test we want to make sure that both pods are running on separate cluster nodes. So we create block-pod-a and block-pod-b:\n $ oc create -f block-pod-a.yml $ oc create -f block-pod-b.yml   After a few seconds we can check the state of our pods:\n $ oc get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES block-pod-a 1/1 Running 0 46s 10.130.6.4 infra02.lan.stderr.at \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; block-pod-b 0/1 ContainerCreating 0 16s \u0026lt;none\u0026gt; infra01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Hm, block-pod-b is in the state ContainerCreating, let’s check the events. Also note that it is running on another node (infra01) then block-pod-a (infra02).\n 10s Warning FailedAttachVolume pod/block-pod-b Multi-Attach error for volume \u0026#34;pvc-bd68be5d-c312-4c31-86a8-63a0c22de844\u0026#34; Volume is already used by pod(s) block-pod-a   Ah, so because of our block device with RWO access mode and block-pod-b running on separate cluster node, OpenShift or K8s can’t attach the volume to our block-pod-b.\n But let’s try another test and let’s create a third pod block-pod-c that should run on the same node as block-pod-a:\n $ oc create -f block-pod-c.yml   Now let’s check the status of block-pod-c:\n $ oc get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES block-pod-a 1/1 Running 0 6m49s 10.130.6.4 infra02.lan.stderr.at \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; block-pod-b 0/1 ContainerCreating 0 6m19s \u0026lt;none\u0026gt; infra01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; block-pod-c 1/1 Running 0 14s 10.130.6.5 infra02.lan.stderr.at \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Oh, block-pod-c is running on node infra02 and mounted the RWO volume. Let’s check the events for block-pod-c:\n 3m6s Normal Scheduled pod/block-pod-c Successfully assigned blockdevices/block-pod-c to infra02.lan.stderr.at 2m54s Normal AddedInterface pod/block-pod-c Add eth0 [10.130.6.5/23] 2m54s Normal Pulled pod/block-pod-c Container image \u0026#34;registry.redhat.io/ubi8/ubi:8.3\u0026#34; already present on machine 2m54s Normal Created pod/block-pod-c Created container block-pod-c 2m54s Normal Started pod/block-pod-c Started container block-pod-c   When we compare this with the events for block-pod-a:\n 9m41s Normal Scheduled pod/block-pod-a Successfully assigned blockdevices/block-pod-a to infra02.lan.stderr.at 9m41s Normal SuccessfulAttachVolume pod/block-pod-a AttachVolume.Attach succeeded for volume \u0026#34;pvc-bd68be5d-c312-4c31-86a8-63a0c22de844\u0026#34; 9m34s Normal AddedInterface pod/block-pod-a Add eth0 [10.130.6.4/23] 9m34s Normal Pulled pod/block-pod-a Container image \u0026#34;registry.access.redhat.com/ubi8/ubi:8.3\u0026#34; already present on machine 9m34s Normal Created pod/block-pod-a Created container block-pod-a 9m34s Normal Started pod/block-pod-a Started container block-pod-a   So the AttachVolume.Attach message is missing in the events for block-pod-c. Because the volume is already attached to the node, interesting.\n     Even with RWO block device volumes it is possible to use the same volume in multiple pods if the pods a running on the same node.     I was not aware of this possibility and always had the believe with an RWO block device only one pod can access the volume. That’s the problem with believing :-)\n Thanks or reading this far.\n   "},{"uri":"https://blog.stderr.at/tags/kubernetes/","title":"Kubernetes","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/operator/","title":"Operator","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2021-01-27-writingoperatoransible/","title":"Writing Operator using Ansible","tags":["Operator","OpenShift","OCP","Kubernetes"],"description":"Example of an Kubernetes Operator based on Ansible","content":"This quick post shall explain, without any fancy details, how to write an Operator based on Ansible. It is assumed that you know what purpose an Operator has.\n As a short summary: Operators are a way to create custom controllers in OpenShift or Kubernetes. It watches for custom resource objects and creates the application based on the parameters in such custom resource object. Often written in Go, the SDK supports Ansible, Helm and (new) Java as well.\n In this example we will install Gogs, a painless self-hosted Git services.\n As general prerequisites we have:\n   Installed OpenShift 4.6+ cluster (could be Minicube)\n  Possibility to execute Ansible scripts and oc/kubectl commands\n  Commands: make, docker (or podman)\n    Install Operator SDK As explained at https://sdk.operatorframework.io/docs/installation/ the following prerequisites must be met prior installing the SDK at least:\n   Docker v17.03+ or podman v1.9.3+ or buildah v1.7+\n  OpenShift CLU v4.6+\n  Kubernetes/OpenShift cluster\n  Access to container registry, for example quay.io\n  Optional: Go v1.13+ (for Operators based on Golang)\n  Ansible v2.9.0+\n   Following the instructions of the SDK documentation, the operator-sdk command will be installed.\n   Creating the Operator To begin we create a new folder for the Operator and initialize the Operator project.\n mkdir gogs-operator cd gogs-operator operator-sdk init --plugins=ansible --domain=example.com.at operator-sdk create api --group gogs --version=v1alpha1 --kind Gogs --generate-playbook   This will create a new project structure with the following parameters:\n --plugin: Type of Operator (Ansible or Helm)\n --domain: Defines the api endpoint together with group and version.\n --group: Usually short product name\n --version: Defines version of API endpoint\n The folder structure which will be created automatically looks as follows:\n . |-- Dockerfile |-- Makefile |-- PROJECT |-- config | |-- crd | | |-- bases | | | |-- gogs.example.com.at_gogs.yaml | | |-- kustomization.yaml | |-- default | | |-- kustomization.yaml | | |-- manager_auth_proxy_patch.yaml | |-- manager | | |-- kustomization.yaml | | |-- manager.yaml | |-- prometheus | | |-- kustomization.yaml | | |-- monitor.yaml | |-- rbac | | |-- auth_proxy_client_clusterrole.yaml | | |-- auth_proxy_role.yaml | | |-- auth_proxy_role_binding.yaml | | |-- auth_proxy_service.yaml | | |-- gogs_editor_role.yaml | | |-- gogs_viewer_role.yaml | | |-- kustomization.yaml | | |-- leader_election_role.yaml | | |-- leader_election_role_binding.yaml | | |-- role.yaml | | |-- role_binding.yaml | |-- samples | | |-- gogs_v1alpha1_gogs.yaml | | |-- kustomization.yaml | |-- scorecard | | |-- bases | | | |-- config.yaml | | |-- kustomization.yaml | | |-- patches | | |-- basic.config.yaml | | |-- olm.config.yaml | |-- testing | |-- debug_logs_patch.yaml | |-- kustomization.yaml | |-- manager_image.yaml | |-- pull_policy | |-- Always.yaml | |-- IfNotPresent.yaml | |-- Never.yaml |-- molecule | |-- default | | |-- converge.yml | | |-- create.yml | | |-- destroy.yml | | |-- kustomize.yml | | |-- molecule.yml | | |-- prepare.yml | | |-- tasks | | | |-- gogs_test.yml | | |-- verify.yml | |-- kind | |-- converge.yml | |-- create.yml | |-- destroy.yml | |-- molecule.yml |-- playbooks | |-- gogs.yml |-- requirements.yml |-- roles |-- watches.yaml   The watches.yaml file maps Custom Resources (identified by Group, Version, and Kind [GVK]) to Ansible Roles and Playbooks. It tells the Operator where to find the actual Ansible playbook.\n --- # Use the \u0026#39;create api\u0026#39; subcommand to add watches to this file. - version: v1alpha1 group: gogs.example.com.at kind: Gogs playbook: playbooks/gogs.yml # +kubebuilder:scaffold:watch   Other files, especially inside playbooks and roles are created as placeholders. These files (or folders) are waiting for you to add the Ansible logic.\n   Defining Roles and Playbook With the folder structure above, a playbook and different roles can be created in order to tell the Operator what it needs to do.\n     Since the Operator will constantly watch for changes, all tasks must be idempotent     In our example we will try to install Gogs, a Git service. It contains a Postgres database system and a webservice. To use some example roles and not fully start from scratch let’s clone the following repository and copy the folders to our Operator.\n cd .. https://github.com/tjungbauer/ansible-operator-roles cd gogs-operator # Remove placeholder rm -Rf roles/ # Copy Postgres deployment role cp -R ../ansible-operator-roles/roles/postgresql-ocp ./roles # Copy Gogs Deplyoment role cp -R ../ansible-operator-roles/roles/gogs-ocp ./roles   When we examine the folder, we see 2 typical Ansible roles. The simple purpose is, to create all required OpenShift objects, like Deployment, Route, Service and so on, fully automated by the Operator.\n |-- playbooks | |-- gogs.yaml |-- roles |-- gogs-ocp | |-- README.adoc | |-- defaults | | |-- main.yml | |-- meta | | |-- main.yml | |-- tasks | | |-- main.yml | |-- templates | |-- config_map.j2 | |-- deployment.j2 | |-- persistent_volume_claim.j2 | |-- route.j2 | |-- service.j2 | |-- service_account.j2 |-- postgresql-ocp |-- README.adoc |-- defaults | |-- main.yml |-- meta | |-- main.yml |-- tasks | |-- main.yml |-- templates |-- deployment.j2 |-- persistent_volume_claim.j2 |-- secret.j2 |-- service.j2   Copy (or create) the following playbook under playbooks/gogs.yaml. As you can see there are 2 tasks: the first one will create the postgres application, the seconds one the Gogs service.\n --- # Persistent Gogs deployment playbook. # # The Playbook expects the following variables to be set in the CR: # (Note that Camel case gets converted by the ansible-operator to Snake case) # - PostgresqlVolumeSize # - GogsVolumeSize # - GogsSSL # The following variables come from the ansible-operator # - ansible_operator_meta.namespace # - ansible_operator_meta.name (from the name of the CR) - hosts: localhost gather_facts: no tasks: - name: Set up PostgreSQL include_role: name: ../roles/postgresql-ocp (1) vars: (2) _postgresql_namespace: \u0026#34;{{ ansible_operator_meta.namespace }}\u0026#34; _postgresql_name: \u0026#34;postgresql-gogs-{{ ansible_operator_meta.name }}\u0026#34; _postgresql_database_name: \u0026#34;gogsdb\u0026#34; _postgresql_user: \u0026#34;gogsuser\u0026#34; _postgresql_password: \u0026#34;gogspassword\u0026#34; _postgresql_volume_size: \u0026#34;{{ postgresql_volume_size|d(\u0026#39;4Gi\u0026#39;) }}\u0026#34; _postgresql_image: \u0026#34;{{ postgresql_image|d(\u0026#39;registry.redhat.io/rhscl/postgresql-10-rhel7\u0026#39;) }}\u0026#34; _postgresql_image_tag: \u0026#34;{{ postgresql_image_tag|d(\u0026#39;latest\u0026#39;) }}\u0026#34; _postgresql_size: 1 - name: Set Gogs Service name to default value set_fact: gogs_service_name: \u0026#34;gogs-{{ ansible_operator_meta.name }}\u0026#34; when: gogs_service_name is not defined - name: Set up Gogs include_role: name: ../roles/gogs-ocp (3) vars: (4) _gogs_namespace: \u0026#34;{{ ansible_operator_meta.namespace }}\u0026#34; _gogs_name: \u0026#34;{{ gogs_service_name }}\u0026#34; _gogs_ssl: \u0026#34;{{ gogs_ssl|d(False)|bool }}\u0026#34; _gogs_route: \u0026#34;{{ gogs_route | d(\u0026#39;\u0026#39;) }}\u0026#34; _gogs_image_tag: \u0026#34;{{ gogs_image_tag | d(\u0026#39;latest\u0026#39;) }}\u0026#34; _gogs_volume_size: \u0026#34;{{ gogs_volume_size|d(\u0026#39;4Gi\u0026#39;) }}\u0026#34; _gogs_postgresql_service_name: \u0026#34;postgresql-gogs-{{ ansible_operator_meta.name }}\u0026#34; _gogs_postgresql_database_name: gogsdb _gogs_postgresql_user: gogsuser _gogs_postgresql_password: gogspassword _gogs_size: 1     1 Path to Postgres Role   2 Parameters for Postgres service   3 Path to Gogs Role   4 Parameters for Gogs service      Operator Permissions The Operator will require correct permissions in order to create objects like Routes or Services in OpenShift. The SDK automatically created a default role.yaml which can be modified. Open the file config/rbac/role.yaml and add permissions for:\n  for apiGroups \u0026#34;\u0026#34;\n services\n  routes\n  peristentvlumeclaims\n  serviceaccounts\n  configmaps\n     for apiGroups: route.operanshift.io the resource routes\n   At the end, the role.yaml should look like this:\n --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: manager-role rules: ## ## Base operator rules ## - apiGroups: - \u0026#34;\u0026#34; resources: - secrets - pods - pods/exec - pods/log - services - routes - configmaps - persistentvolumeclaims - serviceaccounts verbs: - create - delete - get - list - patch - update - watch - apiGroups: - apps resources: - deployments - daemonsets - replicasets - statefulsets verbs: - create - delete - get - list - patch - update - watch ## ## Rules for gogs.example.com.at/v1alpha1, Kind: Gogs ## - apiGroups: - gogs.example.com.at resources: - gogs - gogs/status - gogs/finalizers verbs: - create - delete - get - list - patch - update - watch - apiGroups: - route.openshift.io resources: - routes verbs: - create - update - delete - get - list - watch - patch - apiGroups: - route.openshift.io resources: - routes verbs: - create - update - delete - get - list - watch - patch     Building and Deploy the Operator Now it is time to build the Operator and push it to a repository. In this example a repository was created at quay.io and is called gogs-operator. The SDK will automatically create a Makefile during the initialization, which we will use now.\n     The Makefile is prepared for docker. If you use podman some modifications must be done first. Run the command sed -i \u0026#39;s/docker/podman/g\u0026#39; Makefile to replace all docker commands inside the Makefile.     The next commands will build, push, install and deploy the Operator. Before we start we must be logged in to you Registry of choice (i.e. docker login …​) as well as into our OpenShift cluster. Moreover, it is required that the IMG environment variable is exported with the correct value.\n  Build the Operator and push into the registry\n# export IMG, be sure that the correct tag is used export IMG=quay.io/tjungbau/gogs-operator:v1.0.0 # Build and push into registry make podman-build podman-push podman build . -t quay.io/tjungbau/gogs-operator:v1.0.0 STEP 1: FROM quay.io/operator-framework/ansible-operator:v1.3.0 STEP 2: COPY requirements.yml ${HOME}/requirements.yml --\u0026gt; Using cache 4f84e7064b066c2cac5179b56490a0ef85591170c501ec8a480b617d6e91cff3 STEP 3: RUN ansible-galaxy collection install -r ${HOME}/requirements.yml \u0026amp;\u0026amp; chmod -R ug+rwx ${HOME}/.ansible --\u0026gt; Using cache 2a3a5d44451a45a4c38e1c314e8887c6c45f2551cbef87ef0d1ce518c1969c0d STEP 4: COPY watches.yaml ${HOME}/watches.yaml --\u0026gt; Using cache 642f8361a7b358b89d2e4e5211c1c7a1e22488c53bba0bf1ba2ba275fd56ee69 STEP 5: COPY roles/ ${HOME}/roles/ --\u0026gt; Using cache 93c1af8782bad84d8b81d2d2294c405caab70e2d01c232440f7eb8e5001746c1 STEP 6: COPY playbooks/ ${HOME}/playbooks/ --\u0026gt; Using cache 1cdeee1456ac67d70d4233b0f9ed8052465aaa2cded6bd8ae962dfcc848e5b92 STEP 7: COMMIT quay.io/tjungbau/gogs-operator:v1.0.0 --\u0026gt; 1cdeee1456a 1cdeee1456ac67d70d4233b0f9ed8052465aaa2cded6bd8ae962dfcc848e5b92 podman push quay.io/tjungbau/gogs-operator:v1.0.0 Getting image source signatures Copying blob d5ca8c3b3d34 skipped: already exists Copying blob 4b036ae478b7 skipped: already exists Copying blob 5cfcd0621ffc skipped: already exists Copying blob c6f3d1432bd0 skipped: already exists Copying blob 92538e92de29 skipped: already exists Copying blob eb7bf34352ca skipped: already exists Copying blob 80c43a11288f done Copying blob 803eb2035c9a done Copying blob 40d943ae1834 done Copying blob f4d9024614ee done Copying blob 5143a36c6002 done Copying blob 5050e1080446 skipped: already exists Copying config 1cdeee1456 done Writing manifest to image destination Copying config 1cdeee1456 [--------------------------------------] 0.0b / 6.2KiB Writing manifest to image destination Writing manifest to image destination Storing signatures     Install the CRD into OpenShift\n# Install the custom resource definition make install /root/projects/gogs-operator/bin/kustomize build config/crd | kubectl apply -f - customresourcedefinition.apiextensions.k8s.io/gogs.gogs.example.com.at created     Deploy the Operator and all required objects into OpenShift\n# Deploy the Operator into OpenShift make deploy cd config/manager \u0026amp;\u0026amp; /root/projects/gogs-operator/bin/kustomize edit set image controller=quay.io/tjungbau/gogs-operator:v1.0.0 /root/projects/gogs-operator/bin/kustomize build config/default | kubectl apply -f - namespace/gogs-operator-system created customresourcedefinition.apiextensions.k8s.io/gogs.gogs.example.com.at unchanged role.rbac.authorization.k8s.io/gogs-operator-leader-election-role created clusterrole.rbac.authorization.k8s.io/gogs-operator-manager-role created clusterrole.rbac.authorization.k8s.io/gogs-operator-metrics-reader created clusterrole.rbac.authorization.k8s.io/gogs-operator-proxy-role created rolebinding.rbac.authorization.k8s.io/gogs-operator-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-proxy-rolebinding created service/gogs-operator-controller-manager-metrics-service created deployment.apps/gogs-operator-controller-manager created      This will create a new project in OpenShift called gogs-operator-system. Here, the Operator is running and waiting that somebody creates a CRD of the kind Gogs. Once this happens the Operator will execute the playbooks and therefore create a Postgres and a Gogs pod.\n # Operator Namespace oc get pods -n gogs-operator-system NAME READY STATUS RESTARTS AGE gogs-operator-controller-manager-6747bb6c6-s8794 2/2 Running 0 6m8s     Using the Operator Now we need to create a CRD of the kind Gogs. This will happen in a new project, where the Gogs service shall be hosted.\n  Create a new OpenShift project\noc new-project gogs     Verify the sample resource\ncat config/samples/gogs_v1alpha1_gogs.yaml apiVersion: gogs.example.com.at/v1alpha1 kind: Gogs metadata: name: gogs-sample spec: foo: bar     Apply the sample resource\noc apply -f config/samples/gogs_v1alpha1_gogs.yaml -n gogs      This will create two services:\n  postgresql\n  Gogs\n   The Operator will be responsible to roll out all required objects. This includes the Deployments for the container, the Openshift service and the route.\n oc get all -n gogs NAME READY STATUS RESTARTS AGE pod/gogs-gogs-sample-57778fd76-ghg8j 1/1 Running 0 74s pod/postgresql-gogs-gogs-sample-bbc49b794-mnltb 1/1 Running 0 115s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/gogs-gogs-sample ClusterIP 172.30.47.31 \u0026lt;none\u0026gt; 3000/TCP 80s service/postgresql-gogs-gogs-sample ClusterIP 172.30.47.158 \u0026lt;none\u0026gt; 5432/TCP 117s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/gogs-gogs-sample 1/1 1 1 74s deployment.apps/postgresql-gogs-gogs-sample 1/1 1 1 115s NAME DESIRED CURRENT READY AGE replicaset.apps/gogs-gogs-sample-57778fd76 1 1 1 74s replicaset.apps/postgresql-gogs-gogs-sample-bbc49b794 1 1 1 115s NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/gogs-gogs-sample gogs-gogs-sample-gogs.apps.ocp.ispworld.at gogs-gogs-sample \u0026lt;all\u0026gt; None   At the end, all Pods are alive, ready and are fully controlled by the Operator. We can access the Gogs web interface via the route and start using our own Git service.\n   Updating Operator While the Operator is running fine now, at some point you might want to do some changes. For example, let’s run the Gog service with a replica of 3.\n Perform the following actions:\n  Set the variable _gogs_size to 3 in playbooks/gogs.yml\n  Build and push the new version\nexport IMG=quay.io/tjungbau/gogs-operator:v1.0.8 make podman-build podman-push podman build . -t quay.io/tjungbau/gogs-operator:v1.0.8 STEP 1: FROM quay.io/operator-framework/ansible-operator:v1.3.0 STEP 2: COPY requirements.yml ${HOME}/requirements.yml --\u0026gt; Using cache 4f84e7064b066c2cac5179b56490a0ef85591170c501ec8a480b617d6e91cff3 STEP 3: RUN ansible-galaxy collection install -r ${HOME}/requirements.yml \u0026amp;\u0026amp; chmod -R ug+rwx ${HOME}/.ansible --\u0026gt; Using cache 2a3a5d44451a45a4c38e1c314e8887c6c45f2551cbef87ef0d1ce518c1969c0d STEP 4: COPY watches.yaml ${HOME}/watches.yaml --\u0026gt; Using cache 642f8361a7b358b89d2e4e5211c1c7a1e22488c53bba0bf1ba2ba275fd56ee69 STEP 5: COPY roles/ ${HOME}/roles/ --\u0026gt; Using cache 55785493e215d933ef7a93fe000afa6fbb088d87eeffcdddeea4e7fd1896f5b5 STEP 6: COPY playbooks/ ${HOME}/playbooks/ STEP 7: COMMIT quay.io/tjungbau/gogs-operator:v1.0.8 --\u0026gt; bb9d6a995d0 bb9d6a995d059eab7758f9ac17d3ce12f8759518e231f77d32a4b820e4b14396 podman push quay.io/tjungbau/gogs-operator:v1.0.8 Getting image source signatures Copying blob 5cfcd0621ffc skipped: already exists Copying blob d5ca8c3b3d34 skipped: already exists Copying blob eb7bf34352ca skipped: already exists Copying blob 4b036ae478b7 skipped: already exists Copying blob c6f3d1432bd0 skipped: already exists Copying blob 92538e92de29 skipped: already exists Copying blob 41e53e538a36 done Copying blob 5050e1080446 skipped: already exists Copying blob 40d943ae1834 skipped: already exists Copying blob 803eb2035c9a skipped: already exists Copying blob 80c43a11288f skipped: already exists Copying blob ee0361a14e3b skipped: already exists Copying config bb9d6a995d done Writing manifest to image destination Copying config bb9d6a995d [--------------------------------------] 0.0b / 6.2KiB Writing manifest to image destination Writing manifest to image destination Storing signatures     Deploy the new version\nmake deploy cd config/manager \u0026amp;\u0026amp; /root/projects/gogs-operator/bin/kustomize edit set image controller=quay.io/tjungbau/gogs-operator:v1.0.8 /root/projects/gogs-operator/bin/kustomize build config/default | kubectl apply -f - namespace/gogs-operator-system unchanged customresourcedefinition.apiextensions.k8s.io/gogs.gogs.example.com.at unchanged role.rbac.authorization.k8s.io/gogs-operator-leader-election-role unchanged clusterrole.rbac.authorization.k8s.io/gogs-operator-manager-role unchanged clusterrole.rbac.authorization.k8s.io/gogs-operator-metrics-reader unchanged clusterrole.rbac.authorization.k8s.io/gogs-operator-proxy-role unchanged rolebinding.rbac.authorization.k8s.io/gogs-operator-leader-election-rolebinding unchanged clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-manager-rolebinding unchanged clusterrolebinding.rbac.authorization.k8s.io/gogs-operator-proxy-rolebinding unchanged service/gogs-operator-controller-manager-metrics-service unchanged deployment.apps/gogs-operator-controller-manager configured      The Operator will restart with a new version. After a while the changes will take affect and 3 Gogs pods will run.\n oc get pods -n gogs NAME READY STATUS RESTARTS AGE gogs-gogs-sample-57778fd76-4m98m 1/1 Running 0 12m gogs-gogs-sample-57778fd76-5hrdn 1/1 Running 0 6m23s gogs-gogs-sample-57778fd76-xgh2f 1/1 Running 0 6m24s postgresql-gogs-gogs-sample-bbc49b794-z84wt 1/1 Running 0 13m     What Else? - References Above example is a very quick overview about what can be done. There are many other options. You can create Operators using Go or Helm.\n The best starting points are the following websites:\n   Certified Operator Build Guide\n  Operator SDK Documentation\n    "},{"uri":"https://blog.stderr.at/tags/grafana/","title":"Grafana","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/thanos/","title":"Thanos","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2020-12-10-thanos-vs-thanos/","title":"Thanos Querier vs Thanos Querier","tags":["Grafana","Thanos","OpenShift","OCP"],"description":"How to leverage different Thanos Querier services.","content":"OpenShift comes per default with a static Grafana dashboard, which will present cluster metrics to cluster administrators. It is not possible to customize this Grafana instance.\n However, many customers would like to create their own dashboards, their own monitoring and their own alerting while leveraging the possibilities of OpenShift at the same time and without installing a completely separated monitoring stack.\n So how can you create your own queries? How can you visualize them on custom dashboards, without the need to install Prometheus or Alertmanager a second time?\n The solution is simple: Since OpenShift 4.5 (as TechPreview) and since OpenShift 4.6 (as GA) the default monitoring stack of OpenShift has been extended to support monitoring of user-defined projects. This additional configuration will help to observe your own projects.\n In this article we will see how to deploy the Grafana operator and what the possible issues can occur, when simply connecting Grafana to the OpenShift monitoring.\n Overview As a developer in OpenShift, you can create an application which provides your custom statistics of your application at the endpoint /metrics. Here the example from the official OpenShift documentation:\n # HELP http_requests_total Count of all HTTP requests # TYPE http_requests_total counter http_requests_total{code=\u0026#34;200\u0026#34;,method=\u0026#34;get\u0026#34;} 4 http_requests_total{code=\u0026#34;404\u0026#34;,method=\u0026#34;get\u0026#34;} 2 # HELP version Version information about this binary # TYPE version gauge version{version=\u0026#34;v0.1.0\u0026#34;} 1   This metric can then be viewed inside OpenShift in the developer view under the menu Monitoring. If you go to \u0026#34;Monitoring \u0026gt; Metrics\u0026#34; and select \u0026#34;Custom Query\u0026#34; from the drop down, you can enter, for example, the following PromQL query:\n sum(rate(http_requests_total[2m]))   The following graph will be the result:\n  Figure 1. Custom Query  This is great! But …​ what happens if a customer would like to see his very own super fancy Grafana dashboard? You cannot change the cluster dashboard. However, you can install your own Grafana instance and one way to do so is using the Custom Grafana Operator.\n   Before we begin Before we start the following should be prepared already:\n  OpenShift 4.5+\n  Enabled user-define workload monitoring\n  A project with user-defined workload monitoring. This is explained in the official documentation at https://docs.openshift.com/container-platform/4.6/monitoring/enabling-monitoring-for-user-defined-projects.html.\n   During this blog, we will use the namespace ns1 with a custom metric\n   Deploy Custom Grafana Operator As for any community operator the following must be considered:\n     Community Operators are operators which have not been vetted or verified by Red Hat. Community Operators should be used with caution because their stability is unknown. Red Hat provides no support for Community Operators.     The community Grafana operator must be deployed to its own namespace, for example grafana. Create this namespace first (oc new-project grafana) and search and intall the Grafana Operator from the OperatorHub. You can use the default values, just be sure to select the wanted namespace.\n After a few minutes, the operator should be available:\n  Figure 2. Installed Community Grafana Operator    Setup Grafana Operator Before we can use Grafana to draw beautiful images it must be configured. We need to create an instance of Grafana. Ideally, OpenShift OAuth is already leveraged, to avoid the need to creating user account manually, inside Grafana.\n OAuth requires some objects, which must be created before the actual Grafana instance. The following YAMLs are taken from the operator documentation. Create the following inside the Grafana namespace:\n  Session secret for the proxy …​ change the password!!\n  a cluster role grafana-proxy\n  a cluster role binding for the role\n  a config map injecting trusted CA bundles\n   apiVersion: v1 data: session_secret: Y2hhbmdlIG1lCg== kind: Secret metadata: name: grafana-k8s-proxy type: Opaque --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: grafana-proxy rules: - apiGroups: - authentication.k8s.io resources: - tokenreviews verbs: - create - apiGroups: - authorization.k8s.io resources: - subjectaccessreviews verbs: - create --- apiVersion: authorization.openshift.io/v1 kind: ClusterRoleBinding metadata: name: grafana-proxy roleRef: name: grafana-proxy subjects: - kind: ServiceAccount name: grafana-serviceaccount namespace: grafana userNames: - system:serviceaccount:grafana:grafana-serviceaccount --- apiVersion: v1 kind: ConfigMap metadata: labels: config.openshift.io/inject-trusted-cabundle: \u0026#34;true\u0026#34; name: ocp-injected-certs   Now you can create the following instance under: \u0026#34;Installed Operators \u0026gt; Grafana Operator \u0026gt; Grafana \u0026gt; Create Grafana \u0026gt; YAML View\u0026#34; (or, as an alternative, via the CLI)\n apiVersion: integreatly.org/v1alpha1 kind: Grafana metadata: name: grafana-oauth namespace: grafana spec: config: (1) auth: disable_login_form: false disable_signout_menu: true auth.anonymous: enabled: false auth.basic: enabled: true log: level: warn mode: console security: (2) admin_password: secret admin_user: root secrets: - grafana-k8s-tls - grafana-k8s-proxy client: preferService: true dataStorage: (3) accessModes: - ReadWriteOnce class: managed-nfs-storage size: 10Gi containers: (4) - args: - \u0026#39;-provider=openshift\u0026#39; - \u0026#39;-pass-basic-auth=false\u0026#39; - \u0026#39;-https-address=:9091\u0026#39; - \u0026#39;-http-address=\u0026#39; - \u0026#39;-email-domain=*\u0026#39; - \u0026#39;-upstream=http://localhost:3000\u0026#39; - \u0026#39;-tls-cert=/etc/tls/private/tls.crt\u0026#39; - \u0026#39;-tls-key=/etc/tls/private/tls.key\u0026#39; - \u0026gt;- -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token - \u0026#39;-cookie-secret-file=/etc/proxy/secrets/session_secret\u0026#39; - \u0026#39;-openshift-service-account=grafana-serviceaccount\u0026#39; - \u0026#39;-openshift-ca=/etc/pki/tls/cert.pem\u0026#39; - \u0026#39;-openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\u0026#39; - \u0026#39;-openshift-ca=/etc/grafana-configmaps/ocp-injected-certs/ca-bundle.crt\u0026#39; - \u0026#39;-skip-auth-regex=^/metrics\u0026#39; - \u0026gt;- -openshift-sar={\u0026#34;namespace\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;services\u0026#34;, \u0026#34;verb\u0026#34;: \u0026#34;get\u0026#34;} image: \u0026#39;quay.io/openshift/origin-oauth-proxy:4.8\u0026#39; name: grafana-proxy ports: - containerPort: 9091 name: grafana-proxy resources: {} volumeMounts: - mountPath: /etc/tls/private name: secret-grafana-k8s-tls readOnly: false - mountPath: /etc/proxy/secrets name: secret-grafana-k8s-proxy readOnly: false ingress: enabled: true targetPort: grafana-proxy termination: reencrypt service: annotations: service.alpha.openshift.io/serving-cert-secret-name: grafana-k8s-tls ports: - name: grafana-proxy port: 9091 protocol: TCP targetPort: grafana-proxy serviceAccount: annotations: serviceaccounts.openshift.io/oauth-redirectreference.primary: \u0026gt;- {\u0026#34;kind\u0026#34;:\u0026#34;OAuthRedirectReference\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;reference\u0026#34;:{\u0026#34;kind\u0026#34;:\u0026#34;Route\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;grafana-route\u0026#34;}} configMaps: - ocp-injected-certs dashboardLabelSelector: - matchExpressions: - key: app operator: In values: - grafana     1 Some default settings, which can be modified if required   2 A default administrative user   3 A datastore to use a persistent volume. Other options would be to use ephemeral storage, or another database. This might be especially important, if you would like HA for your Grafana.   4 Container arguments, most important the openshift-sar line which is important for the OAuth    After a few moments, the operator will pick up the change and creates a Grafana pod.\n   Adding a Data Source The next step is to connect your custom Grafana to Prometheus, or actually to the Thanos Querier. To do so, you will need to add a role to the Grafana service account and to create a CRD GrafanaDataSource.\n At this moment, we will work with the cluster role cluster-monitoring-view. The problem this might bring is discussed later.\n  Add the role to the Grafana serviceaccount\noc adm policy add-cluster-role-to-user cluster-monitoring-view -z grafana-serviceaccount     Retrieve the token of the service account\noc serviceaccounts get-token grafana-serviceaccount -n grafana     Create the following Grafana Data Source, either via UI or via CLI. Be sure to change \u0026lt;TOKEN\u0026gt; with the token from step #2.\napiVersion: integreatly.org/v1alpha1 kind: GrafanaDataSource metadata: name: prometheus-grafanadatasource namespace: grafana spec: datasources: - access: proxy editable: true isDefault: true jsonData: httpHeaderName1: Authorization timeInterval: 5s tlsSkipVerify: true name: Prometheus secureJsonData: httpHeaderValue1: \u0026gt;- Bearer \u0026lt;TOKEN\u0026gt; (1) type: prometheus url: \u0026#39;https://thanos-querier.openshift-monitoring.svc.cluster.local:9091\u0026#39; (2) name: prometheus-grafanadatasource.yaml     1 enter token from step #2   2 Thanos default querier URL…​. this might cause problems (see below)       The operator will now restart the Grafana pod to add the newest changes, which should not take more than a few seconds. Grafana can be used now. Dashboards can be created …​ but lets run some tests with PromQL queries instead.\n   Let’s Test Log in to your Grafana using OAuth and a cluster administrator.\n     You could also use a non cluster administrator, if the user is able to GET the services of the Grafana namespace. The reason is the following line in the Grafana CRD: -openshift-sar={\u0026#34;namespace\u0026#34;: \u0026#34;grafana\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;services\u0026#34;,\u0026#34;verb\u0026#34;: \u0026#34;get\u0026#34;} which defines, that OAuth will work for everybody who can get the service. This might be changed according to personal needs, but for this test it is good enough.     Then use the credentials for the admin account, which have been defined while creating the Grafana instance.\n You will be logged in now and since there are no Dashboards, lets go to Explore to enter some custom PromQL queries, for instance our example from above:\n sum(rate(http_requests_total[2m]))    Figure 3. First Query  This is looking good.\n Let’s give it another try and sort by namespaces.\n sum(rate(http_requests_total[2m])) by (namespace)    Figure 4. Second Query - showing internal namespace  What is this? I see a namespace which is actually meant for the cluster (openshift-monitoring).\n Let’s try another query using a different metric:\n sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate) by (namespace)    Figure 5. Third Query - shows even more namespaces  Ok, so we have access to all namespaces on the cluster.\n   Why do I see all namespaces? What does this mean? Well, it means that we have access to all namespaces of the cluster. We see everything. This makes sense, since we assign the cluster role \u0026#34;cluster-monitoring-view\u0026#34; to the serviceaccount of Grafana. But what if we want to show only objects from a specific namespace? If we want, for example, give the developers the possibility to create their own dashboards, without having view access to the whole cluster.\n The first test might be to remove the cluster-monitoring-view privileges from the Grafana serviceaccount. This will lead to an error on Grafana itself, since it cannot access the Thanos Querier, which we configured with: https://thanos-querier.openshift-monitoring.svc.cluster.local:9091\n How does the Openshift WebUI actually work, when you are a developer and would like to search one of the above queries. Let’s try that:\n  Figure 6. Query using the OpenShift UI  It works! It shows the namespace of the developer and only this namespace. When you inspect the actual network traffic, you will see that OpenShift automatically adds the URL parameter namespace=ns1 to the request URL:\n https://your-cluster/api/prometheus-tenancy/api/v1/query?namespace=ns1\u0026amp;query=sum%28node_namespace_pod_container%3Acontainer_cpu_usage_seconds_total%3Asum_rate%29+by+%28namespace%29   This is good information, let’s try this using the Grafana Data Source.\n     It is currently not possible to perform this configuration using the GrafanaDataSource CRD. Instead, it must be done directly at the Grafana Dashboard configuration. There is an open ticket at: https://github.com/integr8ly/grafana-operator/issues/309     Login to Grafana as administrator and switch to \u0026#34;Configuration \u0026gt; Data Source \u0026gt; Prometheus \u0026gt;\u0026#34;. At the very bottom add namespace=ns1 to the Custom query parameters\n  Figure 7. Configure Grafana Data Source      At this point the Grafana serviceaccount has cluster_monitoring_view privileges.     As you can see in the following image, this configuration did not help.\n  Figure 8. Query after Data Source has manually been modified    Thanos Querier vs. Thanos Querier To summarize, in the OpenShift UI everything works, but when using the Grafana dashboard, we see all namespaces from the cluster. Let’s try to find out how OpenShift does this.\n When we check the Thanos services we will see 3 ports:\n ports: - name: web protocol: TCP port: 9091 targetPort: web - name: tenancy protocol: TCP port: 9092 targetPort: tenancy - name: tenancy-rules protocol: TCP port: 9093 targetPort: tenancy-rules   Currently we configured port 9091, but there is another one, which is called tenancy, maybe this is what we need? Let’s try it:\n  Change the CRD GrafanaDataSource to use port 9092 (instead of 9091). This will restart the pod and remove the custom query parameter we configured earlier.\n  Remove the cluster-role\noc adm policy remove-cluster-role-from-user cluster-monitoring-view -z grafana-serviceaccount     The serviceaccount of Grafana, must be able to view the project we want to show in the dashboards. Therefore, allow the Grafana serviceaccount to view the project ns1:\noc adm policy add-role-to-user view system:serviceaccount:grafana:grafana-serviceaccount -n ns1     Log into Grafana as administrator and manually change the Data Source and add namespace=ns1 to the setting Custom query parameters\n  Rerun the Query …​ as you see you will now see one namespace only.\n Figure 9. Query with Thanos Querier on port 9092       What happened? So what actually happened here? We have two ports for our Thanos Querier which are important: 9091 and 9092.\n When we check the Deployment of the Thanos Querier for these ports we will see:\n For the port 9091 it looks like the following:\n spec: [...] containers: [...] - resources: [...] ports: - name: web containerPort: 9091 protocol: TCP [...] args: [...] - \u0026#39;-openshift-sar={\u0026#34;resource\u0026#34;: \u0026#34;namespaces\u0026#34;, \u0026#34;verb\u0026#34;: \u0026#34;get\u0026#34;}\u0026#39;   There is an OAuth setting which says: you have to have the privilege to GET the objects \u0026#34;namespace\u0026#34;.\n The only cluster role which has exactly this privilege and which is also mentioned by the official OpenShift documentation is cluster-monitoring-view\n - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-monitoring-view rules: - apiGroups: - \u0026#34;\u0026#34; resources: - namespaces verbs: - get   As we have seen above, this will show you all namespaces available on the cluster.\n When you check port 9092 there is no such OAuth configuration. This service is actually in front of the container kube-rbac-proxy. It does not require OAuth, but instead the namespace URL parameter.\n Details can be found at: https://github.com/openshift/enhancements/blob/master/enhancements/monitoring/user-workload-monitoring.md\n In short the whole setup looks like this:\n  Figure 10. Thanos interconnecting containers  While port 9091 goes directly to Thanos it will require that you have the cluster-monitoring-view role. Port 9092 does not require this, but instead you MUST send the URL parameter namespace=.\n   Summary While both options are valid, some considerations must be done when using the Grafana Operator.\n   Currently the URL parameter can be set in Grafana directly only. The operator will ignore it. The ticket in the project shall address this, but is not yet implemented: https://github.com/integr8ly/grafana-operator/issues/309\n  The URL parameter setting will be gone, when the Grafana pods is restarted, which might lead to a problem.\n  While the Grafana serviceaccount does not require cluster permissions, it will require permission to view the appropriate namespace\n  All above also means, that you actually would need to create a new DataSource for every project you want to monitor. I was not able to find a way, to send multiple namespaces in the URL parameter.\n   Is it useful to leverage the Grafana operator then at all? Probably yes, since Operators are the future and it is actively developed. Nevertheless, it is always possible to deploy Grafana manually.\n  "},{"uri":"https://blog.stderr.at/tags/argocd/","title":"ArgoCD","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/gitops/","title":"Gitops","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2020-08-06-argocd/","title":"GitOps - Argo CD","tags":["Gitops","OpenShift","OCP","ArgoCD"],"description":"Using Argo CD to manage OpenShift resources","content":"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. GitOps itself uses Git pull request to manager infrastructure and application configuration.\n Let’s try to install and use a simple usecase in order to demonstrate the basic possibilities.\n Without going into the very detail, typical GitOps usecases are:\n   Apply configurations from Git\n  Detect, (auto-)sync and notify configuration drifts\n  Manage multiple clusters and keep the configuration equal\n  …​\n   and much more. Further information about the theory behind can be found at https://www.openshift.com/blog/introduction-to-gitops-with-openshift.\n In short: there is no reason why not to use GitOps and to leverage tools like Argo CD to manage configurations. In this tutorial, the Argo CD operator gets installed and a simple use case is shown to demonstrate the possibilities of this software.\n As for architectural overview of Argo CD, please read the official documentation: https://argoproj.github.io/argo-cd/operator-manual/architecture/, which explains very well the core components. No need to rewrite it here.\n  Prerequisites You need an Openshift 4 cluster. :)\n Watch the video at: https://demo.openshift.com/en/latest/argocd/\n   Install Argo CD operator Before you begin, create a new project:\n oc new-project argocd   In this project the operator will be deployed.\n Look for the Operatorhub in the OpenShift WebUI and \u0026#34;argocd\u0026#34; and select the \u0026#34;Argo CD Community\u0026#34; operator. Subscribe to this operator. Just be sure that the newly created project is selected. Other settings can stay as default.\n  Figure 1. Argo CD: Operator  This will install the operator. You can monitor this process by clicking \u0026#34;Installed Operators\u0026#34;. After a while it should switch from \u0026#34;Installing\u0026#34; to \u0026#34;Succeeded\u0026#34;.\n   Deploy ArgoCD instance Select the installed operator \u0026#34;Argo CD\u0026#34;, select the tab \u0026#34;ArgoCD\u0026#34; and hit the button \u0026#34;Create ArgoCD\u0026#34;\n Enter the following yaml:\n apiVersion: argoproj.io/v1alpha1 kind: ArgoCD metadata: name: argocd namespace: argocd spec: dex: image: quay.io/redhat-cop/dex openShiftOAuth: true version: v2.22.0-openshift rbac: policy: | g, argocdadmins, role:admin scopes: \u0026#39;[groups]\u0026#39; server: route: enabled: true   This yaml extends the default example by:\n   using OpenShift authentication\n  Allow all users from the group \u0026#34;argocdadmins\u0026#34; admin permissions inside Argo CD\n  create a route to access argocd web interface\n   Once this configuration is created, the operator will automatically start to roll out the different pods, which are required. No worries, it will take quite long until everything is up and running.\n   Create a new group and assign a user to it In the ArgoCD resource we have defined the group argocdadmins and all users in this group will get administrator privileges in Argo CD. This group must be created and in addition we assign the user admin to it.\n For example with the following commands:\n oc adm groups new argocdadmins oc adm groups add-users argocdadmins admin     Login to Argo CD Now it is time to login to Argo CD. Just fetch the route which was created by the operator (for example with: oc get routes -n argocd).\n On the login page select \u0026#34;Login via OpenShift\u0026#34; and enter the credentials of the user you would like to use. (well, the one which you can admin permissions in the step above).\n  Figure 2. Argo CD: Login Screen  This will open the Argo CD Interface.\n   First test with Argo CD Let’s create an application in Argo CD to demonstrate the possibilities about application management with GitOps. We will use a simple application which draws a blue (or green) box in your browser.\n Click on the button \u0026#34;Create App\u0026#34; and enter the following parameters:\n   Name: bgd\n  Project: default (This is the project inside Argo CD, not OpenShift)\n  Sync Policy: Can stay at manual for now\n  Repository URL: https://github.com/tjungbauer/gitops-examples (This is a fork of christianh814/gitops-examples)\n  Revision: master\n  Path: bgd/\n  Cluster: https://kubernetes.devault.svc (This is the local default cluster Argo CD created. Other Clusters may be defined)\n  Namespace: bgd (This is the OpenShift namespace which will be created)\n   At the end, it should look like this:\n  Figure 3. Argo CD: Create an Application  Press the \u0026#34;Create\u0026#34; button and your application is ready to be synchronized. Since no synchronization happens yet, Argo CD will complain that the application is out of sync.\n Sync application Since we set the Sync Policy to manual, the synchronization process must be started, guess what, manually. Click on the \u0026#34;Sync\u0026#34; button and Argo CD will open a side panel, which shows the resources are out of sync and other options.\n  Figure 4. Argo CD: Sync an Application  One notable option is the \u0026#34;Prune\u0026#34; setting. By selecting this, changes which have been done directly on OpenShift, are removed and replaced by the ones which are stored at Git.\n     This is a very good option, to force everyone to follow the GitOps process :)     Press the \u0026#34;Synchronize\u0026#34; button and select the application. As you see the sync process has started and after a while, all resources are synced to OpenShift.\n  Figure 5. Argo CD: Application Syncing   Figure 6. Argo CD: Application Synced     Verifying objects Now that Argo CD says that the application has been synchronized, we should check the objects, which have been created in OpenShift.\n As you can see in the Git repository, there are 4 objects which should exist now:\n   a namespace (bgd)\n  a deployment\n  a service\n  a route\n    Figure 7. Argo CD: Git Repo  To verify the existence either check via the WebUI or simply try:\n oc get all -n bgd NAME READY STATUS RESTARTS AGE pod/bgd-6b9b64d94d-5fqdg 1/1 Running 0 6m2s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/bgd ClusterIP 172.30.233.30 \u0026lt;none\u0026gt; 8080/TCP 6m7s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/bgd 1/1 1 1 6m4s NAME DESIRED CURRENT READY AGE replicaset.apps/bgd-6b9b64d94d 1 1 1 6m3s NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/bgd bgd-bgd.apps.ocp.example.test bgd 8080 None   Obviously, the namespace exists and with it also the other objects, which hae been synchronized.\n When you now open the route http://bgd-bgd.apps.ocp.example.test in your browser, you will see a nice blue box.\n  Figure 8. Argo CD: The Blue Box  As you can see all objects have been synchronized and the application has been deployed correctly. The source of truth is in Git and all changes should be done there.\n   I want a green box So you want a green box? Maybe you think of doing this:\n Modify the Deployment and change the environment COLOR from blue to green:\n ... spec: containers: - name: bgd image: \u0026#39;quay.io/redhatworkshops/bgd:latest\u0026#39; env: - name: COLOR value: green # change from blue to green ...   This will trigger a re-deployment and …​ fine …​ you have a green box:\n  Figure 9. Argo CD: The Green Box  But is this the correct way to do that? NO, it is not. Argo CD will immediately complain that the application is out of sync.\n  Figure 10. Argo CD: Out of Sync  When you sync the application it will end up with a blue box again.\n  Figure 11. Argo CD: The Blue Box  But you really really want a green box? Fair enough, the correct way would be to change the deployment configuration on Git. Simply change the file bgd/bgd-deployment.yaml and set the COLOR to green:\n ... spec: containers: - image: quay.io/redhatworkshops/bgd:latest name: bgd env: - name: COLOR value: \u0026#34;green\u0026#34; resources: {}   Again Argo CD will complain that it is out of sync.\n  Figure 12. Argo CD: Git Update  By synchronizing the changes, it will deploy the latest version found at Git and …​ yes, you have a green box now (When deployment on OpenShift side has finished).\n  Figure 13. Argo CD: The Green Box   "},{"uri":"https://blog.stderr.at/categories/tekton/","title":"Tekton","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/container-security/","title":"Container Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/05/enable-automatic-route-creation/","title":"Enable Automatic Route Creation","tags":["Istio","Service Mesh","OpenShift","OCP","Route"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 13 - Automatic route creation","content":"Red Hat Service Mesh 1.1 allows you to enable a \u0026#34;Automatic Route Creation\u0026#34; which will take care about the routes for a specific Gateway. Instead of defining * for hosts, a list of domains can be defined. The Istio OpenShift Routing (ior) synchronizes the routes and creates them inside the Istio namespace. If a Gateway is deleted, the routes will also be removed again.\n This new features makes the manual creation of the route obsolete, as it was explained here: Openshift 4 and Service Mesh 4 - Ingress with custom domain\n Enable Automatic Route Creation Before this feature can be used, it must be enabled. To do so the ServiceMeshContolPlace, typically found in the namespace istio-system must be modified. Add the line ior_enabled: true to the istio-ingressgate configuration.\n ... spec: istio: gateways: istio-egressgateway: autoscaleEnabled: false istio-ingressgateway: autoscaleEnabled: false ior_enabled: true ...     Verify current service Let’s check our tutorial application, if it is still working.\n oc project tutorial export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) curl $GATEWAY_URL/customer customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 30   Let’s review and remove the current used Gateway. As you can see the hosts is set to \u0026#39;*\u0026#39;\n oc get istio-io oc get gateway.networking.istio.io/customer-gateway -o yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;networking.istio.io/v1alpha3\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Gateway\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;customer-gateway\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;tutorial\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;selector\u0026#34;:{\u0026#34;istio\u0026#34;:\u0026#34;ingressgateway\u0026#34;},\u0026#34;servers\u0026#34;:[{\u0026#34;hosts\u0026#34;:[\u0026#34;*\u0026#34;],\u0026#34;port\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;http\u0026#34;,\u0026#34;number\u0026#34;:80,\u0026#34;protocol\u0026#34;:\u0026#34;HTTP\u0026#34;}}]}} creationTimestamp: \u0026#34;2020-05-13T07:52:20Z\u0026#34; generation: 1 name: customer-gateway namespace: tutorial resourceVersion: \u0026#34;41370056\u0026#34; selfLink: /apis/networking.istio.io/v1alpha3/namespaces/tutorial/gateways/customer-gateway uid: 96e82ed9-e870-493c-941f-bfa83c892b94 spec: selector: istio: ingressgateway servers: - hosts: - \u0026#39;*\u0026#39; port: name: http number: 80 protocol: HTTP     Create a new Gateway First let’s remove the current Gateway\n oc delete gateway.networking.istio.io/customer-gateway   Now lets create a new Gateway, but this time we define some names for the hosts section:\n cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; Gateway-ior.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: customer-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - www.example.com - svc.example.com EOF oc apply -f Gateway-ior.yaml -n tutorial   When you now check the routes, 2 new routes have been added:\n oc get routes -n istio-system NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD ... tutorial-customer-gateway-kmqrl www.example.com istio-ingressgateway http2 None tutorial-customer-gateway-ks7q7 svc.example.com istio-ingressgateway http2 None   To test the connectivity, you need to be sure that the hosts, used in the Gateway, are resolvable. If they are then you can access your service:\n curl www.example.com/customer customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 31 curl svc.example.com/customer customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 32    "},{"uri":"https://blog.stderr.at/tags/istio/","title":"Istio","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/quay/","title":"Quay","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/quay/2020-05-13-quay-tutorial1/","title":"Red Hat Quay Registry - Overview and Installation","tags":["Quay","Registry","OpenShift","Container Security"],"description":"Red Hat Quay Registry - Overview and Installation","content":"Red Hat Quay is an enterprise-quality container registry, which is responsible to build, scan, store and deploy containers. The main features of Quay include:\n   High Availability\n  Security Scanning (with Clair)\n  Registry mirroring\n  Docker v2\n  Continuous integration\n   and much more.\n Quay can be installed as:\n   Standalone single service\n  High Availibility service\n  On OpenShift with an Operator\n  On OpenShift without an Operator.\n   For the following Quay Tutorial Quay version 3.3 on OpenShift 4.3/4.4 with an Operator is used. Moreover, a local storage is used, to make the deployment easier and independent to a storage provider. In any case, be sure that for a production environment a \u0026#34;real\u0026#34; storage is used.\n Further details can be found at the official Red Hat documentation at: Deploy Red Hat Quay on OpenShift with an Operator\n Quay Architecture Several components are used to build Quay\n   Database: used to store metadata (not images)\n  Redis: stores live builder logs\n  Quay container registry: Runs Quay as a service\n   Two types of storages are supported:\n   Public cloud storage, like Amazon S3, Google Cloud Storage\n  Private cloud storage, S3 compliant Object Store, Like Ceph RADOS or OpenStack Swift.\n       Since I perform the example setup in a limited lab environment, I used local storage. This is not supported for production installations.       Installation This example installation, covers the configuration of all credentials, but it ignores the configuration of a storage engine. Instead it is using local storage.\n Create a new project and install the Quay Operator Note: Before you begin create a new project called \u0026#34;quay\u0026#34;.\n From the OpenShift console goto Operators \u0026gt; OperationHub and search for \u0026#34;Red Hat Quay Operator\u0026#34;. Be sure NOT to select the community version. Install the Operator using the following settings:\n   Installation Mode: select the namespace \u0026#34;quay\u0026#34;\n  Leave the other settings as default.\n    Figure 1. Operator Installation  Once you select \u0026#34;Subscribe\u0026#34;, the installation process will take a few minutes. At the end, the operator pods should run inside your namespace and is ready to bse used.\n  Figure 2. Operator Running   Configure Quay To configure Quay the Custom Resource Definition \u0026#34;QuayEcosystem\u0026#34; must be defined. Below example shows a ready to use example to test the deployment. Several secrets are used in this QuayEcosystem which must be created first. If they are not specified in the QuayEcosystem, then default values would be used. However, it always makes sense to specify passwords :)\n Create a Pull Secret to pull from Quay.io Check the article Accessing Red Hat Quay to find the appropriate credentials you need to fetch images from quay.io. Find the section \u0026#34;Red Hat Quay v3 on OpenShift\u0026#34;, copy the secret into the file redhat_secret.yaml and create the object in OpenShift:\n oc create -f redhat_secret.yaml    Create Quay Superuser Credentials The Quay superuser will be able to manage other users or projects. Create the secret by defining username, password and e-mail address:\n     The password must have a minimum length of 8 characters.     oc create secret generic quay-credentials \\ --from-literal=superuser-username=\u0026lt;username\u0026gt; \\ --from-literal=superuser-password=\u0026lt;password\u0026gt; \\ --from-literal=superuser-email=\u0026lt;email\u0026gt;    Create Quay Configuration Credentials The Quay configuration is done, by a separate pod, with a separate accessible route. To set the password, create the following secret:\n     The password must have a minimum length of 8 characters.     oc create secret generic quay-config-app \\ --from-literal=config-app-password=\u0026lt;password\u0026gt;    Specify database credentials As next let’s create the credentials for the database:\n oc create secret generic \u0026lt;secret_name\u0026gt; \\ --from-literal=database-username=\u0026lt;username\u0026gt; \\ --from-literal=database-password=\u0026lt;password\u0026gt; \\ --from-literal=database-root-password=\u0026lt;root-password\u0026gt; \\ --from-literal=database-name=\u0026lt;database-name\u0026gt;       It is also possible to use and existing database. To configure this, create the secret as described and add the server parameter, containing the hostname, to the QuayEcosystem definition+      Setting Redis password By default, the operator would install Redis without any password. To specify a password, create the following secret:\n oc create secret generic quay-redis-password \\ --from-literal=password=\u0026lt;password\u0026gt;     Create QuayEcosystem Resource With all the secrets created above, it is time to create the QuayEcosystem. Once it is defined, the operator will automatically start all required services.\n The following is an example, using the different secret names (The names should be self explaining) In addition, the following has been defined:\n   volumeSize = 10GI for the database\n  keepConfigDeployment to false, this will remove the configuration pod after the deployment.\n  hostname: to reach the Quay registry under a defined hostname (otherwise a default name would be created)\n  Clair container scanning is enabled\n   apiVersion: redhatcop.redhat.io/v1alpha1 kind: QuayEcosystem metadata: name: quayecosystem spec: quay: imagePullSecretName: redhat-quay-pull-secret superuserCredentialsSecretName: quay-credentials configSecretName: quay-config-app deploymentStrategy: Recreate skipSetup: false keepConfigDeployment: false externalAccess: hostname: quay.apps.ocp.ispworld.at database: volumeSize: 10Gi credentialsSecretName: quay-database-credential registryBackends: - name: local local: storagePath: /opt/quayregistry redis: credentialsSecretName: quay-redis-password imagePullSecretName: redhat-quay-pull-secret clair: enabled: true imagePullSecretName: redhat-quay-pull-secret    Quay WebUI Once the Quay Operator has deployed all containers, you should see one route (or 2 if you kept Configuration Deployment Container) and can access your Quay installation.\n  Figure 3. Quay WebUI      Optional: Disable self account creation Many customers want to disable the \u0026#34;Create Account\u0026#34; link on the login page (see Figure #3), to prevent that anybody could create a new account. To remove this option, the configuration pod must run.\n Verify if the Configuration pod is running If the following does not return anything, then the container is not running:\n oc get routes -n quay | grep config   If this is the case, modify the resource QuayEcosystem to enable the Configuration UI.\n Edit:\n oc edit QuayEcosystem/quayecosystem   and set \u0026#34;KeepConfigDeployment\u0026#34; to true:\n quay: [...] keepConfigDeployment: true   After a few minutes another pod, called \u0026#34;quayecosystem-quay-config\u0026#34; will be started and a new route is created:\n oc get routes -n quay | grep config quayecosystem-quay-config quayecosystem-quay-config-quay.apps.ocp.ispworld.at quayecosystem-quay-config 8443 passthrough/Redirect None    Configure Account Creation and Anonymous Access Login to the Configuration Web Interface with the credentials you specified during the deployment and scroll down to the section \u0026#34;Access Settings\u0026#34;.\n  Figure 4. Quay Configuration  There remove the checkbox from:\n   Anonymous Access\n  User Creation\n   and save and build the configuration.\n This will trigger a change on the Quay pod. After it has been recreated (this will take a few minutes), the feature to create a new account is removed from the Login page.\n     Working with Quay The following quick steps through Quay are the steps of the Quay tutorial, which can be seen at the Quay WebUI at the \u0026#34;Tutorial\u0026#34; tab.\n Login via Docker CLI To login via docker CLI simply use:\n docker login \u0026lt;your selected hostname for quay\u0026gt;       Docker expects a valid certificate. Such certificate could be added to the definition of QuayEcosystem. However, I did not create a certificate for this lab. To allow untrusted certificates, on a Mac, simply download the certificates (For Chrome: you can drag and drop the certificate from the browser to your Desktop, for Firefox, you need to open the Options menu and export the certificates.). After that double click both certificates (the root and the site certificate), which will install them on you local Keychain. Open the Keychain on you Mac, find the appropriate certificates and set both to \u0026#34;Always trust\u0026#34;      Create an example container The next step to create a new image is to create a container. For this example the busybox base image is used.\n docker run busybox echo \u0026#34;fun\u0026#34; \u0026gt; newfile   This will pull the latest image of busybox and create a container:\n Unable to find image \u0026#39;busybox:latest\u0026#39; locally latest: Pulling from library/busybox d9cbbca60e5f: Pulling fs layer d9cbbca60e5f: Verifying Checksum d9cbbca60e5f: Download complete d9cbbca60e5f: Pull complete Digest: sha256:836945da1f3afe2cfff376d379852bbb82e0237cb2925d53a13f53d6e8a8c48c Status: Downloaded newer image for busybox:latest   With \u0026#34;docker ps\u0026#34; the running container is shown. Remember the Container ID for further steps. In this case fc3e9bb1e9da.\n docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fc3e9bb1e9da busybox \u0026#34;echo fun\u0026#34; 3 minutes ago Exited (0) 3 minutes ago relaxed_proskuriakova    Create the image Once a container has terminated in Docker, the next step is to commit the container to an image. To do so we will use \u0026#34;docker commit\u0026#34; command. As name for the repository I took superapp.\n docker commit fc3e9bb1e9da quay.apps.ocp.ispworld.at/quay/superapp    Push the image to Red Hat Quay The final step is to push the image to our repository, where it will be stored for future use.\n docker push quay.apps.ocp.ispworld.at/quay/superapp    Figure 5. Quay Repository    Test Container Security Scanner Clair is used to scan containers about possible security risks. It imports vulnerability data permanently from a known source and creates a list of threats for an image.\n To test such scanning, we pull the \u0026#34;Universal Base Image RHEL 7\u0026#34; from Red Hat. I am using version 7.6 since this is already quite old and we expect some known vulnerabilities for this image.\n First you need to login to Red Hat Registry:\n docker login registry.redhat.io Username: \u0026lt;Username\u0026gt; Password: \u0026lt;Password\u0026gt; Login Succeeded   Then let’s pull the UBI Image 7.6 (instead of the latest)\n docker pull registry.redhat.io/ubi7/ubi:7.6   Before we can push it to our Quay registry, we need to tag it:\n docker tag registry.redhat.io/ubi7/ubi:7.6 quay.apps.ocp.ispworld.at/quay/ubi7:7.6   If we now check the local images, we see that there are two UBI images.\n docker images REPOSITORY TAG IMAGE ID CREATED SIZE quay.apps.ocp.ispworld.at/quay/ubi7 7.6 247ee58855fd 10 months ago 204MB registry.redhat.io/ubi7/ubi 7.6 247ee58855fd 10 months ago 204MB   Now it is possible to push the image to the Quay repository.\n docker push quay.apps.ocp.ispworld.at/quay/ubi7:7.6   Finally the image is available inside our Registry and Clair will queue it for a security scan.\n Once the scan is finished, possible found issues are shown under the \u0026#34;Repository Tags\u0026#34;.\n  Figure 6. Clair Security Scanning  When you click on then, you will see a detailed result page, with all vulnerabilities found:\n  Figure 7. Clair Security Scanning Result    "},{"uri":"https://blog.stderr.at/tags/registry/","title":"Registry","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/route/","title":"Route","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/service-mesh/","title":"Service Mesh","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/service-mesh/","title":"Service Mesh","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/authorization/","title":"Authorization","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/05/authorization-rbac/","title":"Authorization (RBAC)","tags":["Istio","Service Mesh","OpenShift","OCP","Authorization","Security"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 12 - Authorization Role Based Access Control (RBAC)","content":"Per default all requests inside a Service Mesh are allowed, which can be a problem security-wise. To solve this, authorization, which verifies if the user is allowed to perform a certain action, is required. Istio’s authorization provides access control on mesh-level, namespace-level and workload-level.\n With the resource AuthorizationPolicy granular policies can be defined. These policies are loaded to and verified by the Envoy Proxy which then authorizes a request.\n Implicit enablement To enable authorization the only thing you need is to do is to define the AuthorizationPolicy. If the resource is not defined, then no access control will be used, instead any traffic is allowed. If AuthorizationPolicy is applied to a workload, then by default any traffic is denied unless it is explicitly allowed.\n     This is applicable to Service Mesh version 1.1+        Preparing Environment The following steps will configure an example Role Based Access Control (RBAC). It will start from scratch. If you just want to quickly configure the authorization and have anything else in place, you can start form here: Configure Authentication Policy\n  Create a new project\noc new-project tutorial     Be sure that a Service Mesh Member Roll exists for this new project\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; memberroll.yaml apiVersion: maistra.io/v1 kind: ServiceMeshMemberRoll metadata: name: default spec: members: - tutorial EOF oc apply -f memberroll.yaml -n istio-system     Clone and install the example application\ngit clone https://github.com/redhat-developer-demos/istio-tutorial/ istio-tutorial oc apply -f istio-tutorial/customer/kubernetes/Deployment.yml -n tutorial oc apply -f istio-tutorial/customer/kubernetes/Service.yml -n tutorial oc expose service customer oc apply -f istio-tutorial/preference/kubernetes/Deployment.yml -n tutorial oc apply -f istio-tutorial/preference/kubernetes/Service.yml -n tutorial oc apply -f istio-tutorial/recommendation/kubernetes/Deployment.yml -n tutorial oc apply -f istio-tutorial/recommendation/kubernetes/Service.yml -n tutorial   Wait until all pods are running. There should be 2 containers for all pods:\n oc get pods -w     Create Gateway and VirtualService\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; Gateway_VirtualService.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: customer-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: customer-gateway spec: hosts: - \u0026#34;*\u0026#34; gateways: - customer-gateway http: - match: - uri: prefix: /customer rewrite: uri: / route: - destination: host: customer port: number: 8080 EOF oc apply -f Gateway_VirtualService.yaml -n tutorial     Verify if the application is working You can either use the run.sh from previous tutorials, or simply try the following curl\nexport GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;); echo $GATEWAY_URL curl $GATEWAY_URL/customer   This should return the following line:\n customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1     Optionally check the connection from inside the customer container\nget pods name and enter it:\n oc get pods NAME READY STATUS RESTARTS AGE customer-6948b8b959-dhsm9 2/2 Running 0 177m preference-v1-7fdb89c86b-dvzs9 2/2 Running 0 177m recommendation-v1-69db8d6c48-cjcpn 2/2 Running 0 177m   Connect into the container pod and try to reach the different microservices\n oc rsh customer-6948b8b959-dhsm9 sh-4.4$ curl customer:8080 customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 2 sh-4.4$ curl preference:8080 preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3 sh-4.4$ curl recommendation:8080 recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 4         Configure Authentication Policy  Enabling User-End authentication\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; authentication-policy.yaml apiVersion: \u0026#34;authentication.istio.io/v1alpha1\u0026#34; kind: \u0026#34;Policy\u0026#34; metadata: name: \u0026#34;customerjwt\u0026#34; spec: targets: - name: customer - name: preference - name: recommendation origins: - jwt: issuer: \u0026#34;testing@secure.istio.io\u0026#34; jwksUri: \u0026#34;https://gist.githubusercontent.com/lordofthejars/7dad589384612d7a6e18398ac0f10065/raw/ea0f8e7b729fb1df25d4dc60bf17dee409aad204/jwks.json\u0026#34; principalBinding: USE_ORIGIN EOF oc apply -f authentication-policy.yaml -n tutorial     Access should be denied after a few seconds\ncurl $GATEWAY_URL/customer Origin authentication failed.%     Use token to authenticate\ntoken=$(curl https://gist.githubusercontent.com/lordofthejars/a02485d70c99eba70980e0a92b2c97ed/raw/f16b938464b01a2e721567217f672f11dc4ef565/token.simple.jwt -s) curl -H \u0026#34;Authorization: Bearer $token\u0026#34; $GATEWAY_URL/customer   This will result in a correct response\n customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 5        Configure Role Based Access Control (RBAC)  Create the resource AuthorizationPolicy\nThis is a new resources, supported since Service Mesh 1.1. It will allow GET method when the role equals to \u0026#34;customer\u0026#34;\n cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; AuthorizationPolicy.yaml apiVersion: \u0026#34;security.istio.io/v1beta1\u0026#34; kind: \u0026#34;AuthorizationPolicy\u0026#34; metadata: name: \u0026#34;customer\u0026#34; spec: rules: - to: - operation: methods: [\u0026#34;GET\u0026#34;] when: - key: request.auth.claims[role] values: [\u0026#34;customer\u0026#34;] EOF oc apply -f AuthorizationPolicy.yaml -n tutorial     Get a token for the role and retry to connect to the service,\ntoken=$(curl https://gist.githubusercontent.com/lordofthejars/f590c80b8d83ea1244febb2c73954739/raw/21ec0ba0184726444d99018761cf0cd0ece35971/token.role.jwt -s) curl -H \u0026#34;Authorization: Bearer $token\u0026#34; $GATEWAY_URL/customer   This results in:\n customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 8     Let’s verify the setting and change the AuthorizationPolicy. This will break the authorization, since the token provides roles=customer and we set the Policy to \u0026#34;whereistherole\u0026#34;\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; AuthorizationPolicy-Hack.yaml apiVersion: \u0026#34;security.istio.io/v1beta1\u0026#34; kind: \u0026#34;AuthorizationPolicy\u0026#34; metadata: name: \u0026#34;customer\u0026#34; spec: rules: - to: - operation: methods: [\u0026#34;GET\u0026#34;] when: - key: request.auth.claims[role] values: [\u0026#34;whereistherole\u0026#34;] EOF oc replace -f AuthorizationPolicy-Hack.yaml -n tutorial   If you now try to access the service, with the token, which provides \u0026#34;customer\u0026#34; as role, it will lead to an error:\n token=$(curl https://gist.githubusercontent.com/lordofthejars/f590c80b8d83ea1244febb2c73954739/raw/21ec0ba0184726444d99018761cf0cd0ece35971/token.role.jwt -s) curl -H \u0026#34;Authorization: Bearer $token\u0026#34; $GATEWAY_URL/customer RBAC: access denied       "},{"uri":"https://blog.stderr.at/general/2020/05/basic-usage-of-git/","title":"Basic usage of git","tags":["git","github"],"description":"Using git for contributing to projects hosted on github.com","content":"This is a very short and hopefully simple introduction on how to use Git when you would like to contribute to projects hosted on github.com. The same workflow should also work for projects on gitlab.com.\n Introduction There is this fancy mega application hosted on github called megaapp that you would like to contribute to. It’s perfect but there’s just this little feature missing to make it even more perfect.\n This is how we would tackle this.\n     rocket science ahead       Glossary     Term Definition     fork\n A (personal) copy of a repository you created on github or gitlab.\n   upstream\n When creating forks of repositories on github or gitlab, the original repository hosting the project\n   index\n The staging area git uses before you can commit to a repository\n   remote repository\n A repository hosted on a server shared by developers\n   local repository\n A local copy of a repository stored on you machine.\n      Step 1: Fork the repository on github.com Login to you Github account and navigate to the project you would like to fork, megaapp in our example.\n Click on the the fork button, as depicted in the image below:\n   If you are a member of several projects on github.com, github is going to ask you into which project you would like to clone this repository.\n After selecting the project or your personal account, github is going to clone the repository into the project you selected. For this example I’m going to use my personal github account \u0026#34;tosmi\u0026#34;.\n   Step 2: Clone the repository to you workstation Next we are going to clone our fork from Step 1: Fork the repository on github.com to our workstation and start working on the new feature.\n After forking the upstream project you are redirect to your personal copy of the project. Click on the \u0026#34;Clone or download\u0026#34; button and select the link. You can choose between SSH and HTTPS protocols for downloading the project. We are going to use SSH.\n   Copy the link into a terminal and execute the git clone command:\n $ git clone git@github.com:tosmi/megaapp.git     Step 3: Create a feature branch for your new fancy feature Change into the directory of the project you downloaded in Step 2: Clone the repository to you workstation\n cd megaapp   Now we create a feature branch with a short name that describes our new feature:\n git checkout -b tosmi/addoption   Because we would like to add a new option to megaapp we call this feature branch addoption.\n We are also prefixing the feature branch with our github username so that it is clear for the upstream project maintainer(s) who is contributing this.\n How you name you branches is opinionated, so we would search for upstream project guidelines and if there are none maybe look at some existing pull request how other people are naming there branches. If we find no clue upstream we sticking with \u0026lt;github username\u0026gt;/\u0026lt;branch name\u0026gt;.\n We can now start adding our mega feature to the project.\n   Step 4: Add you changes to the Git index Before we can commit our changes, we have to place the changes made in the so called index or staging area:\n $ git add \u0026lt;path to file you have changed\u0026gt;   If we would like to place all of our changes onto the index we could execute\n $ git add -A     Step 5: Commit your changes After adding our changes to the Git index we can commit with\n $ git commit   This will open our favorite editor and we can type a commit message. The first line should be a short description of our change, probably not longer than 70 to 80 characters. After two newlines we can enter a detailed explanation of your changes.\n This is an example commit message\n Added a `version` option to output the current version of megaapp This change introduces a `version` option to megaapp. The purpose is to output the current version of megaapp for users. This might be helpful when users open a bug report so we can see what version is affected.   After saving the message and we have successfully created a commit.\n     Remember this is now only stored in the local copy of the repository! We still have to push our changes to github.     There is also the option to add the commit comment directly on the command line\n $ git commit -m \u0026#39;Added a `version` option to output the current version of megaapp This change introduces a `version` option to megaapp. The purpose is to output the current version of megaapp for users. This might be helpful when users open a bug report so we can see what version is affected.\u0026#39;     Step 6: Pushing our local changes to our forked repo on github.com We execute\n $ git push   to push our local changes to the forked repository hosted on github.com.\n   Step 7: Creating a pull request on github.com We navigate to our personal project page of the forked repository on github. For the fork we are using in this example this is http://github.com/tosmi/megaapp.\n Github is going to show us a button \u0026#34;Compare \u0026amp; pull request\u0026#34;:\n   After clicking on that button we are able to review the changes we would like to include in this pull request.\n If we are happy with our changes we click on \u0026#34;Create pull request\u0026#34;. The upstream owner of the repository will get notified and we can see our open pull request on the upstream project page under \u0026#34;Pull requests\u0026#34;.\n If there are CI test configured for that project they will start to run and we can see if our pull request is going to pass all test configured.\n   Rebasing to current upstream if required Sometimes a upstream project maintainer asks you to rebase your work on the current upstream master branch. The following steps explain the basic workflow.\n First we are going to create a new remote location of our repository called upstream. Upstream points to the upstream project repository. We will not push to this location, in most cases this is not possible because you do not have write access to a remote upstream repository. It is just used for pulling upstream changes in our forked repository.\n Execute the following commands to add the upstream repository as a new remote location and display all remote locations currently defined.\n $ git remote add upstream https://github.com/rhatservices/megaapp.git $ git remote -v origin git@github.com:tosmi/megaapp.git (fetch) origin git@github.com:tosmi/megaapp.git (push) upstream https://github.com/rhatservices/megaapp.git (fetch) upstream https://github.com/rhatservices/megaapp.git (push)   As we hopefully implemented our new feature in feature branch, we can pull changes from the upstream master branch into our local copy of the master branch. Remember we are using a feature branch and master should be kept clean from local changes.\n $ git checkout master Switched to branch \u0026#39;master\u0026#39; Your branch is up to date with \u0026#39;origin/master\u0026#39;.   So now we have this older copy of the upstream master branch checked out and we would like to update it to the latest and greatest from the upstream master branch.\n $ git pull upstream master remote: Enumerating objects: 10, done. remote: Counting objects: 100% (10/10), done. remote: Compressing objects: 100% (3/3), done. remote: Total 6 (delta 2), reused 6 (delta 2), pack-reused 0 Unpacking objects: 100% (6/6), 630 bytes | 157.00 KiB/s, done. From https://github.com/rhatservices/megaapp * branch master -\u0026gt; FETCH_HEAD * [new branch] master -\u0026gt; upstream/master Updating 4d8584e..ddfd077 Fast-forward cmd/megaapp/main.go | 2 ++ cmd/megaapp/rule.go | 20 ++++++++++++++++++++ 2 files changed, 22 insertions(+) create mode 100644 cmd/megaapp/rule.go   With the pull command above you pulled all changes from the upstream master branch into you local copy of master. Just to be sure let’s display all available branches, local and remote ones.\n Branches with a name remote/\u0026lt;remote name\u0026gt;/\u0026lt;branch name\u0026gt; are remote branches that git knows about. Origin points to our forked repository and is also the default location for push operations.\n $ git branch -a master * tosmi/megafeature remotes/origin/HEAD -\u0026gt; origin/master remotes/origin/master remotes/origin/tosmi/megafeature remotes/upstream/master   So finally to rebase our feature branch to the upstream master branch we first need to checkout our feature branch via\n $ git checkout tosmi/megafeature   Now we are able to rebase our changes to upstream master. Git basically pulls in all changes from the master branch and re-applies the changes we did in our feature branch.\n git rebase upstream/master Successfully rebased and updated refs/heads/tosmi/megafeature.   There might be merge conflicts when git tries to apply you changes from your feature branch. You have to fix those changes, git add the fixed files and execute git rebase continue. Luckily this is not the case for your megafeature.\n As we have successfully rebased our feature branch to upstream master we can now try to push changes made to our forked github repository.\n $ git push To github.com:tosmi/megaapp.git ! [rejected] tosmi/megafeature -\u0026gt; tosmi/megafeature (non-fast-forward) error: failed to push some refs to \u0026#39;git@github.com:tosmi/megaapp.git\u0026#39; hint: Updates were rejected because the tip of your current branch is behind hint: its remote counterpart. Integrate the remote changes (e.g. hint: \u0026#39;git pull ...\u0026#39;) before pushing again. hint: See the \u0026#39;Note about fast-forwards\u0026#39; in \u0026#39;git push --help\u0026#39; for details.   Oh, this fails of course! The reason is that our local feature branch and the remote feature branch have a different commit history. The remote feature branch is missing the commits from master that we applied when rebasing on the current master branch.\n So let’s try again, this time using the --force-with-lease option. You could also use -f or --force but --force-with-lease will stop you if someone else (our you) has modified the remote feature branch meanwhile. If you push with -f or --force anyways you might loose changes.\n $ git push --force-with-lease Enumerating objects: 5, done. Counting objects: 100% (5/5), done. Delta compression using up to 8 threads Compressing objects: 100% (3/3), done. Writing objects: 100% (3/3), 295 bytes | 295.00 KiB/s, done. Total 3 (delta 2), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To github.com:tosmi/megaapp.git + acf66a3...39357b2 tosmi/megafeature -\u0026gt; tosmi/megafeature (forced update)   But as no one modified the remote feature branch while we did our rebase the force push goes through.\n Our merge request (if we opened one already) is now updated to the latest upstream master branch and merging our feature should be a breeze. You might notify the upstream project maintainer that you feature branch is up to date and ready for merging\n   Using git’s interactive rebase to change you commit history When working with upstream projects it might be that a project maintainer requests that you rework your git history before he is willing to merge your changes. For example this could be that case if you have plenty of commits with very small changes (e.g. fixed typos).\n The general rule is that one commit should implement one change. This is not a hard rule, but usually works.\n Let’s look at an example. For the implementation of our new feature that we would like to bring upstream we have the following commit history\n $ git log --oneline 0a5221d (HEAD -\u0026gt; tosmi/megafeature) fixed typo 0e60d12 update README bf2ef3c update   We have updated README.md in the repository but there a three commits for this little change. Before bringing this upstream in our pull request, we would like to convert those three commits into a single one and also make the commit message a little more meaningful.\n We execute the following command to start reworking our commit history\n $ git rebase -i   Git will drop us into our beloved editor (vi in this case), under Linux you could change the editor git uses by modifying the $EDITOR environment variable. We are going to see the following output:\n pick bf2ef3c update pick 0e60d12 update README pick 0a5221d fixed typo # Rebase 39357b2..0a5221d onto 39357b2 (3 commands) # # Commands: # p, pick \u0026lt;commit\u0026gt; = use commit # r, reword \u0026lt;commit\u0026gt; = use commit, but edit the commit message # e, edit \u0026lt;commit\u0026gt; = use commit, but stop for amending # s, squash \u0026lt;commit\u0026gt; = use commit, but meld into previous commit # f, fixup \u0026lt;commit\u0026gt; = like \u0026#34;squash\u0026#34;, but discard this commit\u0026#39;s log message # x, exec \u0026lt;command\u0026gt; = run command (the rest of the line) using shell # b, break = stop here (continue rebase later with \u0026#39;git rebase --continue\u0026#39;) # d, drop \u0026lt;commit\u0026gt; = remove commit # l, label \u0026lt;label\u0026gt; = label current HEAD with a name # t, reset \u0026lt;label\u0026gt; = reset HEAD to a label # m, merge [-C \u0026lt;commit\u0026gt; | -c \u0026lt;commit\u0026gt;] \u0026lt;label\u0026gt; [# \u0026lt;oneline\u0026gt;] # . create a merge commit using the original merge commit\u0026#39;s # . message (or the oneline, if no original merge commit was # . specified). Use -c \u0026lt;commit\u0026gt; to reword the commit message. # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. #   Git automatically selected commit id bf2ef3c as the basis for our rebase. We could also have specified the commit id where we would like to start our rebase operation e.g.\n git rebase -i bf2ef3c   In our editor of choice we can now tell git what it should do with the selected commits. Please go ahead and read the helpfull explanation text in comments (prefixed with \u0026#39;#\u0026#39;) to get a better understanding of the operations supported.\n In our case we would like to squash the last commits. So we change the lines with pick to squash until it looks like the following:\n pick bf2ef3c update squash 0e60d12 update README squash 0a5221d fixed typo   We would like to squash commits 0a5221d and 0e60d12 onto commit bf2ef3c. Keep in mind that git actually reverses the order of commits. So 0a5221d is the last commit we added.\n If we save the file and quit our editor (I’m using vi here), git drops us into another buffer where we can finally modify the commits\n This is a combination of 3 commits. # This is the 1st commit message: update # This is the commit message #2: update README # This is the commit message #3: fixed typo # Please enter the commit message for your changes. Lines starting # with \u0026#39;#\u0026#39; will be ignored, and an empty message aborts the commit. # # Date: Mon May 18 15:46:37 2020 +0200 # # interactive rebase in progress; onto 39357b2 # Last commands done (3 commands done): # squash 0e60d12 update README # squash 0a5221d fixed typo # No commands remaining. # You are currently rebasing branch \u0026#39;tosmi/megafeature\u0026#39; on \u0026#39;39357b2\u0026#39;. # # Changes to be committed: # modified: README.md #   We can see all three commit message and we are going to modify those messages until we are happy\n # This is a combination of 3 commits. # This is the 1st commit message: updated README.md to megafeature as we added megafeature, it makes sense to include a short note about it also in README.md # Please enter the commit message for your changes. Lines starting # with \u0026#39;#\u0026#39; will be ignored, and an empty message aborts the commit. # # Date: Mon May 18 15:46:37 2020 +0200 # # interactive rebase in progress; onto 39357b2 # Last commands done (3 commands done): # squash 0e60d12 update README # squash 0a5221d fixed typo # No commands remaining. # You are currently rebasing branch \u0026#39;tosmi/megafeature\u0026#39; on \u0026#39;39357b2\u0026#39;. # # Changes to be committed: # modified: README.md #   When we are happy with new commit message we just save and quit our editor. Git will now rewirte the history and when we take look at the commit history again we will see our changes:\n $ git log --oneline 91d1ae2 (HEAD -\u0026gt; tosmi/megafeature) updated README.md to megafeature 39357b2 (origin/tosmi/megafeature) added a mega feature ddfd077 (upstream/master, master) added rule command 4d8584e (origin/master, origin/HEAD) Update README.md eb6ccbc Create README.md 60fcabc start using cobra for argument parsing 5140ed0 import .gitignore d2b55d1 import a simple Makefile 2ecb412 initial import   We only have commit 91d1ae2 now , which includes all three changes from the commits before.\n     Rewriting the history of a repository is a dangerous operation. Especially when you are working in a team. It is not advised to change the history of commits that got already pushed to a remote location. Otherwise your teammates will get confused next time they try to push or pull from the shared repository.     So it’s OK to change the commit history of a feature branch that only you are using, but be careful when working on branches more than one developer is using.\n  "},{"uri":"https://blog.stderr.at/categories/general/","title":"General","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/git/","title":"Git","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/git/","title":"git","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/github/","title":"github","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/bookinfo/","title":"Bookinfo","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/deploy-example-bookinfo-application/","title":"Deploy Example Bookinfo Application","tags":["Istio","Service Mesh","OpenShift","OCP","Bookinfo","Example"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 11 - Deploy Example Bookinfo Application","content":"To test a second application, a bookinfo application shall be deployed as an example.\n The following section finds it’s origin at:\n   Istio - Bookinfo Application\n  OpenShift 4 - Example Application\n   The Bookinfo application displays information about a book, similar to a single catalog entry of an online book store. Displayed on the page is a description of the book, book details (ISBN, number of pages, and other information), and book reviews. The Bookinfo application consists of these microservices: * The productpage microservice calls the details and reviews microservices to populate the page. * The details microservice contains book information. * The reviews microservice contains book reviews. It also calls the ratings microservice. * The ratings microservice contains book ranking information that accompanies a book review. There are three versions of the reviews microservice: * Version v1 does not call the ratings Service. * Version v2 calls the ratings Service and displays each rating as one to five black stars. * Version v3 calls the ratings Service and displays each rating as one to five red stars. The end-to-end architecture of the application is shown below.    Figure 1. Bookinfo Application End2End Overview  To use the bookinfo application inside service mesh, no code changes are required. Instead an Envoy proxy is added as a sidecar container to all containers (product, review, details) which intercepts the traffic.\n  Installation Let’s start right away:\n  Create a new project\noc new-project bookinfo     Add the new project to our Service Mesh\napiVersion: maistra.io/v1 kind: ServiceMeshMember metadata: name: default namespace: bookinfo spec: controlPlaneRef: name: basic-install namespace: istio-system     Create the application\noc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-1.1/samples/bookinfo/platform/kube/bookinfo.yaml     Create the Gateway and the VirtuaService\noc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-1.1/samples/bookinfo/networking/bookinfo-gateway.yaml     Check if the services and pods are up and running\noc get svc,pods NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/details ClusterIP 172.30.178.172 \u0026lt;none\u0026gt; 9080/TCP 7m16s service/productpage ClusterIP 172.30.78.96 \u0026lt;none\u0026gt; 9080/TCP 7m13s service/ratings ClusterIP 172.30.154.12 \u0026lt;none\u0026gt; 9080/TCP 7m15s service/reviews ClusterIP 172.30.138.174 \u0026lt;none\u0026gt; 9080/TCP 7m14s NAME READY STATUS RESTARTS AGE pod/details-v1-d7db4d55b-mwzsk 2/2 Running 0 7m14s pod/productpage-v1-5f598fbbf4-svkbc 2/2 Running 0 7m11s pod/ratings-v1-85957d89d8-v2lrs 2/2 Running 0 7m11s pod/reviews-v1-67d9b4bcc-x6s2v 2/2 Running 0 7m11s pod/reviews-v2-67b465c497-zpz6z 2/2 Running 0 7m11s pod/reviews-v3-7bd659b757-j6rwn 2/2 Running 0 7m11s        Verify that application is accessible  Export the Gateway URL into a variable\nexport GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;)     Verify if the productpage is accessible\ncurl -s http://${GATEWAY_URL}/productpage | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; \u0026lt;title\u0026gt;Simple Bookstore App\u0026lt;/title\u0026gt;     You can also access the Productpage in your browser. When you reload the page several times, you will see different results for the Reviews. This comes due to 3 different versions: one without any rating, one with black stars and one with red stars. http://${GATEWAY_URL}/productpage\n Figure 2. Bookinfo Application       Adding default Destination Rule oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-1.1/samples/bookinfo/networking/destination-rule-all-mtls.yaml   This will add default routing to all endpoints with same weight. As you can see in Kiali, the Reviews microservice is contacted equally.\n  Figure 3. Kiali: Bookinfo Application  Feel free to play with other DestinationRules to controll your traffic.\n  "},{"uri":"https://blog.stderr.at/tags/example/","title":"Example","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/2020/04/ansible-azure-resource-manager-example/","title":"Ansible - Azure Resource Manager Example","tags":["Ansible","Azure"],"description":"Using Ansible Azure Resource Manager to create a Virtual Machine","content":"Using Ansible Resource Manager with an ARM template and a simple Ansible playbook to deploy a Virtual Machine with Disk, virtual network, public IP and so on.\n Introduction Source: [1]\n In order to deploy a Virtual Machine and all depended resources using the Azure Resource Manager (ARM) template with Ansible, you will need three things:\n   The ARM Template\n  The parameters you want to use\n  The Ansible playbook\n   All can be found below. Store them and simply call:\n ansible-playbook Azure/create_azure_deployment.yml       it will take several minutes, until everything has been deployment in Azure.         Instead of using a json file locally you can upload the template file (as well as a parameters file) to a version control system and use it from there.       Additional Resources Ansible Playbook: Create-Azure-Deplyoment.yml --- - name: Get facts of a VM hosts: localhost connection: local become: false gather_facts: false tasks: #- name: Destroy Azure Deploy # azure_rm_deployment: # resource_group: tju-ResourceGroup # name: tju-testDeployment # state: absent - name: Create Azure Resource Group deployment azure_rm_deployment: state: present resource_group_name: tju-ResourceGroup name: tju-testDeployment #template_link: \u0026#39;\u0026lt;YOUR RAW Github template file\u0026gt;\u0026#39; #parameters_link: \u0026#39;\u0026lt;YOUR RAW Github parameters file\u0026gt;\u0026#39; template: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;ResourceManagerTemplate.json\u0026#39;) }}\u0026#34; parameters: projectName: value: tjuProject location: value: \u0026#34;East US\u0026#34; adminUsername: value: tjungbauer adminPublicKey: value: \u0026#34;{{ lookup(\u0026#39;file\u0026#39;, \u0026#39;/Users/tjungbauer/.ssh/id_rsa.pub\u0026#39;) }}\u0026#34; operatingSystem: value: CentOS operatingSystemPublisher: value: OpenLogic operatingSystemSKU: value: \u0026#39;7.1\u0026#39; vmSize: value: Standard_D2s_v3 register: azure - name: Add new instance to host group add_host: hostname: \u0026#34;{{ item[\u0026#39;ips\u0026#39;][0].public_ip }}\u0026#34; groupname: azure_vms loop: \u0026#34;{{ azure.deployment.instances }}\u0026#34;    ResourceManagerTemplate.json { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;projectName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies a name for generating resource names.\u0026#34; } }, \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the location for all resources.\u0026#34; } }, \u0026#34;adminUsername\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies a username for the Virtual Machine.\u0026#34; } }, \u0026#34;adminPublicKey\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the SSH rsa public key file as a string. Use \\\u0026#34;ssh-keygen -t rsa -b 2048\\\u0026#34; to generate your SSH key pairs.\u0026#34; } }, \u0026#34;operatingSystem\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the Operating System. i.e. CentOS\u0026#34; } }, \u0026#34;operatingSystemPublisher\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the publisher. i.e. OpenLogic\u0026#34; } }, \u0026#34;operatingSystemSKU\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the version of the OS. i.e. 7.1\u0026#34; } }, \u0026#34;vmSize\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the the VM size. i.e. Standard_D2s_v3\u0026#34; } } }, \u0026#34;variables\u0026#34;: { \u0026#34;vNetName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-vnet\u0026#39;)]\u0026#34;, \u0026#34;vNetAddressPrefixes\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;vNetSubnetName\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;vNetSubnetAddressPrefix\u0026#34;: \u0026#34;10.0.0.0/24\u0026#34;, \u0026#34;vmName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-vm\u0026#39;)]\u0026#34;, \u0026#34;publicIPAddressName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-ip\u0026#39;)]\u0026#34;, \u0026#34;networkInterfaceName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-nic\u0026#39;)]\u0026#34;, \u0026#34;networkSecurityGroupName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;projectName\u0026#39;), \u0026#39;-nsg\u0026#39;)]\u0026#34;, \u0026#34;networkSecurityGroupName2\u0026#34;: \u0026#34;[concat(variables(\u0026#39;vNetSubnetName\u0026#39;), \u0026#39;-nsg\u0026#39;)]\u0026#34; }, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/networkSecurityGroups\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;networkSecurityGroupName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;securityRules\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ssh_rule\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Locks inbound down to ssh default port 22.\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;Tcp\u0026#34;, \u0026#34;sourcePortRange\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;destinationPortRange\u0026#34;: \u0026#34;22\u0026#34;, \u0026#34;sourceAddressPrefix\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;destinationAddressPrefix\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;access\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;priority\u0026#34;: 123, \u0026#34;direction\u0026#34;: \u0026#34;Inbound\u0026#34; } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/publicIPAddresses\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;publicIPAddressName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;publicIPAllocationMethod\u0026#34;: \u0026#34;Dynamic\u0026#34; }, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Basic\u0026#34; } }, { \u0026#34;comments\u0026#34;: \u0026#34;Simple Network Security Group for subnet [variables(\u0026#39;vNetSubnetName\u0026#39;)]\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/networkSecurityGroups\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-08-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;networkSecurityGroupName2\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;securityRules\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;default-allow-22\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;priority\u0026#34;: 1000, \u0026#34;access\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;direction\u0026#34;: \u0026#34;Inbound\u0026#34;, \u0026#34;destinationPortRange\u0026#34;: \u0026#34;22\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;Tcp\u0026#34;, \u0026#34;sourceAddressPrefix\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;sourcePortRange\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;destinationAddressPrefix\u0026#34;: \u0026#34;*\u0026#34; } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/virtualNetworks\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vNetName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkSecurityGroups\u0026#39;, variables(\u0026#39;networkSecurityGroupName2\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;addressSpace\u0026#34;: { \u0026#34;addressPrefixes\u0026#34;: [ \u0026#34;[variables(\u0026#39;vNetAddressPrefixes\u0026#39;)]\u0026#34; ] }, \u0026#34;subnets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnetName\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;addressPrefix\u0026#34;: \u0026#34;[variables(\u0026#39;vNetSubnetAddressPrefix\u0026#39;)]\u0026#34;, \u0026#34;networkSecurityGroup\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkSecurityGroups\u0026#39;, variables(\u0026#39;networkSecurityGroupName2\u0026#39;))]\u0026#34; } } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Network/networkInterfaces\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-11-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;networkInterfaceName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;Microsoft.Network/publicIPAddresses\u0026#39;, variables(\u0026#39;publicIPAddressName\u0026#39;))]\u0026#34;, \u0026#34;[resourceId(\u0026#39;Microsoft.Network/virtualNetworks\u0026#39;, variables(\u0026#39;vNetName\u0026#39;))]\u0026#34;, \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkSecurityGroups\u0026#39;, variables(\u0026#39;networkSecurityGroupName\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;ipConfigurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ipconfig1\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;privateIPAllocationMethod\u0026#34;: \u0026#34;Dynamic\u0026#34;, \u0026#34;publicIPAddress\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/publicIPAddresses\u0026#39;, variables(\u0026#39;publicIPAddressName\u0026#39;))]\u0026#34; }, \u0026#34;subnet\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/virtualNetworks/subnets\u0026#39;, variables(\u0026#39;vNetName\u0026#39;), variables(\u0026#39;vNetSubnetName\u0026#39;))]\u0026#34; } } } ] } }, { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Compute/virtualMachines\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2018-10-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vmName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkInterfaces\u0026#39;, variables(\u0026#39;networkInterfaceName\u0026#39;))]\u0026#34; ], \u0026#34;properties\u0026#34;: { \u0026#34;hardwareProfile\u0026#34;: { \u0026#34;vmSize\u0026#34;: \u0026#34;[parameters(\u0026#39;vmSize\u0026#39;)]\u0026#34; }, \u0026#34;osProfile\u0026#34;: { \u0026#34;computerName\u0026#34;: \u0026#34;[variables(\u0026#39;vmName\u0026#39;)]\u0026#34;, \u0026#34;adminUsername\u0026#34;: \u0026#34;[parameters(\u0026#39;adminUsername\u0026#39;)]\u0026#34;, \u0026#34;linuxConfiguration\u0026#34;: { \u0026#34;disablePasswordAuthentication\u0026#34;: true, \u0026#34;ssh\u0026#34;: { \u0026#34;publicKeys\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;[concat(\u0026#39;/home/\u0026#39;, parameters(\u0026#39;adminUsername\u0026#39;), \u0026#39;/.ssh/authorized_keys\u0026#39;)]\u0026#34;, \u0026#34;keyData\u0026#34;: \u0026#34;[parameters(\u0026#39;adminPublicKey\u0026#39;)]\u0026#34; } ] } } }, \u0026#34;storageProfile\u0026#34;: { \u0026#34;imageReference\u0026#34;: { \u0026#34;publisher\u0026#34;: \u0026#34;[parameters(\u0026#39;operatingSystemPublisher\u0026#39;)]\u0026#34;, \u0026#34;offer\u0026#34;: \u0026#34;[parameters(\u0026#39;operatingSystem\u0026#39;)]\u0026#34;, \u0026#34;sku\u0026#34;: \u0026#34;[parameters(\u0026#39;operatingSystemSKU\u0026#39;)]\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;osDisk\u0026#34;: { \u0026#34;createOption\u0026#34;: \u0026#34;fromImage\u0026#34; } }, \u0026#34;networkProfile\u0026#34;: { \u0026#34;networkInterfaces\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Network/networkInterfaces\u0026#39;, variables(\u0026#39;networkInterfaceName\u0026#39;))]\u0026#34; } ] } } } ], \u0026#34;outputs\u0026#34;: { \u0026#34;adminUsername\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;adminUsername\u0026#39;)]\u0026#34; } } }      Sources   [1]: azure_rm_deployment – Create or destroy Azure Resource Manager template deployments\n    "},{"uri":"https://blog.stderr.at/tags/azure/","title":"Azure","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2020-04-16-tekton/","title":"OpenShift Pipelines - Tekton Introduction","tags":["Pipelines","OpenShift","OCP","Tekton"],"description":"Using OpenShift 4.x and OpenShift Pipelines using Tekton for your CI/CD process","content":"OpenShift Pipelines is a cloud-native, continuous integration and delivery (CI/CD) solution for building pipelines using Tekton. Tekton is a flexible, Kubernetes-native, open-source CI/CD framework that enables automating deployments across multiple platforms (Kubernetes, serverless, VMs, etc) by abstracting away the underlying details. [1]\n OpenShift Pipelines features Source: [1]\n   Standard CI/CD pipeline definition based on Tekton\n  Build images with Kubernetes tools such as S2I, Buildah, Buildpacks, Kaniko, etc\n  Deploy applications to multiple platforms such as Kubernetes, serverless and VMs\n  Easy to extend and integrate with existing tools\n  Scale pipelines on-demand\n  Portable across any Kubernetes platform\n  Designed for microservices and decentralized teams\n  Integrated with the OpenShift Developer Console\n     Prerequisites  OpenShift 4.x cluster. Try yourself at https://try.openshift.com\n  Optional: Tekton CLI - Optional for now, since you could do everything via UI as well.\n  oc/kubectl CLI or WebUI\n       The Tekton CLI is optional in case you prefer to do everything via the OpenShift WebUI. However, below examples make use of the Tekton CLI and I personally would recommend to at least install it. (Like oc client it is good to have a CLI option as well). In any case, all described action can be done directly via the WebUI as well.       Basic Concepts Tekton makes use of several custom resources (CRD).\n These CRDs are:\n  Task: each step in a pipeline is a task, while a task can contain several steps itself, which are required to perform a specific task. For each Task a pod will be allocated and for each step inside this Task a container will be used. This helps in better scalability and better performance throughout the pipeline process.\n  Pipeline: is a series of tasks, combined to work together in a defined (structured) way\n  TaskRun: is the result of a Task, all combined TaskRuns are used in the PipelineRun\n  PipelineRun: is the actual execution of a whole Pipeline, containing the results of the pipeline (success, failed…​)\n   Pipelines and Tasks should be generic and never define possible variables, like input git repository, directly in their definition. For this, the concept of PipelineResources has been created, which defines these parameters and which are used during a PipelineRun.\n   Installation The OpenShift Pipeline is an operator which can e installed using the following yaml:\n cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-OpenShift-Pipelines.yaml apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: openshift-pipelines-operator namespace: openshift-operators spec: channel: dev-preview name: openshift-pipelines-operator source: community-operators sourceNamespace: openshift-marketplace EOF   oc create -f deploy-OpenShift-Pipelines.yaml   As an alternative, you can also use the WebUI to rollout the operator:\n  Search for \u0026#34;OpenShift Pipeline\u0026#34; under OperatorHub and install it\n  Select:\n All Namespaces: since the operator needs to watch for Tekton Custom Resources across all namespaces.\n  Channel: Dev Preview\n  Approve Strategy: Automatic\n        Prepare a tutorial project To test our OpenShift Pipelines, we need to deploy an example application. This application let’s you vote what pet you like more: Cats or Dogs? It contais of a backend and a frontend part, which both will be deployed in a namespace.\n Let’s first create a new project:\n oc new-project pipelines-tutorial   The OpenShift Pipeline operator will automatically create a pipeline serviceaccount with all required permissions to build and push an image and which is used by PipelineRuns:\n oc get sa pipeline NAME SECRETS AGE pipeline 2 15s      Create a Task A Task is the smallest block of a Pipeline which by itself can contain one or more steps which are executed in order to process a specific element. For each Task a pod is allocated and each step is running in a container inside this pod. Tasks are reusable by other Pipelines. Input and Output specifications can be used to interact with other Tasks.\n Let’s create two tasks Source: Pipeline-Tutorial\n cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-Example-Tasks.yaml apiVersion: tekton.dev/v1alpha1 kind: Task metadata: name: apply-manifests spec: inputs: resources: - {type: git, name: source} params: - name: manifest_dir description: The directory in source that contains yaml manifests type: string default: \u0026#34;k8s\u0026#34; steps: - name: apply image: quay.io/openshift/origin-cli:latest workingDir: /workspace/source command: [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;] args: - |- echo Applying manifests in $(inputs.params.manifest_dir) directory oc apply -f $(inputs.params.manifest_dir) echo ----------------------------------- --- apiVersion: tekton.dev/v1alpha1 kind: Task metadata: name: update-deployment spec: inputs: resources: - {type: image, name: image} params: - name: deployment description: The name of the deployment patch the image type: string steps: - name: patch image: quay.io/openshift/origin-cli:latest command: [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;] args: - |- oc patch deployment $(inputs.params.deployment) --patch=\u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{ \u0026#34;containers\u0026#34;:[{ \u0026#34;name\u0026#34;: \u0026#34;$(inputs.params.deployment)\u0026#34;, \u0026#34;image\u0026#34;:\u0026#34;$(inputs.resources.image.url)\u0026#34; }] }}}}\u0026#39; EOF   oc create -f deploy-Example-Tasks.yaml   Verify that the two tasks have been created using the Tekton CLI:\n tkn task ls NAME AGE apply-manifests 52 seconds ago update-deployment 52 seconds ago      Create a Pipeline A pipeline is a set of Tasks, which should be executed in a defined way to achieve a specific goal.\n The example Pipeline below uses two resources:\n   git-repo: defines the Git-Source\n  image: Defines the target at a repository\n   It first uses the Task buildah, which is a standard Task the OpenShift operator created automatically. This task will build the image. The resulted image is pushed to an image registry, defined in the output parameter. After that our created tasks apply-manifest and update-deployment are executed. The execution order of these tasks is defined with the runAfter Parameter in the yaml definition.\n     The Pipeline should be re-usable accross multiple projects or environments, thats why the resources (git-repo and image) are not defined here. When a Pipeline is executed, these resources will get defined.     cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-Example-Pipeline.yaml apiVersion: tekton.dev/v1alpha1 kind: Pipeline metadata: name: build-and-deploy spec: resources: - name: git-repo type: git - name: image type: image params: - name: deployment-name type: string description: name of the deployment to be patched tasks: - name: build-image taskRef: name: buildah kind: ClusterTask resources: inputs: - name: source resource: git-repo outputs: - name: image resource: image params: - name: TLSVERIFY value: \u0026#34;false\u0026#34; - name: apply-manifests taskRef: name: apply-manifests resources: inputs: - name: source resource: git-repo runAfter: - build-image - name: update-deployment taskRef: name: update-deployment resources: inputs: - name: image resource: image params: - name: deployment value: $(params.deployment-name) runAfter: - apply-manifests EOF   oc create -f deploy-Example-Pipeline.yaml   Verify that the Pipeline has been created using the Tekton CLI:\n tkn pipeline ls NAME AGE LAST RUN STARTED DURATION STATUS build-and-deploy 3 seconds ago --- --- --- ---      Trigger Pipeline After the Pipeline has been created, it can be triggered to execute the Tasks.\n Create PipelineResources Since the Pipeline is generic, we need to define 2 PipelineResources first, to execute a Pipepline. Our example application contains a frontend (vote-ui) AND a backend (vote-api), therefore 4 PipelineResources will be created. (2 times git repository to clone the source and 2 time output image)\n Quick overview:\n   ui-repo: will be used as git_repo in the Pipepline for the Frontend\n  ui-image: will be used as image in the Pipeline for the Frontend\n  api-repo: will be used as git_repo in the Pipepline for the Backend\n  api-image: will be used as image in the Pipeline for the Backend\n   cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-Example-PipelineResources.yaml apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: ui-repo spec: type: git params: - name: url value: http://github.com/openshift-pipelines/vote-ui.git --- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: ui-image spec: type: image params: - name: url value: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-ui:latest --- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: api-repo spec: type: git params: - name: url value: http://github.com/openshift-pipelines/vote-api.git --- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: api-image spec: type: image params: - name: url value: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-api:latest EOF   oc create -f deploy-Example-PipelineResources.yaml   The resources can be listed with:\n tkn resource ls NAME TYPE DETAILS api-repo git url: http://github.com/openshift-pipelines/vote-api.git ui-repo git url: http://github.com/openshift-pipelines/vote-ui.git api-image image url: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-api:latest ui-image image url: image-registry.openshift-image-registry.svc:5000/pipelines-tutorial/vote-ui:latest    Execute Pipelines We start a PipelineRune for the backend and frontend of our application.\n cat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; deploy-Example-PipelineRun.yaml apiVersion: tekton.dev/v1alpha1 kind: PipelineRun metadata: name: build-deploy-api-pipelinerun spec: pipelineRef: name: build-and-deploy resources: - name: git-repo resourceRef: name: api-repo - name: image resourceRef: name: api-image params: - name: deployment-name value: vote-api --- apiVersion: tekton.dev/v1alpha1 kind: PipelineRun metadata: name: build-deploy-ui-pipelinerun spec: pipelineRef: name: build-and-deploy resources: - name: git-repo resourceRef: name: ui-repo - name: image resourceRef: name: ui-image params: - name: deployment-name value: vote-ui EOF   oc create -f deploy-Example-PipelineRun.yaml   The PipelineRuns can be listed with\n tkn pipelinerun ls NAME STARTED DURATION STATUS build-deploy-api-pipelinerun 3 minutes ago --- Running build-deploy-ui-pipelinerun 3 minutes ago --- Running   Moreover, the logs can be viewed with the following command and select the appropriate PipelineRun:\n tkn pipeline logs -f ? Select pipelinerun: [Use arrows to move, type to filter] \u0026gt; build-deploy-api-pipelinerun started 2 minutes ago build-deploy-ui-pipelinerun started 2 minutes ago    Checking your application Now our Pipeline built and deployed the voting application, where you can vote if you prefere cats or dogs (Cats or course :) )\n Get the route of your project and open the URL in the browser. (Should be something like vote-ui-pipelines-tutorial.apps.yourclustername)\n  Figure 1. Tekton: Example Application      OpenShift WebUI With the OpenShift Pipeline operator a new menu item is introduced on the WebUI of OpenShift. All Tekton CLI command which are used above, can actually be replaced with the web interface, in case you prefere this. The big advantage is th graphical presentation of Pipelines and their lifetime.\n I will not create screenshots for every screen, but for example pipelines:\n Under Pipelines a list of pipelines will be shown.\n  Figure 2. OpenShift UI: List of Pipelines    Additional Resources   Sources   [1]: OpenShift Pipelines Tutorial\n  Tekon\n  Tekton Task Catalog\n    "},{"uri":"https://blog.stderr.at/tags/pipelines/","title":"Pipelines","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/tekton/","title":"Tekton","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/general/2020/04/red-hat-satellite-cheat-sheet/","title":"Red Hat Satellite Cheat Sheet","tags":["Satellite"],"description":"Cheat sheet for Red Hat Satellite","content":"Cheat sheet for various Red Hat Satellite tasks from a newbie to a newbie.\n Requirements   up to Satellite 6.7 RHEL 7.X\n  4 CPU Cores\n  20 GB of RAM\n  300 GB disk space\n   for more info see the prerequistes guide\n   Installation Satellite up to version 6.7 uses puppet for installation. You can use\n puppet filebucket   to restore files modified by puppet.\n Satellite requires the Red Hat Satellite Infrastructure Subscription, check if it’s available with\n subscription-manager list --all --available --matches \u0026#39;Red Hat Satellite Infrastructure Subscription\u0026#39;   If not attach it with\n subscription-manager attach --pool=pool_id   Next disable all repos and enable only supported repostories via\n subscription-manager repos --disable \u0026#34;*\u0026#34;   and enable required repositories\n subscription-manager repos --enable=rhel-7-server-rpms \\ --enable=rhel-7-server-satellite-6.6-rpms \\ --enable=rhel-7-server-satellite-maintenance-6-rpms \\ --enable=rhel-server-rhscl-7-rpms \\ --enable=rhel-7-server-ansible-2.8-rpms   then clean cached all repo data via\n yum clean all   and install satellite packages via\n yum install satellite   Install satellite with\n satellite-installer --scenario satellite \\ --foreman-initial-organization \u0026#34;initial_organization_name\u0026#34; \\ --foreman-initial-location \u0026#34;initial_location_name\u0026#34; \\ --foreman-initial-admin-username admin_user_name \\ --foreman-initial-admin-password admin_password     Backup / Restore / Cloning Use satellite-maintain for doing offline and online backups\n satellite-maintain backup offline /backup/   when using the online option make sure that no new content view or content view versions should be created while the backup is running. basically satellite should be idle.\n Cloning Satellite The online/offline options also backup /var/lib/pulp, which contains all downloaded packages. This could be huge. There’s an option to skip this so\n satellite-maintain backup offline --skip-pulp-tar /backup/       For a restore you always need the content of /var/lib/pulp.     This is mainly usefull for cloning satellite. You backup everything except /var/lib/pulp, copy the backup to a second system and rsync /var/lib/pulp to the new system. Then restore the backup and satellite should work as normal on the clone.\n  Snaphot backups Satellite also supports backups via LVM snapshots. For more information see Snapshot backup\n    Upgrades  Read the Satellite release notes\n  Do a offline backup see [Backup / Restore]\n  You could clone satellite to a other system\n  If there are local changes to dhcpd or dns configurations use\nsatellite-installer --foreman-proxy-dns-managed=false --foreman-proxy-dhcp-managed=false   to stop satellite-install from overwriting those files.\n   install the latest version of satellite-maintain via\nyum install rubygem-foreman_maintain     check for available satellite versions with\nsatellite-maintain upgrade list-versions     test the possible upgrade with\nsatellite-maintain upgrade check --target-version 6.7     and finally run the upgrade and PRAY!\nsatellite-maintain upgrade run --target-version 6.7        Various tips and tricks Installing packages via yum Satellite installs a yum plugin called foreman-protector. If you try to install a package via yum you get the following message\n WARNING: Excluding 12190 packages due to foreman-protector. Use foreman-maintain packages install/update \u0026lt;package\u0026gt; to safely install packages without restrictions. Use foreman-maintain upgrade run for full upgrade.   so use\n satellite-maintain install \u0026lt;package name\u0026gt;    OS package upgrade This should be done via satellite-maintain because all packages are locked by default (see Installing packages via yum).\n This basically comes down to running\n oreman-maintain upgrade run --target-version 6.6.z   for upgrading OS packages if you have satellite 6.6 installed.\n   "},{"uri":"https://blog.stderr.at/tags/satellite/","title":"Satellite","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/categories/satellite/","title":"Satellite","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/service-mesh-1.1-released/","title":"Service Mesh 1.1 released","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"OpenShift 4.x Service Mesh Update.","content":"April 10th 2020 Red Hat released Service Mesh version 1.1 which supports the following versions:\n   Istio - 1.4.6\n  Kiali - 1.12.7\n  Jaeger - 1.17.1\n   Update To update an operator like Service Mesh, the Operator Life Cycle Manager takes care and automatically updates everything (unless it was configured differently).\n For the Service Mesh 1.1 update consult Upgrading Red Hat OpenShift Service Mesh It is important to add the version number to the ServiceMeshControlPlane object. The easiest way to do so is:\n   Log into OpenShift\n  Select the Namespace istio-system\n  Goto _\u0026#34;Installed Operators \u0026gt; Red Hat OpenShift Service Mesh \u0026gt; ServiceMeshControlPlanes \u0026gt; basic-install \u0026gt; YAML\u0026#34;\n  Under spec add the following:\nspec: version: v1.1        Notable Changes ServiceMeshMember Object With the ServiceMeshMember object it is now possible that a project administrator can add a service to the service mesh, instead relying on the cluster administrator to configure the ServiceMeshMemberRoll. To do so create the following object (i.e. under the namespace tutorial)\n apiVersion: maistra.io/v1 kind: ServiceMeshMember metadata: name: default namespace: tutorial spec: controlPlaneRef: name: basic-install (1) namespace: istio-system (2)     1 Name of the ServiceMeshControlPlane object   2 name of the service mesh namespace      "},{"uri":"https://blog.stderr.at/service-mesh/2020/04/authentication-jwt/","title":"Authentication JWT","tags":["Istio","Service Mesh","OpenShift","OCP","JWT"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 10 - Authentication with JWT","content":"Welcome to tutorial 10 of OpenShift 4 and Service Mesh, where we will discuss authentication with JWT. JSON Web Token (JWT) is an open standard that allows to transmit information between two parties securely as a JSON object. It is an authentication token, which is verified and signed and therefore trusted. The signing can be achieved by using a secret or a public/private key pair.\n Service Mesh can be used to configure a policy which enables JWT for your services.\n Preparation Be sure that you have at least the Gateway and VirtualService configured:\n oc get istio-io -n tutorial   Which should return the following:\n NAME AGE gateway.networking.istio.io/ingress-gateway-exampleapp 45h NAME HOST AGE destinationrule.networking.istio.io/recommendation recommendation 29h NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/ingress-gateway-exampleapp [ingress-gateway-exampleapp] [*] 45h   Run some texample traffic, to be sure that our application is still working as expected\n export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) sh ~/run.sh 1000 $GATEWAY_URL   # 0: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 31622 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 33056 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 31623 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 33057 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 31624 # 5: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 33058     Enabling End-User Authentication To test this feature we will need a valid token (JWT). More details can be found at the Istio example\n All we need to create a Policy object\n apiVersion: \u0026#34;authentication.istio.io/v1alpha1\u0026#34; kind: \u0026#34;Policy\u0026#34; metadata: name: \u0026#34;jwt-example\u0026#34; spec: targets: - name: customer origins: - jwt: issuer: \u0026#34;testing@secure.istio.io\u0026#34; jwksUri: \u0026#34;https://raw.githubusercontent.com/istio/istio/release-1.2/security/tools/jwt/samples/jwks.json\u0026#34; (1) principalBinding: USE_ORIGIN     1 Path to test a public key    After a few seconds the requests will fail with an \u0026#34;authentication failed\u0026#34; error:\n sh ~/run.sh 1000 $GATEWAY_URL   # 0: Origin authentication failed. # 1: Origin authentication failed. # 2: Origin authentication failed. # 3: Origin authentication failed. # 4: Origin authentication failed. # 5: Origin authentication failed.   In Kiali we see a 100% failure rate.\n  Figure 1. Kiali: failing because of authentication error.  To be able to connect to our application we first need to fetch a valid token and put this into the header while sending curl.\n export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) export TOKEN=$(curl https://raw.githubusercontent.com/istio/istio/release-1.1/security/tools/jwt/samples/demo.jwt -s) for x in $(seq 1 1000); do curl --header \u0026#34;Authorization: Bearer $TOKEN\u0026#34; $GATEWAY_URL -s; done   In Kiali the traffic is now working again and authenticated.\n  Figure 2. Kiali: Traffic authenticated.    Clean Up Remove the policy again, to be ready for the next tutorial.\n oc delete policy jwt-example -n tutorial    "},{"uri":"https://blog.stderr.at/tags/jwt/","title":"JWT","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/mtls/","title":"mTLS","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/mutual-tls-authentication/","title":"Mutual TLS Authentication","tags":["Istio","Service Mesh","OpenShift","OCP","mTLS"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 9 - Mutual TLS/mTLS Authentication","content":"When more and more microservices are involved in an application, more and more traffic is sent on the network. It should be considered to secure this traffic, to prevent the possibility to inject malicious packets. Mutual TLS/mTLS authentication or two-way authentication offers a way to encrypt service traffic with certificates.\n With Red Hat OpenShift Service Mesh, Mutual TLS can be used without the microservice knowing that it is happening. The TLS is managed completely by the Service Mesh Operator between two Envoy proxies using a defined mTLS policy.\n Issue 9 of OpenShift 4 and Service Mesh will explain how to enable Mutual TLS inside the Service Mesh to secure the traffic between the different microservices.\n How does it work?  If a microservice sends a request to a server, it must pass the local sidecar Envoy proxy first.\n  The proxy will intercept the outbound request and starts a mutual TLS handshake with the proxy at the server side. During this handshake the certificates are exchanged and loaded into the proxy containers by Service Mesh.\n  The client side Envoy starts a mutual TLS handshake with the server side Envoy.\n  The client proxy does a secure naming check on the server’s certificate to verify that the identity in the certificate is authorized.\n  A mutual TLS connection is established between the client and the server.\n  The Envoy proxy at the server sides decrypts the traffic and forwards it to the application through a local TCP connection.\n     Preparations  Before we can start be sure that the services are setup like in Issue #3. In addition, be sure that the following DestinationRule already exists:\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation subsets: - labels: version: v1 name: version-v1 - labels: version: v2 name: version-v2     Now we will create a pod, which is running outside of the Service Mesh. It will not have a sidecar proxy and will simply curl our application.\nStore the following yaml and create the object in our cluster.\n apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: app: curl version: v1 name: curl spec: replicas: 1 selector: matchLabels: app: curl version: v1 template: metadata: labels: app: curl version: v1 annotations: (1) sidecar.istio.io/proxyCPU: \u0026#34;500m\u0026#34; sidecar.istio.io/proxyMemory: 400Mi spec: containers: - image: quay.io/maistra_demos/curl:latest command: [\u0026#34;/bin/sleep\u0026#34;, \u0026#34;3650d\u0026#34;] imagePullPolicy: Always name: curl     1 since no sidecar is injected (sidecar.istio.io/inject: \u0026#34;true\u0026#34;), only 1 container will be started.       The traffic coming from the microservice customer AND from the external client curl must be simulated. To achieve this the following shell script can be used:\n #!/bin/sh export CURL_POD=$(oc get pods -n tutorial -l app=curl | grep curl | awk \u0026#39;{ print $1}\u0026#39; ) export CUSTOMER_POD=$(oc get pods -n tutorial -l app=customer | grep customer | awk \u0026#39;{ print $1}\u0026#39; ) echo \u0026#34;A load generating script is running in the next step. Ctrl+C to stop\u0026#34; while :; do echo \u0026#34;Executing curl in curl pod\u0026#34; oc exec -n tutorial $CURL_POD -- curl -s http://preference:8080 \u0026gt; /dev/null sleep 0.5 echo \u0026#34;Executing curl in customer pod\u0026#34; oc exec -n tutorial $CUSTOMER_POD -c customer -- curl -s http://preference:8080 \u0026gt; /dev/null sleep 0.5 done   By executing this, it will first execute a curl command out of the curl pod and then the same curl command out of the customer container. Kepp this script running\n   Enabling Mutual TLS Lets execute the shell script above and verify Kiali. As you notice there are requests coming from the customer microservice and from the source called unknown, which is the curl-service running outside the Service Mesh.\n  Figure 1. Kiali: traffic coming from customer microserver and external pod  Enable the policy by creating the following object:\n apiVersion: \u0026#34;authentication.istio.io/v1alpha1\u0026#34; kind: \u0026#34;Policy\u0026#34; metadata: name: \u0026#34;preference-mutualtls\u0026#34; spec: targets: - name: preference peers: - mtls: mode: STRICT (1)     1 We are enforcing mtls for the target preference    After a few seconds the curl pod cannot reach the application anymore:\n Executing curl in curl pod command terminated with exit code 56 Executing curl in customer pod Executing curl in curl pod command terminated with exit code 56 Executing curl in customer pod Executing curl in curl pod command terminated with exit code 5   This is expected, since the preference service allows traffic over mutual TLS only. This was enforced by the Policy object (STRICT mode). The customer service, which is running inside the Service Mesh receives the error \u0026#34;5053 Service Unavalable\u0026#34; since it tries to send traffic, but it does not know yet to use mTLS.\n In Kiali you will see the following:\n  Figure 2. Kiali: traffic is blocked      The curl pod is greyed out, since the traffic it tries to send, never reaches the preference service and is therefor not counted in the metric.     To make customer aware that mutual TLS shall be used, a DestinationRule must be configured:\n apiVersion: \u0026#34;networking.istio.io/v1alpha3\u0026#34; kind: \u0026#34;DestinationRule\u0026#34; metadata: name: \u0026#34;preference-destination-rule\u0026#34; spec: host: \u0026#34;preference\u0026#34; trafficPolicy: tls: mode: ISTIO_MUTUAL (1)     1 Let’s use mTLS    This defines that ISTIO_MUTUAL shall be used for the service preference. The customer service recognizes this and automatically enables mTLS. After a few minutes the traffic graph in Kiali will show \u0026#34;green\u0026#34; traffic from customer through preference to _recommendation:\n  Figure 3. Kiali: traffic for Service Mesh components is fine again.     Mutual TLS Migration As you can see in the previous section, the curl pod cannot reach the application inside the Service Mesh. This happens because prefernce is strictly enforcing encrypted traffic, but curl only sends plain text. Luckily, Istio provides a method to gradually monitor the traffic and migrate to mTLS. Instead of STRICT mode PERMISSIVE can be used. Enabling permissive mode, preference will accept both, encrypted and plain-text traffic.\n Replace the Policy object with the following configuration:\n apiVersion: \u0026#34;authentication.istio.io/v1alpha1\u0026#34; kind: \u0026#34;Policy\u0026#34; metadata: name: \u0026#34;preference-mutualtls\u0026#34; spec: targets: - name: preference peers: - mtls: mode: PERMISSIVE   oc replace -f Policy-permissive.yaml   Now let’s wait a few minutes and observe Kiali, which should end up with:\n  Figure 4. Kiali: Encrypted and Plain-Text traffic  As you can see with the lock icon, the traffic between cunstomer and preference is encrypted, while the traffic from unknown (which is our curl pod), is plain-text.\n     The errors you may see in Kiali happen due a known issue: https://issues.jboss.org/browse/MAISTRA-1000       Cleanup Clean up your environment:\n oc delete policy -n tutorial preference-mutualtls oc delete destinationrule -n tutorial preference-destination-rule    "},{"uri":"https://blog.stderr.at/tags/fault-injection/","title":"Fault Injection","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/fault-injection/","title":"Fault Injection","tags":["Istio","Service Mesh","OpenShift","OCP","Fault Injection"],"description":"OpenShift 4.x and Service Mesh/Istio Tutorial 8 - Faul Injection/Chaos Testing","content":"Tutorial 8 of OpenShift 4 and Service Mesh tries to cover Fault Injection by using Chaos testing method to verify if your application is running. This is done by adding the property HTTPFaultInjection to the VirtualService. The settings for this property can be for example: delay, to delay the access or abort, to completely abort the connection.\n \u0026#34;Adopting microservices often means more dependencies, and more services you might not control. It also means more requests on the network, increasing the possibility for errors. For these reasons, it’s important to test your services’ behavior when upstream dependencies fail.\u0026#34; [1]\n Preparation Before we start this tutorial, we need to clean up our cluster. This is especially important when you did the previous training Limit Egress/External Traffic.\n oc delete deployment recommendation-v3 oc scale deployment recommendation-v2 --replicas=1 oc delete serviceentry worldclockapi-egress-rule oc delete virtualservice worldclockapi-timeout   Verify that 2 pods for the recommendation services are running (with 2 containers)\n oc get pods -l app=recommendation -n tutorial NAME READY STATUS RESTARTS AGE recommendation-v1-69db8d6c48-h8brv 2/2 Running 0 4d20h recommendation-v2-6c5b86bbd8-jnk8b 2/2 Running 0 4d19h      Abort Connection with HTTP Error 503 For the first example, we will need to modify the VirtualService and the DestinationRule. The VirtualService must be extended with a http fault section, which will abort the traffic 50% of the time.\n  Create the VirtualService\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - fault: abort: httpStatus: 503 percent: 50 route: - destination: host: recommendation subset: app-recommendation   Apply the change\n oc replace -f VirtualService-abort.yaml       Existing VirtualService with the name recommendation will be overwritten.       Create the DestinationRule\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation subsets: - labels: app: recommendation name: app-recommendation   Apply the change\n oc replace -f destinationrule-faultinj.yaml       Existing Destination with the name recommendation will be overwritten.       Check the traffic and verify that 50% of the connections will end with a 503 error:\nexport INGRESS_GATEWAY=$(oc get route customer -n tutorial -o \u0026#39;jsonpath={.spec.host}\u0026#39;) sh ~/run.sh 1000 $GATEWAY_URL      Clean Up oc delete virtualservice recommendation       Test slow connection with Delay More interesting, in my opinion, to test is a slow connection. This can be tested by adding the fixedDelay property into the VirtualService. Like in the example below, we will use a VirtualService. This time delay instead of abort is used. The fixDelay defines a delay of 7 seconds for 50% of the traffic.\n apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - fault: delay: fixedDelay: 7.000s percent: 50 route: - destination: host: recommendation subset: app-recommendation   If you now send traffic into the application, you will see that some answers will have a delay of 7 seconds. Keep sending traffic in a loop.\n Even more visible it will be, when you goto \u0026#34;Distributed Tracing\u0026#34; at the Kiali UI, select the service recommendation and a small lookback of maybe 5min. You will find that some requests are very fast, while other will tage about 7 seconds.\n  Figure 1. Jaeger with delayed traffic.     Retry on errors If a microservice is answering with an error, Service Mesh/Istio will automatically try to reach another pod providing the service. These retries can be modified. In order to make everything visible, we will use Kiali to monitor the traffic.\n  \n  We start by sending traffic into the application. This should be split evenly between v1 and v2 of the recommendation microservice\nsh ~/run.sh 1000 $GATEWAY_URL   # 8329: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 11145 # 8330: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 9712 # 8331: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 11146 # 8332: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 9713 # 8333: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 11147   In Kiali this ia visible in the Graphs, using the settings: \u0026#34;Versioned app graph\u0026#34; and \u0026#34;Requests percentage\u0026#34;\n  Figure 2. Traffic is split by 50% between recommendation v1 nd v2    As second step we need to enable the nasty mode for the microservice v2. This will simulate an outage, respoding with error 503 all the time. This change must be done inside the container:\noc exec -it $(oc get pods|grep recommendation-v2|awk \u0026#39;{ print $1 }\u0026#39;|head -1) -c recommendation /bin/bash     Inside the container use the following command and exit the container again\ncurl localhost:8080/misbehave   Kiali will now show that v1 will get 100% of the traffic, while v2 is shown as red. When you select the red square of v2 and then move the mouse over the red cross for the failing application, you will see that the pd itself is ready, but that 100% of the traffic is currently failing.\n  Figure 3. Traffic for v2 is failing    revert the change and fix v2 service\noc exec -it $(oc get pods|grep recommendation-v2|awk \u0026#39;{ print $1 }\u0026#39;|head -1) -c recommendation /bin/bash   curl localhost:8080/behave   Verify in Kiali that everything is \u0026#34;green\u0026#34; again and that the traffic is split by 50% between v1 and v2.\n       Sources   [1]: Istio By Example - Fault Injection\n    "},{"uri":"https://blog.stderr.at/tags/do410/","title":"DO410","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/ansible/2020/04/do410-ansible-and-ansible-tower-training-notes/","title":"DO410 Ansible and Ansible Tower training notes","tags":["Ansible","Ansible Tower","DO410"],"description":"Notes taken during the DO410 online training course","content":"Notes taken during Red Hat course D410 Ansible and Ansible Tower.\n Ansible installation   make sure that libselinux-python is installed\n  Ansible 2.7 requires python 2.6 or 3.5\n   yum list installed python     windows modules implemented in powershell\n  ansible requires at least .net 4.0\n     Configuration files Ansible searches for ansible.cfg in the following order:\n   $ANSIBLE_CFG\n  ansible.cfg in the current directory\n  $HOME/ansible.cfg\n  /etc/ansible/ansible.cfg\n   whichever it finds first will be used.\n use\n ansible --version   to see which config file is currently used. you can view/dump/see what changed with\n ansible-config [list|dump|view]   Default modules List all available modules via\n ansible-doc -l   For getting help on a specific module use\n ansible-doc ping      Ad-hoc commmands To display ansible output on a single line per host for easier readablility use the -o option\n ansible all -m command -a /bin/hostname -o   Use the raw module for directly executing commands on remote systems that do not have python installed.\n ansible -m raw     Custom Facts Ansible uses custom facts from /etc/ansible/facts.d/. Facts can be stored in .ini style or you can place executable scripts in this directory. The script needs to output JSON. Custom facts are available via ansible_facts.ansible_local.\n   Magic variables available   hostvars: variables defined for this host\n  group_names: list of groups this host is a member of\n  groups: list of all groups and hosts in the inventory\n  inventory_hostname: host name of the current host as configured in the inventory\n     Matching hosts in the inventory Some examples on how to match hosts defined in the inventory\n   \u0026#39;*.lab.com\u0026#39;: match all hosts starting with lab.com\n  \u0026#39;lab,datacenter\u0026#39;: match all hosts either in lab or datacenter\n  \u0026#39;datacenter*\u0026#39;: match all host and host groups starting with datacenter\n  \u0026#39;lab,\u0026amp;datacenter\u0026#39;: match hosts in the lab and datacenter group\n  \u0026#39;datacenter,!test.lab.com\u0026#39;: match all hosts in datacenter, except test.lab.com\n     Dynamic inventory Example scripts for dynamic inventories can be found at https://github.com/ansible/ansible/tree/devel/contrib/inventory.\n You can use ansible-inventory to take a look a the current inventory as json. This also works for static inventories.\n Inventories can be combined. Just create a directory containing a static inventory and script to create a dynamic inventory, ansible will happily execute the scripts and merge everything together.\n   Debugging The following might be useful when debugging ansible roles and playbooks\n ansible-playbook play.yml --syntax-check ansible-playbook play.yml --step ansible-playbook play.yml --start-at-task=\u0026#34;start httpd service\u0026#34; ansible-playbook --check play.yml ansible-playbook --check --diff play.yml     Ansible Tower Notes on deploying and working with ansible tower.\n Installation System requirements:\n   at least 4GB of RAM\n  actual requirement depends on forks variable\n  recommendation is 100MB memory for each for + 2GB of memory for tower services\n  20GB of disk storage, at least 10GB in /var\n   Steps for installing:\n   download setup tar.gz from http://releases.ansible.com/ansible-tower/setup/\n  set passwords in inventory\n  run ./setup.sh\n    Authentication Authentication settings can be changed under Settings / Authentication. E.g for configuring Azure AD authentication we are going to need\n   an Azure AD oauth2 key and\n  a Azure AD oauth2 secret\n    RBAC   separate roles for organizations and inventories\n  you need to assign roles to organizations and inventories\n    The Tower Flow These are the steps to run playbooks against managed nodes in Tower:\n   Create an organization if required\n  Create users\n  Create teams and assign users\n  Create credentials for accessing managed nodes\n  Assign credential to organization\n  Create credentials for accessing SCM repositories (e.g. git)\n  Assign credentials to users or teams\n  Create a project\n  Assign Teams to project\n  Create a job template for executing playbooks\n    Ansible Roles support If the project includes a requirements.txt file in the roles/ folder, tower will automatically run\n ansible-galaxy install -r roles/requirements.yml -p ./roles/ --force   at the end of an update. So this could be used to include external dependencies (like SAP ansible roles).\n  Job Templates Ansible playbooks are stored in GIT repositories. A job template defines\n   the inventory used for this job template\n  the project for executing this job\n  this connects the GIT repository used in this project with the template\n  the playbook to execute\n  the credentials for executing jobs\n  permissions for users / teams (e.g. admin, execute)\n   Tower creates jobs from those templates, which are ansible runs executed against managed nodes.\n  Fact Caching It might be a good idea to use the tower facts cache. To speed up playbook runs set gather_facts: no in the play. Then enable the facts cache in tower.\n   In tower settings set a timeout for the cache\n  In job templates enable Use facts cache\n  Create a playbook that runs on a regular basis to gather facts, e.g.\n   - name: Refresh fact cache hosts: all gather_facts: yes    Inventory options These are the options for creating inventories in Ansible Tower\n   static inventory defined in tower\n  importing static inventories via awx-manage\n  static inventory defined in git repository\n  dynamic inventory via a custom script\n  dynamic inventory provides by tower (e.g. satellite)\n   A special feature in Tower are so called smart inventories. A smart inventory combines all static and dynamic inventories and allows filtering based on facts. Filtering requires a valid fact cache.\n  Troubleshooting Tower uses the following components:\n   postgresql\n  nginx\n  memcached\n  rabbitmq\n  supervisord\n   Useful tools\n   ansible-tower-service (e.g. status / restart)\n  supervisorctl (e.g. status)\n  awx-manage\n   Tower stores log files in\n   /var/log/tower/ (e.g. tower.log).\n  /var/log/supervisor/\n  /var/log/nginx/\n   Other important directories\n   /var/lib/awx/public/static static files served by django\n  /var/lib/awx/projects stores all project related files e.g. git checkouts)\n  /var/lib/awx/jobs_status job status output\n       by default playbook runs are confined to /tmp this might lead to problems with tasks running on the local system.     In case of a lost admin password you can use awx-manage to reset the password or create a new superuser:\n awx-manage changepassword admin awx-manage createsuperuser    Replacing the default TLS certificates Ansible tower uses nginx to service it’s web interface over TLS. Nginx uses the configuration file /etc/nginx/nginx.conf.\n To deploy custom TLS certificates used by tower replace the certificate and private key in /etc/tower. You have to replace\n   /etc/tower/tower.crt and\n  /etc/tower/tower.key\n   It might be a good idea to create a backup copy before overwriting those files.\n  Backup and restore Of course backup and restore are done via ansible. The ansible tower setup script setup.sh provides a wrapper around these playbooks. Execute\n setup.sh -b   to perform a backup. This creates a backup .tar.gz file in the current directory.\n To restore a backup use\n setup.sh -r   this restores the latest backup per default.\n    Things to remember   Workflow job templates\n  add autocmd FileType yaml setlocal ai ts=2 sw=2 et to .vimrc\n  use sudo yum install python-cryptography if there are many vault files to speed up ansible\n    "},{"uri":"https://blog.stderr.at/tags/egress/","title":"Egress","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/limit-egress/external-traffic/","title":"Limit Egress/External Traffic","tags":["Istio","Service Mesh","OpenShift","OCP","Egress"],"description":"OpenShift 4.x and Service Mesh/istio Tutorial 7 - Test and control your egress/external traffic.","content":"Sometimes services are only available from outside the OpenShift cluster (like external API) which must be reached. Part 7 of OpenShift 4 and Service Mesh takes care and explains how to control the egress or external traffic. All operations have been successdully tested on OpenShift 4.3.\n Preparation Before this tutorial can be started, ensure that 3 microservices are deployed (recommendation may have 2 versions) and that the objects Gateway and VirtualService are configured. The status should be like in Issue #4..6\n You can verify this the following way:\n export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) curl $GATEWAY_URL   which should simply print:\n customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 7123     Setup recommendation-v3 We need to deploy version 3 of our recommendation microservice. This will perform an external API call to http://worldclockapi.com to retrieve the current time.\n To deploy the Deployment v3:\n cd ~/istio-tutorial/recommendation oc apply -f kubernetes/Deployment-v3.yml -n tutorial       If you list the pods at this moment, you will see that only one container (Ready 1/1) is started. This happens because the Deployment yaml file is missing an annotation.     Fixing missing proxy sidecar container After you applied the Deployment-v3.yml, only 1 container is started. The proxy sidecar is not injected, because an annotation is missing in the configuration for the Deployment.\n To fix this use the following command:\n oc patch deployment recommendation-v3 -n tutorial -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;sidecar.istio.io/inject\u0026#34;:\u0026#34;true\u0026#34;}}}}}\u0026#39;   This will automatically restart the pod with 2 containers.\n    Create DestinationRule and VirtualService Use the following definition to create (overwrite) the DestinationRule for recommendation-v3.\n apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation subsets: - labels: version: v3 name: version-v3       Only version 3 is used for now. The other versions are still there, but ignored for our tests.     Apply the change\n oc apply -f DestinationRule_v3.yaml   Define the VirtualService and send 100% of the traffic to v3 of the recommendation microservice.\n apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - route: - destination: host: recommendation subset: version-v3 weight: 100       As an alternative, you can also edit the existing VirtualService and add the section for version-v3 with a weight of 100, while changing the weight of v1 and v2 to 0.       Test egress traffic As usual we test our application by sending traffic to it. The following command should print successful connection requests:\n sh ~/run.sh 1000 $GATEWAY_URL   # 0: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 1 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 2 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 3 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 4 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-06T18:31+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 5   As you can see 100% of the traffic is sent to v3 AND a new field enters the output. The current time is now shown as well. The information for this field is fetched with an external API call to http://worldclockapi.com.\n     The traffic is simply sent to an external destination. There is not limit yet. Readers of the Istio documentation will miss the object ServiceEntry which somebody should think is required. However, Openshift is currently(?) configured in a way to simply allow ANY traffic. This is defined in a ConfigMap which might be changed to modify the default behavior. However, as soon as ServiceEntry and the appropriate VirtualService is configured, the traffic will be limited as well.       Limit/Control external access As you can see above you can simply send egress traffic without any control about what is allowed or not. In order to limit your outgoing traffic a new object called ServiceEntry must be defined as well as a change in your VirtualService will be required.\n Define the ServiceEntry and apply it to your cluster:\n apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: worldclockapi-egress-rule spec: hosts: - worldclockapi.com ports: - name: http-80 number: 81 (1) protocol: http     1 Wrong port 81 is set on purpose for demonstration        The port number: 81 is set on purpose, to prove that the traffic will not work with a wrong ServiceEntry.     oc create -f ServiceEntry.yaml   To actually limit the traffic a link between the ServiceEntry and a VirtualService, which defines the external destination, must be created. Moreover, a timeout is set for possible connection errors, to keep the application responding even when the external API is down.\n apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: worldclockapi-timeout (1) spec: hosts: - worldclockapi.com (2) http: - timeout: 3s (3) route: - destination: host: worldclockapi.com weight: 100 (4)     1 The name of the object   2 The external hostname we want to reach   3 The timeout setting in seconds   4 The destination route, which is sending 100% of the external traffic to the host above    oc apply -f VirtualService-worldclockapi.yaml   If you now run a connection test you will still get an error.\n sh ~/run.sh 1 $GATEWAY_URL # customer =\u0026gt; Error: 503 - preference =\u0026gt; Error: 500 ...   Fix ServiceEntry This happens, because we misconfigured the ServiceEntry on purpose to demonstrate that the traffic is sent to worldclockapi.com:80.\n Fix the ServiceEntry object and apply to your cluster:\n apiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: worldclockapi-egress-rule spec: hosts: - worldclockapi.com ports: - name: http-80 number: 80 (1) protocol: http     1 Changed from 81 to 80    oc apply -f ServiceEntry.yaml   Now the traffic should work and gives you back a connection to microservice and a current time:\n sh ~/run.sh 10 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 138 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 139 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 140 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 141 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 142 # 5: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 143 # 6: customer =\u0026gt; preference =\u0026gt; recommendation v3 2020-04-07T07:47+02:00 from \u0026#39;83bbb6d11a7e\u0026#39;: 144      Verify Kiali  Figure 1. Kiali shows traffic to the external service     OPTIONAL: Disallow ANY connections     This is a change in the default ConfigMap of the ServiceMesh. Do this on your own risk and always consult the latest documentation of OCP.     As explained above, we are able to connect to an external service without any limitation. The ServiceEntry object together with the VirtualService define the actual destination and would disallow traffic if they are wrongly configured, but if you forget these entries, it would still be possible to establish an egress connection.\n In OpenShift a ConfigMap in the istio-system namespace defines the default behavior. There are two possibilities:\n   ALLOW_ANY - outbound traffic to unknown destinations will be allowed, in case there are no services or ServiceEntries for the destination port\n  REGISTRY_ONLY - restrict outbound traffic to services defined in the service registry as well\n Let’s Cleanup the ServiceEntry and the VirtualService which have been created above\noc delete serviceentry worldclockapi-egress-rule serviceentry.networking.istio.io \u0026#34;worldclockapi-egress-rule\u0026#34; deleted oc delete virtualservice worldclockapi-timeout virtualservice.networking.istio.io \u0026#34;worldclockapi-timeout\u0026#34; deleted       Now traffic to the external service will be allowed again       Modify the ConfigMap istio in the namespace istio-system\noc get configmap istio -n istio-system -o yaml | sed \u0026#39;s/mode: ALLOW_ANY/mode: REGISTRY_ONLY/g\u0026#39; | oc replace -n istio-system -f -         Wait a few seconds and try to connect. You will see that the connection is not possible anymore.\n     If you now re-create the ServiceEntry the connection will be possible again, since the service is registered to the Service Mesh.      "},{"uri":"https://blog.stderr.at/service-mesh/2020/04/advanced-routing-example/","title":"Advanced Routing Example","tags":["Istio","Service Mesh","OpenShift","OCP","Grayscale","Canary","DestinationRule","Mirror","Loadbalancer"],"description":"OpenShift 4.x and Service Mesh/istio Tutorial 6 - Advanced Routing. Try routing traffic based on canary, mirroring or loadbalancing traffic.","content":"Welcome to part 6 of OpenShift 4 and Service Mesh Advanced routing, like Canary Deployments, traffic mirroring and loadbalancing are discussed and tested. All operations have been successdully tested on OpenShift 4.3.\n Advanced Routing During Issue #5 some simple routing was implemented. The traffic was split by 100% to a new version (v2) of the recommendation microservice. This section shall give a brief overview of advanced routing possibilities.\n Canary Deployments A canary deployment is a strategy to roll out a new version of your service by using traffic splitting. A small amount of traffic (10%) will be sent to the new version, while most of the traffic will be sent to the old version still. The traffic to the new version can be analysed and if everything works as expected more and more traffic can be sent to the new version.\n To enable split traffic, the VirtualService must be update:\n apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - route: - destination: host: recommendation subset: version-v1 weight: 90 - destination: host: recommendation subset: version-v2 weight: 10   Apply the change\n oc apply -f VitualService_split_v1_and_v1.yaml   Test the traffic and verify that 10% will be sent to v2\n sh ~/run.sh 100 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1060 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1061 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 2060 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1062 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 1063 ...       If an error is shown, then you most probably forget to configure the DestinationRule as described here.      Figure 1. Kiali split traffic 90/10    Routing based on user-agent header It is possible to send traffic to different versions based on the browser type which is calling the application. In our test application the service customer is setting the header baggage-user-agent and propagates it to the other services.\n     \u0026gt;\u0026gt; headers.putSingle(\u0026#34;baggage-user-agent\u0026#34;, userAgent);     Create the following file\n apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - match: - headers: baggage-user-agent: regex: .*Safari.* route: - destination: host: recommendation subset: version-v2 - route: - destination: host: recommendation subset: version-v1   and apply the change\n oc apply -f VitualService_safari.yaml   In order to test the result, either use the appropriate browser or use curl to set the user-agent. As expected, request from Safari are sent to v2, other are sent to v1.\n Safari\n curl -v -A Safari $GATEWAY_URL [...] \u0026gt; User-Agent: Safari [...] customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 2365   Firefox\n curl -v -A Firefox $GATEWAY_URL [...] \u0026gt; User-Agent: Firefox [...] customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3762     Mirroring Traffic Mirroring Traffic, aka Dark Launch, will duplicate the traffic to another service, allowing you to analyse it before sending production data to it. Responses of the mirrored requests are ignored.\n Run the following command and be sure that recommendation-v1 and recommendation-v2 are both running:\n oc get pod -n tutorial| grep recommendation recommendation-v1-69db8d6c48-h8brv 2/2 Running 0 24h recommendation-v2-6c5b86bbd8-jnk8b 2/2 Running 0 23h   Update the VirtualService, so that version v2 will receive mirrored traffic, while the actual request will be sent to v1:\n apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - route: - destination: host: recommendation subset: version-v1 mirror: (1) host: recommendation subset: version-v2     1 This must be set to \u0026#39;mirror\u0026#39;    Apply the change\n oc apply -f VitualService_mirrored-traffic.yaml   Now lets open and follow the logs of recommandation-v2 in order to see that traffic will reach this service, but responses are ignored:\n oc logs -f $(oc get pods|grep recommendation-v2|awk \u0026#39;{ print $1 }\u0026#39;) -c recommendation   In a second terminal window send some traffic to our service.\n sh ~/run.sh 100 $GATEWAY_URL   You will see that only v1 answers, while in the 2nd window, v2 gets the same traffic.\n  Load Balancing In the default OpenShift environment the kube-proxy forwards all requests to pods randomly. With Red Hat ServiceMesh it is possible to add more complexity and let the Envoy proxy handle load balancing for your services.\n Three methods are supported:\n   random\n  round-robin\n  least connection\n   The round robin function is used by default, when there is no DestinationRule configured. We can use the DestinationRule to use the least connection option to see how the traffic is sent.\n Before we start we need to delete the VirtualService for the recommendation microservice\n oc delete virtualservice recommendation   The we scale version v2 to 3:\n oc scale deployment recommendation-v2 --replicas=3   After a few seconds the folling pods should run now:\n NAME READY STATUS RESTARTS AGE customer-6948b8b959-jdjlg 2/2 Running 1 25h preference-v1-7fdb89c86b-nktqn 2/2 Running 0 25h recommendation-v1-69db8d6c48-h8brv 2/2 Running 0 25h recommendation-v2-6c5b86bbd8-6lgz6 2/2 Running 0 91s recommendation-v2-6c5b86bbd8-dnc8b 2/2 Running 0 91s recommendation-v2-6c5b86bbd8-jnk8b 2/2 Running 0 24h   If you send traffic to the application, you would see that 3 quarter are sent to v1 and one is sent to v1.\n With the following DestinationRule the traffic will be sent randomly to the application\n apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation trafficPolicy: loadBalancer: simple: RANDOM   If you now sent traffic to the service, you will see that the traffic is sent randomly to the versions. (verify the serial number)\n sh ~/run.sh 100 $GATEWAY_URL # 140: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 5729 # 141: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 7119 # 142: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 361 # 143: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 362 # 144: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 5730 # 145: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 362 # 146: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 7120 # 147: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 7121 # 148: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 363 ...     "},{"uri":"https://blog.stderr.at/tags/canary/","title":"Canary","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/destinationrule/","title":"DestinationRule","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/grayscale/","title":"Grayscale","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/loadbalancer/","title":"Loadbalancer","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/mirror/","title":"Mirror","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/openshift/2020-04-01-oc-commands/","title":"Helpful oc / kubectl commands","tags":["oc","kubectl","OpenShift","OCP"],"description":"Openshift 4.x - Collection of usefull oc/kubectl commands.","content":"This is a list of useful oc and/or kubectl commands so they won’t be forgotton. No this is not a joke…​\n List all pods in state Running oc get pods --field-selector=status.phase=Running   List all pods in state Running and show there resource usage oc get pods --field-selector=status.phase=Running -o json|jq \u0026#34;.items[] | {name: .metada ta.name, res: .spec.containers[].resources}\u0026#34;      List events sort by time created oc get events --sort-by=\u0026#39;.lastTimestamp\u0026#39;     Explain objects while specifing the api version Sometimes when you run oc explain you get a message in DESCRIPTION that this particular version is deprecated, e.g. you are running oc explain deployment and get\n DESCRIPTION: DEPRECATED - This group version of Deployment is deprecated by apps/v1/Deployment. See the release notes for more information. Deployment enables declarative updates for Pods and ReplicaSets.       Note the DEPRECTATED message above     if you want to see the documentation for the object that has not been deprecated you can use\n oc explain deployment --api-version=apps/v1     Magic with oc set oc set is actually a very versatile command. Studying oc set -h is a good idea, here are some examples\n Set route weights when alternateBackends in a route are defined oc set route-backends bluegreen blue=1 green=9    Set resources on the command line oc set resources dc cakephp-mysql-example --limits=memory=1Gi,cpu=200m     "},{"uri":"https://blog.stderr.at/tags/kubectl/","title":"kubectl","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/tags/oc/","title":"oc","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/service-mesh/2020/04/routing-example/","title":"Routing Example","tags":["Istio","Service Mesh","OpenShift","OCP","Grayscale","Canary","DestinationRule"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 5 - Routing Examples. Learn about routing and create your first definitions for VirtualService and DesitnationRule","content":"In part 5 of the OpenShift 4 and Service Mesh tutorials, basic routing, using the objects VirtualService and DesitnationRule, are described. All operations have been successfully tested on OpenShift 4.3.\n Some Theory In this section another version of the recommendation microservice will be deployed. The traffic to the new version will be controlled with different settings of the VirtualService. Multiple scenarios can be realized with ServiceMesh. In general these are defined as follows ([1]):\n Blue-Green Deployments In a Blue-Green deployment the old version (green) is kept running, while a new version (blue) is deployed and tested. When testing is successful, the 100% of the traffic is switched to the new version. If there is any error, the traffic could be switched back to the green version.\n  A/B Deployments A/B deployments, in difference to Blue-Green deployments, will enable you to try a new version of the application in a limited way in the production environment. It is possible to specify that the production version gets most of the user requests, while a limited number of requests is sent to the new version. This could be specified by location of the user for example, so that all users from Vienna are sent to the new version, while all others are still using the old version.\n  Canary Deployments Canary releases can be used to allow a small, minimum amount of traffic to the new version of your application. This traffic can be increased gradually until all traffic is sent to the new version. If any issues are found, you can roll back and send the traffic to the old version.\n    Prerequisites It is assumed that an OpenShift environment is up and running and that Issues #1 - #3 are done at least:\n   Openshift 4 and ServiceMesh 1 - Installation\n  Openshift 4 and ServiceMesh 2 - Deploy Microservices\n  Openshift 4 and ServiceMesh 3 - Ingress Traffic\n     Prepare Simple Routing OPTIONAL: Build the recommendation microservice If you want to locally build the microservice, you must change the source code from version v1 to v2 the following way:\n Open the file:\n istio-tutorial/recommendation/java/vertx/src/main/java/com/redhat/developer/demos/recommendation/RecommendationVerticle.java   and change the following line from v1 to v2\n private static final String RESPONSE_STRING_FORMAT = \u0026#34;recommendation v2 from \u0026#39;%s\u0026#39;: %d\\n\u0026#34;;   Now you can build the image:\n cd istio-tutorial/recommendation/java/vertx mvn package podman build -t example/recommendation:v2 . (1)     1 Note the v2 tag     Create second deployment with version2 A deployment with our recommendation:v2 microservice must be created. A service object must not be created this time, as it already exists.\n cd ~/istio-tutorial/recommendation/ oc apply -f kubernetes/Deployment-v2.yml -n tutorial oc get pods -w   If you want to diff v1 and v2 deployment, you will notice that the main change is the image which gets pulled.\n diff recommendation/kubernetes/Deployment-v2.yml recommendation/kubernetes/Deployment.yml 6,7c6,7 \u0026lt; version: v2 \u0026lt; name: recommendation-v2 --- \u0026gt; version: v1 \u0026gt; name: recommendation-v1 13c13 \u0026lt; version: v2 --- \u0026gt; version: v1 18c18 \u0026lt; version: v2 --- \u0026gt; version: v1 27c27 \u0026lt; image: quay.io/rhdevelopers/istio-tutorial-recommendation:v2.1 --- \u0026gt; image: quay.io/rhdevelopers/istio-tutorial-recommendation:v1.1    Call application Execute the test command to access the application. Since no rules are defined yet, the traffic is split by 50% to version 1 and version 2 (round robin):\n sh ~/run.sh 10 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 27 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 27 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 28 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 28 ...   In Kiali presents this as well:\n  Figure 1. Kiali sends 50% to v1 and v2     Send all traffic to recommendation:v2 To route the traffic accordingly a DestinationRule and a VirtualService must be created for recommendation. While the DesinationRule will add a name to each version, VirtualService specifies the actual destination of the traffic.\n Define DestinationRule for recommendation The object DestinationRule will define the versions in subsets.\n apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: recommendation spec: host: recommendation subsets: - labels: version: v1 name: version-v1 - labels: version: v2 name: version-v2   Create the object with the command: oc create -f \u0026lt;filename\u0026gt;\n  Define VirtualService for recommendation The VirtualService defines that 100% (weight) of the traffic for recomendation (host) will be sent to the subset (version-v2), which is defined in the DefinationRule\n apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: recommendation spec: hosts: - recommendation http: - route: - destination: host: recommendation subset: version-v2 weight: 100   Create the object with the command: oc create -f \u0026lt;filename\u0026gt;\n  Call application If you now call the application, only traffic to v2 should be shown:\n sh ~/run.sh 1000 $GATEWAY_URL # 0: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 27 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 27 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v2 from \u0026#39;3cbba7a9cde5\u0026#39;: 28 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 28 ...   In Kiali presents this as well and send 100% of the traffic to recommendation:v2:\n  Figure 2. Kiali sends 100% to v2     Sources   [1]: DZone: Traffic Management With Istio\n    "},{"uri":"https://blog.stderr.at/service-mesh/2020/03/ingress-with-custom-domain/","title":"Ingress with custom domain","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 4 - Use a custom domain to get your traffic into your Service Mesh","content":"    Since Service Mesh 1.1, there is a better way to achieve the following. Especially the manual creation of the route is not required anymore. Check the following article to Enable Automatic Route Creation.     Often the question is how to get traffic into the Service Mesh when using a custom domains. Part 4 our our tutorials series OpenShift 4 and Service Mesh will use a dummy domain \u0026#34;hello-world.com\u0026#34; and explains the required settings which must be done.\n Modify Gateway and VirtualService Issue #3 explains how to get ingress traffic into the Service Mesh, by defining the Gateway and the VirtualService. We are currently using the default ingress route defined in the istio_system project. But what if a custom domain shall be used? In such case another route must be defined in the istio-system project and small configuration changes must be applied.\n  First lets create a slightly modified Gateway.yaml:\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: ingress-gateway-exampleapp spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;hello-world.com\u0026#34; (1)     1 add you custom domain here    The only difference is at the hosts which was changed from \u0026#39;*\u0026#39; to \u0026#39;hello-world.com\u0026#39;\n   As second change, the VirtualService must be modified as well with the custom domain:\nVirtualService.yaml:\n apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: ingress-gateway-exampleapp spec: hosts: - \u0026#34;hello-world.com\u0026#34; (1) gateways: - ingress-gateway-exampleapp http: - match: - uri: exact: / route: - destination: host: customer port: number: 8080     1 add you custom domain here      Replace current objects in OpenShift:\noc replace -f Gateway.yaml -n tutorial oc replace -f VirtualService.yaml -n tutorial     Create a new route under the project istio-system:\napiVersion: route.openshift.io/v1 kind: Route metadata: name: hello-world.com (1) namespace: istio-system (2) spec: host: hello-world.com (3) to: kind: Service name: istio-ingressgateway (4) port: targetPort: 8080     1 add you custom domain here   2 the route must be created at istio-system   3 add you custom domain here   4 this is the service as it was created by the operator         OPTIONAL: Add custom domain to local hosts file The custom domain hello-world.com must be resolvable somehow, pointing to the ingress router of OpenShift. This can be done, by adding the domain into the local hosts file (with all limitations this brings with it)\n # Get IP address of: oc -n istio-system get route istio-ingressgateway echo \u0026#34;x.x.x.x hello-world.com\u0026#34; \u0026gt;\u0026gt; /etc/hosts     Create some example traffic We will reuse the script of Issue #3 to simulate traffic. Since we changed the domain, the connection will go to hello-world.com\n sh run-check.sh 1000 hello-world.com   This will send 1000 requests to our application:\n # 0: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6626 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6627 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6628 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6629 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6630 # 5: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 6631 ...    "},{"uri":"https://blog.stderr.at/service-mesh/2020/03/ingress-traffic/","title":"Ingress Traffic","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 3 - Get traffic into your Service Mesh and use Kiali to make your traffic visible.","content":"Part 3 of tutorial series OpenShift 4 and Service Mesh will show you how to create a Gateway and a VirtualService, so external traffic actually reaches your Mesh. It also provides an example script to run some curl in a loop.\n Configure Gateway and VirtualService Example With the microservices deployed during Issue #2, it makes sense to test the access somehow. In order to bring traffic into the application a Gateway object and a VirtualService object must be created.\n The Gateway will be the entry point which forward the traffic to the istio ingressgateway\n apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: ingress-gateway-exampleapp spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34;   As 2nd object a VirtualService must be created:\n apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: ingress-gateway-exampleapp spec: hosts: - \u0026#34;*\u0026#34; gateways: - ingress-gateway-exampleapp http: - match: - uri: exact: / route: - destination: host: customer port: number: 8080   Get all istio-io related objects of your project. These objects represent the network objects of Service Mesh, like Gateway, VirtualService and DestinationRule (explained later)\n oc get istio-io -n tutorial NAME HOST AGE destinationrule.networking.istio.io/recommendation recommendation 3d21h NAME AGE gateway.networking.istio.io/ingress-gateway 4d15h NAME GATEWAYS HOSTS AGE virtualservice.networking.istio.io/ingress-gateway [ingress-gateway] [*] 4d15h     Create some example traffic Before we start, lets fetch the default route of our Service Mesh:\n export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath=\u0026#39;{.spec.host}\u0026#39;)   This should return: istio-ingressgateway-istio-system.apps.\u0026lt;clustername\u0026gt;\n Now, let’s create a shell script to run some curl commands in a loop and can be easily reused for other scenarios:\n #!/bin/bash numberOfRequests=$1 host2check=$2 if [ $# -eq 0 ]; then echo \u0026#34;better define: \u0026lt;script\u0026gt; #ofrequests hostname2check\u0026#34; echo \u0026#34;Example: run.sh 100 hello.com\u0026#34; let \u0026#34;numberOfRequests=100\u0026#34; else let \u0026#34;i = 0\u0026#34; while [ $i -lt $numberOfRequests ]; do echo -n \u0026#34;# $i: \u0026#34;; curl $2 let \u0026#34;i=$((i + 1))\u0026#34; done fi   Run the script and check the output:\n sh run-check.sh 1000 $GATEWAY_URL   This will send 1000 requests to our application:\n # 0: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3622 # 1: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3623 # 2: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3624 # 3: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3625 # 4: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3626 # 5: customer =\u0026gt; preference =\u0026gt; recommendation v1 from \u0026#39;f11b097f1dd0\u0026#39;: 3627 ...     Verify in Kiali To verify in Kiali our application, open the URL in your browser and login using your OpenShift credentials.\n     If you do not know the URL for Kiali, execute the following command oc get route kiali -n istio-system     Switch the the Graph view and you should see the following picture:\n  Figure 1. Kiali Graph   "},{"uri":"https://blog.stderr.at/service-mesh/2020/03/deploy-microservices/","title":"Deploy Microservices","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 2 - Deploy some example microservices, which are used in future tutorials.","content":"The second tutorials explains how to install an example application containing thee microservices. All operations have been successfully tested on OpenShift 4.3.\n Introduction As quickly explained at Issue #1, OpenShift 4.3 and the Service Mesh shall be installed already. At this point you should have all 4 operators installed and ready. The test application used in this scenario is based on Java and contains 3 microservices in the following traffic flow:\n Customer ⇒ Preference ⇒ Recomandation\n The microservices are the same as used at the Interactive Learning Portal.\n   Deploy microservices  First lets create a new project for our tests\noc new-project tutorial     EITHER: Add tutorial to ServiceMeshMemberRoll\n       Service Mesh 1.1 now supports the object ServiceMember which can created under the application namespace and which automatically configures the ServiceMemberRoll. However, below description still works.     + OpenShift will auto-inject the sidecar proxy to pods of a namespace, if the namespace is configured in the ServiceMeshMemberRoll object which was created for the operator.\n  Create the file memberroll.yaml:\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; memberroll.yaml apiVersion: maistra.io/v1 kind: ServiceMeshMemberRoll metadata: name: default spec: members: - tutorial EOF     Add the namespace to the member roll:\noc apply -f memberroll.yaml -n istio-system          If you already have namespaces configured for ServiceMeshMemberRoll, better modify the object manually. Custom Resource Definitions (CRD) do not like to be modified on the fly (currently?)      OR: Create ServiceMeshMember object: Available since ServiceMesh 1.1\napiVersion: maistra.io/v1 kind: ServiceMeshMember metadata: name: default namespace: tutorial spec: controlPlaneRef: name: basic-install namespace: istio-system     Download the tutorial application locally\ngit clone https://github.com/redhat-developer-demos/istio-tutorial/      Deploy microservice: customer To deploy the first microservice 2 objects (Deployment and Service) must be created. Moreover, the service will be exposed, since the service customer is our entry point. Verify ~/istio-tutorial/customer/kubernetes/Deployment.yml to check where the actual image is coming from: quay.io/rhdevelopers/istio-tutorial-customer:v1.1\n cd ~/istio-tutorial/customer oc apply -f kubernetes/Deployment.yml -n tutorial oc apply -f kubernetes/Service.yml -n tutorial oc expose service customer   Check if pods are running with two containers:\n oc get pods -w   The result should look somehow like this:\n NAME READY STATUS RESTARTS AGE customer-6948b8b959-g77bs 2/2 Running 0 52m       If there is only 1 container running, indicated by READY = 1/1, then most likely the ServiceMeshMemberRoll was not updated with the name tutorial or contains a wrong project name.      Deploy microservice: preference The deployment of the microservice preference is exactly like it is done for customer, except that no service must be exposed:\n cd ~/istio-tutorial/preference/ oc apply -f kubernetes/Deployment.yml -n tutorial oc apply -f kubernetes/Service.yml -n tutorial   Check if pods are running with two containers:\n oc get pods -w   The result should look somehow like this:\n NAME READY STATUS RESTARTS AGE customer-6948b8b959-g77bs 2/2 Running 0 4d15h preference-v1-7fdb89c86b-gkk5g 2/2 Running 0 4d14h    Deploy microservice: recommendation The deployment of the microservice recommendation is exactly like it is done for preference:\n cd ~/istio-tutorial/recommendation/ oc apply -f kubernetes/Deployment.yml -n tutorial oc apply -f kubernetes/Service.yml -n tutorial   Check if pods are running with two containers:\n oc get pods -w   The result should look somehow like this:\n NAME READY STATUS RESTARTS AGE customer-6948b8b959-g77bs 2/2 Running 0 4d15h preference-v1-7fdb89c86b-gkk5g 2/2 Running 0 4d14h recommendation-v1-69db8d6c48-p9w2b 2/2 Running 0 4d14h      Optional: build the images It is possible (and probably a good training) to build the microservices locally to understand how this works. In order to achieve this the packages maven and podman must be installed.\n Build customer Go to the source folder of customer application and build it:\n cd ~/projects/istio-tutorial/customer/java/springboot mvn package   It will take a few seconds, but it should give \u0026#34;BUILD SUCCESS\u0026#34; as output, if everything worked.\n Now the image will be built using podman\n podman build -t example/customer .    Build preference The image build process of the second microservice follows the same flow as customer:\n cd ~/istio-tutorial/preference/java/springboot mvn package podman build -t example/preference:v1 .       the \u0026#34;v1\u0026#34; tag at the image name is important and must be used.      Build recommendation The image build process of the third microservice follows the same flow as preference:\n cd ~/projects/istio-tutorial/recommendation/java/vertx mvn package podman build -t example/recommendation:v1 .       the \u0026#34;v1\u0026#34; tag at the image name is important and must be used. Later other versions will be deployed.       "},{"uri":"https://blog.stderr.at/service-mesh/2020/03/installation/","title":"Installation","tags":["Istio","Service Mesh","OpenShift","OCP"],"description":"Openshift 4.x and Service Mesh/Istio Tutorial 1 - Installation and First Steps","content":"Everything has a start, this blog as well as the following tutorials. This series of tutorials shall provide a brief and working overview about OpenShift Service Mesh. It is starting with the installation and the first steps, and will continue with advanced settings and configuration options.\n OpenShift 4.x and ServiceMesh     UPDATE: At 10th April 2020 Red Hat released Service Mesh version 1.1 which supports:       Istio - 1.4.6\n  Kiali - 1.12.7\n  Jaeger - 1.17.1\n   The following tutorials for OpenShift Service Mesh are based on the official documentation: OpenShift 4.3 Service Mesh and on the Interactive Learning Portal. All operations have been successfully tested on OpenShift 4.3. Currently OpenShift supports Istio 1.4.6, which shall be updated in one of the future releases.\n To learn the basics of Service Mesh, please consult the documentation as they are not repeated here.\n It is assumed that OpenShift has access to external registries, like quay.io.\n Other resource I can recommend are:\n   Istio By Example: A very good and brief overview of different topics by Megan O’Keefe.\n  Istio: The Istio documentation\n     Prerequisites At the very beginning OpenShift must be installed. This tutorial is based on OpenShift 4.3 and a Lab installation on Hetzner was used. Moreover, it is assumed that the OpenShift Client and Git are installed on the local system.\n During the tutorials an example application in the namespace tutorial will be deployed. This application will contain 3 microservices:\n   customer (the entry point)\n  preference\n  recommendation\n   This application is also used at the Interactive Learning Portal which can be tested there interactively. However, the training is still based on OpenShift version 3.\n   Install Red Hat Service Mesh To deploy Service Mesh on Openshift 4 follow the guide at Installing Red Hat OpenShift Service Mesh.\n In short, multiple operators must be installed:\n   Elasticsearch\n  Jaeger\n  Kiali\n  Service Mesh\n       Elasticsearch is a very memory intensive application. Per default it will request 16GB of memory which can be reduced on Lab environments.       Link Jaeger and Grafana to Kiali In the lab environment it happened that Kiali was not able to auto detect Grafana or Jaeger. This is visible when the link Distributed Tracing is missing in the left menu.\n To fix this the ServiceMeshControlPlane object in the istio-system namespace must be updated with 3 lines:\n oc edit ServiceMeshControlPlane -n istio-system kiali: # ADD THE FOLLOWING LINES dashboard: grafanaURL: https://grafana-istio-system.apps.\u0026lt;your clustername\u0026gt; jaegerURL: https://jaeger-istio-system.apps.\u0026lt;your clustername\u0026gt;   This change will take a few minutes to be effective.\n  "},{"uri":"https://blog.stderr.at/categories/compliance/security/","title":"Compliance/Security","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/rss/","title":"RSS Feeds","tags":[],"description":"","content":""},{"uri":"https://blog.stderr.at/","title":"YAUB Yet Another Useless Blog","tags":[],"description":"","content":"Welcome to YAUB - Yet Another Useless Blog The articles in this blog shall help to easily tests and understand specific issues so they can be reproduced and tested on local environments.\nYou can find the most recent posts on this site or walk through the different categories via the left navigation.\n"}]