<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>etcd on TechBlog about OpenShift/Ansible/Satellite and much more</title><link>https://blog.stderr.at/day-2/etcd/</link><description>TechBlog about OpenShift/Ansible/Satellite and much more</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Toni Schmidbauer &amp; Thomas Jungbauer</copyright><atom:link href="https://blog.stderr.at/day-2/etcd/index.xml" rel="self" type="application/rss+xml"/><item><title>Automated ETCD Backup</title><link>https://blog.stderr.at/day-2/etcd/2022-01-29-automatedetcdbackup/</link><pubDate>Sat, 29 Jan 2022 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/etcd/2022-01-29-automatedetcdbackup/</guid><description>&lt;div class="paragraph">
&lt;p>Securing ETCD is one of the major Day-2 tasks for a Kubernetes cluster. This article will explain how to create a backup using OpenShift Cronjob.&lt;/p>
&lt;/div>
&lt;div class="admonitionblock caution">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-caution" title="Caution">&lt;/i>
&lt;/td>
&lt;td class="content">
There is absolutely no warranty. Verify your backups regularly and perform restore tests.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_prerequisites">Prerequisites&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>The following is required:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>OpenShift Cluster 4.x&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Integrated Storage, might be NFS or anything. Best practice would be a RWX enabled storage.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_configure_project_cronjob">Configure Project &amp;amp; Cronjob&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Create the following objects in OpenShift. This fill create:&lt;/p>
&lt;/div>
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>A Project called &lt;code>ocp-etcd-backup&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A PersistentVolumeClaim to store the backups. &lt;strong>Change to your appropriate StorageClass and accessMode&lt;/strong>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A ServiceAccount called &lt;code>openshift-backup&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A dedicated ClusterRole which is able to start (debug pods)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A ClusterRoleBinding between the created ServiceAccount and the customer ClusterRole&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A 2nd ClusterRoleBinding, which gives our ServiceAccount the permission to start privileged containers. This is required to start a debug pod on a control plane node.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A CronJob which performs the backup …​ see Callouts for inline explanations.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;div class="admonitionblock note">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-note" title="Note">&lt;/i>
&lt;/td>
&lt;td class="content">
A helm chart, which would create these objects below, can be found at: &lt;a href="https://github.com/tjungbauer/ocp-auto-backup" class="bare">https://github.com/tjungbauer/ocp-auto-backup&lt;/a>. This is probably a better way to manage the variables via the values.yaml file.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">kind: Namespace
apiVersion: v1
metadata:
name: ocp-etcd-backup
annotations:
openshift.io/description: Openshift Backup Automation Tool
openshift.io/display-name: Backup ETCD Automation
openshift.io/node-selector: &amp;#39;&amp;#39;
spec: {}
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
name: etcd-backup-pvc
namespace: ocp-etcd-backup
spec:
accessModes:
- ReadWriteOnce &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>
resources:
requests:
storage: 100Gi
storageClassName: gp2
volumeMode: Filesystem
---
kind: ServiceAccount
apiVersion: v1
metadata:
name: openshift-backup
namespace: ocp-etcd-backup
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
name: cluster-etcd-backup
rules:
- apiGroups: [&amp;#34;&amp;#34;]
resources:
- &amp;#34;nodes&amp;#34;
verbs: [&amp;#34;get&amp;#34;, &amp;#34;list&amp;#34;]
- apiGroups: [&amp;#34;&amp;#34;]
resources:
- &amp;#34;pods&amp;#34;
- &amp;#34;pods/log&amp;#34;
verbs: [&amp;#34;get&amp;#34;, &amp;#34;list&amp;#34;, &amp;#34;create&amp;#34;, &amp;#34;delete&amp;#34;, &amp;#34;watch&amp;#34;]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
name: openshift-backup
subjects:
- kind: ServiceAccount
name: openshift-backup
namespace: ocp-etcd-backup
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: cluster-etcd-backup
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
name: etcd-backup-scc-privileged
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: system:openshift:scc:privileged
subjects:
- kind: ServiceAccount
name: openshift-backup
namespace: ocp-etcd-backup
---
kind: CronJob
apiVersion: batch/v1
metadata:
name: cronjob-etcd-backup
namespace: ocp-etcd-backup
labels:
purpose: etcd-backup
spec:
schedule: &amp;#39;*/5 * * * *&amp;#39; &lt;i class="conum" data-value="2">&lt;/i>&lt;b>(2)&lt;/b>
startingDeadlineSeconds: 200
concurrencyPolicy: Forbid
suspend: false
jobTemplate:
metadata:
creationTimestamp: null
spec:
backoffLimit: 0
template:
metadata:
creationTimestamp: null
spec:
nodeSelector:
node-role.kubernetes.io/master: &amp;#39;&amp;#39; &lt;i class="conum" data-value="3">&lt;/i>&lt;b>(3)&lt;/b>
restartPolicy: Never
activeDeadlineSeconds: 200
serviceAccountName: openshift-backup
schedulerName: default-scheduler
hostNetwork: true
terminationGracePeriodSeconds: 30
securityContext: {}
containers:
- resources:
requests:
cpu: 300m
memory: 250Mi
terminationMessagePath: /dev/termination-log
name: etcd-backup
command: &lt;i class="conum" data-value="4">&lt;/i>&lt;b>(4)&lt;/b>
- /bin/bash
- &amp;#39;-c&amp;#39;
- &amp;gt;-
oc get no -l node-role.kubernetes.io/master --no-headers -o
name | grep `hostname` | head -n 1 | xargs -I {} -- oc debug
{} -- bash -c &amp;#39;chroot /host sudo -E
/usr/local/bin/cluster-backup.sh /home/core/backup&amp;#39; ; echo
&amp;#39;Moving Local Master Backups to target directory (from
/home/core/backup to mounted PVC)&amp;#39;; mv /home/core/backup/*
/etcd-backup/; echo &amp;#39;Deleting files older than 30 days&amp;#39; ; find
/etcd-backup/ -type f -mtime +30 -exec rm {} \;
securityContext:
privileged: true
runAsUser: 0
imagePullPolicy: IfNotPresent
volumeMounts:
- name: temp-backup
mountPath: /home/core/backup &lt;i class="conum" data-value="5">&lt;/i>&lt;b>(5)&lt;/b>
- name: etcd-backup
mountPath: /etcd-backup &lt;i class="conum" data-value="6">&lt;/i>&lt;b>(6)&lt;/b>
terminationMessagePolicy: FallbackToLogsOnError
image: registry.redhat.io/openshift4/ose-cli
serviceAccount: openshift-backup
volumes:
- name: temp-backup
hostPath:
path: /home/core/backup
type: &amp;#39;&amp;#39;
- name: etcd-backup
persistentVolumeClaim:
claimName: etcd-backup-pvc
dnsPolicy: ClusterFirst
tolerations:
- operator: Exists
effect: NoSchedule
- operator: Exists
effect: NoExecute
successfulJobsHistoryLimit: 5
failedJobsHistoryLimit: 5&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="colist arabic">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td>&lt;i class="conum" data-value="1">&lt;/i>&lt;b>1&lt;/b>&lt;/td>
&lt;td>RWO is used here, since I have no other available storage on my test cluster.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="2">&lt;/i>&lt;b>2&lt;/b>&lt;/td>
&lt;td>How often shall the job be executed. Here, every 5 minutes.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="3">&lt;/i>&lt;b>3&lt;/b>&lt;/td>
&lt;td>Bind the job to &amp;#34;Master&amp;#34; nodes.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="4">&lt;/i>&lt;b>4&lt;/b>&lt;/td>
&lt;td>Command to be executed…​ It fetches the actual local master nodename and starts a debugging Pod there. The backup script is called and moves the backup to /home/core/backup which is a folder on the control plane itself. The move command will move the backups from the local folder to the actual backup target volume. Finally, it will remove backups older than 30 days.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="5">&lt;/i>&lt;b>5&lt;/b>&lt;/td>
&lt;td>Mounted /home/core/backup on the master nodes, here the command will store the backups before they are moved&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="6">&lt;/i>&lt;b>6&lt;/b>&lt;/td>
&lt;td>Target destination for the etcd backup on the mounted PVC&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_start_a_job">Start a Job&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>If you do not want to wait until the CronJob is triggered, you can manually start the Job using the following commands:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc create job backup --from=cronjob/cronjob-etcd-backup -n ocp-etcd-backup&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This will start a Pod which will do the backup:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code>Starting pod/ip-10-0-196-187us-east-2computeinternal-debug ...
To use host binaries, run `chroot /host`
found latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-15
found latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-10
found latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-9
found latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-3
etcdctl is already installed
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1638199790.980932,&amp;#34;caller&amp;#34;:&amp;#34;snapshot/v3_snapshot.go:119&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;created temporary db file&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/home/core/backup/snapshot_2021-11-29_152949.db.part&amp;#34;}
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:&amp;#34;2021-11-29T15:29:50.991Z&amp;#34;,&amp;#34;caller&amp;#34;:&amp;#34;clientv3/maintenance.go:200&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;opened snapshot stream; downloading&amp;#34;}
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1638199790.9912837,&amp;#34;caller&amp;#34;:&amp;#34;snapshot/v3_snapshot.go:127&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;fetching snapshot&amp;#34;,&amp;#34;endpoint&amp;#34;:&amp;#34;https://10.0.196.187:2379&amp;#34;}
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:&amp;#34;2021-11-29T15:29:53.306Z&amp;#34;,&amp;#34;caller&amp;#34;:&amp;#34;clientv3/maintenance.go:208&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;completed snapshot read; closing&amp;#34;}
Snapshot saved at /home/core/backup/snapshot_2021-11-29_152949.db
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1638199793.3482974,&amp;#34;caller&amp;#34;:&amp;#34;snapshot/v3_snapshot.go:142&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;fetched snapshot&amp;#34;,&amp;#34;endpoint&amp;#34;:&amp;#34;https://10.0.196.187:2379&amp;#34;,&amp;#34;size&amp;#34;:&amp;#34;180 MB&amp;#34;,&amp;#34;took&amp;#34;:2.367303503}
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1638199793.348459,&amp;#34;caller&amp;#34;:&amp;#34;snapshot/v3_snapshot.go:152&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;saved&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/home/core/backup/snapshot_2021-11-29_152949.db&amp;#34;}
{&amp;#34;hash&amp;#34;:1180914745,&amp;#34;revision&amp;#34;:10182252,&amp;#34;totalKey&amp;#34;:19360,&amp;#34;totalSize&amp;#34;:179896320}
snapshot db and kube resources are successfully saved to /home/core/backup
Removing debug pod ...
Moving Local Master Backups to target directory (from /home/core/backup to mounted PVC)&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_verifying_the_backup">Verifying the Backup&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Let’s start a dummy Pod which can access the PVC to verify if the backup is really there.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">apiVersion: v1
kind: Pod
metadata:
name: verify-etcd-backup
spec:
containers:
- name: verify-etcd-backup
image: registry.access.redhat.com/ubi8/ubi
command: [&amp;#34;sleep&amp;#34;, &amp;#34;3000&amp;#34;]
volumeMounts:
- name: etcd-backup
mountPath: /etcd-backup
volumes:
- name: etcd-backup
persistentVolumeClaim:
claimName: etcd-backup-pvc&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Logging into that Pod will show the available backups stored at /etcd-backup which is the mounted PVC.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc rsh -n ocp-etcd-backup verify-etcd-backup ls -la etcd-backup
total 1406196
drwxr-xr-x. 3 root root 4096 Nov 29 17:00 .
dr-xr-xr-x. 1 root root 25 Nov 29 17:06 ..
drwx------. 2 root root 16384 Nov 29 15:21 lost+found
-rw-------. 1 root root 179896352 Nov 29 15:21 snapshot_2021-11-29_152150.db
-rw-------. 1 root root 179896352 Nov 29 15:29 snapshot_2021-11-29_152949.db
-rw-------. 1 root root 179896352 Nov 29 15:32 snapshot_2021-11-29_153159.db
-rw-------. 1 root root 179896352 Nov 29 15:36 snapshot_2021-11-29_153618.db
-rw-------. 1 root root 179896352 Nov 29 15:55 snapshot_2021-11-29_155513.db
-rw-------. 1 root root 179896352 Nov 29 16:00 snapshot_2021-11-29_160020.db
-rw-------. 1 root root 179896352 Nov 29 16:55 snapshot_2021-11-29_165521.db
-rw-------. 1 root root 179896352 Nov 29 17:00 snapshot_2021-11-29_170020.db
-rw-------. 1 root root 89875 Nov 29 15:21 static_kuberesources_2021-11-29_152150.tar.gz
-rw-------. 1 root root 89875 Nov 29 15:29 static_kuberesources_2021-11-29_152949.tar.gz
-rw-------. 1 root root 89875 Nov 29 15:32 static_kuberesources_2021-11-29_153159.tar.gz
-rw-------. 1 root root 89875 Nov 29 15:36 static_kuberesources_2021-11-29_153618.tar.gz
-rw-------. 1 root root 89875 Nov 29 15:55 static_kuberesources_2021-11-29_155513.tar.gz
-rw-------. 1 root root 89875 Nov 29 16:00 static_kuberesources_2021-11-29_160020.tar.gz
-rw-------. 1 root root 89875 Nov 29 16:55 static_kuberesources_2021-11-29_165521.tar.gz
-rw-------. 1 root root 89875 Nov 29 17:00 static_kuberesources_2021-11-29_170020.tar.gz&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div></description></item></channel></rss>