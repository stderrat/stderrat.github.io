<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Pod Placement on TechBlog about OpenShift/Ansible/Satellite and much more</title><link>https://blog.stderr.at/day-2/pod-placement/</link><description>TechBlog about OpenShift/Ansible/Satellite and much more</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Toni Schmidbauer &amp; Thomas Jungbauer</copyright><atom:link href="https://blog.stderr.at/day-2/pod-placement/index.xml" rel="self" type="application/rss+xml"/><item><title>Introduction</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-26-podplacement-intro/</guid><description>&lt;div class="paragraph">
&lt;p>Pod scheduling is an internal process that determines placement of new pods onto nodes within the cluster. It is probably one of the most important tasks for a Day-2 scenario and should be considered at a very early stage for a new cluster. OpenShift/Kubernetes is already shipped with a &lt;strong>default scheduler&lt;/strong> which schedules pods as they get created accross the cluster, without any manual steps.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>However, there are scenarios where a more advanced approach is required, like for example using a specifc group of nodes for dedicated workload or make sure that certain applications do not run on the same nodes. Kubernetes provides different options:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>Controlling placement with node selectors&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with pod/node affinity/anti-affinity rules&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with taints and tolerations&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Controlling placement with topology spread constraints&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This series will try to go into the detail of the different options and explains in simple examples how to work with pod placement rules.
It is not a replacement for any official documentation, so always check out Kubernetes and or OpenShift documentations.&lt;/p>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_pod_placement_series">Pod Placement Series&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-nodeselector/">NodeSelector&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Affinity and Anti Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-node-affinity/">Node Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-taints-and-tolerations">Taints and Tolerations&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-topology-spread-constraints/">Topology Spread Constraints&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/descheduler/">Descheduler&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_prerequisites">Prerequisites&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="admonitionblock note">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-note" title="Note">&lt;/i>
&lt;/td>
&lt;td class="content">
The following prerequisites are used for all examples.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Let’s image that our cluster (OpenShift 4) has 4 compute nodes&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get node --selector=&amp;#39;node-role.kubernetes.io/worker&amp;#39;
NAME STATUS ROLES AGE VERSION
compute-0 Ready worker 7h1m v1.19.0+d59ce34
compute-1 Ready worker 7h1m v1.19.0+d59ce34
compute-2 Ready worker 7h1m v1.19.0+d59ce34
compute-3 Ready worker 7h1m v1.19.0+d59ce34&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>An example application (from the catalog Django + Postgres) has been deployed in the namespace &lt;code>podtesting&lt;/code>. It contains by default 1 pod for a PostGresql database and one pod for a frontend web application.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide | grep Running
django-psql-example-1-h6kst 1/1 Running 0 20m 10.130.2.97 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
postgresql-1-4pcm4 1/1 Running 0 21m 10.131.0.51 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Without any configuration the OpenShift scheduler will try to spread the pods evenly accross the cluster.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Let’s increase the replica of the web frontend:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc scale --replicas=4 dc/django-psql-example -n podtesting&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Eventually 4 additional pods will be started accross the compute nodes of the cluster:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide | grep Running
django-psql-example-1-842fl 1/1 Running 0 2m7s 10.131.0.65 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-1-h6kst 1/1 Running 0 24m 10.130.2.97 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-1-pxhlv 1/1 Running 0 2m7s 10.128.2.13 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-1-xms7x 1/1 Running 0 2m7s 10.129.2.10 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
postgresql-1-4pcm4 1/1 Running 0 26m 10.131.0.51 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>As you can see, the scheduler already tries to spread the pods evenly, so that every worker node will host one frontend pod.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>However, let’s try to apply a more advanced configuration for the pod placement, starting with the
&lt;a href="https://blog.stderr.at/openshift/day-2/2021-08-27-podplacement/">Pod Placement - NodeSelector&lt;/a>&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Node Affinity</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-29-node-affinity/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-29-node-affinity/</guid><description>&lt;div class="paragraph">
&lt;p>Node Affinity allows to place a pod to a specific group of nodes. For example, it is possible to run a pod only on nodes with a specific CPU or disktype. The disktype was used as an example for the &lt;code>nodeSelector&lt;/code> and yes …​ Node Affinity is conceptually similar to nodeSelector but allows a more granular configuration.&lt;/p>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_pod_placement_series">Pod Placement Series&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Please check out other ways of pod placements:&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
&lt;a>Expand me...&lt;/a>
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-nodeselector/">NodeSelector&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Affinity and Anti Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-node-affinity/">Node Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-taints-and-tolerations">Taints and Tolerations&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-topology-spread-constraints/">Topology Spread Constraints&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/descheduler/">Descheduler&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_using_node_affinity">Using Node Affinity&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Currently two types of affinity settings are known:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>requiredDuringSchedulingIgnoreDuringExecuption (short &lt;em>required&lt;/em>) - a hard requirement which must be met before a pod can be scheduled&lt;/p>
&lt;/li>
&lt;li>
&lt;p>preferredDuringSchedulingIgnoredDuringExecution (short &lt;em>preferred&lt;/em>) - a soft requirement the scheduler &lt;strong>tries&lt;/strong> to meet, but does not guarantee it&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="admonitionblock note">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-note" title="Note">&lt;/i>
&lt;/td>
&lt;td class="content">
Both types can be specified. In such case the node must first meet the required rule and then attempt to meet the preferred rule.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="sect2">
&lt;h3 id="_preparing_node_labels">Preparing node labels&lt;/h3>
&lt;div class="admonitionblock note">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-note" title="Note">&lt;/i>
&lt;/td>
&lt;td class="content">
Remember the prerequisites explained in the . &lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Placement - Introduction&lt;/a>. We have 4 compute nodes and an example web application up and running.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Before we start with affinity rules we need to label all nodes. Let’s create 2 zones (east and west) for our compute nodes.&lt;/p>
&lt;/div>
&lt;div class="admonitionblock note">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-note" title="Note">&lt;/i>
&lt;/td>
&lt;td class="content">
You can skip this, if these labels are still set.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="imageblock">
&lt;div class="content">
&lt;img src="https://blog.stderr.at/day-2/images/affinity-kubernetes.zones.png" alt="Node Zones"/>
&lt;/div>
&lt;div class="title">Figure 1. Node Zones&lt;/div>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc label nodes compute-0 compute-1 topology.kubernetes.io/zone=east
oc label nodes compute-2 compute-3 topology.kubernetes.io/zone=west&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect2">
&lt;h3 id="_configure_node_affinity_rule">Configure node affinity rule&lt;/h3>
&lt;div class="paragraph">
&lt;p>Like pod affinity the node affinity is defined on the pod specification:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">kind: DeploymentConfig
apiVersion: apps.openshift.io/v1
metadata:
name: django-psql-example
namespace: podtesting
[...]
spec:
[...]
template:
[...]
spec:
[...]
affinity:
nodeAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
nodeSelectorTerms:
- matchExpressions:
- key: topology.kubernetes.io/zone
operator: In
values:
- west&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>In this example the pods are started only on nodes of the zone &amp;#34;West&amp;#34;. Since the value is an array, multiple zones can be defined letting the web application be executed on West and East for example.
With this setup you can control on which node a specific application shall be executed. For example: you have a group of nodes which provide a GPU and your GPU application must be started only on this group of nodes.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Like with pod affinity you can combine required and preferred settings.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_what_happened_to_node_anti_affinity">What happened to Node Anti-Affinity?&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Unlike Pod Anti-Affinity, there is no concept to define a node Anti-Affinity. Instead you can use the &lt;code>NotIn&lt;/code> and &lt;code>DoesNotExist&lt;/code> operators to achieve this bahaviour.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_cleanup">Cleanup&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>As cleanup simply remove the affinity specification from the DeploymentConf. The node labels can stay as they are since they do not hurt.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_summary">Summary&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>This concludes the quick overview of the node affinity. Further information can be found at &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity" target="_blank" rel="noopener">Node Affinity&lt;/a>&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>NodeSelector</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-27-podplacement/</guid><description>&lt;div class="paragraph">
&lt;p>One of the easiest ways to tell your Kubernetes cluster where to put certain pods is to use a &lt;code>nodeSelector&lt;/code> specification. A nodeSelector defines a key-value pair and are defined inside the specification of the pods and as a label on one or multiple nodes (or machine set or machine config). Only if selector matches the node label, the pod is allowed to be scheduled on that node.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Kubernetes distingushes between 2 types of selectors:&lt;/p>
&lt;/div>
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>&lt;em>cluster-wide node selectors&lt;/em>: defined by the cluster administrators and valid for the whole cluster&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>project node selectors&lt;/em>: to place new pods inside projects into specific nodes.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_pod_placement_series">Pod Placement Series&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Please check out other ways of pod placements:&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
&lt;a>Expand me...&lt;/a>
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-nodeselector/">NodeSelector&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Affinity and Anti Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-node-affinity/">Node Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-taints-and-tolerations">Taints and Tolerations&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-topology-spread-constraints/">Topology Spread Constraints&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/descheduler/">Descheduler&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_using_nodeselector">Using nodeSelector&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>As previously described, we have a cluster with an example application scheduled accross the worker nodes evenly by the scheduler.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide | grep Running
django-psql-example-1-842fl 1/1 Running 0 2m7s 10.131.0.65 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-1-h6kst 1/1 Running 0 24m 10.130.2.97 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-1-pxhlv 1/1 Running 0 2m7s 10.128.2.13 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-1-xms7x 1/1 Running 0 2m7s 10.129.2.10 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
postgresql-1-4pcm4 1/1 Running 0 26m 10.131.0.51 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>However, our 4 compute nodes are assembled with different hardware specification and are using different harddisks (sdd vs hdd).&lt;/p>
&lt;/div>
&lt;div class="imageblock">
&lt;div class="content">
&lt;img src="https://blog.stderr.at/day-2/images/nodeselector-disktypes.png" alt="Node with different disktypes"/>
&lt;/div>
&lt;div class="title">Figure 1. Nodes with Different Specifications&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Since our web application must run on fast disks must configure the cluster to schedule the pods on nodes with SSD only.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>To start using nodeSelectors we first &lt;strong>label our nodes&lt;/strong> accordingly:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>compute-0 and compute-1 are faster nodes with an SSD attached.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>compute-2 and compute-2 have a HDD attached.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc label nodes compute-0 compute-1 disktype=ssd &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>
oc label nodes compute-2 compute-3 disktype=hdd&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="colist arabic">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td>&lt;i class="conum" data-value="1">&lt;/i>&lt;b>1&lt;/b>&lt;/td>
&lt;td>as key we are using &lt;strong>disktype&lt;/strong>&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>As crosscheck we can list nodes with a specific label:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get nodes -l disktype=ssd
NAME STATUS ROLES AGE VERSION
compute-0 Ready worker 7h32m v1.19.0+d59ce34
compute-1 Ready worker 7h31m v1.19.0+d59ce34
oc get nodes -l disktype=hdd
NAME STATUS ROLES AGE VERSION
compute-2 Ready worker 7h32m v1.19.0+d59ce34
compute-3 Ready worker 7h32m v1.19.0+d59ce34&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="admonitionblock warning">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-warning" title="Warning">&lt;/i>
&lt;/td>
&lt;td class="content">
If no matching label is found, the pod cannot be scheduled. Therefore, &lt;strong>always&lt;/strong> label the nodes first.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The 2nd step is to add the node selector to the specification of the pod. In our example we are using a DeploymentConfig, so let’s add it there:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc patch dc django-psql-example -n podtesting --patch &amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;template&amp;#34;:{&amp;#34;spec&amp;#34;:{&amp;#34;nodeSelector&amp;#34;:{&amp;#34;disktype&amp;#34;:&amp;#34;ssd&amp;#34;}}}}}&amp;#39;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This adds the nodeSelector into: spec/template/spec&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml"> nodeSelector:
disktype: ssd&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Kubernetes will now trigger a restart of the pods on the supposed nodes.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide | grep Running
django-psql-example-3-4j92k 1/1 Running 0 42s 10.129.2.7 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-3-d7hsd 1/1 Running 0 42s 10.129.2.8 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-3-fkbfm 1/1 Running 0 14m 10.128.2.18 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-3-psskb 1/1 Running 0 14m 10.128.2.17 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>As you can see, only nodes with a SSD (compute-0 and compute-1) are being used.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_controlling_pod_placement_with_project_wide_selector">Controlling pod placement with project-wide selector&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Adding a nodeSelector to a deployment seems fine…​ until somebody forgets to add it. Then the pods would be started anywhere the scheduler finds suitable. Therefore, it might make sense to use a project-wide node selector, which will automatically be applied on all pods on that project. The project selector is added by the cluster administrator to the &lt;strong>Namespace&lt;/strong> object (no matter what the OpenShift documentation says in it’s example) as &lt;code>openshift.io/node-selector&lt;/code> parameter.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Let’s remove our previous configuration and add the setting to our namespace &lt;em>podtesting&lt;/em>:&lt;/p>
&lt;/div>
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>Cleanup&lt;/p>
&lt;div class="paragraph">
&lt;p>Remove the nodeSelector from the deployment configuration and wait until all pods have been reshuffeld&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc patch dc django-psql-example -n podtesting --type=&amp;#39;json&amp;#39; -p=&amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;remove&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/template/spec/nodeSelector&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;disktype=ssd&amp;#34; }]&amp;#39;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>Add the label to the project&lt;/p>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc annotate ns/podtesting openshift.io/node-selector=&amp;#34;disktype=ssd&amp;#34;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The OpenShift scheduler will now spread the Pods accross compute-0 or compute-1 again, but not on compute-2 or 3.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>We can prove that by stressing our cluster (and nodes) a little bit and scale our frontend application to 10:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide | grep Running
django-psql-example-4-2jn2l 1/1 Running 0 27s 10.128.2.8 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-6g7ks 1/1 Running 0 7m47s 10.129.2.23 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-752nm 1/1 Running 0 7m47s 10.128.2.7 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-c5jvm 1/1 Running 0 27s 10.129.2.4 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-f5kwg 1/1 Running 0 27s 10.129.2.5 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-g7bcs 1/1 Running 0 7m47s 10.129.2.24 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-h5tgb 1/1 Running 0 27s 10.129.2.6 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-spvpp 1/1 Running 0 28s 10.128.2.5 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-v9qwj 1/1 Running 0 7m48s 10.129.2.22 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-4-zgwcv 1/1 Running 0 27s 10.128.2.6 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>As you can see compute-0 and compute-1 are the only nodes which are used.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_well_known_labels">Well-Known Labels&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>nodeSelector is one of the easiest ways to control where an application shall be started. Working with labels is therefore very important as soon as workload shall be added to the cluster.
Kubernetes reserves some labels which can be leveraged and some are already predefined on the nodes, for example:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>beta.kubernetes.io/arch=amd64&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kubernetes.io/hostname=compute-0&lt;/p>
&lt;/li>
&lt;li>
&lt;p>kubernetes.io/os=linux&lt;/p>
&lt;/li>
&lt;li>
&lt;p>node-role.kubernetes.io/worker=&lt;/p>
&lt;/li>
&lt;li>
&lt;p>node.openshift.io/os_id=rhcos&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>A list of all known can be found at: [&lt;a href="#source_1">1&lt;/a>]&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Two of them I would like to mention here, since they might become very important when designing the placement of pods:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>topology.kubernetes.io/zone&lt;/p>
&lt;/li>
&lt;li>
&lt;p>topology.kubernetes.io/region&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>With these two labels you can create availability zones for your cluster. A &lt;strong>zone&lt;/strong> can be seen a logical failure domain and a cluster is typically spanned across multiple zones. This could be a rack in a data center for example, hardware which is sharing the same switch or simply different data centers. Zones are seen as independent to each other.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>A &lt;strong>region&lt;/strong> is made up of one or more zones. A cluster is usually not spanned across multiple region.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Kubernetes makes a few assumptions about the structure of zones and regions:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>regions and zones are hierarchical: zones are strict subsets of regions and no zone can be in 2 regions&lt;/p>
&lt;/li>
&lt;li>
&lt;p>zone names are unique across regions; for example region &amp;#34;africa-east-1&amp;#34; might be comprised of zones &amp;#34;africa-east-1a&amp;#34; and &amp;#34;africa-east-1b&amp;#34;&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_cleanup">Cleanup&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>This concludes the chapter about nodeSelectors. For the next chapter of the Pod Placement Series (&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Affinity and Anti Affinity&lt;/a>) we need to cleanup our configuration.&lt;/p>
&lt;/div>
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>Scale the frontend down to 2&lt;/p>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc scale --replicas=2 dc/django-psql-example -n podtesting&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>Remove the label from the namespace&lt;/p>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc annotate ns/podtesting openshift.io/node-selector- &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="colist arabic">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td>&lt;i class="conum" data-value="1">&lt;/i>&lt;b>1&lt;/b>&lt;/td>
&lt;td>The minus at the end defines that this annotation shall be removed&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>And, just to be sure if you have not done this before, remove the nodeSelector from the DeploymentConfig&lt;/p>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc patch dc django-psql-example -n podtesting --type=&amp;#39;json&amp;#39; -p=&amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;remove&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/template/spec/nodeSelector&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;disktype=ssd&amp;#34; }]&amp;#39;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_sources">Sources&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>&lt;a id="source_1">&lt;/a>[1]: &lt;a href="https://kubernetes.io/docs/reference/labels-annotations-taints/" target="_blank" rel="noopener">Well-Known Labels, Annotations and Taints&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Pod Affinity/Anti-Affinity</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-28-affinity/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-28-affinity/</guid><description>&lt;div class="paragraph">
&lt;p>While noteSelector provides a very easy way to control where a pod shall be scheduled, the affinity/anti-affinity feature, expands this configuration with more expressive rules like logical AND operators, constraints against labels on other pods or soft rules instead of hard requirements.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The feature comes with two types:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>pod affinity/anti-affinity - allows constrains against other pod labels rather than node labels.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>node affinity - allows pods to specify a group of nodes they can be placed on&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_pod_placement_series">Pod Placement Series&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Please check out other ways of pod placements:&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
&lt;a>Expand me...&lt;/a>
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-nodeselector/">NodeSelector&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Affinity and Anti Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-node-affinity/">Node Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-taints-and-tolerations">Taints and Tolerations&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-topology-spread-constraints/">Topology Spread Constraints&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/descheduler/">Descheduler&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_pod_affinity_and_anti_affinity">Pod Affinity and Anti-Affinity&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>&lt;strong>Affinity&lt;/strong> and &lt;strong>Anti-Affinity&lt;/strong> controls the nodes on which a pod should (or should not) be scheduled &lt;em>based on labels on Pods that are already scheduled on the node&lt;/em>. This is a different approach than nodeSelector, since it does not directly take the node labels into account. That said, one example for such setup would be: You have dedicated nodes for developement and production workload and you have to be sure that pods of dev or prod applications do not run on the same node.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Affinity and Anti-Affinity are shortly defined as:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>Pod affinity - tells scheduler to put a new pod onto the same node as other pods (selection is done using label selectors)&lt;/p>
&lt;div class="paragraph">
&lt;p>For example:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>I want to run where this other labelled pod is already running (Pods from same service shall be running on same node.)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>Pod Anti-Affinity - prevents the scheduler to place a new pod onto the same nodes with pods with the same labels&lt;/p>
&lt;div class="paragraph">
&lt;p>For example:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>I definitely do not want to start a pod where this other pod with the defined label is running (to prevent that all Pods of same service are running in the same availability zone.)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="admonitionblock warning">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-warning" title="Warning">&lt;/i>
&lt;/td>
&lt;td class="content">
&lt;div class="paragraph">
&lt;p>As described in the official &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity" target="_blank" rel="noopener">Kubernetes documention&lt;/a> two things should be considered:&lt;/p>
&lt;/div>
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>The affinity feature requires processing which can slow down the cluster. Therefore, it is not recommended for cluster with more than 100 nodes.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The feature requires that all nodes are consistently labelled (&lt;code>topologyKey&lt;/code>). Unintended behaviour might happen if labels are missing.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_using_affinityanti_affinity">Using Affinity/Anti-Affinity&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Currently two types of pod affinity/anti-affinity are known:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>&lt;strong>requiredDuringSchedulingIgnoreDuringExecuption&lt;/strong> (short &lt;em>required&lt;/em>) - a hard requirement which must be met before a pod can be scheduled&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>preferredDuringSchedulingIgnoredDuringExecution&lt;/strong> (short &lt;em>preferred&lt;/em>) - a soft requirement the scheduler &lt;strong>tries&lt;/strong> to meet, but does not guarantee it&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="admonitionblock note">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-note" title="Note">&lt;/i>
&lt;/td>
&lt;td class="content">
Both types can be defined in the same specification. In such case the node must first meet the required rule and then attempt based on best effort to meet the preferred rule.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="sect2">
&lt;h3 id="_preparing_node_labels">Preparing node labels&lt;/h3>
&lt;div class="admonitionblock note">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-note" title="Note">&lt;/i>
&lt;/td>
&lt;td class="content">
Remember the prerequisites explained in the . &lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Placement - Introduction&lt;/a>. We have 4 compute nodes and an example web application up and running.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Before we start with affinity rules we need to label all nodes. Let’s create 2 zones (east and west) for our compute nodes using the well-known label &lt;code>topology.kubernetes.io/zone&lt;/code>&lt;/p>
&lt;/div>
&lt;div class="imageblock">
&lt;div class="content">
&lt;img src="https://blog.stderr.at/day-2/images/affinity-kubernetes.zones.png" alt="Node Zones"/>
&lt;/div>
&lt;div class="title">Figure 1. Node Zones&lt;/div>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc label nodes compute-0 compute-1 topology.kubernetes.io/zone=east
oc label nodes compute-2 compute-3 topology.kubernetes.io/zone=west&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect2">
&lt;h3 id="_configure_pod_affinity_rule">Configure pod affinity rule&lt;/h3>
&lt;div class="paragraph">
&lt;p>In our example we have one database pod and multiple web application pods. Let’s image we would like to always run these pods in the same zone.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>&lt;strong>The pod affinity defines that a pod can be scheduled onto a node ONLY if that node is in the same zone as at least one already-running pod with a certain label.&lt;/strong>&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This means we must first label the postgres pod accordingly. Let’s labels the pod with &lt;code>security=zone1&lt;/code>&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc patch dc postgresql -n podtesting --type=&amp;#39;json&amp;#39; -p=&amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;add&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/template/metadata/labels/security&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;zone1&amp;#34; }]&amp;#39;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>After a while the postgres pod is restarted on one of the 4 compute nodes (since we did not specify in which zone this single pod shall be started) - here compute-1 (zone == east):&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide | grep Running
postgresql-5-6v5h6 1/1 Running 0 119s 10.129.2.24 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>As a second step, the deployment configuration of the web application must be modified. Remember, we want to run web application pods only on nodes located in the same zone as the postgres pods. In our example this would be either compute-0 or compute-1.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Modify the config accordingly:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">kind: DeploymentConfig
apiVersion: apps.openshift.io/v1
[...]
namespace: podtesting
spec:
[...]
template:
[...]
spec:
[...]
affinity:
podAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: security &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>
operator: In &lt;i class="conum" data-value="2">&lt;/i>&lt;b>(2)&lt;/b>
values:
- zone1 &lt;i class="conum" data-value="3">&lt;/i>&lt;b>(3)&lt;/b>
topologyKey: topology.kubernetes.io/zone &lt;i class="conum" data-value="4">&lt;/i>&lt;b>(4)&lt;/b>&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="colist arabic">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td>&lt;i class="conum" data-value="1">&lt;/i>&lt;b>1&lt;/b>&lt;/td>
&lt;td>The key of the label of a pod which is already running on that node is &amp;#34;security&amp;#34;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="2">&lt;/i>&lt;b>2&lt;/b>&lt;/td>
&lt;td>As operator &amp;#34;In&amp;#34; is used the postgres pod must have a matching key (security) containing the value (zone1). Other options like &amp;#34;NotIn&amp;#34;, &amp;#34;DoesNotExist&amp;#34; or &amp;#34;Exact&amp;#34; are available as well&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="3">&lt;/i>&lt;b>3&lt;/b>&lt;/td>
&lt;td>The value must be &amp;#34;zone1&amp;#34;&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="4">&lt;/i>&lt;b>4&lt;/b>&lt;/td>
&lt;td>As topology the topology.kubernetes.io/zone is used. The application can be deployed on nodes with the same label&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Setting this (and maybe scaling the replicas up a little bit) will start all frontend pods either on compute-0 or on compute-1.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>&lt;strong>In other words: On nodes of the same zone, where the postgres pod with the label security=zone1 is running.&lt;/strong>&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide | grep Running
django-psql-example-13-4w6qd 1/1 Running 0 67s 10.128.2.58 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-13-655dj 1/1 Running 0 67s 10.129.2.28 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-13-9d4pj 1/1 Running 0 67s 10.129.2.27 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-13-bdwhb 1/1 Running 0 67s 10.128.2.61 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-13-d4jrw 1/1 Running 0 67s 10.128.2.57 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-13-dm9qk 1/1 Running 0 67s 10.128.2.60 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-13-ktmfm 1/1 Running 0 67s 10.129.2.25 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-13-ldm56 1/1 Running 0 77s 10.128.2.55 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-13-mh2f5 1/1 Running 0 67s 10.129.2.29 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-13-qfkhq 1/1 Running 0 67s 10.129.2.26 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-13-v88qv 1/1 Running 0 67s 10.128.2.56 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-13-vfgf4 1/1 Running 0 67s 10.128.2.59 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
postgresql-5-6v5h6 1/1 Running 0 3m18s 10.129.2.24 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect2">
&lt;h3 id="_configure_pod_anti_affinity_rule">Configure pod anti-affinity rule&lt;/h3>
&lt;div class="paragraph">
&lt;p>For now the database pod and the web application pod are running on nodes of the same zone. However, somebody is asking us to configure it vice versa: the web application should not run in the same zone as postgresql.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Here we can use the Anti-Affinity feature.&lt;/p>
&lt;/div>
&lt;div class="admonitionblock note">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-note" title="Note">&lt;/i>
&lt;/td>
&lt;td class="content">
As an alternative, it would also be possible to change the operator in the affinity rule from &amp;#34;In&amp;#34; to &amp;#34;NotIn&amp;#34;
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">kind: DeploymentConfig
apiVersion: apps.openshift.io/v1
[...]
namespace: podtesting
spec:
[...]
template:
[...]
spec:
[...]
affinity:
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: security &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>
operator: In &lt;i class="conum" data-value="2">&lt;/i>&lt;b>(2)&lt;/b>
values:
- zone1 &lt;i class="conum" data-value="3">&lt;/i>&lt;b>(3)&lt;/b>
topologyKey: topology.kubernetes.io/zone &lt;i class="conum" data-value="4">&lt;/i>&lt;b>(4)&lt;/b>&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This will force the web application pods to run only on &amp;#34;west&amp;#34; zone nodes.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">django-psql-example-16-4n9h5 1/1 Running 0 40s 10.131.1.53 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-16-blf8b 1/1 Running 0 29s 10.130.2.63 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-16-f9plb 1/1 Running 0 29s 10.130.2.64 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-16-tm5rm 1/1 Running 0 28s 10.131.1.55 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-16-x8lbh 1/1 Running 0 29s 10.131.1.54 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-16-zb5fg 1/1 Running 0 28s 10.130.2.65 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
postgresql-5-6v5h6 1/1 Running 0 18m 10.129.2.24 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_combining_required_and_preferred_affinities">Combining required and preferred affinities&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>It is possible to combine requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution. In such case the required affinity MUST be met, while the preferred affinity is tried to be met. The following examples combines these two types in an affinity and anti-affinity specification.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The &lt;code>podAffinity&lt;/code> block defines the same as above: schedule the pod on a node of the same zone, where a pod with the label &lt;code>security=zone1&lt;/code> is running.
The &lt;code>podAntiAffinity&lt;/code> defines that the pod should not be started on a node if that node has a pod running with the label &lt;code>security=zone2&lt;/code>. However, the scheduler might decide to do so as long the &lt;code>podAffinity&lt;/code> rule is met.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
affinity:
podAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: security
operator: In
values:
- zone1
topologyKey: topology.kubernetes.io/zone
podAntiAffinity:
preferredDuringSchedulingIgnoredDuringExecution:
- weight: 100 &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>
podAffinityTerm:
labelSelector:
matchExpressions:
- key: security
operator: In
values:
- zone2
topologyKey: topology.kubernetes.io/zone&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="colist arabic">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td>&lt;i class="conum" data-value="1">&lt;/i>&lt;b>1&lt;/b>&lt;/td>
&lt;td>The &lt;code>weight&lt;/code> field is used by the scheduler to create a scoring. The higher the scoring the more preferred is that node.&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="sect2">
&lt;h3 id="_topologykey">topologyKey&lt;/h3>
&lt;div class="paragraph">
&lt;p>It is important to understand the &lt;code>topologyKey&lt;/code> setting. This is the key for the node label. If an affinity rule is met, Kubernetes will try to find suitable nodes which are labelled with the topologyKey. All nodes must be labelled consistently, otherwise unintended behaviour might occur.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>As described in the Kubernetes documentation at &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity" target="_blank" rel="noopener">Pod Affinity and Anit-Affinity&lt;/a>, the topologyKey has some constraints:&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>&lt;em>Quote Kubernetes:&lt;/em>&lt;/p>
&lt;/div>
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>&lt;em>For pod affinity, empty &lt;code>topologyKey&lt;/code> is not allowed in both &lt;code>requiredDuringSchedulingIgnoredDuringExecution&lt;/code> and &lt;code>preferredDuringSchedulingIgnoredDuringExecution&lt;/code>.&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>For pod anti-affinity, empty &lt;code>topologyKey&lt;/code> is also not allowed in both &lt;code>requiredDuringSchedulingIgnoredDuringExecution&lt;/code> and &lt;code>preferredDuringSchedulingIgnoredDuringExecution&lt;/code>.&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>For &lt;code>requiredDuringSchedulingIgnoredDuringExecution&lt;/code> pod anti-affinity, the admission controller &lt;code>LimitPodHardAntiAffinityTopology&lt;/code> was introduced to limit &lt;code>topologyKey&lt;/code> to &lt;code>kubernetes.io/hostname&lt;/code>. If you want to make it available for custom topologies, you may modify the admission controller, or disable it.&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Except for the above cases, the &lt;code>topologyKey&lt;/code> can be any legally label-key.&lt;/em>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;div class="paragraph">
&lt;p>&lt;em>End of quote&lt;/em>&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_cleanup">Cleanup&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>As cleanup simply remove the affinity specification from the DeploymentConf. The node labels can stay as they are since they do not hurt.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_summary">Summary&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>This concludes the quick overview of the pod affinity. The next chapter will discuss &lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-node-affinity/">Node Affinity&lt;/a> rules, which allows affinity based on node specifications.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Taints and Tolerations</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-30-taints/</guid><description>&lt;div class="paragraph">
&lt;p>While Node Affinity is a property of pods that attracts them to a set of nodes, taints are the exact opposite. Nodes can be configured with one or more taints, which mark the node in a way to only accept pods that do tolerate the taints. The tolerations themselves are applied to pods, telling the scheduler to accept a taints and start the workload on a tainted node.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>A common use case would be to mark certain nodes as infrastructure nodes, where only specific pods are allowed to be executed or to taint nodes with a special hardware (i.e. GPU).&lt;/p>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_pod_placement_series">Pod Placement Series&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Please check out other ways of pod placements:&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
&lt;a>Expand me...&lt;/a>
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-nodeselector/">NodeSelector&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Affinity and Anti Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-node-affinity/">Node Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-taints-and-tolerations">Taints and Tolerations&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-topology-spread-constraints/">Topology Spread Constraints&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/descheduler/">Descheduler&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_understanding_taints_and_tolerations">Understanding Taints and Tolerations&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Matching taints and tolerations is defined by a key/value pair and a taint effect.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>For example, a node can be tainted as:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
spec:
taints:
- effect: NoExecute
key: key1
value: value1
[...]&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>And a pod can have the matching toleration:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
spec:
tolerations:
- key: &amp;#34;key1&amp;#34;
operator: &amp;#34;Equal&amp;#34;
value: &amp;#34;value1&amp;#34;
effect: &amp;#34;NoExecute&amp;#34;
tolerationSeconds: 3600
[...]&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This means that the pod is tolerating the taint of the node with the pair &lt;strong>key1=value1&lt;/strong>.&lt;/p>
&lt;/div>
&lt;div class="sect2">
&lt;h3 id="_parameter_effect">Parameter: effect&lt;/h3>
&lt;div class="paragraph">
&lt;p>Above example is using as effect &lt;code>NoExecute&lt;/code>. The following effects are possible:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>&lt;code>NoSchedule&lt;/code> - new pods are not scheduled, existing pods remain&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>PreferNoSchedule&lt;/code> - new pods are not preferred but can still be scheduled but the scheduler tries to avoid that, existing pods remain&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>NoExecute&lt;/code> - new pods are not scheduled, existing pods (without matching toleration) are &lt;strong>removed&lt;/strong>! The setting &lt;code>tolerationSeconds&lt;/code> is used to define a maximum time until a pod is allowed to stay.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;div class="sect2">
&lt;h3 id="_parameter_operator">Parameter: operator&lt;/h3>
&lt;div class="paragraph">
&lt;p>For the operator two options are possible:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>&lt;code>Exists&lt;/code> - simply checks if a key exists and key and effect matches. No value should be configured in this case.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Equal&lt;/code> (default) - with this option key, value and effect must match exactly.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_configuring_taints_and_tolerations">Configuring Taints and Tolerations&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Our compute-0 node is a special node with a GPU installed. We would like that our web application is running on this node and ONLY our web application is running on that node. To achieve this, we will taint the node accordingly with the key/value &lt;code>gpu=enabled&lt;/code> and the effect &lt;code>NoExecute&lt;/code> and configure a toleration to the DeploymentConfig of our web fronted.&lt;/p>
&lt;/div>
&lt;div class="imageblock">
&lt;div class="content">
&lt;img src="https://blog.stderr.at/day-2/images/tainting-node.png" alt="Tainting compute-0"/>
&lt;/div>
&lt;div class="title">Figure 1. Tainting Node&lt;/div>
&lt;/div>
&lt;div class="admonitionblock warning">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-warning" title="Warning">&lt;/i>
&lt;/td>
&lt;td class="content">
Always configure tolerations before you taint a node. Otherwise the scheduler might not be able to start a pod until it has been configured to tolerate the taints.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>First we will set the toleration to the DeploymentConfig:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc edit dc/django-psql-example -n podtesting&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>And add a toleration for the key/value pair, the effect NoExecute and a tolerationSeconds of X seconds.&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
spec:
tolerations:
- key: &amp;#34;gpu&amp;#34;
value: &amp;#34;enabled&amp;#34;
operator: &amp;#34;Equal&amp;#34;
effect: &amp;#34;NoExecute&amp;#34;
tolerationSeconds: 30 &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="colist arabic">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td>&lt;i class="conum" data-value="1">&lt;/i>&lt;b>1&lt;/b>&lt;/td>
&lt;td>I am setting the tolerationSeconds to 30 seconds to get it done quicker.&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Before we taint our node, let’s check which pods are currently running there:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -A -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,STATE:.status.phase | grep compute-0 | grep Running
tuned-hrvl4 compute-0 Running
dns-default-ln9s4 compute-0 Running
node-resolver-qk24n compute-0 Running
node-ca-lxrvf compute-0 Running
ingress-canary-fv5w6 compute-0 Running
machine-config-daemon-558v4 compute-0 Running
grafana-78ccdb8c9d-8rqsp compute-0 Running
node-exporter-rn8h4 compute-0 Running
prometheus-adapter-66976bf759-fxdcd compute-0 Running
multus-additional-cni-plugins-29qgq compute-0 Running
multus-mkd87 compute-0 Running
network-metrics-daemon-64hrw compute-0 Running
network-check-target-l6l7n compute-0 Running
ovnkube-node-hrtln compute-0 Running
django-psql-example-24-c599b compute-0 Running
django-psql-example-24-l7znv compute-0 Running
django-psql-example-24-zpg77 compute-0 Running&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Multiple different pods are running here, as well as two of our web frontend (django-psql-example)&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The other django-psql-example pods are started accross the cluster:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide
oc get pods -n podtesting -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
django-psql-example-25-8ppxc 1/1 Running 0 21s 10.128.0.61 master-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-8rv76 1/1 Running 0 21s 10.130.2.92 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-9m8k2 1/1 Running 0 21s 10.129.0.67 master-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-fhvxg 1/1 Running 0 31s 10.128.2.126 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-kqgwz 0/1 Running 0 21s 10.130.0.71 master-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-m66nn 1/1 Running 0 21s 10.130.0.70 master-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-ntjqb 1/1 Running 0 21s 10.128.0.60 master-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-nxxqh 1/1 Running 0 21s 10.130.2.93 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-p8nbz 1/1 Running 0 21s 10.131.1.183 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-ttr9g 1/1 Running 0 21s 10.129.2.57 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-xn4fp 1/1 Running 0 21s 10.129.2.56 compute-1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-xpqf4 1/1 Running 0 21s 10.128.2.127 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-25-xwmwv 1/1 Running 0 21s 10.131.1.184 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>To taint the node we can simply execute the following command. Be sure to use the same values as in the toleration:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc adm taint nodes compute-0 gpu=enabled:NoExecute&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This will create the following specification in the node object:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
taints:
- effect: NoExecute
key: gpu
value: enabled&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>OpenShift will allow pods, which are not tolerating the taints, to keep on running for 30 seconds. After that, these pods will be evicted and started elsewhere.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>When we check after the tolerationSeconds time has passed which pods are running on the node compute-0, we will see that most pods have disappeared, except the pods for the webapplication and pods which are part of DaemonSets. (DaemonSets are defined to run on all or specific nodes and are not evicted. The cluster-DaemonSets are tolerating everything)&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -A -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,STATE:.status.phase | grep compute-0 | grep Running
tuned-hrvl4 compute-0 Running
node-resolver-qk24n compute-0 Running
node-ca-lxrvf compute-0 Running
machine-config-daemon-558v4 compute-0 Running
node-exporter-rn8h4 compute-0 Running
multus-additional-cni-plugins-29qgq compute-0 Running
multus-mkd87 compute-0 Running
network-metrics-daemon-64hrw compute-0 Running
network-check-target-l6l7n compute-0 Running
ovnkube-node-hrtln compute-0 Running
django-psql-example-25-8dzqh compute-0 Running
django-psql-example-25-9p4sb compute-0 Running
django-psql-example-25-pqbvn compute-0 Running
django-psql-example-25-sb6hr compute-0 Running&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_removing_taints">Removing taints&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Taints can be simply removed with the oc command added a trailing &lt;code>-&lt;/code>&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc adm taint nodes compute-0 gpu=enabled:NoExecute-&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_built_in_taints_taint_nodes_by_condition">Built-in Taints - Taint Nodes by Condition&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Several taints are built into OpenShift and are set during certain events (aka Taint Nodes by Condition) and cleared when the condition is resolved.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The following list is quoted from the OpenShift documentation and provides a list of taints which are automatically set. For example, when a node becomes unavailable:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>&lt;code>node.kubernetes.io/not-ready&lt;/code>: The node is not ready. This corresponds to the node condition Ready=False.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>node.kubernetes.io/unreachable&lt;/code>: The node is unreachable from the node controller. This corresponds to the node condition Ready=Unknown.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>node.kubernetes.io/out-of-disk&lt;/code>: The node has insufficient free space on the node for adding new pods. This corresponds to the node condition OutOfDisk=True.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>node.kubernetes.io/memory-pressure&lt;/code>: The node has memory pressure issues. This corresponds to the node condition MemoryPressure=True.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>node.kubernetes.io/disk-pressure&lt;/code>: The node has disk pressure issues. This corresponds to the node condition DiskPressure=True.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>node.kubernetes.io/network-unavailable&lt;/code>: The node network is unavailable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>node.kubernetes.io/unschedulable&lt;/code>: The node is unschedulable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>node.cloudprovider.kubernetes.io/uninitialized&lt;/code>: When the node controller is started with an external cloud provider, this taint is set on a node to mark it as unusable. After a controller from the cloud-controller-manager initializes this node, the kubelet removes this taint.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Depending on the condition, the node will either have the effect &lt;code>NoSchedule&lt;/code>, which means no new pods will be started there (unless a toleration is configured) or the effect &lt;code>NoExecute&lt;/code>, which will evict pods with no tolerations from the node. Typical examples for NoSchedule condition would be: memory-pressure or disk-pressure. If a node is unreachable or not-ready, then it will be automatically tainted with the effect NoExecute. &lt;code>tolerationSeconds&lt;/code> (default 300) will be respected.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_tolerating_all_taints">Tolerating all taints&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Tolerations can be configured to tolerate all possible taints. In such case no &lt;code>value&lt;/code> or &lt;code>key&lt;/code> is configured and &lt;code>operator: &amp;#34;Exists&amp;#34;&lt;/code> is used:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
spec:
tolerations:
- operator: “Exists”&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="admonitionblock note">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td class="icon">
&lt;i class="fa icon-note" title="Note">&lt;/i>
&lt;/td>
&lt;td class="content">
Some cluster daemonsets (i.e. tuned) are configured this way.
&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_cleanup">Cleanup&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Remove the taint from the node:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc adm taint nodes compute-0 gpu=enabled:NoExecute-&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>And remove the toleration specification from the DeploymentConfig&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
spec:
tolerations:
- key: &amp;#34;gpu&amp;#34;
value: &amp;#34;enabled&amp;#34;
operator: &amp;#34;Equal&amp;#34;
effect: &amp;#34;NoExecute&amp;#34;
tolerationSeconds: 30 &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Topology Spread Constraints</title><link>https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-08-31-topologyspreadcontraints/</guid><description>&lt;div class="paragraph">
&lt;p>&lt;strong>Topology spread constraints&lt;/strong> is a new feature since Kubernetes 1.19 (OpenShift 4.6) and another way to control where pods shall be started. It allows to use failure-domains, like zones or regions or to define custom topology domains. It heavily relies on configured node labels, which are used to define topology domains. This feature is a more granular approach than affinity, allowing to achieve higher availability.&lt;/p>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_pod_placement_series">Pod Placement Series&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Please check out other ways of pod placements:&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
&lt;a>Expand me...&lt;/a>
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-nodeselector/">NodeSelector&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Affinity and Anti Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-node-affinity/">Node Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-taints-and-tolerations">Taints and Tolerations&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-topology-spread-constraints/">Topology Spread Constraints&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/descheduler/">Descheduler&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_understanding_topology_spread_constraints">Understanding Topology Spread Constraints&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Imagine, like for pod affinity, we have two zones (east and west) in our 4 compute node cluster.&lt;/p>
&lt;/div>
&lt;div class="imageblock">
&lt;div class="content">
&lt;img src="https://blog.stderr.at/day-2/images/affinity-kubernetes.zones.png" alt="Node Zones"/>
&lt;/div>
&lt;div class="title">Figure 1. Node Zones&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>These zones are defined by node labels, which can be created with the following commands:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc label nodes compute-0 compute-1 topology.kubernetes.io/zone=east
oc label nodes compute-2 compute-3 topology.kubernetes.io/zone=west&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Now let’s scale our web application to 3 pods. The OpenShift scheduler will try to evenly spread the pods accross the cluster:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc get pods -n podtesting -o wide
django-psql-example-31-jrhsr 0/1 Running 0 8s 10.130.2.112 compute-2 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-31-q66hk 0/1 Running 0 8s 10.131.1.219 compute-3 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
django-psql-example-31-xv7jc 0/1 Running 0 8s 10.128.3.115 compute-0 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="imageblock">
&lt;div class="content">
&lt;img src="https://blog.stderr.at/day-2/images/topologyspreadconstraints1.png" alt="Running Pods"/>
&lt;/div>
&lt;div class="title">Figure 2. Running Pods&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>If a new pod is started the scheduler may try to start it on compute-1. However, this is done based on best effort. The scheduler does not guarantee that and may try to start it on one of the nodes in zone &amp;#34;West&amp;#34;. With the configuration &lt;code>topologySpreadConstraints&lt;/code> this can be controlled and incoming pods can only be scheduled on a node of zone &amp;#34;East&amp;#34; (either on compute-0 or compute-1).&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_configure_topologyspreadcontraint">Configure TopologySpreadContraint&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Let’s configure our topology by adding the following into the DeploymentConfig:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
topologySpreadConstraints:
- maxSkew: 1
topologyKey: topology.kubernetes.io/zone
whenUnsatisfiable: DoNotSchedule
labelSelector:
matchLabels:
name: django-psql-example&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This defines the following parameter:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>&lt;code>maxSkew&lt;/code> - defines the degree to which pods may be unevenly distributed (must be greater than zero). Depending on the &lt;code>whenUnsatisfiable&lt;/code> parameter the bahaviour differs:&lt;/p>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>whenUnsatisfiable == DoNotSchedule: would not schedule if the maxSkew is is not met.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>whenUnsatisfiable == ScheduleAnyway: the scheduler will still schedule and gives the topology which would decrease the skew a better score&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>&lt;code>topologyKey&lt;/code> - key of node lables&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>whenUnsatisfiable&lt;/code> - defines what to do with a pod if the spread constraint is not met. Can be either &lt;code>DoNotSchedule&lt;/code> (default) or &lt;code>ScheduleAnyway&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>lableSelector&lt;/code> - used to find matching pods. Found pods are considered to be part of the topology domain. In our example we simply use a default label of the application: &lt;code>name: django-psql-example&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>If now a 4th pod shall be started the topologySpreadConstraints will allow this pod on one of the nodes of zone=east (either compute-0 or compute-1). It cannot be scheduled on nodes in zone=west since it would violate the &lt;code>maxSkew: 1&lt;/code>&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre>Pod distribution would be 1 in zone east and 3 on zone west.
--&amp;gt; the skew would then be 3 - 1 = 2 which is not equal to 1&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_configure_multiple_topologyspreadcontraint">Configure multiple TopologySpreadContraint&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>It is possible to configure multiple TopologySpreadConstraints. In such a case all contrains must meet the requirement (logical AND). For example:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
topologySpreadConstraints:
- maxSkew: 1
topologyKey: topology.kubernetes.io/zone
whenUnsatisfiable: DoNotSchedule
labelSelector:
matchLabels:
name: django-psql-example
- maxSkew: 1
topologyKey: kubernetes.io/hostname
whenUnsatisfiable: DoNotSchedule
labelSelector:
matchLabels:
name: django-psql-example&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The first part is identical to our first example and would allow the schedule to start a pod in &lt;code>topology.kubernetes.io/zone: east&lt;/code> (compute-0 or compute-1). The second configuration defines not a zone but a node hostname. Now the scheduler can only deploy into zone=east AND onto node=compute-1&lt;/p>
&lt;/div>
&lt;div class="sect2">
&lt;h3 id="_conflicts_with_multiple_topologyspreadconstraints">Conflicts with multiple TopologySpreadConstraints&lt;/h3>
&lt;div class="paragraph">
&lt;p>If you use multiple constraints conflicts are possible. For example, you have a 3 node cluster and the 2 zones east and west. In such cases the maxSkew might be increased or the &lt;code>whenUnsatisfiable&lt;/code> might be set to ScheduleAnyway&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>See &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#example-multiple-topologyspreadconstraints" class="bare">https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#example-multiple-topologyspreadconstraints&lt;/a> for further information.&lt;/p>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_cleanup">Cleanup&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Remove the topologySpreadConstraint&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">spec:
[...]
template:
[...]
topologySpreadConstraints:
- maxSkew: 1
topologyKey: topology.kubernetes.io/zone
whenUnsatisfiable: DoNotSchedule
labelSelector:
matchLabels:
name: django-psql-example&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div></description></item><item><title>Using Descheduler</title><link>https://blog.stderr.at/day-2/pod-placement/2021-09-01-descheduler/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/day-2/pod-placement/2021-09-01-descheduler/</guid><description>&lt;div class="paragraph">
&lt;p>&lt;strong>Descheduler&lt;/strong> is a new feature which is GA since OpenShift 4.7. It can be used to evict pods from nodes based on specific strategies. The evicted pod is then scheduled on another node (by the Scheduler) which is more suitable.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This feature can be used when:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>nodes are under/over-utilized&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pod or node affinity, taints or labels have changed and are no longer valid for a running pod&lt;/p>
&lt;/li>
&lt;li>
&lt;p>node failures&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pods have been restarted too many times&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_pod_placement_series">Pod Placement Series&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>Please check out other ways of pod placements:&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;div class="expand">
&lt;div class="expand-label" style="cursor: pointer;" onclick="$h = $(this);$h.next('div').slideToggle(100,function () {$h.children('i').attr('class',function () {return $h.next('div').is(':visible') ? 'fas fa-chevron-down' : 'fas fa-chevron-right';});});">
&lt;i style="font-size:x-small;" class="fas fa-chevron-right">&lt;/i>
&lt;span>
&lt;a>Expand me...&lt;/a>
&lt;/span>
&lt;/div>
&lt;div class="expand-content" style="display: none;">
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-nodeselector/">NodeSelector&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-pod-affinity/">Pod Affinity and Anti Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-node-affinity/">Node Affinity&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-taints-and-tolerations">Taints and Tolerations&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/pod-placement-topology-spread-constraints/">Topology Spread Constraints&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://blog.stderr.at/openshift/day-2/descheduler/">Descheduler&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_descheduler_profiles">Descheduler Profiles&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>The following descheduler profiles are known and one or multiple can be configured for the Descheduler. Each profiles enables certain strategies which the Descheduler is leveraging.
Since the strategies names are more or less self explaining, I did not add their full description here. Instead detailed information can be found at: &lt;a href="https://docs.openshift.com/container-platform/4.8/nodes/scheduling/nodes-descheduler.html#nodes-descheduler-profiles_nodes-descheduler" target="_blank" rel="noopener">Descheduler Profiles&lt;/a>&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>&lt;code>AffinityAndTaints&lt;/code> - removes pods that violates affinity and anti-affinity rules or taints&lt;/p>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>RemovePodsViolatingInterPodAntiAffinity&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RemovePodsViolatingNodeAffinity&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RemovePodsViolatingNodeTaints&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>&lt;code>TopologyAndDuplicates&lt;/code> - evicts pods which are not evenly spreaded or which are violating the topology domain&lt;/p>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>RemovePodsViolatingTopologySpreadConstraint&lt;/p>
&lt;/li>
&lt;li>
&lt;p>RemoveDuplicates&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>&lt;code>LifecycleAndUtilization&lt;/code> - evicts long-running pods to balance resource usage of nodes&lt;/p>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>RemovePodsHavingTooManyRestarts - Pods that are restarted more than 100 times&lt;/p>
&lt;/li>
&lt;li>
&lt;p>LowNodeUtilization - removes pods from overutilized nodes.&lt;/p>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>A node is considered underutilized if its usage is below 20% for all thresholds (CPU, memory, and number of pods).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A node is considered overutilized if its usage is above 50% for any of the thresholds (CPU, memory, and number of pods).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>PodLifeTime - evicts pods that are too old&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_descheduler_mechanism">Descheduler mechanism&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>The following rules are followed by the Descheduler to ensure that eviction of pods does not go wild. Therefore the following pods will never be evicted:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>pods in openshift-* or kube-system namespaces&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pods with priorityClassName equal to &lt;code>system-cluster-critical&lt;/code> or &lt;code>system-node-critical&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pods which cannot be recreated, for example: static or stand-alone pods/jobs or pods without a replication controller or replica set&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pods of a daemon set&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pods with local storage&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pods which are violating the pod disruption budget&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="sect1">
&lt;h2 id="_installing_the_descheduler">Installing the Descheduler&lt;/h2>
&lt;div class="sectionbody">
&lt;div class="paragraph">
&lt;p>The Descheduler is not installed by default and must be installed after the cluster has been initiated. This is done by installed the &lt;strong>Kube Descheduler&lt;/strong> Operator.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>First we create a separate namespace for our operator, including a label:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-bash" data-lang="bash">oc adm new-project openshift-kube-descheduler-operator
oc label ns/openshift-kube-descheduler-operator openshift.io/cluster-monitoring=true&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Then we search for the &lt;strong>Kube Descheduler&lt;/strong> operator and install it, using the newly created namespace:&lt;/p>
&lt;/div>
&lt;div class="imageblock">
&lt;div class="content">
&lt;img src="https://blog.stderr.at/day-2/images/descheduler-install.png?height=400px" alt="Install Descheduler Operator"/>
&lt;/div>
&lt;div class="title">Figure 1. Install Descheduler Operator&lt;/div>
&lt;/div>
&lt;div class="paragraph">
&lt;p>After a few moments the operator will be installed.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>You can now create a Descheduler instance either via UI (wizard) or by using the following specification:&lt;/p>
&lt;/div>
&lt;div class="listingblock">
&lt;div class="content">
&lt;pre class="highlight">&lt;code class="language-yaml" data-lang="yaml">apiVersion: operator.openshift.io/v1
kind: KubeDescheduler
metadata:
name: cluster
namespace: openshift-kube-descheduler-operator
spec:
deschedulingIntervalSeconds: 3600 &lt;i class="conum" data-value="1">&lt;/i>&lt;b>(1)&lt;/b>
logLevel: Normal &lt;i class="conum" data-value="2">&lt;/i>&lt;b>(2)&lt;/b>
managementState: Managed
operatorLogLevel: Normal &lt;i class="conum" data-value="3">&lt;/i>&lt;b>(3)&lt;/b>
profiles: &lt;i class="conum" data-value="4">&lt;/i>&lt;b>(4)&lt;/b>
- AffinityAndTaints
- TopologyAndDuplicates
- LifecycleAndUtilization&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;div class="colist arabic">
&lt;table>
&lt;tbody>&lt;tr>
&lt;td>&lt;i class="conum" data-value="1">&lt;/i>&lt;b>1&lt;/b>&lt;/td>
&lt;td>Defines the time interval the descheduler is running. Default is 3600 seconds&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="2">&lt;/i>&lt;b>2&lt;/b>&lt;/td>
&lt;td>Defines logging for overall component. Can be Normal, Debug, Trace or TraceAll&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="3">&lt;/i>&lt;b>3&lt;/b>&lt;/td>
&lt;td>Defines logging for the operator itself. Can be Normal, Debug, Trace or TraceAll&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;i class="conum" data-value="4">&lt;/i>&lt;b>4&lt;/b>&lt;/td>
&lt;td>Enables on or multiple profiles the Descheduler should consider&lt;/td>
&lt;/tr>
&lt;/tbody>&lt;/table>
&lt;/div>
&lt;/div>
&lt;/div></description></item></channel></rss>