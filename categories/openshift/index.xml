<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>OpenShift on TechBlog about OpenShift/Ansible/Satellite and much more</title><link>https://blog.stderr.at/categories/openshift/</link><description>Recent content in OpenShift on TechBlog about OpenShift/Ansible/Satellite and much more</description><generator>Hugo</generator><language>en-us</language><copyright>Toni Schmidbauer &amp; Thomas Jungbauer</copyright><lastBuildDate>Sat, 08 Jun 2024 07:58:31 +0200</lastBuildDate><atom:link href="https://blog.stderr.at/categories/openshift/index.xml" rel="self" type="application/rss+xml"/><item><title>Update Cluster Version using GitOps approach</title><link>https://blog.stderr.at/gitopscollection/2024-06-07-update-cluster-version-with-gitops/</link><pubDate>Fri, 07 Jun 2024 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2024-06-07-update-cluster-version-with-gitops/</guid><description>&lt;div class="paragraph">
&lt;p>During a GitOps journey at one point, the question arises, how to update a cluster? Nowadays it is very easy to update a cluster using CLI or WebUI, so why bother with GitOps in that case? The reason is simple: Using GitOps you can be sure that all clusters are updated to the correct, required version and the version of each cluster is also managed in Git.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>All you need is the &lt;strong>channel&lt;/strong> you want to use and the desired cluster &lt;strong>version&lt;/strong>. Optionally, you can define the exact image SHA. This might be required when you are operating in a restricted environment.&lt;/p>
&lt;/div></description></item><item><title>Multiple Sources for Applications in Argo CD</title><link>https://blog.stderr.at/gitopscollection/2024-06-02-multisources-for-application-in-argocd/</link><pubDate>Sun, 02 Jun 2024 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2024-06-02-multisources-for-application-in-argocd/</guid><description>&lt;div class="paragraph">
&lt;p>Argo CD or OpenShift GitOps uses Applications or ApplicationSets to define the relationship between a source (Git) and a cluster. Typically, this is a 1:1 link, which means one Application is using one source to compare the cluster status. This can be a limitation. For example, if you are working with Helm Charts and a Helm repository, you do not want to re-build (or re-release) the whole chart just because you made a small change in the values file that is packaged into the repository. You want to separate the configuration of the chart with the Helm package.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The most common scenarios for multiple sources are (see: &lt;a href="https://argo-cd.readthedocs.io/en/stable/user-guide/multiple_sources/" target="_blank" rel="noopener">Argo CD documentation&lt;/a>):&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>Your organization wants to use an external/public Helm chart&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You want to override the Helm values with your own local values&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You don’t want to clone the Helm chart locally as well because that would lead to duplication and you would need to monitor it manually for upstream changes.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This small article describes three different ways with a working example and tries to cover the advantages and disadvantages of each of them. They might be opinionated but some of them proved to be easier to use and manage.&lt;/p>
&lt;/div></description></item><item><title>Installing OpenShift Logging using GitOps</title><link>https://blog.stderr.at/gitopscollection/2024-05-24-install-openshift-logging/</link><pubDate>Fri, 24 May 2024 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2024-05-24-install-openshift-logging/</guid><description>&lt;div class="paragraph">
&lt;p>&lt;a href="https://docs.openshift.com/container-platform/4.15/observability/logging/logging_release_notes/logging-5-9-release-notes.html" target="_blank" rel="noopener">OpenShift Logging&lt;/a> is one of the more complex things to install and configure on an OpenShift cluster. Not because the service or Operators are so complex to understand, but because of the dependencies logging has. Besides the logging operator itself, the Loki operator is required, the Loki operator requires access to an object storage, that might be configured or is already available.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>In this article, I would like to demonstrate the configuration of the full stack using an object storage from OpenShift Data Foundation. This means:&lt;/p>
&lt;/div>
&lt;div class="ulist">
&lt;ul>
&lt;li>
&lt;p>Installing the logging operator into the namespace openshift-logging&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Installing the Loki operator into the namespace openshift-operators-redhat&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Creating a new BackingStore and BucketClass&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Generating the Secret for Loki to authenticate against the object storage&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Configuring the LokiStack resource&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Configuring the ClusterLogging resource&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;div class="paragraph">
&lt;p>All steps will be done automatically. In case you have S3 storage available, or you are not using OpenShift Data Foundation, the setup will be a bit different. For example, you do not need to create a BackingStore or the Loki authentication Secret.&lt;/p>
&lt;/div></description></item><item><title>Configure Buckets in MinIO using GitOps</title><link>https://blog.stderr.at/gitopscollection/2024-05-17-configure-minio-buckets/</link><pubDate>Fri, 17 May 2024 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2024-05-17-configure-minio-buckets/</guid><description>&lt;div class="paragraph">
&lt;p>&lt;a href="https://min.io/" target="_blank" rel="noopener">MinIO&lt;/a> is a simple, S3-compatible object storage, built for high-performance and large-scale environments. It can be installed as an Operator to Openshift. In addition, to a command line tool, it provides a WebUI where all settings can be done, especially creating and configuring new buckets. Currently, this is not possible in a declarative GitOps-friendly way.
Therefore, I created the Helm chart &lt;a href="https://github.com/tjungbauer/helm-charts/tree/main/charts/minio-configurator" target="_blank" rel="noopener">minio configurator&lt;/a>, that will start a Kubernetes Job, which will take care of the configuration.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Honestly, when I say I have created it, the truth is, that it is based on an existing &lt;a href="https://github.com/bitnami/charts/tree/main/bitnami/minio" target="_blank" rel="noopener">MinIO Chart by Bitnami&lt;/a>, that does much more than just set up a bucket. I took out the bucket configuration part, streamlined it a bit and added some new features, which I required.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This article shall explain how to achieve this.&lt;/p>
&lt;/div></description></item><item><title>Setup &amp; Configure Advanced Cluster Security using GitOps</title><link>https://blog.stderr.at/gitopscollection/2024-04-28-installing-advanced-cluster-security/</link><pubDate>Sun, 28 Apr 2024 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2024-04-28-installing-advanced-cluster-security/</guid><description>&lt;div class="paragraph">
&lt;p>Today I want to demonstrate the deployment and configuration of &lt;strong>Advanced Cluster Security&lt;/strong> (ACS) using a GitOps approach. The required operator shall be installed, verified if it is running and then ACS shall be initialized. This initialization contains the deployment of several components:&lt;/p>
&lt;/div>
&lt;div class="olist arabic">
&lt;ol class="arabic">
&lt;li>
&lt;p>Central - as UI and as a main component of ACS&lt;/p>
&lt;/li>
&lt;li>
&lt;p>SecuredClusters - installs a Scanner, Controller pods etc.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Console link into OpenShift UI - to directly access the ACS Central UI&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Job to create an initialization bundle to install the Secured Cluster&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Job to configure authentication using OpenShift&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Let’s start …​&lt;/p>
&lt;/div></description></item><item><title>Setup &amp; Configure Compliance Operator using GitOps</title><link>https://blog.stderr.at/gitopscollection/2024-04-25-installing-compliance-operator/</link><pubDate>Thu, 25 Apr 2024 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2024-04-25-installing-compliance-operator/</guid><description>&lt;div class="paragraph">
&lt;p>In the previous articles, we have discussed the &lt;a href="https://blog.stderr.at/gitopscollection/2023-12-28-gitops-repostructure/">Git repository folder structure&lt;/a> and the configuration of the &lt;a href="gitopscollection/2024-04-02-configure_app_of_apps/">App-Of-Apps&lt;/a>. Now it is time to deploy our first configuration. One of the first things I usually deploy is the &lt;a href="https://docs.openshift.com/container-platform/4.15/security/compliance_operator/co-overview.html" target="_blank" rel="noopener">Compliance Operator&lt;/a>. This Operator is recommended for any cluster and can be deployed without any addition to the Subscription.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>In this article, I will describe how it is installed and how the Helm Chart is configured.&lt;/p>
&lt;/div></description></item><item><title>Configure App-of-Apps</title><link>https://blog.stderr.at/gitopscollection/2024-04-02-configure_app_of_apps/</link><pubDate>Tue, 02 Apr 2024 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2024-04-02-configure_app_of_apps/</guid><description>&lt;div class="paragraph">
&lt;p>In the article &lt;a href="https://blog.stderr.at/gitopscollection/2024-02-02-setup-argocd/">Install GitOps to the cluster&lt;/a> OpenShift GitOps is deployed using a shell script. This should be the very first installation and the only deployment that is done manually on a cluster. This procedure automatically installs the so-called &lt;strong>App-of-Apps&lt;/strong> named &lt;strong>Argo CD Resources Manager&lt;/strong> which is responsible for all further Argo CD Applications and ApplicationSets. No other configuration should be done manually if possible.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This article will demonstrate how to configure the App-of-Apps in an easy and declarative way, using ApplicationSet mainly.&lt;/p>
&lt;/div></description></item><item><title>OpenShift Data Foundation - Noobaa Bucket Data Retention (Lifecycle)</title><link>https://blog.stderr.at/openshift/2024/02/openshift-data-foundation-noobaa-bucket-data-retention-lifecycle/</link><pubDate>Mon, 12 Feb 2024 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2024/02/openshift-data-foundation-noobaa-bucket-data-retention-lifecycle/</guid><description>&lt;div class="paragraph">
&lt;p>Data retention or lifecycle configuration for S3 buckets is done by the S3 provider directly. The provider keeps track and files are automatically rotated after the requested time.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>This article is a simple step-by-step guide to configure such lifecycle for OpenShift Data Foundation (ODF), where buckets are provided by Noobaa. Knowledge about ODF is assumed, however similar steps can be reproduced for any S3-compliant storage operator.&lt;/p>
&lt;/div></description></item><item><title>Setup OpenShift GitOps/Argo CD</title><link>https://blog.stderr.at/gitopscollection/2024-02-02-setup-argocd/</link><pubDate>Fri, 02 Feb 2024 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2024-02-02-setup-argocd/</guid><description>&lt;div class="paragraph">
&lt;p>„&lt;em>If it is not in GitOps, it does not exist&lt;/em>“ - is a mantra I hear quite often and also try to practice at customer engagements. The idea is to have Git as the only source of truth on what happens inside the environment. That said, &lt;a href="https://openpracticelibrary.com/practice/everything-as-code/">Everything as Code&lt;/a> is a practice that treats every aspect of the system as a code. Storing this code in Git provides a shared understanding, traceability and repeatability of changes.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>While there are many articles about how to get GitOps into the deployment process of applications, this one rather sets the focus on the &lt;strong>cluster configuration&lt;/strong> and tasks system administrators usually have to do.&lt;/p>
&lt;/div></description></item><item><title>GitOps - Choosing the right Git repository structure</title><link>https://blog.stderr.at/gitopscollection/2023-12-28-gitops-repostructure/</link><pubDate>Thu, 28 Dec 2023 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2023-12-28-gitops-repostructure/</guid><description>&lt;div class="paragraph">
&lt;p>One of the most popular questions asked before adopting the GitOps approach is how to deploy an application to different environments (Test, Dev, Production, etc.) in a safe and repeatable way.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Each organisation has different requirements, and the choice will depend on a multitude of factors that also include non-technical aspects.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>Therefore, it is important to state: &amp;#34;&lt;strong>There is no unique “right” way, there are common practices&lt;/strong>&amp;#34;.&lt;/p>
&lt;/div></description></item><item><title>Introducing the GitOps Approach</title><link>https://blog.stderr.at/gitopscollection/2023-12-11-gitops-intro/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/gitopscollection/2023-12-11-gitops-intro/</guid><description>&lt;div class="paragraph">
&lt;p>When managing one or more clusters, the question arises as to how cluster configurations and applications can be installed securely, regularly, and in the same way.
This is where the so-called GitOps approach helps, according to the mantra: &amp;#34;&lt;strong>If it is not in Git, it does not exist&lt;/strong>&amp;#34;.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The idea is to have Git as the only source of truth on what happens inside the environment. While there are many articles about how to get GitOps into the deployment process of applications, this series of articles tries to set the focus on the &lt;strong>cluster configuration&lt;/strong> and tasks system administrators usually have to do, for example: Setup an Operator.&lt;/p>
&lt;/div></description></item><item><title>Running Falco on OpenShift 4.12</title><link>https://blog.stderr.at/openshift/2023/11/running-falco-on-openshift-4.12/</link><pubDate>Sun, 26 Nov 2023 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2023/11/running-falco-on-openshift-4.12/</guid><description>&lt;p>
As mentioned in our &lt;a href="https://blog.stderr.at/openshift/2023-10-23-openshift-falco/">previous post&lt;/a> about &lt;a href="https://falco.org/">Falco&lt;/a>, Falco is a security
tool to monitor kernel events like system calls or Kubernetes audit
logs to provide real-time alerts.&lt;/p>
&lt;p>
In this post I&amp;#39;ll show to customize Falco for a specific use case.
We would like to monitor the following events:&lt;/p>
&lt;ul>
&lt;li>An interactive shell is opened in a container&lt;/li>
&lt;li>Log all commands executed in an interactive shell in a container&lt;/li>
&lt;li>Log read and writes to files within an interactive shell inside a container&lt;/li>
&lt;li>Log commands execute via `kubectl/oc exec` which leverage the
&lt;code>pod/exec&lt;/code> K8s endpoint&lt;/li>
&lt;/ul></description></item><item><title>Quay Deployment and Configuration using GitOps</title><link>https://blog.stderr.at/openshift/2023/11/quay-deployment-and-configuration-using-gitops/</link><pubDate>Fri, 03 Nov 2023 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2023/11/quay-deployment-and-configuration-using-gitops/</guid><description>&lt;div class="paragraph">
&lt;p>Installing and configuring Quay Enterprise using a GitOps approach is not as easy as it sounds.
On the one hand, the operator is deployed easily, on the other hand, the configuration of Quay is quite tough to do in a declarative way and syntax rules must be strictly followed.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>In this article, I am trying to explain how I solved this issue by using a Kubernetes Job and a Helm Chart.&lt;/p>
&lt;/div></description></item><item><title>Setting up Falco on OpenShift 4.12</title><link>https://blog.stderr.at/openshift/2023/10/setting-up-falco-on-openshift-4.12/</link><pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2023/10/setting-up-falco-on-openshift-4.12/</guid><description>&lt;p>
&lt;a href="https://falco.org/">Falco&lt;/a> is a security tool to monitor kernel events like system calls to
provide real-time alerts. In this post I&amp;#39;ll document the steps taken
to get Open Source &lt;a href="https://falco.org/">Falco&lt;/a> running on an OpenShift 4.12 cluster.&lt;/p>
&lt;p>
&lt;a href="https://blog.stderr.at/openshift/2023-10-23-openshift-falco/#headline-4">UPDATE&lt;/a>: Use the &lt;code>falco-driver-loader-legacy&lt;/code> image for OpenShift 4.12 deployments.&lt;/p></description></item><item><title>How to force a MachineConfig rollout</title><link>https://blog.stderr.at/openshift/2023/10/how-to-force-a-machineconfig-rollout/</link><pubDate>Wed, 18 Oct 2023 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/openshift/2023/10/how-to-force-a-machineconfig-rollout/</guid><description>&lt;p>
While playing around with &lt;a href="https://falco.org/">Falco&lt;/a> (worth another post) I had to force a
MachineConfig update even so the actual configuration of the machine
did not change.&lt;/p>
&lt;p>
This posts documents the steps taken.&lt;/p></description></item><item><title>Introduction to a Secure Supply Chain</title><link>https://blog.stderr.at/securesupplychain/2023-06-15-securesupplychain-intro/</link><pubDate>Wed, 28 Jun 2023 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/securesupplychain/2023-06-15-securesupplychain-intro/</guid><description>&lt;div class="paragraph">
&lt;p>The goal of the following (&amp;#34;short&amp;#34;) series is to build a secure CI/CD pipeline step by step using OpenShift Pipelines (based on Tekton).
The whole build process shall pull and build an image, upload it to a development environment and subsequently update the production environment.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The main focus here is security. Several steps and tools shall help to build and deploy a &lt;strong>Secure Supply Chain&lt;/strong>.&lt;/p>
&lt;/div>
&lt;div class="paragraph">
&lt;p>The whole process is part of a Red Hat workshop which can present to your organization. I did some tweaks and created a step-by-step plan in order
to remember it …​ since I am getting old :)&lt;/p>
&lt;/div></description></item><item><title>Step 1 - Listen to Events</title><link>https://blog.stderr.at/securesupplychain/2023-06-16-securesupplychain-step1/</link><pubDate>Wed, 28 Jun 2023 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/securesupplychain/2023-06-16-securesupplychain-step1/</guid><description>&lt;div class="paragraph">
&lt;p>In this first step, we will simply prepare our environment to be able to retrieve calls from Git. In Git we will fork a prepared source code into a repository and any time a developer pushes a new code into our repository a webhook will notify OpenShift Pipelines to start the pipeline. Like most pipelines, the first task to be executed is to fetch the source code so it can be used for the next steps.
The application I am going to use is called &lt;a href="https://github.com/tjungbauer/globex-ui" target="_blank" rel="noopener">globex-ui&lt;/a> and is an example webUI build with Angular.&lt;/p>
&lt;/div></description></item><item><title>Step 2 - Pipelines</title><link>https://blog.stderr.at/securesupplychain/2023-06-17-securesupplychain-step2/</link><pubDate>Wed, 28 Jun 2023 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/securesupplychain/2023-06-17-securesupplychain-step2/</guid><description>&lt;div class="paragraph">
&lt;p>We will now create the Pipeline and try to trigger it for the first time to verify if our Webhook works as intended.&lt;/p>
&lt;/div></description></item><item><title>Step 3 - SonarQube</title><link>https://blog.stderr.at/securesupplychain/2023-06-18-securesupplychain-step3/</link><pubDate>Wed, 28 Jun 2023 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/securesupplychain/2023-06-18-securesupplychain-step3/</guid><description>&lt;div class="paragraph">
&lt;p>After the Pipeline has been created and tested we will add another Task to verify the source code and check for possible security issues, leveraging the tool &lt;a href="https://www.sonarsource.com/products/sonarqube/" target="_blank" rel="noopener">SonarQube&lt;/a> by Sonar.&lt;/p>
&lt;/div></description></item><item><title>Step 4 - Verify Git Commit</title><link>https://blog.stderr.at/securesupplychain/2023-06-19-securesupplychain-step4/</link><pubDate>Wed, 28 Jun 2023 00:00:00 +0000</pubDate><guid>https://blog.stderr.at/securesupplychain/2023-06-19-securesupplychain-step4/</guid><description>&lt;div class="paragraph">
&lt;p>Besides checking the source code quality, we should also verify if the commit into Git was done by someone/something we trust. It is a good practice to sign all commits to Git. You need to prepare your Git account and create trusted certificates.&lt;/p>
&lt;/div></description></item></channel></rss>